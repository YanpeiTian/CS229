Id,CreationDate,Score,ViewCount,Body,OwnerUserId,LastEditorUserId,LastEditDate,CommentCount,BodyLength,UserDisplayName,UserCreationDate,UserReputation,UserViews,UserUpVotes,UserDownVotes,ParentId,ParentAcceptedAnswerId,ParentCreationDate,ParentScore,ParentViewCount,ParentBody,ParentOwnerUserId,ParentLastEditorUserId,ParentLastEditDate,ParentTitle,ParentTags,ParentAnswerCount,ParentCommentCount,ParentBodyLength
"50634377","2018-06-01 00:01:14","1","","<p>The reason this does not work is because you are refering to <code>self</code> in the class body, where it is not defined. Here are two solutions.</p>

<h1>Store the serial object as class attribute</h1>

<p>If you store the <code>MySerial</code> instance as a <em>class</em> attribute, then it sill be accessible in the class body:</p>

<pre><code>class App():
    ser = MySerial()

    @ser.decorator
    def myfunc(self):
        return 'yummy_bytes'
</code></pre>

<h1>Decorate on each instantiation</h1>

<p>Or if you need a different <code>MySerial</code> instance for every <code>App</code> instance, then you will need to wait for the instance to be created to define an <em>instance</em> attribute <code>my_func</code>. This means the function is decorated dynamically on every instance creation, in which case, the <code>@</code> decorator syntax must be replaced by a function call.</p>

<pre><code>class App():
    def __init__(self):
        self.ser = MySerial()
        self.my_func = self.ser.decorator(self.myfunc)

    def myfunc(self):
        return 'yummy_bytes'
</code></pre>

<p>This solution generalizes to decorating multiple methods or conditionally deactivating serializing, say in a test environment.</p>

<pre><code>import env

class App():
    def __init__(self):
        self.ser = MySerial()

        to_decorate = [] if env.test else ['myfunc']

        for fn_name in to_decorate:
            fn = getattr(self, fn_name)
            setattr(self, fn_name, self.ser.decorator(fn))
</code></pre>
","5079316","5079316","2018-06-01 03:59:38","10","1531","Olivier Melançon","2015-07-04 00:30:37","15837","1134","1990","511","50634303","50634330","2018-05-31 23:49:09","4","514","<p>I am trying to create a class (<code>MySerial</code>) that instantiates a serial object so that I can write/read to a serial device (UART). There is an instance method that is a decorator which wraps around a function that belongs to a completely different class (<code>App</code>). So decorator is responsible for writing and reading to the serial buffer.</p>

<p>If I create an instance of <code>MySerial</code> inside the <code>App</code> class, I can't use the decorator instance method that is created from <code>MySerial</code>.
I have tried foregoing instance methods and using class methods as explained in <a href=""https://stackoverflow.com/questions/1263451/python-decorators-in-classes"">this second answer</a>, but I really need to instantiate <code>MySerial</code>, thus create an instance using <code>__init__</code>.</p>

<p>How can this be accomplished? Is it impossible?</p>

<ul>
<li>Create a decorator that is an instance method.</li>
<li>Use this decorator within another class</li>
</ul>

<hr>

<pre><code>class MySerial():
    def __init__(self):
        pass # I have to have an __init__
    def write(self):
        pass # write to buffer
    def read(self):
        pass # read to buffer
    def decorator(self, func):
        def func_wrap(*args, **kwargs):
            self.write(func(*args, **kwars))
            return self.read()
        return func_wrap

class App():
    def __init__(self):
        self.ser = MySerial()

    @self.ser.decorator  # &lt;-- does not work here.
    def myfunc(self):
        # 'yummy_bytes' is written to the serial buffer via 
        # MySerial's decorator method
        return 'yummy_bytes'

if __name__ == '__main__':
    app = App()
</code></pre>
","4358938","","","Use an instance method as a decorator within another class","<python><mocking><instance><patch><python-decorators>","3","2","1718"
"50634386","2018-06-01 00:03:18","2","","<p>If you want to compare all the strings within the nested list (assuming that the sublists only have a depth of 1), you can do a nested <code>for</code> loop.</p>

<pre><code>list = [[""I am a boy""], [""I am a girl""]] 
for sub_list in list:
    for string in sub_list:
        if 'boy' in string:
            print(""You got it"")
</code></pre>
","9238796","5606836","2018-06-01 02:07:27","0","343","Andrii Glukhyi","2018-01-19 07:15:50","101","24","0","0","50634291","","2018-05-31 23:46:55","0","124","<p>I have a nested list. I need to check whether a particular string is in an element on the list - How do I do it? I am checking if ""boy"" is in list[0]</p>

<pre><code>list = [[""I am a boy""],[""I am a girl""]]
print (list[0])
if ""boy"" in list[0]:
    print (""You got it"")
</code></pre>
","9831309","2308683","2018-06-01 02:17:52","Word within a Nested list python","<python>","4","2","285"
"50634395","2018-06-01 00:04:02","0","","<p>This code will give you a list of the indexes of all sublists containing ""boy"". I changed the name to <code>alist</code>, as <strong>list</strong> is a reserved word.</p>

<pre><code>indexes = [count for count,item in enumerate(alist) if 'boy' in item[0]]
print(indexes)
</code></pre>
","8172933","2308683","2018-06-01 02:18:23","1","288","figbeam","2017-06-16 16:36:56","4157","270","14","2","50634291","","2018-05-31 23:46:55","0","124","<p>I have a nested list. I need to check whether a particular string is in an element on the list - How do I do it? I am checking if ""boy"" is in list[0]</p>

<pre><code>list = [[""I am a boy""],[""I am a girl""]]
print (list[0])
if ""boy"" in list[0]:
    print (""You got it"")
</code></pre>
","9831309","2308683","2018-06-01 02:17:52","Word within a Nested list python","<python>","4","2","285"
"50634413","2018-06-01 00:06:20","1","","<p>Here is the right approach that is fairly general:</p>

<pre><code>df[""c""] = df.groupby(""id"")[""a""].transform(lambda x: np.arange(1, len(x)+1))
</code></pre>
","9814693","","","0","160","Alex","2018-05-19 05:41:06","461","46","34","2","50634224","50634281","2018-05-31 23:35:36","1","368","<p>I have a df that has groups. For each group I would like to form a new column that contains integers from 1 to the number of rows in that group. The following attempt doesn't work as it creates one row with multiple columns instead of one column with multiple rows. Why does this happen?</p>

<pre><code>df = pd.DataFrame(data = {""a"": np.arange(5), ""b"": np.arange(5)[::-1], ""id"": [1,1,1,2,2]}).set_index(""id"")
df[""c""] = df.groupby(""id"").apply(lambda x: np.arange(1, len(x)+1))
</code></pre>

<p>Alternatively if I try to return a <code>Series</code> object as in:</p>

<pre><code>df[""c""] = df.groupby(""id"").apply(lambda x: pd.Series(data = np.arange(1, len(x)+1)))
</code></pre>

<p>This raises an exception saying the indices are incompatible (which makes sense since the returned <code>Series</code> now has a <code>MultiIndex</code>)</p>

<p><strong>Update</strong>: Let me make this question a bit more general: how does one do <code>groupby</code> on a dataframe returning a <code>Series</code> with the same index as the dataframe so that the resulting series can be merged into the original dataframe?</p>
","9814693","9814693","2018-05-31 23:58:00","pandas: create a column from 1 to length of each group","<python><python-3.x><pandas>","2","0","1116"
"50634439","2018-06-01 00:10:12","2","","<p>You can do that with python library <em>beautifulsoup</em>.</p>
","9210919","","","0","67","mswedr","2018-01-12 21:15:49","31","6","0","0","50634419","50634449","2018-06-01 00:07:20","-4","67","<p>I Am Working on project  and in a specific part i need to search for some information and get the results of this search from the internet so how the data can be fetched from the web page to use it ? </p>
","9420246","","","how to fetch data from webpage in java or python","<java><python><search>","2","0","208"
"50634441","2018-06-01 00:10:30","0","","<p>You're actually not updating the existing dataframes, in either version; you're just creating new dataframes.</p>

<p>The difference is that in the repetitive code, you're rebinding each of the variables <code>X</code>, <code>X1</code>, etc. to each of those new dataframes, while in your loop, you're rebinding <code>num_x</code> over and over again to each new dataframe, so at the end, only the last one is stored anywhere.</p>

<p>Ideally, you probably want to rewrite your code to not have those separate variables in the first place, and only have a list of 9 dataframes—or maybe a dict of 9 dataframes keyed by names <code>X</code>, <code>X1</code>, etc., if the names really are meaningful.</p>

<p>But if that's too big of a change, you can do something like this:</p>

<pre><code>all_x = [X, X1, X2, X3, X4, X5, X6, X7, X8]
idx = 0
for num_x in all_x:
   encoder=LabelEncoder()
   (num_x) = (num_x).apply(encoder.fit_transform)   
   onehotencoder = OneHotEncoder(categorical_features='all')
   # store the new dataframe back into the list
   all_x[idx] = num_x = onehotencoder.fit_transform(num_x).toarray()
   np.save('X%d' % idx, num_x)
   idx+=1
   print(num_x)
# store the list of new dataframes back into the original variable names
X, X1, X2, X3, X4, X5, X6, X7, X8 = all_x
</code></pre>

<p>There are ways you can make this cleaner (e.g., using <code>for idx, num_x in enumerate(all_x):</code>), but I stuck as closely as possible to your existing code.</p>
","908494","","","0","1479","abarnert","2011-08-23 20:55:30","272076","19605","5576","4509","50634310","","2018-05-31 23:49:57","0","89","<p>I am preprocessing data for a deep neural network and I have variables that I need to one hot encode. So far, this is what I am doing and it is working fine. However, I was wondering if I could implement this in a for loop as that may be more efficient?</p>

<pre><code># Only Educational Establishment Type
X6 = X.drop(['Sex', 'Applicant Domicile (High Level)', 'Applicant Domicile (Low Level)', 'Age Band (5 Levels)', 'POLAR3 Quintile', 'Subject Group (Detailed Level)', 'Subject Group (Summary Level)'], axis=1)
X6 = onehotencoder.fit_transform((X6).apply(encoder.fit_transform)).toarray()
# print(X6.head)

# Only Subject Group (Detailed Level)
X7 = X.drop(['Sex', 'Applicant Domicile (High Level)', 'Applicant Domicile (Low Level)', 'Age Band (5 Levels)', 'POLAR3 Quintile', 'Educational Establishment Type', 'Subject Group (Summary Level)'], axis=1)
X7 = onehotencoder.fit_transform((X7).apply(encoder.fit_transform)).toarray()
# print(X7.head)

# Only Subject Group (Summary Level)
X8 = X.drop(['Sex', 'Applicant Domicile (High Level)', 'Applicant Domicile (Low Level)', 'Age Band (5 Levels)', 'POLAR3 Quintile', 'Educational Establishment Type', 'Subject Group (Detailed Level)'], axis=1)
X8 = onehotencoder.fit_transform((X8).apply(encoder.fit_transform)).toarray()
</code></pre>

<p>I also need to save the encoded array in a npy format to recall later. I attempted to implement all of this in a for loop as follows; however, it does not save a file for each array as desired and it does not actually update the existing dataframes into one hot encoded arrarys.</p>

<pre><code>all_x = [X, X1, X2, X3, X4, X5, X6, X7, X8]

idx = 0
for num_x in all_x: 
   encoder=LabelEncoder()
   (num_x) = (num_x).apply(encoder.fit_transform)   
   onehotencoder = OneHotEncoder(categorical_features='all')
   (num_x) = onehotencoder.fit_transform(num_x).toarray()
   np.save('X%d' % idx, num_x)
   idx+=1
   print(num_x)
</code></pre>
","9877672","","","Encoding and saving multiple arrays in a for loop python","<python><arrays><numpy><for-loop><one-hot-encoding>","1","1","1934"
"50634442","2018-06-01 00:10:32","-1","","<p><a href=""https://stackoverflow.com/a/50257412/1807800"">https://stackoverflow.com/a/50257412/1807800</a></p>

<p>Check out the above link regarding Search Preferences. It explains how to limit columns returned to those only in the search.</p>
","1807800","","","2","245","Suite Resources","2012-11-08 00:46:30","1006","583","11","2","50634375","","2018-06-01 00:00:58","1","318","<p>I'm using Python 2.7 and <a href=""http://docs.python-zeep.org/en/master/"" rel=""nofollow noreferrer"">Zeep</a> to call SuiteTalk <a href=""https://webservices.netsuite.com/wsdl/v2017_2_0/netsuite.wsdl"" rel=""nofollow noreferrer"">v2017_2_0</a>, the SOAP-based NetSuite web service API. The command I'm running is <code>search</code> like so:</p>

<pre><code>from zeep import Client
netsuite = Client(WSDL)

TransactionSearchAdvanced = netsuite.get_type(
    'ns19:TransactionSearchAdvanced')
TransactionSearchRow = netsuite.get_type('ns19:TransactionSearchRow')

# login removed for brevity

r = netsuite.service.search(TransactionSearchAdvanced(
    savedSearchId=search, columns=TransactionSearchRow()))
</code></pre>

<p>Now the results of this include all the data I want but I can't figure out how (if at all) I can determine the display columns that the website would show for this saved search and the order they go in.</p>

<p>I figure I could probably <code>netsuite.service.get()</code> and pass the <code>internalId</code> of the saved search but what type do I specify? Along those lines, has anyone found a decent reference for all the objects, type enumerations, etc.?</p>
","9642","","","Determine display columns from Saved Search via SuiteTalk / NetSuite?","<python><netsuite><zeep><suitetalk>","1","3","1185"
"50634449","2018-06-01 00:11:12","2","","<p>Use Selenium - Here's a nice tutorial <a href=""https://www.guru99.com/selenium-tutorial.html"" rel=""nofollow noreferrer"">https://www.guru99.com/selenium-tutorial.html</a></p>

<p>Selenium is a java library, that supports all major browsers. The download and documentation links can be found on its website - <a href=""https://www.seleniumhq.org/docs/"" rel=""nofollow noreferrer"">https://www.seleniumhq.org/docs/</a></p>
","4406032","4406032","2018-06-01 00:13:51","0","420","NRJ","2014-12-30 16:29:55","121","4","6","0","50634419","50634449","2018-06-01 00:07:20","-4","67","<p>I Am Working on project  and in a specific part i need to search for some information and get the results of this search from the internet so how the data can be fetched from the web page to use it ? </p>
","9420246","","","how to fetch data from webpage in java or python","<java><python><search>","2","0","208"
"50634468","2018-06-01 00:14:30","1","","<p>I found a solution using <code>brew</code> to install <code>pyenv</code>.  Solution was found <a href=""https://github.com/awslabs/aws-sam-cli#install-with-pyenv"" rel=""nofollow noreferrer"">here</a> but I only needed the parts up to when it uses <code>pip</code>.</p>

<pre><code># Install PyEnv (https://github.com/pyenv/pyenv#installation)
$ brew update
$ brew install pyenv

# Initialize pyenv using bash_profile
$ echo -e 'if command -v pyenv 1&gt;/dev/null 2&gt;&amp;1; then\n  eval ""$(pyenv init -)""\nfi\nexport PATH=""~/.pyenv/bin:$PATH""' &gt;&gt; ~/.bash_profile
# or using zshrc
$ echo -e 'if command -v pyenv 1&gt;/dev/null 2&gt;&amp;1; then\n  eval ""$(pyenv init -)""\nfi\nexport PATH=""~/.pyenv/bin:$PATH""' &gt;&gt; ~/.zshrc

# restart the shell
$ exec ""$SHELL""

# Install Python 2.7
$ pyenv install 2.7.14
$ pyenv local 2.7.14
</code></pre>

<p>After completing those steps I ran <code>sudo easy_install pip</code> again and eureka!  It worked. </p>
","1196031","","","0","961","DroidT","2012-02-08 00:34:46","1993","537","449","3","50616037","","2018-05-31 03:03:20","3","3256","<p>I'm having trouble executing the <code>sudo easy_install pip</code> command on my mac.  I'm not behind a firewall.  I'm using python version 2.7.10. The version of macOS Sierra is 10.12.4.  Here is the error I receive:</p>

<pre><code>Searching for pip
Reading https://pypi.python.org/simple/pip/
Download error on https://pypi.python.org/simple/pip/: [SSL: TLSV1_ALERT_PROTOCOL_VERSION] tlsv1 alert protocol version (_ssl.c:590) -- Some packages may not be found!
Couldn't find index page for 'pip' (maybe misspelled?)
Scanning index of all packages (this may take a while)
Reading https://pypi.python.org/simple/
Download error on https://pypi.python.org/simple/: [SSL: TLSV1_ALERT_PROTOCOL_VERSION] tlsv1 alert protocol version (_ssl.c:590) -- Some packages may not be found!
No local packages or download links found for pip
error: Could not find suitable distribution for Requirement.parse('pip')
</code></pre>
","1196031","","","sudo easy_install pip not working","<python><pip><easy-install>","1","4","919"
"50634537","2018-06-01 00:26:34","3","","<p>Assuming you actually want to take arbitrary Python code as user input and run it—which you usually really, really don't want to do, but let's assume here that you have a good reason…</p>

<p>But first: You mentioned trying to do this with SymPy. If you're actually trying to create functions out of SymPy expressions, e.g., by using <code>sympify</code> or <code>lambdify</code>—that ought to work, and if it doesn't, you need to show us your code if you want help debugging it.</p>

<p>But let's stop stalling and get to how you can do what you asked for, even though there's a good chance it isn't actually what you want.</p>

<hr>

<p>Remember that decorators are just functions, which take a function and return another function, and you can call them normally. So, all you have to do is turn that arbitrary Python code into a function, and you can pass it to the decorator.</p>

<hr>

<p>If that arbitrary Python code is just an expression, you can wrap it in a <code>lambda</code> expression, <code>eval</code> the result, and you've got a function that applies that expression:</p>

<pre><code>lambdastr = f'lambda x: {user_string}'
lambdafunc = eval(lambdastr)
numbafunc = numba.jit(nopython=True)(lambdafunc)
</code></pre>

<p>Or, if you prefer:</p>

<pre><code>numbafunc = numba.jit(nopython=True)(eval(f'lambda x: {user_string}'))
</code></pre>

<p>If you're thinking ""But wait, <code>eval</code> is dangerous""—well, yeah, <code>eval</code> is dangerous because it evaluates arbitrary user strings as code, which is exactly what you want to do. There's no non-dangerous way to do that.</p>

<p>So, if your user passes you the string <code>x * x</code>, you've now got a function that squares its input, and if the user passes you the string <code>__import__('os').system('rm -rf /')</code>, you've now got a function that tries to erase your entire hard drive.</p>

<hr>

<p>If you want to take a statement, you can effectively do the same same thing by wrapping it in a <code>def</code> and calling <code>exec</code>:</p>

<pre><code>defstr = f'def __(x): {user_string}'
deffunc = exec(defstr)
numbafunc = numba.jit(nopython=True)(deffunc)
</code></pre>

<hr>

<p>If that arbitrary Python code can be a block of statements, it's slightly more complicated by the fact that you need to deal with indentation, but that's not too hard:</p>

<pre><code>user_lines = '\n'.join(' '+line for line in user_string.splitlines())
defstr = f'def __(x):\n{user_lines}'
deffunc = exec(defstr)
numbafunc = numba.jit(nopython=True)(deffunc)
</code></pre>
","908494","908494","2018-06-01 00:33:55","1","2554","abarnert","2011-08-23 20:55:30","272076","19605","5576","4509","50634434","50634537","2018-06-01 00:09:12","0","377","<p>I'm trying to write a program that takes in a user input function as a string and does a number of calculations using that function. These calculations are done using numba.jit, and the code works if I hardcode my function, but I'm having trouble figuring out how to parse the string in such a way that I can turn it into a jitted function with nopython=True.</p>

<p>For example, my code runs with the function </p>

<pre><code>@jit(nopython=True)
def f(x):
    return x*x
</code></pre>

<p>but I want to, instead, take the user input string 'x*x' and create the same function. I've tried using SymPy, but I couldn't make it play nicely with jit. Any ideas?</p>
","2914093","","","Using numba.jit on a user input string","<python><numba>","1","2","666"
"50634540","2018-06-01 00:27:06","5","","<p>Always remember when sending an <em>array(list)</em> or <em>dictionary</em> in the <strong><em>HTTP POST</em></strong> request, do use <strong>json argument</strong> in the post function and set its value to your <strong>array(list)/dictionary</strong>. </p>

<p>In your case it will be like:</p>

<blockquote>
  <p>r = requests.post(url, headers=headers, json=data)</p>
</blockquote>

<p><strong>Note:</strong> <em>POST requests implicitly convert parameter's content type for body to application/json.</em></p>

<p>For a quick intro read <a href=""https://realpython.com/api-integration-in-python/"" rel=""noreferrer"">API-Integration-In-Python</a></p>
","9865853","7311767","2018-06-01 00:50:37","1","654","MHBN","2018-05-29 17:35:20","51","3","0","0","31168819","","2015-07-01 18:20:59","45","40057","<p>I'm trying to send an array(list) of requests to the WheniWork API using requests.post, and I keep getting one of two errors. When I send the list as a list, I get an unpacking error, and when I send it as a string, I get an error asking me to submit an array. I think it has something to do with how requests handles lists. Here are the examples:</p>

<pre><code>url='https://api.wheniwork.com/2/batch'
headers={""W-Token"": ""Ilovemyboss""}
data=[{'url': '/rest/shifts', 'params': {'user_id': 0,'other_stuff':'value'}, 'method':'post',{'url': '/rest/shifts', 'params': {'user_id': 1,'other_stuff':'value'}, 'method':'post'}]
r = requests.post(url, headers=headers,data=data)
print r.text

# ValueError: too many values to unpack
</code></pre>

<p>Simply wrapping the value for data in quotes:</p>

<pre><code>url='https://api.wheniwork.com/2/batch'
headers={""W-Token"": ""Ilovemyboss""}
data=""[]"" #removed the data here to emphasize that the only change is the quotes
r = requests.post(url, headers=headers,data=data)
print r.text

#{""error"":""Please include an array of requests to make."",""code"":5000}
</code></pre>
","5025679","","","How to send an array using requests.post (Python)? ""Value Error: Too many values to unpack""","<python><api><python-requests>","3","0","1114"
"50634551","2018-06-01 00:28:38","0","","<p>You want a numeric sort, but 'chr2' is not a number. you need a preprocessing step of splitting the first column into 2 columns, the text part and the number part.</p>

<p><code>gawk 'match($1, /([^0-9])*([0-9]*)/, a) {print a[1], a[2], $2}' /tmp/abc | 
sort -t ' ' -k1,1 -k2,2n -k3,3n</code></p>

<p>use gawk to split on a regex, non numeric then numeric, then column 2 (separated by single spaces now).</p>

<p>Sort on single space separated columns.</p>

<p><code>gawk '{print $1 $2, $3}'</code> to recombine the columns.</p>

<p>You may need to modify these to maintain whatever whitespace is needed. </p>
","3936601","","","0","613","Evan Benn","2014-08-13 08:45:33","559","80","145","20","50634189","50634782","2018-05-31 23:31:01","0","49","<p>I have very large data which I want to sort on </p>

<ul>
<li>column 1: numerically and then alphanumerically</li>
<li>then on column 2: numerically.</li>
</ul>

<p>So, my final output would be something like this:</p>

<pre><code>1    11  
1    13
1    15
2    3
2    5
chr2   6
chr2   15
chr15   3
chr15   9
</code></pre>

<p>I am using <code>sort</code> on unix. But, I either keep getting <strong>chr2</strong> on the top or on the bottom with any sort I try. Here are some of the sort I tried: which fail to give me the desired output:</p>

<pre><code>sort -V -k1,1n -k2n final_merged.txt &gt; merged-sort.txt
sort -k1,1n -k2n final_merged.txt &gt; merged-sort.txt 
sort -k1,1h -k2n final_merged.txt &gt; merged-sort.txt
sort -k1,1 -k2n final_merged.txt &gt; merged-sort.txt
</code></pre>

<p><strong>Post edit:</strong> Any way to fix this issue without overloading the memory while using</p>

<ul>
<li>sort or other unix utilities</li>
<li>python </li>
</ul>

<p>Thanks,</p>
","6346698","6346698","2018-05-31 23:46:50","sort numerically then alphnumerically on the same column","<python><shell><sorting><ubuntu><unix>","3","4","985"
"50634575","2018-06-01 00:33:31","0","","<p>Let's take a look first to this specific line <code>cl.exe /c /nologo /Ox /MD /W3 /GS- /DNDEBUG -DWIN32=1 -Ic:\wpdpack\Include -Ic:\python27\include -Ic:\python27\PC /Tppcapdumper.cc /Fobuild\temp.win-amd64-2.7\Release\pcapdumper.obj</code></p>

<p>As you can see there, when pip installing, setup.py will try to use <a href=""https://www.winpcap.org/archive/"" rel=""nofollow noreferrer"">winpcap</a> as a dependency to compile <a href=""https://github.com/CoreSecurity/pcapy/blob/master/pcapdumper.cc"" rel=""nofollow noreferrer"">pcapdumper.cc</a> and the location is expected to be <code>c:\wpdpack</code>.</p>

<p>To make it work you just need to download and extract the latest <a href=""https://www.winpcap.org/archive/4.1.1-WpdPack.zip"" rel=""nofollow noreferrer"">stable winpcap library</a> version (ie: not beta suffix) and uncompress it on c:. Then you just open a visual command prompt and try again <code>pip install pcapy</code>.</p>

<p>On my case I've tried using vs2015+python3.6.x and it's been built smoothly. In any case, make sure you read its docs carefully, specially the part where it talks about <a href=""https://github.com/CoreSecurity/pcapy"" rel=""nofollow noreferrer"">requirements</a>.</p>

<p>Also, one last hint, I recommend you take a look to this <a href=""https://stackoverflow.com/a/39155105/3809375"">answer</a> which explains very briefly how to proceed each time you want to install tricky libraries like this pcapy.</p>
","3809375","","","0","1447","BPL","2014-07-06 10:47:36","4786","1268","252","137","50634329","50634575","2018-05-31 23:53:10","2","681","<p>I tried to install <code>pcapy</code> using <code>pip install pcapy</code>, but I encoutered an error stating that the file <code>pcap.h</code> does not exist as following:</p>

<pre><code>Installing collected packages: pcapy
  Running setup.py install for pcapy ... error
    Complete output from command c:\python27\python.exe -u -c ""import setuptools
tokenize;__file__='c:\\users\\username\\appdata\\local\\temp\\pip-install-1tyk
yr\\pcapy\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" 
install --
record c:\users\username\appdata\local\temp\pip-record-u_q6qm\install-record.txt
 --single-version-externally-managed --compile:
    running install
    running build
    running build_ext
    building 'pcapy' extension
    creating build
    creating build\temp.win-amd64-2.7
    creating build\temp.win-amd64-2.7\Release
    creating build\temp.win-amd64-2.7\Release\win32
    C:\Users\UserName\AppData\Local\Programs\Common\Microsoft\Visual C++ for
Python\9.0\VC\Bin\amd64\cl.exe /c /nologo /Ox /MD /W3 /GS- /DNDEBUG -DWIN32=1 -I
c:\wpdpack\Include -Ic:\python27\include -Ic:\python27\PC /Tppcapdumper.cc /Fobuild\temp.win-amd64-2.7\Release\pcapdumper.obj
    pcapdumper.cc
    pcapdumper.cc(11) : fatal error C1083: Cannot open include file: 'pcap.h': N
o such file or directory
    error: command 'C:\\Users\\UserName\\AppData\\Local\\Programs\\Common\\Microsoft\\Visual C++ for Python\\9.0\\VC\\Bin\\amd64\\cl.exe' failed with exit status 2

Command ""c:\python27\python.exe -u -c ""import setuptools, tokenize;__file__='c:\\users\\username\\appdata\\local\\temp\\pip-install-1tykyr\\pcapy\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record c:\users\username\appdata\local\temp\pip-record-u_q6qm\install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in c:\users\username\appdata\local\temp\pip-install-1tykyr\pcapy\
</code></pre>

<p>I tried to upgrade <code>setuptools</code> but I got the same result. I tried to install <code>libcap</code> by running <code>pip install libcap</code> but I also got the same problem. How can I fix this problem?</p>
","8808047","3809375","2018-06-01 00:35:36","pip install pcapy cannot open include file 'pcap.h'","<python><python-2.x><pcap><winpcap>","1","0","2296"
"50634597","2018-06-01 00:36:51","0","","<p>My understanding is that the file is changing in time so that you want to be able to check if minimum was detected. Think you can do this if you watch for file changes. Below I provide the simplest approach but you could ""fortify"" it by adding some time-outs.</p>

<pre><code>import os
import numpy as np
stat_prev = os.stat(fname)
while True:
    data = np.genfromtxt(fname, dtype=np.int, delimiter=',', names=True)
    min_idx = np.argmin(data['Area'])
    if min_idx &lt; len(data) - 1 and data['Area'][min_idx] &lt; data['Area'][min_idx+1]:
        data = data[:min_idx + 1] # &lt;-- remove +1 if min row is the last one
        break # exit main loop;
    # wait for the file to change
    stat_now = os.stat(fname)
    while stat_prev == stat_now: # add some time-out, if you want
        stat_prev = os.stat(fname)
</code></pre>

<p>Also, if do not want a structured array and just a simple array, then you can convert <code>data</code> to a simple array using <a href=""https://stackoverflow.com/a/5957555/8033585"">this recipe</a>:</p>

<pre><code>data.view(data.dtype[0]).reshape(data.shape + (-1,))
</code></pre>
","8033585","8033585","2018-06-01 07:33:26","0","1125","AGN Gazer","2017-05-18 21:39:33","6041","460","707","281","50633132","","2018-05-31 21:36:02","0","40","<p>I am using <code>matplotlib</code> and <code>numpy</code>, and I am making graphs. The data format I am using is <code>.csv</code>. In the <code>csv</code> file I am using there are three columns. I wonder, is there a way to only import data up until the peak/lowest values of one of my columns? </p>

<p>Context: I am using Langmuir troughs with lipid monolayers and compressing and expanding barriers to increase/decrease the area
I am trying to plot pressure and fluorescence against the area. However, the program that takes this data performs a complete cycle of compression and expansion and I cannot stop the data collection simply when the trough is at its minimum area. So I would like to have Python only import until the area value gets to its lowest point.</p>

<p><code>example of how my data looks
Area    | Presure | Intensity
12500   |3        | 1
11500   |6        | 12
etc     |8        |25
3000    |12       |38
3500    |19       |54      &lt;==want it to stop importing here
4500    |16       |47</code></p>

<p>Is this possible??</p>

<p>I have added what Phi has put and It doesn't seem to be working? I still get all of the values included into my graphs code looks like this 
    import matplotlib.pyplot as plt
    import numpy as np
    import pandas as pd</p>

<pre><code>df = pd.read_csv(""C:\\Users\\Owner\\Desktop\\Thunberg,Dametre\\5-29 Data and 
movies\\New folder (2)\\Data 2.csv"", sep=',')
rowmin = df.area.idxmax()
df[:(1 + rowmin)]
fig, ax1 = plt.subplots()
area, pressure, pixel = np.loadtxt 
(""C:\\Users\\Owner\\Desktop\\Thunberg,Dametre\\5-29 Data and movies\\New 
folder 
(2)\\Data 2.csv"", delimiter="","", skiprows=1, unpack=True)
plt.plot(area,pressure, label='area/pressure!',color='b')

plt.xlabel('area', color='b')
plt.ylabel('Pressure', color='b')
ax1.tick_params('y', colors='b')
ax2 = ax1.twinx()
this ax2 creates a second x axis 
ax2.set_ylabel('Intensity (measured by average pixel value)', color='r')
this labels the secondary axis and chooses its color
ax2.tick_params('y', colors='r')
this Chooses the color of the ticks in the axis
ax2.plot(area,pixel, color='r')
this is what actually plots the second graph of area vs intensity
plt.title('Kibron Trough Pressure-Area-Intensity Graph')
plt.legend()
plt.show()
</code></pre>
","9872610","9872610","2018-06-01 17:13:08","Only importing up to the maximum value of one of my columns","<python><numpy><matplotlib>","2","2","2280"
"50634640","2018-06-01 00:42:47","2","","<p>Your current approach is not very efficient, as you will iterate over <code>txt</code>, multiple times, 2 (<code>in</code> and <code>find()</code>) for each operator.  </p>

<p>You could use <code>index()</code> instead of <code>find()</code> and just ignore the <code>ValueError</code> exception , e.g.:</p>

<pre><code>def findNextOpr(txt):
    for o in '+-*/':
        try:
            return txt.index(o)
        except ValueError:
            pass
    return -1
</code></pre>

<p>You can do this in a single (perhaps more readable) pass by <code>enumerate()</code>ing the <code>txt</code> and return if you find the character, e.g.:</p>

<pre><code>def findNextOpr(txt):
    for i, c in enumerate(txt):
        if c in '+-*/':
            return i
    return -1
</code></pre>

<p>Note: if you wanted all of the operators you could change the <code>return</code> to <code>yield</code>, and then just iterate over the generator, e.g.:</p>

<pre><code>def findNextOpr(txt):
    for i, c in enumerate(txt):
        if c in '+-*/':
            yield i

In []:
for op in findNextOpr('1+2-3+4'):
    print(op)

Out[]:
1
3
5
</code></pre>
","2750492","2750492","2018-06-01 01:45:43","2","1139","AChampion","2013-09-05 11:19:58","22583","2373","75","181","50634600","","2018-06-01 00:37:05","0","55","<p>I would understand how to do this assuming that I was only looking for one specific character, but in this instance I am looking for any of the 4 operators, '+', '-', '*', '/'. The function returns -1 if there is no operator in the passed string, txt, otherwise it returns the position of the leftmost operator. So I'm thinking find() would be optimal here.</p>

<p>What I have so far:</p>

<pre><code>def findNextOpr(txt):
# txt must be a nonempty string.
    if len(txt) &lt;= 0 or not isinstance(txt, str):
        print(""type error: findNextOpr"")
        return ""type error: findNextOpr""
    if '+' in txt:
        return txt.find('+')
    elif '-' in txt:
        return txt.find('-')
    else
        return -1
</code></pre>

<p>I think if I did what I did for the '+' and '-' operators for the other operators, it wouldn't work for multiple instances of that operator in one expression. Can a loop be incorporated here?</p>
","9672407","","","How do I detect any of 4 characters in a string and then return their index?","<python>","3","2","934"
"50634648","2018-06-01 00:43:30","2","","<p>Using <code>groupby</code> with columns</p>

<pre><code>pd.concat([df1,df2],1).fillna(15).groupby(level=0,axis=1).mean()
Out[408]: 
            July2016  June2016
Chocolate        7.5       8.5
Strawberry      14.5      13.0
Vanilla         21.0      13.0
</code></pre>
","7964527","","","1","273","WeNYoBen","2017-05-04 16:45:29","164847","15327","4764","689","50634552","","2018-06-01 00:28:54","1","257","<p>I have several data frames that have months as the columns, and contain integer values. I am posting 2 for this example. </p>

<pre><code>df1 =     
             June 2016       July 2016
Flavor
Vanilla      17.0            23.0
Chocolate    7.0             12.0
Strawberry   11.0            14.0

df2 =        
             June 2016       July 2016
Flavor
Vanilla      9.0            19.0
Chocolate    10.0           3.0
</code></pre>

<p>How can I iterate through each dataframe and perform a calculation dependent on the row and column name of the dataframe when they have to match? For example, I want to calculate the average for Vanilla for July, which would be (23 + 19)/2. If a <code>Flavor</code> also does not exist in a data frame, then I would also like to assign a constant value (say 15 in this example) per month in that data frame. Would I append the data frames together then apply <code>.mean()</code>?</p>

<p>Thanks in advance, and sorry for any abruptness, I am currently on the go, traveling.</p>

<p>Thanks!</p>
","9343043","4492932","2018-06-01 00:30:16","Iterate through multiple dataframes and perform calculation based on specific column","<python><pandas><dataframe><average>","2","1","1039"
"50634655","2018-06-01 00:45:34","1","","<p>You can improve your code a bit because you keep looking at the string a lot of times. <code>'+' in txt</code> actually searches through the string just like <code>txt.find('+')</code> does. So you can combine those easily to avoid having to search through it twice:</p>

<pre><code>pos = txt.find('+')
if pos &gt;= 0:
    return pos
</code></pre>

<p>But this still leaves you with the problem that this will return for the first operator you are looking for if that operator is contained anywhere within the string. So you don’t actually get the first position <em>any</em> of these operators is within the string.</p>

<p>So what you want to do is look for all operators separately, and then return the lowest non-negative number since that’s the first occurence of any of the operators within the string:</p>

<pre><code>plusPos = txt.find('+')
minusPos = txt.find('-')
multPos = txt.find('*')
divPos = txt.find('/')

return min(pos for pos in (plusPos, minusPos, multPos, divPos) if pos &gt;= 0)
</code></pre>
","216074","216074","2018-06-01 01:39:21","6","1018","poke","2009-11-21 13:29:25","238811","20921","2535","771","50634600","","2018-06-01 00:37:05","0","55","<p>I would understand how to do this assuming that I was only looking for one specific character, but in this instance I am looking for any of the 4 operators, '+', '-', '*', '/'. The function returns -1 if there is no operator in the passed string, txt, otherwise it returns the position of the leftmost operator. So I'm thinking find() would be optimal here.</p>

<p>What I have so far:</p>

<pre><code>def findNextOpr(txt):
# txt must be a nonempty string.
    if len(txt) &lt;= 0 or not isinstance(txt, str):
        print(""type error: findNextOpr"")
        return ""type error: findNextOpr""
    if '+' in txt:
        return txt.find('+')
    elif '-' in txt:
        return txt.find('-')
    else
        return -1
</code></pre>

<p>I think if I did what I did for the '+' and '-' operators for the other operators, it wouldn't work for multiple instances of that operator in one expression. Can a loop be incorporated here?</p>
","9672407","","","How do I detect any of 4 characters in a string and then return their index?","<python>","3","2","934"
"50634668","2018-06-01 00:47:51","1","","<p>Try it:</p>

<pre><code>import json
from django.core import serializers
from django.http import JsonResponse

def index(request):
    if request.method == 'GET' and request.is_ajax():
        # Return objects
        entry = Entry.objects.filter()[:5]
        # serializers
        entry2 = serializers.serialize('json', entry)
        # convert JSON
        entry3 = [d['fields'] for d in json.loads(entry2)]
        data = dict()
        data[""entry""] = entry3
    return JsonResponse(data)
</code></pre>
","5587826","","","3","510","André Thadeu Souza","2015-11-20 23:50:29","31","1","0","0","50631477","","2018-05-31 19:28:27","4","909","<p>I want to retrieve data from the database when the bottom of the page is hit.</p>

<p>Now, what I have so far:</p>

<p><strong>urls.py</strong>
<br></p>

<pre><code>urlpatterns = [
   url(r'^$', feedViews.index, name='index'),
   url(r'^load/$', feedViews.load, name='load'),
]
</code></pre>

<p><br>
<strong>views.py</strong></p>

<pre><code>def index(request):
    if request.method == 'GET':
        context = {
                'entry_list': Entry.objects.filter()[:5],
            }
        return render(request,'index.html',context)
    else:
        return HttpResponse(""Request method is not a GET"")

def load(request):
    if request.method == 'GET':
        context = {
                'entry_list': Entry.objects.filter()[:1],
            }
        return render(request,'index.html',context)
    else:
        return HttpResponse(""Request method is not a GET"")
</code></pre>

<p><br>
<strong>index.html</strong></p>

<pre><code>...
    &lt;script&gt;
$(window).on(""scroll"", function() {
       if ((window.innerHeight + window.scrollY) &gt;= document.body.offsetHeight) {
         console.log( ""TEST"" );

        $.ajax(
        {
            type:""GET"",
            url: ""/load"",
            data:{

            },


         })
       }
   });
&lt;/script&gt;
...
</code></pre>

<p>Basicaly it loads 5 items at the beginning and what I try to achieve is that it loads 1 more as soon as I hit the bottom of the page.
So jQuery works beacuase the console.log('Test') works and in my terminal it says</p>

<blockquote>
  <p>""GET /load/ HTTP/1.1"" 200 484</p>
</blockquote>

<p>which is fine as well.</p>

<p>I think I messed up the ajax somehow. I am not sure though.</p>

<p>As you can probably tell I am a nooby but any help is highly appreciated.</p>
","8144583","","","How to pass data through an AJAX request in django?","<python><jquery><ajax><django>","2","3","1767"
"50634685","2018-06-01 00:51:05","0","","<p>There's a lot of hidden pitfalls that make this a risky design, however it is a great learning example.</p>

<p>First off, the call to 'self' when decorating fails because there is no self at that scope. It only exists inside the methods. Now that the easy one is out of the way...</p>

<p>myfunc is an attribute of App class. When you create an instance of App, it is always that one function that gets called. Even when it becomes methodfied, that only happens once.</p>

<pre><code>a1 = App()
a2 = App()
assert a1.myfunc.__func__ is a2.myfunc.__func__
assert id(a1.myfunc) is id(a2.myfunc)  # Methods have some weirdness that means that won't equate but id's show they are the same 
</code></pre>

<p>This is why self is needed to get a unique namespace for the instance. It is also why you won't be able to get decorator that is unique to the instance in this way.
Another way to think about it is that Class must be defined before you can produce instances. Therefore, you can't use an instance in the defination of a Class.</p>

<h2>Solution</h2>

<p>The decorator needs to be written in a way that it won't store any instance attributes. It will access the App instance attributes instead.</p>

<pre><code>class MySerial():
    def __init__(self):
        pass # Possibly don't need to have an __init__
    def write(self, serial_config):
        pass # write to buffer
    def read(self, serial_config):
        pass # read to buffer
    def decorator(self, func):
        def func_wrap(self_app: App, *args, **kwargs):
            self.write(func(self_app, *args, **kwars), self_app.serial_config)
            return self.read(self_app.serial_config)
        return func_wrap

ser = MySerial()

class App():
    def __init__(self, serial_config):
        self.serial_config = serial_config  # This is the instance data for     MySerial

    @ser.decorator
    def myfunc(self):
        # 'yummy_bytes' is written to the serial buffer via 
        # MySerial's decorator method
        return 'yummy_bytes'

if __name__ == '__main__':
    app = App()
</code></pre>

<p>Now I'm assuming MySerial was going to have a unique file, or port or something per instance of App. This is what would be recorded in serial_config. This may not be elegant if the stream is opening an closing but you should be able to improve this for your exact application.</p>
","4368167","4368167","2018-06-01 00:59:24","0","2361","Guy Gangemi","2014-12-16 22:21:28","626","82","156","1","50634303","50634330","2018-05-31 23:49:09","4","514","<p>I am trying to create a class (<code>MySerial</code>) that instantiates a serial object so that I can write/read to a serial device (UART). There is an instance method that is a decorator which wraps around a function that belongs to a completely different class (<code>App</code>). So decorator is responsible for writing and reading to the serial buffer.</p>

<p>If I create an instance of <code>MySerial</code> inside the <code>App</code> class, I can't use the decorator instance method that is created from <code>MySerial</code>.
I have tried foregoing instance methods and using class methods as explained in <a href=""https://stackoverflow.com/questions/1263451/python-decorators-in-classes"">this second answer</a>, but I really need to instantiate <code>MySerial</code>, thus create an instance using <code>__init__</code>.</p>

<p>How can this be accomplished? Is it impossible?</p>

<ul>
<li>Create a decorator that is an instance method.</li>
<li>Use this decorator within another class</li>
</ul>

<hr>

<pre><code>class MySerial():
    def __init__(self):
        pass # I have to have an __init__
    def write(self):
        pass # write to buffer
    def read(self):
        pass # read to buffer
    def decorator(self, func):
        def func_wrap(*args, **kwargs):
            self.write(func(*args, **kwars))
            return self.read()
        return func_wrap

class App():
    def __init__(self):
        self.ser = MySerial()

    @self.ser.decorator  # &lt;-- does not work here.
    def myfunc(self):
        # 'yummy_bytes' is written to the serial buffer via 
        # MySerial's decorator method
        return 'yummy_bytes'

if __name__ == '__main__':
    app = App()
</code></pre>
","4358938","","","Use an instance method as a decorator within another class","<python><mocking><instance><patch><python-decorators>","3","2","1718"
"50634696","2018-06-01 00:52:35","0","","<p>For EC2, it's easy to collect private/public ip addresses like following:</p>

<p><a href=""http://boto3.readthedocs.io/en/latest/reference/services/ec2.html"" rel=""nofollow noreferrer"">EC2 &mdash; Boto 3 Docs 1.7.30 documentation</a>  </p>

<pre><code>import boto3

ec2 = boto3.resource(service_name='ec2',
                     region_name='xxx',
                     aws_access_key_id='xxx',
                     aws_secret_access_key='xxx')

for i in ec2.instances.all():
    print(i.private_ip_address)
    print(i.public_ip_address)
</code></pre>

<p>But, you need to use the iam key for EC2.</p>
","2565527","2565527","2018-06-01 01:00:49","3","603","hiropon","2013-07-09 17:14:22","1054","200","996","5","50634239","","2018-05-31 23:37:44","0","1030","<p>I'm trying to use Python and BOTO3 to list out the IPs of all devices in AWS (ELBs, EC2 etc)</p>

<p>I've been able to pull the whole inventory but How can I assume a role and list out only the IP and device across all of my linked AWS accounts ?</p>
","9877726","","","Use Python and BOTO3 to list AWS IP and devices","<python><bash><amazon-web-services><boto>","2","0","254"
"50634700","2018-06-01 00:53:30","0","","<p>First, you shouldn't be printing or returning error messages; you should be raising exceptions. <code>TypeError</code> and <code>ValueError</code> would be appropriate here. (A string that isn't long enough is the latter, not the former.)</p>

<p>Second, you can simply find the the positions of all the operators in the string using a list comprehension, exclude results of -1, and return the lowest of the positions using <code>min()</code>.</p>

<pre><code>def findNextOpr(text, start=0):
    ops = ""+-/*""
    if not isinstance(text, str):
       raise TypeError(""text must be a string"")
    # ""text must be empty"" isn't strictly true: 
    # you'll get a perfectly sensible result for an empty string
    if not text:
       return ValueError(""text must not be empty"")
    op_idxs = [pos for pos in (text.find(op, start) for op in ops) if pos &gt; -1]
    return min(op_idxs) if op_idxs else -1
</code></pre>

<p>I've added a <code>start</code> argument that can be used to find the next operator: simply pass in the index of the last-found operator, plus 1.</p>
","416467","","","0","1070","kindall","2010-08-10 18:03:46","138012","9545","896","234","50634600","","2018-06-01 00:37:05","0","55","<p>I would understand how to do this assuming that I was only looking for one specific character, but in this instance I am looking for any of the 4 operators, '+', '-', '*', '/'. The function returns -1 if there is no operator in the passed string, txt, otherwise it returns the position of the leftmost operator. So I'm thinking find() would be optimal here.</p>

<p>What I have so far:</p>

<pre><code>def findNextOpr(txt):
# txt must be a nonempty string.
    if len(txt) &lt;= 0 or not isinstance(txt, str):
        print(""type error: findNextOpr"")
        return ""type error: findNextOpr""
    if '+' in txt:
        return txt.find('+')
    elif '-' in txt:
        return txt.find('-')
    else
        return -1
</code></pre>

<p>I think if I did what I did for the '+' and '-' operators for the other operators, it wouldn't work for multiple instances of that operator in one expression. Can a loop be incorporated here?</p>
","9672407","","","How do I detect any of 4 characters in a string and then return their index?","<python>","3","2","934"
"50634737","2018-06-01 01:01:25","1","","<p>Your <code>else</code> only applies to the preceding <code>if</code> (and any <code>elif</code>s attached to it). By doing:</p>

<pre><code>            if searchstring1 in line:
                count_entity += 1
            if searchstring2 in line:
                count_field += 1
            if searchstring3 in line:
                count_other += 1
            else:
                count_none += 1
</code></pre>

<p>you increment <code>count_none</code> every time <code>searchstring3</code> isn't in <code>line</code>, even if <code>searchstring1</code> or <code>searchstring2</code> was in the line (so <code>count_other + count_none</code> will always sum to <code>count_total</code>).</p>

<p>To fix, use <code>elif</code> instead of <code>if</code> for the in between <code>if</code> statements, so the <code>else</code> case only executes if <em>none</em> of the search strings were found:</p>

<pre><code>            if searchstring1 in line:
                count_entity += 1
            elif searchstring2 in line:  # Changed to elif
                count_field += 1
            elif searchstring3 in line:  # Changed to elif
                count_other += 1
            else:
                count_none += 1
</code></pre>

<p>This will prevent you from checking for <code>searchstring2</code> and <code>searchstring3</code> if <code>searchstring1</code> is found (and similarly, if <code>searchstring2</code> is found, you won't check for or increment based on <code>searchstring3</code>). If you need to search all three, but only increment <code>count_none</code> if none of them hit, you'll need to get a little more complicated:</p>

<pre><code>            foundany = False
            if searchstring1 in line:
                foundany = True
                count_entity += 1
            if searchstring2 in line:
                foundany = True
                count_field += 1
            if searchstring3 in line:
                foundany = True
                count_other += 1
            if not foundany:
                count_none += 1
</code></pre>
","364696","","","1","2082","ShadowRanger","2010-06-11 15:51:22","75261","5377","1249","1821","50634714","","2018-06-01 00:56:46","1","43","<p>I'm writing python scripts to count how many times certain strings occur but it seems like the else is not working correctly. </p>

<p>Here is my codes:</p>

<pre><code>import os

user_input = ""#""
directory = os.listdir(user_input)

searchstring1 = 'type=""entity""'

searchstring2 = 'type=""field""'

searchstring3 = 'type=""other""'

searchstring4 = ""type=""

count_entity = 0

count_field = 0

count_other = 0

count_none = 0

counttotal = 0

for fname in directory:
    if os.path.isfile(user_input + os.sep + fname):
        f = open(user_input + os.sep + fname, 'r', encoding=""utf-8"")
        for line in f:
            if ""&lt;noun"" in line:
                counttotal += 1
                if searchstring1 in line:
                    count_entity += 1
                if searchstring2 in line:
                    count_field += 1
                if searchstring3 in line:
                    count_other += 1
                else:
                    count_none += 1

        f.close()

print(""Entity Number"" + str(count_entity))
print(""Field Number"" + str(count_field))
print(""Other Number"" + str(count_other))
print(""None Number"" + str(count_none))
</code></pre>

<p>If it worked correctly, the count_none should equal to total-entity-field-other. But I don't know why it turns out count_none = counttotal so obvious else is not working correctly.</p>

<p>Can anyone tell me why this happened? Thanks for your help!!</p>
","9878693","364696","2018-06-01 01:01:33","Python else not working correctly","<python><python-3.x><if-statement>","1","1","1429"
"50634771","2018-06-01 01:08:09","1","","<p>you can try to follow the steps from ""<a href=""https://stackoverflow.com/questions/37866877/pycharm-anaconda-import-tensor-flow-library-issue"">Pycharm anaconda import tensor flow library issue</a>"" for solve your issue (""You need to do these following steps:"")</p>
","9793130","","","1","268","Andy Hui","2018-05-15 09:04:37","131","39","31","0","50634751","50634771","2018-06-01 01:04:30","0","6123","<p>I cannot install tensorflow in pycharm on windows 10, though I have tried many different things:</p>

<ol>
<li>went to settings > project interpreter and tried clicking the green plus button to install it, gave me the error: non-zero exit code (1) and told me to try installing via pip in the command line, which was successful, but I can't figure out how to make Pycharm use it when it's installed there</li>
<li>tried changing to a Conda environment, which still would not allow me to run tensorflow since when I input into the python command line: pip.main(['install', 'tensorflow']) it gave me another error and told me to update pip</li>
<li>updated pip then tried step 2 again, but now that I have pip 10.0.1, I get the error 'pip has no attribute main'. I tried reverted pip to 9.0.3 in the command line, but this won't change the version used in pycharm, which makes no sense to me. I reinstalled anaconda, as well as pip, and deleted and made a new project and yet it still says that it is using pip 10.0.1 which makes no sense to me</li>
</ol>

<p>So in summary, I still can't install tensorflow, and I now have the wrong version of pip being used in Pycharm. I realize that there are many other posts about this issue but I'm pretty sure I've been to all of them and either didn't get an applicable answer or an answer that I understand.</p>
","4655531","","","Pycharm Can't install TensorFlow","<python><tensorflow><pip><pycharm><conda>","1","0","1356"
"50634782","2018-06-01 01:11:34","2","","<p>Try:</p>

<pre><code>sort -k1,2 -V final_merged.txt
</code></pre>

<p>Running this using your sample data gives me:</p>

<pre><code>1    11
1    13
1    15
2    3
2    5
chr2   6
chr2   15
chr15   3
chr15   9
</code></pre>
","6190930","","","1","226","pgngp","2016-04-12 01:46:09","1100","112","1731","2","50634189","50634782","2018-05-31 23:31:01","0","49","<p>I have very large data which I want to sort on </p>

<ul>
<li>column 1: numerically and then alphanumerically</li>
<li>then on column 2: numerically.</li>
</ul>

<p>So, my final output would be something like this:</p>

<pre><code>1    11  
1    13
1    15
2    3
2    5
chr2   6
chr2   15
chr15   3
chr15   9
</code></pre>

<p>I am using <code>sort</code> on unix. But, I either keep getting <strong>chr2</strong> on the top or on the bottom with any sort I try. Here are some of the sort I tried: which fail to give me the desired output:</p>

<pre><code>sort -V -k1,1n -k2n final_merged.txt &gt; merged-sort.txt
sort -k1,1n -k2n final_merged.txt &gt; merged-sort.txt 
sort -k1,1h -k2n final_merged.txt &gt; merged-sort.txt
sort -k1,1 -k2n final_merged.txt &gt; merged-sort.txt
</code></pre>

<p><strong>Post edit:</strong> Any way to fix this issue without overloading the memory while using</p>

<ul>
<li>sort or other unix utilities</li>
<li>python </li>
</ul>

<p>Thanks,</p>
","6346698","6346698","2018-05-31 23:46:50","sort numerically then alphnumerically on the same column","<python><shell><sorting><ubuntu><unix>","3","4","985"
"50634792","2018-06-01 01:12:51","-1","","<ul>
<li><blockquote>
  <p>You need a headless browser. – SLaks</p>
</blockquote></li>
<li><blockquote>
  <p>Hi @SLaks, i've searched on google, and it's seems than robobrowser are a headless browser, or i've don't understand what i've read (and it's possible ^^) I've read this github.com/dhamaniasad/HeadlessBrowsers – ecavard</p>
</blockquote></li>
<li><blockquote>
  <p>You need a real browser, not a fake one (the first sections in that page). – SLaks</p>
</blockquote></li>
<li><blockquote>
  <p>Thanks @SLaks, I've imported selenium and i'm actually trying to get webdriver of one of headless browser on the github page. – ecavard</p>
</blockquote></li>
<li><blockquote>
  <p>I've used PhantomJS and it seems to work.. Thanks ;) – ecavard </p>
</blockquote></li>
</ul>
","9152863","","","1","776","ecavard","2017-12-29 11:27:12","31","10","4","0","50634430","50634792","2018-06-01 00:08:50","0","352","<p>I've write a python script to submit a form, and when i try it, it doesn't work..</p>

<p>So I wanted to look at the current page in robobrowser, and i found this:</p>

<pre><code>&lt;div id=""content""&gt;
&lt;div class=""SurveyHolder"" id=""BlockPage""&gt;&lt;!-- Modules --&gt;
&lt;div id=""jsrequired""&gt;
&lt;p&gt;Pour répondre à ce sondage, JavaScript doit être activé sur votre navigateur.  Dès que vous aurez activé JavaScript, vous pourrez répondre aux questions ; vous devrez néanmoins relancer le sondage.   Nous vous prions de nous excuser pour les désagréments occasionnés.&lt;/p&gt;
&lt;p&gt;Vous trouverez toutes les instructions relatives à l'activation de JavaScript sur toutes les versions d'Internet Explorer, Mozilla FireFox 3.X, Opera et Netscape sur &lt;a
href=""http://support.microsoft.com/gp/howtoscript""&gt;http://support.microsoft.com/gp/howtoscript&lt;/a&gt;.&lt;/p&gt;
    &lt;ul&gt;
    &lt;li&gt;Activation de JavaScript dans Safari 4.0
                                                        &lt;ul&gt;
    &lt;li&gt;Cliquez sur « Modifier » dans la barre de menu puis choisissez « Préférences ».&lt;/li&gt;
&lt;li&gt;Dans la boîte de dialogue qui apparaît, cliquez sur « Sécurité » dans la barre de menu située en haut.&lt;/li&gt;
&lt;li&gt;En haut de la section Sécurité, dans la rubrique de Contenu Web, cochez la case « Activer JavaScript ».&lt;/li&gt;
&lt;li&gt;Fermez la boîte de dialogue et toutes les fenêtres du navigateur, puis reprenez le sondage.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=""pusher"" id=""contenttrailer""&gt; &lt;/div&gt;
&lt;/div&gt;
</code></pre>

<p>It's in French but, basically, it requires to activate javascript ...</p>

<p>I tried to find a way to activate javascript in robobrowser, but i don't found any solution...</p>

<p>So.. that is my question: Is there a way to do this ?</p>

<p>Thanks for help.</p>
","9152863","","","Is there a way to activate javascript in robobrowser?","<javascript><python><forms><robobrowser>","1","5","1914"
"50634800","2018-06-01 01:14:08","1","","<p>Unfortunately I cannot help just now with a recursive function, but given that a higher count of letters/characters can easily explode into billions of potential combinations if not filtered during creation I have a quirky alternative by iterating over the known words. Those have to be in memory anyway.</p>

<p>[EDIT] Removed the sorting as it does not really provide any benefit, fixed an issue where I falsely set to true on iteration</p>

<pre><code># Some letters, separated by space
letters = 'c a t b'
# letters = 't t a c b'

# # Assuming a word per line, this is the code to read it
# with open(""words_on_file.txt"", ""r"") as words:
#     words_to_look_for = [x.strip() for x in words]
#     print(words_to_look_for)

# Alternative for quick test
known_words = [
    'cat',
    'bat',
    'a',
    'cab',
    'superman',
    'ac',
    'act',
    'grumpycat',
    'zoo',
    'tab'
]

# Create a list of chars by splitting
list_letters = letters.split("" "")

for word in known_words:
    # Create a list of chars
    list_word = list(word)
    if len(list_word) &gt; len(list_letters):
        # We cannot have longer words than we have count of letters
        # print(word, ""too long, skipping"")
        continue

    # Now iterate over the chars in the word and see if we have
    # enough chars/letters
    temp_letters = list_letters[:]

    # This was originally False as default, but if we iterate over each
    # letter of the word and succeed we have a match
    found = True
    for char in list_word:
        # print(char)
        if char in temp_letters:
            # Remove char so it cannot match again
            # list.remove() takes only the first found
            temp_letters.remove(char)
        else:
            # print(char, ""not available"")
            found = False
            break

    if found is True:
        print(word)
</code></pre>

<p>You could copy&amp;paste a product function from the itertools <a href=""https://docs.python.org/3/library/itertools.html#itertools.product"" rel=""nofollow noreferrer"">documentation</a> and use the code provided by ExtinctSpecie, it has no further dependencies, however I found without tweaking it returns all potential options including duplications of characters which I did not immediately understand.</p>

<pre><code>def product(*args, repeat=1):
    # product('ABCD', 'xy') --&gt; Ax Ay Bx By Cx Cy Dx Dy
    # product(range(2), repeat=3) --&gt; 000 001 010 011 100 101 110 111
    pools = [tuple(pool) for pool in args] * repeat
    result = [[]]
    for pool in pools:
        result = [x+[y] for x in result for y in pool]
    for prod in result:
        yield tuple(prod)
</code></pre>
","9802392","9802392","2018-06-01 18:03:06","8","2674","d parolin","2018-05-16 19:48:35","184","23","11","0","50633762","50648943","2018-05-31 22:39:31","2","154","<p>given letters: example of letters</p>

<pre><code>letters = 'hutfb' 
</code></pre>

<p>I am given a file with a list of words.</p>

<p>I need to write a recursive function that allows me to check all possibilities the letters can make. If the possibility is in the list of words from the file, I need to print that specific word. </p>

<p>so for letters given</p>

<p>they can create the words:</p>

<ul>
<li>a</li>
<li>cat</li>
<li>ac</li>
<li>act</li>
<li>cab</li>
</ul>

<p>and so on and on </p>

<p>each combination the letters make I need to check the file to see if its a valid word. if it is I need to print them. </p>

<p>I don't know how start to write this function.</p>
","9849592","9849592","2018-06-02 20:12:48","recursive function for wordsearch","<python><python-3.x><file><recursion>","4","4","684"
"50634826","2018-06-01 01:19:09","1","","<p><a href=""https://github.com/pandas-dev/pandas/issues/21278"" rel=""nofollow noreferrer"">https://github.com/pandas-dev/pandas/issues/21278</a></p>

<p>Warmup was the issue. (double facepalm). Pandas silently builds and caches a hash index at first use (O(maplen)). Calling the tested function and prebuilding the indexgets much better performance.</p>

<pre><code>numiter = 100
for n in [10, 100000, 1000000, 10000000,]:
    domain = np.arange(0, n)
    range = domain+10
    maptable = pd.Series(range, index=domain) #.sort_index()

    query_vals = pd.Series([1,2,3])

    def f1():
        query_vals.map(maptable)
    f1()
    print ""Pandas1 "", n, timeit.timeit(stmt=f1, number=numiter)/numiter

    def f2():
        query_vals.map(maptable.get)
    f2()
    print ""Pandas2 "", n, timeit.timeit(stmt=f2, number=numiter)/numiter

    maptabledict = maptable.to_dict()
    query_vals_list = pd.Series([1,2,3]).tolist()

    def f3():
        {k: maptabledict[k] for k in query_vals_list}
    f3()
    print ""Py dict "", n, timeit.timeit(stmt=f3, number=numiter)/numiter
    print

pd.show_versions()
Pandas1  10 0.000621199607849
Pandas2  10 0.000686831474304
Py dict  10 2.0170211792e-05

Pandas1  100000 0.00149286031723
Pandas2  100000 0.00118808984756
Py dict  100000 8.47816467285e-06

Pandas1  1000000 0.000708899497986
Pandas2  1000000 0.000479419231415
Py dict  1000000 1.64794921875e-05

Pandas1  10000000 0.000798969268799
Pandas2  10000000 0.000410139560699
Py dict  10000000 1.47914886475e-05
</code></pre>

<p>... although a little depressing that python dictionaries are 10x faster.</p>
","48956","","","0","1602","user48956","2008-12-24 19:29:33","5458","718","590","42","50633939","50634826","2018-05-31 23:00:00","5","519","<p>Some days I just hate using middleware. Take this for example: I'd like to have a lookup table that maps values from a set of inputs (domain) values, to outputs (range) values. The mapping is unique. A Python map can do this, but since the map is quite big I figured, why not use a ps.Series and its index, which has added benefit that I can:</p>

<ul>
<li>pass in multiple values to be mapped as a series (hopefully faster than dictionary lookup)</li>
<li>the original series' index in maintained in the result</li>
</ul>

<p>like so:</p>

<pre><code>domain2range = pd.Series(allrangevals, index=alldomainvals)
# Apply the map
query_vals = pd.Series(domainvals, index=someindex)
result = query_vals.map(domain2range)
assert result.index is someindex # Nice
assert (result.values in allrangevals).all() # Nice
</code></pre>

<p>Works as expected. But not. The above .map's time cost grows with <code>len(domain2range)</code> not (more sensibly) <code>O(len(query_vals))</code> as can be shown:</p>

<pre><code>numiter = 100
for n in [10, 1000, 1000000, 10000000,]:
    domain = np.arange(0, n)
    range = domain+10
    maptable = pd.Series(range, index=domain).sort_index()

    query_vals = pd.Series([1,2,3])
    def f():
        query_vals.map(maptable)
    print n, timeit.timeit(stmt=f, number=numiter)/numiter


10 0.000630810260773
1000 0.000978469848633
1000000 0.00130645036697
10000000 0.0162791204453
</code></pre>

<p><em>facepalm</em>. At n=10000000 its taken (0.01/3) second per mapped value.</p>

<p>So, questions:</p>

<ul>
<li>is <a href=""https://pandas.pydata.org/pandas-docs/version/0.21/generated/pandas.Series.map.html"" rel=""nofollow noreferrer"">Series.map</a> expected to behave like this? Why is it so utterly, ridiculously slow? I <em>think</em> I'm using it as shown in the docs.</li>
<li>is there a fast way to use pandas to do table-lookup. It seems like the above is not it?</li>
</ul>
","48956","48956","2018-05-31 23:30:38","Why is pandas.series.map so shockingly slow?","<python><pandas>","1","11","1918"
"50634834","2018-06-01 01:20:02","0","","<p>A Python solution:</p>

<p>Initialize <a href=""https://stackoverflow.com/questions/19366517/sorting-in-python-how-to-sort-a-list-containing-alphanumeric-values?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa"">Natural Sort</a>.</p>

<pre><code>import re

_nsre = re.compile('([0-9]+)')
def natural_sort_key(s):
    return [int(text) if text.isdigit() else text.lower()
            for text in re.split(_nsre, s)]
</code></pre>

<p>Then sort as you wanted:</p>

<pre><code>sorted_data = sorted(data, key=lambda item: (natural_sort_key(str(item[0])), item[1]))
</code></pre>

<p>sorting primarily on <code>item[0]</code> with <code>natural sort</code>, then on <code>item[1]</code> numerically.</p>
","7228140","","","1","732","Stephen Cowley","2016-11-29 23:51:33","1308","107","88","2","50634189","50634782","2018-05-31 23:31:01","0","49","<p>I have very large data which I want to sort on </p>

<ul>
<li>column 1: numerically and then alphanumerically</li>
<li>then on column 2: numerically.</li>
</ul>

<p>So, my final output would be something like this:</p>

<pre><code>1    11  
1    13
1    15
2    3
2    5
chr2   6
chr2   15
chr15   3
chr15   9
</code></pre>

<p>I am using <code>sort</code> on unix. But, I either keep getting <strong>chr2</strong> on the top or on the bottom with any sort I try. Here are some of the sort I tried: which fail to give me the desired output:</p>

<pre><code>sort -V -k1,1n -k2n final_merged.txt &gt; merged-sort.txt
sort -k1,1n -k2n final_merged.txt &gt; merged-sort.txt 
sort -k1,1h -k2n final_merged.txt &gt; merged-sort.txt
sort -k1,1 -k2n final_merged.txt &gt; merged-sort.txt
</code></pre>

<p><strong>Post edit:</strong> Any way to fix this issue without overloading the memory while using</p>

<ul>
<li>sort or other unix utilities</li>
<li>python </li>
</ul>

<p>Thanks,</p>
","6346698","6346698","2018-05-31 23:46:50","sort numerically then alphnumerically on the same column","<python><shell><sorting><ubuntu><unix>","3","4","985"
"50634840","2018-06-01 01:20:42","1","","<p>I meet the same problem in jupyter notebook, and I run the command below and solve my problem:</p>

<pre><code>!pip install pandas
</code></pre>
","6785808","7311767","2018-06-01 01:37:55","1","148","zhao","2016-09-02 03:02:25","181","13","21","0","40553560","40587125","2016-11-11 17:53:07","22","26026","<p>I am using the Jupyter notebook with Python 3 selected. On the first line of a cell I am entering:</p>

<pre><code>import pandas as pd
</code></pre>

<p>The error I get from the notebook is, ImportError: No module named 'pandas'. How can I install pandas to the jupyter notebook? The computer I launched the Jupyter notebook from definitely has pandas.</p>

<p>I tried doing:</p>

<pre><code>!pip install pandas
</code></pre>

<p>And it says it is already installed but for Python 2.7 at the bottom. My script shows it is a Python 3 script at the top though.</p>

<p>When I do echo $PATH in Ubuntu is shows that '/home/user/anaconda2/bin' is on the first entry. I think I may need to change this to be anaconda3?</p>

<p>UPDATE: When I try and launch a Python3 script through jupyter the command line which launched Jupyter gives me the error ""ImportError: No module named 'IPython.paths'. Then there is a timeout waiting for 'kernel_info' reply. Additionally, I tried removing anaconda but still experience the same error. I have tried to make so many quick fixes now, that I am not sure what the next step is to get this working.</p>
","5240483","214143","2018-09-26 19:08:06","Jupyter python3 notebook cannot recognize pandas","<python><pandas><anaconda><jupyter-notebook>","12","2","1139"
"50634883","2018-06-01 01:26:27","0","","<p>The class is named <code>CurlAsyncHTTPClient</code>, not <code>Curlasync_HTTPClient</code>. It looks like someone did a search-and-replace for <code>async</code> to <code>async_</code> because in Python 3.7 <code>async</code> is a reserved word (but it doesn't matter here because it's in the middle of a class name). </p>
","2805033","","","0","326","Ben Darnell","2013-09-22 20:10:35","18747","2200","55","10","50614711","","2018-05-30 23:38:48","1","71","<p>When I start pyspider by <code>pyspider all</code> in terminal, it pops out an <code>ImportError</code>:</p>

<pre><code>ImportError: cannot import name 'Curlasync_HTTPClient' from 'tornado.curl_httpclient'
(/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tornado/curl_httpclient.py)
</code></pre>

<p>There's some other texts in the error message, but I think this is the main cause.</p>

<p>I tried to reinstall tornado by reinstalling tornado(4.5.3), but it does not seem to be helpful. So any of you guys ever experienced the same problem? Any tips will be appreciated. Thanks in advance.</p>

<p>Config Info:<br>
macOS High Sierra 10.13.4<br>
python version:3.7.0a2<br>
tornado version:4.5.3  </p>
","9786842","2745495","2018-05-31 01:00:24","Getting ImportError when starting pyspider in Terminal","<python><python-3.x><tornado><macos-high-sierra><pyspider>","1","0","736"
"50634886","2018-06-01 01:26:35","1","","<p>I don't think there's any shortcuts here. I would recommend drawing the unzoomed screen as normal, blitting in the zoomed area as a circle, then blitting in the magnifying glass over top. The first and last bits you obviously know how to do already, so let's look at blitting in the zoomed area.</p>

<p>To do this, I would </p>

<ul>
<li>take the portion of the image to be zoomed in as a square, and copy it to a new Surface. (Remember that this will be smaller than the resulting area.)</li>
<li>Use pygame.transform.smoothscale to expand it to be large enough to cover the entire magnifying glass area</li>
<li>Use masking to turn it from a square into a circle. See <a href=""https://stackoverflow.com/questions/16880128/pygame-is-there-any-way-to-only-blit-or-update-in-a-mask"">Pygame - is there any way to only blit or update in a mask</a> for details.</li>
<li>Blit the result.</li>
</ul>

<p>Don't worry too much about performance. You're only doing this once per frame.</p>
","3757614","","","1","986","user3757614","2014-06-19 18:00:22","1606","83","9","0","50630647","50634886","2018-05-31 18:30:18","0","326","<p>I made a magnify feature yesterday as a test for a game I'm going to work on. It looks ok but I'd prefer to only zoom in on the area seen through the glass.</p>

<p>Does anybody know how zoom in on a <em>dynamic</em> radius with python/pygame? Will I have to tileset my image or brute force it with some kind of intensive layer blit? I'm not sure whether or not this is something to ask here.</p>

<p>Here's a video demo to show what I'm talking about - <a href=""https://youtu.be/_pVyb0bns3k"" rel=""nofollow noreferrer"">https://youtu.be/_pVyb0bns3k</a></p>
","7511382","","","Zooming in on an area with pygame?","<python><pygame><zoom>","1","0","559"
"50634892","2018-06-01 01:27:51","2","","<p>You can use a list comprehension:</p>

<pre><code>d = [[1,2,3],[2,3],[2,4,3],[4,5],[5]]
new_d = [i for i in d if not any(all(c in i for c in b) and len(b) &lt; len(i) for b in d)]
</code></pre>

<p>Output:</p>

<pre><code>[[2, 3], [5]]
</code></pre>
","7326738","","","1","253","Ajax1234","2016-12-21 16:39:57","49079","3709","2930","360","50634876","50634980","2018-06-01 01:25:19","5","345","<p>I have a list of lists in Python like the following:  </p>

<pre><code>[[1,2,3],[2,3],[2,4,3],[4,5],[5]]
</code></pre>

<p>I want to remove all the inner lists that are a superset (a list that contain all the elements of another list, but with additional elements) of another inner list. For the example above, removing the supersets should result in the following:</p>

<pre><code>[[2,3],[5]]
</code></pre>

<p>How can I accomplish this?</p>
","9878711","","","How can you remove superset lists from a list of lists in Python?","<python><python-3.x><list><nested-lists>","4","0","446"
"50634909","2018-06-01 01:30:20","0","","<p>yeah, I did run it as administrator but the problem was with the command ...  shutil.copy won't work due to some reason so I used the following command to copy</p>

<p><code>distutils.dir_util.copy_tree(folderPath,r'D:/job/HDrec2_MotoG/Windows/records')</code> </p>

<p>and this worked somehow :D</p>
","9356156","9356156","2018-06-01 03:16:14","0","304","Teddy 1993","2018-02-13 16:57:49","3","21","0","0","50633853","","2018-05-31 22:50:23","0","366","<p>I am trying to copy contents of a folder but it keeps on showing the error of Permission denied.   </p>

<pre><code> import subprocess
 import shutil
 import os


 procId = subprocess.call('adb pull /sdcard/HDrec2.0/records/ tmp/', stdin = subprocess.PIPE)
folderPath = (os.getcwd() + os.sep + 'tmp')
shutil.copyfile(folderPath ,'D:\job\HDrec2_MotoG\Windows\records')
</code></pre>

<p>error message is </p>

<pre><code>traceback (most recent call last):
  File ""D:\job\HDrec2_MotoG\Windows\z2.py"", line 20, in &lt;module&gt;
    shutil.copyfile(folderPath ,'D:\job\HDrec2_MotoG\Windows\records')
  File ""C:\Python27\lib\shutil.py"", line 96, in copyfile
    with open(src, 'rb') as fsrc:
IOError: [Errno 13] Permission denied: 'D:\\job\\HDrec2_MotoG\\Windows\\tmp'
</code></pre>
","9356156","","","shutil.copyfile keep returning error of permission denied","<python><shutil><ioerror>","1","3","782"
"50634922","2018-06-01 01:31:59","6","","<p>Only one parameter for lambda like:</p>

<pre><code>c = list(map(lambda x: sum(x), zip(a, b)))
</code></pre>

<p>But once we are using sum, we can map it directly like:</p>

<pre><code>c = list(map(sum, zip(a, b)))
</code></pre>

<h3>Test Code:</h3>

<pre><code>a = (1, 2)
b = (3, 4)
c = list(map(sum, zip(a, b)))
print(c)
</code></pre>

<h3>Result:</h3>

<pre><code>[4, 6]
</code></pre>
","7311767","7311767","2018-06-01 01:35:06","1","391","Stephen Rauch","2016-12-18 02:06:51","33601","12784","4195","3857","50634912","50634922","2018-06-01 01:30:25","4","84","<p>I'm trying to write a function for adding 2D vectors. 
I'm trying to combine the <code>map()</code> function, getting a list using the <code>zip()</code> function (which will zip 2 tuples).</p>

<p>This is the code:</p>

<pre><code>a = (1, 2)
b = (3, 4)
c = list(map(lambda x, y: x+y, list(zip(a, b))))
print(c)
</code></pre>

<p>So the way I see it, <code>zip(a, b)</code> returns a zip object containing the following tuples: (1, 3), (2, 4). It is then converted to a list. I'm getting the following error:</p>

<blockquote>
  <p>TypeError: () missing 1 required positional argument: 'y'</p>
</blockquote>

<p>So my guess is that the lambda function is not taking the second number in each tuple.</p>

<p>Is there any way to do this? </p>
","6219523","7311767","2018-06-01 01:34:03","Combining map with zip through lambda","<python>","4","0","744"
"50634932","2018-06-01 01:33:15","0","","<ol>
<li><p>The file descriptor returned by <code>open</code> is not a pipe. It's generally legal to use regular files with <code>PipeIOStream</code>, but it's not <em>useful</em> on linux. Such file descriptors are always considered readable, and reading from them or writing to them always blocks. So using PipeIOStream like this is no better than simply doing <code>fd.write(item[1])</code>. </p></li>
<li><p>You've opened the file in write-only mode, but PipeIOStream wraps its file in a read/write wrapper (I'm actually kind of surprised that this ever works, since real pipes are one-way). I <em>think</em> that's where this exception is coming from. If you opened the file in <code>'ab+'</code> mode, I think it would work. I haven't tried this, though. </p></li>
</ol>
","2805033","","","1","777","Ben Darnell","2013-09-22 20:10:35","18747","2200","55","10","50630940","50634932","2018-05-31 18:50:31","1","302","<p>I've a little micro webservice which stores messages locally identified by IDs. To ensure that files won't be written at the same time, I've implemented a queue. The following code works just once, a second file upload throws traceback below, I really don't know how to handle the fd correctly. </p>

<pre><code>from tornado import web, ioloop, gen
from tornado.queues import Queue
from tornado.iostream import PipeIOStream

class Sample:
    def __init__(self):
        self.queue = Queue()

    @gen.coroutine
    def write_queue(self):
        while True:
            item = yield self.queue.get()
            print(""Message with id %s stored"" % item[0])
            fd = open(item[0], 'ab')
            stream = PipeIOStream(fd.fileno())
            yield stream.write(item[1])
            stream.close_fd()

class MainHandler(web.RequestHandler):

    def initialize(self, store):
        self.store = store

    @gen.coroutine
    def put(self, id):
        yield self.store.queue.put((id, self.request.body))


def start(store):
    return web.Application([
        (r""/(.*)"", MainHandler,
         {""store"": store})
    ])

if __name__ == '__main__':
    store = Store()
    app = start(store)
    app.listen(8888)
    ioloop.IOLoop.current().add_callback(store.write_queue)
    ioloop.IOLoop.current().start()




ERROR:tornado.application:Exception in callback functools.partial(&lt;function wrap.&lt;locals&gt;.null_wrapper at 0x7f46657f46a8&gt;, &lt;Future finished exception=OSError(9, 'Bad file descriptor')&gt;)
Traceback (most recent call last):

    stream = PipeIOStream(fd.fileno())
  File ""/usr/local/lib/python3.5/dist-packages/tornado/iostream.py"", line 1643, in __init__
    self._fio = io.FileIO(self.fd, ""r+"")
OSError: [Errno 9] Bad file descriptor
</code></pre>
","9877585","","","Tornado PipeIOStream: OSError: [Errno 9] Bad file descriptor","<python><tornado>","1","0","1791"
"50634934","2018-06-01 01:33:23","4","","<p>In Python2, you can use specific unpacking syntax in a <code>lambda</code>:</p>

<pre><code>a = (1, 2)
b = (3, 4)
c = list(map(lambda (x, y): x+y, list(zip(a, b))))
</code></pre>

<p>However, in Python3, you will have to use <code>sum</code>:</p>

<pre><code>c = list(map(lambda x_y: sum(x_y), list(zip(a, b))))
</code></pre>
","7326738","","","2","329","Ajax1234","2016-12-21 16:39:57","49079","3709","2930","360","50634912","50634922","2018-06-01 01:30:25","4","84","<p>I'm trying to write a function for adding 2D vectors. 
I'm trying to combine the <code>map()</code> function, getting a list using the <code>zip()</code> function (which will zip 2 tuples).</p>

<p>This is the code:</p>

<pre><code>a = (1, 2)
b = (3, 4)
c = list(map(lambda x, y: x+y, list(zip(a, b))))
print(c)
</code></pre>

<p>So the way I see it, <code>zip(a, b)</code> returns a zip object containing the following tuples: (1, 3), (2, 4). It is then converted to a list. I'm getting the following error:</p>

<blockquote>
  <p>TypeError: () missing 1 required positional argument: 'y'</p>
</blockquote>

<p>So my guess is that the lambda function is not taking the second number in each tuple.</p>

<p>Is there any way to do this? </p>
","6219523","7311767","2018-06-01 01:34:03","Combining map with zip through lambda","<python>","4","0","744"
"50634936","2018-06-01 01:33:41","3","","<p><code>zip</code> returns a tuples, so you could sum like this example: </p>

<pre><code>list(map(lambda x: x[0] + x[1], zip(a, b)))
</code></pre>

<p>output:</p>

<pre><code>[4, 6]
</code></pre>
","3926995","","","0","198","Chiheb Nexus","2014-08-10 12:58:13","5825","915","2336","8","50634912","50634922","2018-06-01 01:30:25","4","84","<p>I'm trying to write a function for adding 2D vectors. 
I'm trying to combine the <code>map()</code> function, getting a list using the <code>zip()</code> function (which will zip 2 tuples).</p>

<p>This is the code:</p>

<pre><code>a = (1, 2)
b = (3, 4)
c = list(map(lambda x, y: x+y, list(zip(a, b))))
print(c)
</code></pre>

<p>So the way I see it, <code>zip(a, b)</code> returns a zip object containing the following tuples: (1, 3), (2, 4). It is then converted to a list. I'm getting the following error:</p>

<blockquote>
  <p>TypeError: () missing 1 required positional argument: 'y'</p>
</blockquote>

<p>So my guess is that the lambda function is not taking the second number in each tuple.</p>

<p>Is there any way to do this? </p>
","6219523","7311767","2018-06-01 01:34:03","Combining map with zip through lambda","<python>","4","0","744"
"50634946","2018-06-01 01:34:59","3","","<p>The <code>lambda</code> receives <em>one</em> parameter, which is a 2-tuple returned by <code>zip</code>.</p>

<ul>
<li>You can access it element-wise: <code>lambda pair: pair[0] + pair[1]</code>.</li>
<li>You can apply <code>sum()</code> to it.</li>
<li>In Python 2, you can unpack it right in the signature: <code>lambda (x, y): x + y</code>. </li>
</ul>
","223424","","","0","360","9000","2009-12-03 01:14:45","32123","2971","4620","18","50634912","50634922","2018-06-01 01:30:25","4","84","<p>I'm trying to write a function for adding 2D vectors. 
I'm trying to combine the <code>map()</code> function, getting a list using the <code>zip()</code> function (which will zip 2 tuples).</p>

<p>This is the code:</p>

<pre><code>a = (1, 2)
b = (3, 4)
c = list(map(lambda x, y: x+y, list(zip(a, b))))
print(c)
</code></pre>

<p>So the way I see it, <code>zip(a, b)</code> returns a zip object containing the following tuples: (1, 3), (2, 4). It is then converted to a list. I'm getting the following error:</p>

<blockquote>
  <p>TypeError: () missing 1 required positional argument: 'y'</p>
</blockquote>

<p>So my guess is that the lambda function is not taking the second number in each tuple.</p>

<p>Is there any way to do this? </p>
","6219523","7311767","2018-06-01 01:34:03","Combining map with zip through lambda","<python>","4","0","744"
"50634954","2018-06-01 01:36:15","0","","<p>Yes, this looks fine to me. The key thing to remember is that in <code>worker_A</code> (the function you pass to <code>executor.submit()</code>, you must not call <code>write_message</code>. Instead, return a value and write it after <code>yield executor.submit()</code> has returned.</p>
","2805033","","","2","292","Ben Darnell","2013-09-22 20:10:35","18747","2200","55","10","50633359","","2018-05-31 21:56:42","2","341","<p>I am new to Python and the problem I'm trying to solve is to have an on-going websocket connection that can take in requests, wait some time while doing calcs, return results when ready while not blocking other requests that come from other users/clients/connections. I have achieved this and it works rather nicely but wanted to check if this is the right solution. In this particular snippet they key bit is actions performed upon open: I spin off another thread with sleep.</p>

<pre><code>import tornado.web
import tornado.websocket
import tornado.httpserver
import tornado.ioloop
import time
import json
from tornado import gen
from concurrent.futures import ThreadPoolExecutor
from tornado.options import define, options, parse_command_line

define(""port"", default=8888, type=int)

thread_pool = ThreadPoolExecutor(2)

class WebSocketHandler(tornado.websocket.WebSocketHandler):
    # Make this an asynchronous coroutine
    @gen.coroutine
    def on_message_coroutine(self, message):
        self.write_message('Message:', message)

        def worker_A(websocket, x):
            time.sleep(1)
            print('TICK', x)
            pass
            return x

        print('scheduling and yielding')
        for x in range(0, 30):
            test = yield thread_pool.submit(worker_A, self, x)
            self.write_message(json.dumps(test))

        print('done yielding')

    def open(self, *args):
        print(""New connection"")
        tornado.ioloop.IOLoop.current().spawn_callback(self.on_message_coroutine, 'New Msg')

    def check_origin(self, origin):
        return True

    def on_message(self, message):
        print(""New message {}"".format(message))
        self.write_message(""You sent me this, sending it back in upper: "" + message.upper())

    def on_close(self):
            print(""Connection closed"")

app = tornado.web.Application([
    (r'/ws/', WebSocketHandler),
])

if __name__ == '__main__':
    app.listen(options.port)
    tornado.ioloop.IOLoop.instance().start()
</code></pre>

<p>Output on front-end's console below. ""New message"" bit is when I click the buttons while websocket is continuously returning other output (TICKS):</p>

<pre><code>app.component.ts:17 Response from websocket: 11
app.component.ts:17 Response from websocket: 12
app.component.ts:17 Response from websocket: 13
app.component.ts:17 Response from websocket: 14
app.component.ts:24 new message from client to websocket:  gotta be a string
app.component.ts:17 Response from websocket: You sent me this, sending it back in upper: ""GOTTA BE A STRING""
app.component.ts:17 Response from websocket: 15
app.component.ts:17 Response from websocket: 16
app.component.ts:17 Response from websocket: 17
app.component.ts:24 new message from client to websocket:  gotta be a string
app.component.ts:17 Response from websocket: You sent me this, sending it back in upper: ""GOTTA BE A STRING""
app.component.ts:17 Response from websocket: 18
app.component.ts:17 Response from websocket: 19
app.component.ts:17 Response from websocket: 20
app.component.ts:17 Response from websocket: 21
</code></pre>
","9878239","9878239","2018-05-31 22:02:22","Tornado WebSockets non-blocking requests using threading. Am I doing it right?","<python><websocket><tornado>","1","1","3106"
"50634956","2018-06-01 01:36:30","1","","<p>This is due to the <code>unsafe</code> default-value for the <code>casting</code>argument of <code>astype</code>. In the <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.astype.html"" rel=""nofollow noreferrer"">docs</a> the argument <code>casting</code> is described as such:</p>

<p>""Controls what kind of data casting may occur. <strong>Defaults to ‘unsafe’ for backwards compatibility.</strong>"" (my emphasis)</p>

<p>Any of the other possible castings return a <code>TypeError</code>.</p>

<pre><code>a = np.empty((1,), object)
a[0] = np.array([1.2])
a.astype(float, casting='same_kind')
</code></pre>

<p>Results in:</p>

<pre><code>TypeError: Cannot cast array from dtype('O') to dtype('float64') according to the rule 'same_kind'
</code></pre>

<p>This is true for all castings except <code>unsafe</code>, namely: <code>no</code>, <code>equiv</code>, <code>safe</code>, and <code>same_kind</code>.</p>
","2861681","","","0","937","vmg","2013-10-09 07:23:30","2235","216","157","11","49159074","50634956","2018-03-07 18:38:50","14","307","<p>I'll preface this with the statement that I wouldn't do this in the first place and that I ran across this helping a friend.</p>

<p>Consider the data frame <code>df</code></p>

<pre><code>df = pd.DataFrame(pd.Series([[1.2]]))

df

       0
0  [1.2]
</code></pre>

<p>This is a data frame of objects where the objects are lists.  In my friend's code, they had:</p>

<pre><code>df.astype(float)
</code></pre>

<p>Which breaks as I had hoped</p>

<blockquote>
<pre><code>ValueError: setting an array element with a sequence.
</code></pre>
</blockquote>

<p>However, if those values were numpy arrays instead:</p>

<pre><code>df = pd.DataFrame(pd.Series([np.array([1.2])]))

df

       0
0  [1.2]
</code></pre>

<p>And I tried the same thing:</p>

<pre><code>df.astype(float)

     0
0  1.2
</code></pre>

<p>It's happy enough to do something and convert my 1-length arrays to scalars.  This feels very dirty!</p>

<p>If instead they were not 1-length arrays</p>

<pre><code>df = pd.DataFrame(pd.Series([np.array([1.2, 1.3])]))

df

            0
0  [1.2, 1.3]
</code></pre>

<p>Then it breaks</p>

<blockquote>
<pre><code>ValueError: setting an array element with a sequence.
</code></pre>
</blockquote>

<hr>

<p><strong>Question</strong><br>
Please tell me this is a bug and we can fix it.  Or can someone explain why and in what world this makes sense?</p>

<hr>

<p><strong>Response to @root</strong><br>
You are right.  Is this worth an issue?  Do you expect/want this?</p>

<pre><code>a = np.empty((1,), object)
a[0] = np.array([1.2])

a.astype(float)

array([ 1.2])
</code></pre>

<p>And</p>

<pre><code>a = np.empty((1,), object)
a[0] = np.array([1.2, 1.3])

a.astype(float)
</code></pre>

<blockquote>
<pre><code>ValueError: setting an array element with a sequence.
</code></pre>
</blockquote>
","2336654","2336654","2018-03-07 20:57:50","DataFrame of objects `astype(float)` behaviour different depending if lists or arrays","<python><pandas><numpy>","1","6","1805"
"50634974","2018-06-01 01:40:38","1","","<p>Here is an example using an Explicit Wait with an Excpected Condition.  Once the page is loaded, it will wait up to an additional 10 seconds for the title to match ""Example Domain"", then close the browser.  It polls the DOM while waiting... and will raise a <code>TimeoutException</code> after 10 secs if the title does not match yet.</p>

<pre><code>from selenium import webdriver
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait

driver = webriver.Chrome()
driver.get(https://example.com)
wait = WebDriverWait(driver, 10)
wait.until(EC.title_is(""Example Domain""))
driver.close()
</code></pre>
","16148","","","1","670","Corey Goldberg","2008-09-17 15:03:51","40460","5058","3100","76","50634788","","2018-06-01 01:12:29","0","309","<p>The task is to check all titles on many web pages are correct, I tried to make it fast via close the browser once title is loaded, but did not get the time down. Is selenium always loading all, i.e. until <code>&lt;/html&gt;?</code></p>

<p>I would like to have something like, </p>

<pre><code>        driver.get('webpage_url')
        #t = EC.title_is('title_string')
        if driver.title:
                driver.close()
</code></pre>
","5203458","","","How to make python selenium close browser once title loaded?","<python><selenium><selenium-webdriver>","1","1","443"
"50634980","2018-06-01 01:41:35","3","","<p>A set can only be a subset of another if it is smaller, thus by iterating over the sets in ascending order of size, we can check each element against the previously found minimal subsets to know if it is a minimal subset.</p>

<pre><code>def get_minimal_subsets(sets):
    sets = sorted(map(set, sets), key=len)
    minimal_subsets = []
    for s in sets:
        if not any(minimal_subset.issubset(s) for minimal_subset in minimal_subsets):
            minimal_subsets.append(s)

    return minimal_subsets

l = [[1,2,3],[2,3],[2,4,3],[4,5],[5]]

print(get_minimal_subsets(l))  # [{5}, {2, 3}]
</code></pre>
","5079316","","","1","612","Olivier Melançon","2015-07-04 00:30:37","15837","1134","1990","511","50634876","50634980","2018-06-01 01:25:19","5","345","<p>I have a list of lists in Python like the following:  </p>

<pre><code>[[1,2,3],[2,3],[2,4,3],[4,5],[5]]
</code></pre>

<p>I want to remove all the inner lists that are a superset (a list that contain all the elements of another list, but with additional elements) of another inner list. For the example above, removing the supersets should result in the following:</p>

<pre><code>[[2,3],[5]]
</code></pre>

<p>How can I accomplish this?</p>
","9878711","","","How can you remove superset lists from a list of lists in Python?","<python><python-3.x><list><nested-lists>","4","0","446"
"50634998","2018-06-01 01:44:02","2","","<p>Let's say you have a map as an image, and you want to determine if a random point is on land or not. What you need for that is an image redrawn to have only black and white pixels. You can then use Pygame's Surface.get_at() command to see what color the pixel there is, and make a decision based on that.</p>

<p>Alternately, let's say you have a map as a bunch of polygons, and you want to determine if a random point is on land or not. The logic there is called ray tracing, and is explained better over here: <a href=""https://stackoverflow.com/questions/217578/how-can-i-determine-whether-a-2d-point-is-within-a-polygon"">How can I determine whether a 2D Point is within a Polygon?</a></p>
","3757614","","","0","695","user3757614","2014-06-19 18:00:22","1606","83","9","0","50567595","50634998","2018-05-28 13:29:47","1","174","<p>Is it possible to have red circles to only appear inside a polygon. For a rectangle, you can determine the height and width. </p>

<p>I'm planning to make like a virus simulator, and that there are red circles that only appear only inside the countries. But the countries aren't rectangles, but polygons/images.</p>

<p>I was wondering if it was possible to have only circles to be blit inside a polygon or image. 
Thanks</p>
","9699232","","","Things to be blit only inside a polygon (Pygame)","<python><python-3.x><pygame>","2","5","429"
"50635014","2018-06-01 01:47:09","2","","<p>I ended up with the same idea as @Olivier Melançon. You can use ascending order to throw away subsets and have this run in O(n^2) * O(subset calculation).</p>

<pre><code>input = [[1,2,3],[2,3],[2,4,3],[4,5],[5]]
sets = [set(x) for x in input]
sets.sort(key=len)

subsets = []
while sets != []:
    cur = sets[0]
    subsets.append(cur)
    sets = [x for x in sets[1:] if not cur &lt;= x]

output = [list(x) for x in subsets]
print(output)
</code></pre>
","3029173","3029173","2018-06-01 01:52:46","2","457","AnilRedshift","2013-11-24 20:22:44","4450","333","1194","14","50634876","50634980","2018-06-01 01:25:19","5","345","<p>I have a list of lists in Python like the following:  </p>

<pre><code>[[1,2,3],[2,3],[2,4,3],[4,5],[5]]
</code></pre>

<p>I want to remove all the inner lists that are a superset (a list that contain all the elements of another list, but with additional elements) of another inner list. For the example above, removing the supersets should result in the following:</p>

<pre><code>[[2,3],[5]]
</code></pre>

<p>How can I accomplish this?</p>
","9878711","","","How can you remove superset lists from a list of lists in Python?","<python><python-3.x><list><nested-lists>","4","0","446"
"50635033","2018-06-01 01:50:01","2","","<p>Here it is:</p>

<pre><code>super=[[1,2,3],[2,3],[2,4,3],[4,5],[5]]
subset=[s for s in super if not any(set(s).issuperset(set(i)) and len(s)&gt;len(i) for i in super)]
</code></pre>

<p>Output:</p>

<pre><code>&gt;&gt;&gt; subset
[[2, 3], [5]]
</code></pre>
","4397349","","","0","261","Tin Luu","2014-12-27 07:02:56","1084","48","293","3","50634876","50634980","2018-06-01 01:25:19","5","345","<p>I have a list of lists in Python like the following:  </p>

<pre><code>[[1,2,3],[2,3],[2,4,3],[4,5],[5]]
</code></pre>

<p>I want to remove all the inner lists that are a superset (a list that contain all the elements of another list, but with additional elements) of another inner list. For the example above, removing the supersets should result in the following:</p>

<pre><code>[[2,3],[5]]
</code></pre>

<p>How can I accomplish this?</p>
","9878711","","","How can you remove superset lists from a list of lists in Python?","<python><python-3.x><list><nested-lists>","4","0","446"
"50635051","2018-06-01 01:53:05","1","","<p>In a while loop, your going to have to get the span that contains the time, for example like so.</p>

<p><code>time_element = driver.find_elements_by_class_name(""jquery_server_clock"");</code></p>

<p>Then you can get the time as a string</p>

<p><code>time_element.get_attribute('text')</code></p>

<p>Then you could convert this into a DateTime object and then simply check if the time is more than or equal to a DateTime of 6 o'clock.</p>
","7706630","","","0","444","SillySam","2017-03-14 04:56:41","56","33","16","0","50634669","50635051","2018-06-01 00:48:00","0","162","<p>I am writing a selenium script to perform form fills at a certain time of day. </p>

<p>I currently have the form fills scheduled with Python's schedule module however this uses my system time. The issue is my system time is not synced with the websites server time. Fortunately, the website has a live clock in the page. </p>

<p>I inspected the clock element which is as follows:</p>

<p><code>&lt;span&gt;The Time is: &lt;b class=""jquery_server_clock"" dfc=""pv""&gt;5:24:59 PM&lt;/b&gt;&lt;/span&gt;</code></p>

<p>What I am trying to do is monitor the live clock on the website and when it hits a certain time perform an action - I was thinking with a 'while loop'. </p>

<p>Something like:</p>

<pre><code>while true:
    t = ""jquery_server_clock""
    if t = ""6:00:00 PM""
        driver.find_element_by_name(""submit"").click()
</code></pre>

<p>I can't seem to get the clock time into a variable to monitor at this point. After I get that I'm sure I could get the while loop working. </p>

<p>Thank you for the help!</p>
","7406623","","","Monitor live clock with selenium / python","<python><selenium>","2","3","1026"
"50635080","2018-06-01 01:57:08","0","","<p>can you try this.
Change reader to DictReader and json.dumps(row).
DictReader make input data is python dict. And for in is loop each row in reader, you just try push row is enough</p>

<pre><code>        es = Elasticsearch([{'host': 'localhost', 'port': 9200}])
        print(es)

        def csv_reader(file_obj, delimiter=','):
            reader = csv.DictReader(file_obj)
            i = 1
            results = []
            for row in reader:
                print(row)
                es.index(index='product', doc_type='prod', id=i,
                         body=json.dumps(row))
                i = i + 1

                results.append(row)
                print(row)

          if __name__ == ""__main__"":
           with open(""/home/Documents/csv/acsv.csv"") as f_obj:
           csv_reader(f_obj)
</code></pre>
","5978735","5978735","2018-06-01 02:19:51","0","827","Ngoc Pham","2016-02-25 06:51:20","689","118","7","1","50619742","","2018-05-31 08:18:18","1","3955","<p>I am iterating the rows one by one of a csv file and I want to insert it into es. I'm new to both python and elastic search.How to convert one csv row and insert it into es one by one 
</p>

<pre class=""lang-python prettyprint-override""><code>import csv
import json

from elasticsearch import Elasticsearch

es = Elasticsearch(
  [{'host': 'localhost', 'port': 9200}])
 print(es)


def csv_reader(file_obj, delimiter=','):
   reader = csv.reader(file_obj)
   i = 1
   results = []
   for row in reader:
    print(row)
    es.index(index='product', doc_type='prod', id=i, 
   body=json.dump([row for row in reader], file_obj))
    i = i + 1
    results.append(row)
    print(row)


 if __name__ == ""__main__"":
  with open(""/home/Documents/csv/acsv.csv"") as f_obj:
    csv_reader(f_obj)
</code></pre>

<p>But I'm getting this error:</p>

<blockquote>
  <p>Traceback (most recent call last):</p>
  
  <p>File ""/home/PycharmProjects/CsvReaderForSyncEs/csvReader.py"", line 25, in  csv_reader(f_obj)</p>
  
  <p>File ""/home/PycharmProjects/CsvReaderForSyncEs/csvReader.py"", line 17, in csv_reader</p>
  
  <p>es.index(index='product', doc_type='prod', id=i, body=json.dump([row for row in reader], file_obj))</p>
  
  <p>File ""/usr/lib/python2.7/json/<strong>init</strong>.py"", line 190, in dump fp.write(chunk)</p>
  
  <p>IOError: File not open for writing</p>
</blockquote>
","9870449","4797454","2018-05-31 11:23:11","Read CSV and Upload Data to Elasticsearch","<python><elasticsearch>","3","0","1374"
"50635090","2018-06-01 01:58:36","0","","<pre><code>class CustomerSerializer(serializers.ModelSerializer):
    ship_address = AddressSerializer(read_only=True)
</code></pre>

<p>and modify models.py 
<code>null=True</code> in field that you want to make optional</p>
","7690443","","","6","226","Ravi Bhushan","2017-03-10 13:12:17","380","66","9","3","50631349","","2018-05-31 19:18:25","0","186","<p>I m unable to sort this validation error.</p>

<p>My Address Model has a required field 'locality'.</p>

<pre><code>class Address(models.Model):
      ....
      locality = models.CharField(max_length=20,unique=True)
      ....

class AddressSerializer(serializers.ModelSerializer):
    class Meta:
        model = Address
        fields = ['house_number', 'street', 'area', 'locality', 'address_of']


class CustomerSerializer(serializers.ModelSerializer):
    ship_address = AddressSerializer(required=False)

    class Meta:
        model = Customer
</code></pre>

<p>In Address model locality is a required field.</p>

<p>I want to have the AddressSerializer to be optional in CustomerSerializer. 
Inspite of having the required=False flag, I m getting a validation error : </p>

<pre><code>""ship_address"": {
        ""locality"": [
            ""This field may not be blank.""
        ]
    }
</code></pre>

<p>** Kindly note: This is just a representational one and not the exact replica of my models and serializers thus ignore any minor errors.</p>
","5427043","5427043","2018-06-01 02:43:45","DRF Unable to have required=False Flag in Nested Model Serializer","<python><django-rest-framework>","3","4","1056"
"50635091","2018-06-01 01:58:37","1","","<blockquote>
  <p>Is there an easier, more direct way to do it, without having to export it to a .pb file, then freeze it and so on? </p>
</blockquote>

<p>Yes, as you pointed out in the updated question, it is possible to <a href=""https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/framework/graph_util_impl.py#L206"" rel=""nofollow noreferrer"">freeze the graph</a> and use <a href=""https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/contrib/lite/python/lite.py#L141"" rel=""nofollow noreferrer"">toco_convert</a> in python api directly. It needs the graph to be frozen and the input and output shapes to be determined. In your question, there is no freeze graph step since there are no variables. If you have variables and run toco without converting those to constants first, toco will complain!</p>

<blockquote>
  <p>Now all I have to do is find a way to adapt this to the iris classification model. Any suggestions?</p>
</blockquote>

<p>This one is slightly trickier and needs more work. Basically, you need to load the graph and figure out the input and output tensor names and then freeze the graph and call toco_convert. For finding the input and output tensor names in this case (where you have not defined the graph), you have to poke around the graph generated and determine them based on input shapes, names, etc. Here is the code that you can append at the end of your main function in <code>premade_estimator.py</code> to generate the tflite graph in this case.  </p>

<pre><code>print(""\n====== classifier model_dir, latest_checkpoint ==========="")
print(classifier.model_dir)
print(classifier.latest_checkpoint())
debug = False

with tf.Session() as sess:
    # First let's load meta graph and restore weights
    latest_checkpoint_path = classifier.latest_checkpoint()
    saver = tf.train.import_meta_graph(latest_checkpoint_path + '.meta')
    saver.restore(sess, latest_checkpoint_path)

    # Get the input and output tensors needed for toco.
    # These were determined based on the debugging info printed / saved below.
    input_tensor = sess.graph.get_tensor_by_name(""dnn/input_from_feature_columns/input_layer/concat:0"")
    input_tensor.set_shape([1, 4])
    out_tensor = sess.graph.get_tensor_by_name(""dnn/logits/BiasAdd:0"")
    out_tensor.set_shape([1, 3])

    # Pass the output node name we are interested in.
    # Based on the debugging info printed / saved below, pulled out the
    # name of the node for the logits (before the softmax is applied).
    frozen_graph_def = tf.graph_util.convert_variables_to_constants(
        sess, sess.graph_def, output_node_names=[""dnn/logits/BiasAdd""])

    if debug is True:
        print(""\nORIGINAL GRAPH DEF Ops ==========================================="")
        ops = sess.graph.get_operations()
        for op in ops:
            if ""BiasAdd"" in op.name or ""input_layer"" in op.name:
                print([op.name, op.values()])
        # save original graphdef to text file
        with open(""estimator_graph.pbtxt"", ""w"") as fp:
            fp.write(str(sess.graph_def))

        print(""\nFROZEN GRAPH DEF Nodes ==========================================="")
        for node in frozen_graph_def.node:
            print(node.name)
        # save frozen graph def to text file
        with open(""estimator_frozen_graph.pbtxt"", ""w"") as fp:
            fp.write(str(frozen_graph_def))

tflite_model = tf.contrib.lite.toco_convert(frozen_graph_def, [input_tensor], [out_tensor])
open(""estimator_model.tflite"", ""wb"").write(tflite_model)
</code></pre>

<p><strong>Note:</strong> I am assuming the logits from the final layer (before the Softmax is applied) as the output, corresponding to the node <strong><em>dnn/logits/BiasAdd</em></strong>. If you want the probabilities, I believe it is  <strong><em>dnn/head/predictions/probabilities</em></strong>.</p>
","9789871","","","5","3859","Pannag Sanketi","2018-05-14 17:27:15","1021","220","10","1","50581883","50635091","2018-05-29 10:19:10","2","5562","<p><strong>Background information:</strong></p>

<p>I have written a TensorFlow model very similar to the <a href=""https://www.tensorflow.org/get_started/premade_estimators"" rel=""nofollow noreferrer"">premade iris classification model</a> provided by TensorFlow. The differences are relatively minor: </p>

<ul>
<li>I am classifying football exercises, not iris species.</li>
<li>I have 10 features and one label, not 4 features and one label.</li>
<li>I have 5 different exercises, as opposed to 3 iris species.</li>
<li>My trainData contains around 3500 rows, not only 120.</li>
<li>My testData contains around 330 rows, not only 30.</li>
<li>I am using a DNN classifier with n_classes=6, not 3.</li>
</ul>

<p>I now want to export the model as a <code>.tflite</code> file. But according to the <a href=""https://www.tensorflow.org/mobile/tflite/devguide"" rel=""nofollow noreferrer"">TensorFlow Developer Guide</a>, I need to first export the model to a <code>tf.GraphDef</code> file, then freeze it and only then will I be able to convert it. However, the <a href=""https://github.com/tensorflow/models/blob/master/research/slim/README.md"" rel=""nofollow noreferrer"">tutorial</a>  provided by TensorFlow to create a <code>.pb</code> file from a custom model only seems to be optimized for image classification models. </p>

<p><strong>Question:</strong></p>

<p>So how do I convert a model like the iris classification example model into a <code>.tflite</code> file? Is there an easier, more direct way to do it, without having to export it to a <code>.pb</code> file, then freeze it and so on? An example based on the iris classification code or a link to a more explicit tutorial would be very useful!</p>

<hr>

<p><strong>Other information:</strong></p>

<ul>
<li>OS: macOS 10.13.4 High Sierra</li>
<li>TensorFlow Version: 1.8.0</li>
<li>Python Version: 3.6.4</li>
<li>Using PyCharm Community 2018.1.3</li>
</ul>

<p><strong>Code:</strong></p>

<p>The iris classification code can be cloned by entering the following command:</p>

<p><code>git clone https://github.com/tensorflow/models</code></p>

<p>But in case you don't want to download the whole package, here it is:</p>

<p>This is the classifier file called <strong><code>premade_estimator.py</code></strong>:</p>

<pre><code>    #  Copyright 2016 The TensorFlow Authors. All Rights Reserved.
    #
    #  Licensed under the Apache License, Version 2.0 (the ""License"");
    #  you may not use this file except in compliance with the License.
    #  You may obtain a copy of the License at
    #
    #  http://www.apache.org/licenses/LICENSE-2.0
    #
    #  Unless required by applicable law or agreed to in writing,                         software
    #  distributed under the License is distributed on an ""AS IS"" BASIS,
    #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    #  See the License for the specific language governing permissions and
    #  limitations under the License.
    """"""An Example of a DNNClassifier for the Iris dataset.""""""
    from __future__ import absolute_import
    from __future__ import division
    from __future__ import print_function

    import argparse
    import tensorflow as tf

    import iris_data

    parser = argparse.ArgumentParser()
    parser.add_argument('--batch_size', default=100, type=int, help='batch size')
    parser.add_argument('--train_steps', default=1000, type=int,
                help='number of training steps')


    def main(argv):
        args = parser.parse_args(argv[1:])

        # Fetch the data
        (train_x, train_y), (test_x, test_y) = iris_data.load_data()

        # Feature columns describe how to use the input.
        my_feature_columns = []
        for key in train_x.keys():
                    my_feature_columns.append(tf.feature_column.numeric_column(key=key))

        # Build 2 hidden layer DNN with 10, 10 units respectively.
        classifier = tf.estimator.DNNClassifier(
            feature_columns=my_feature_columns,
            # Two hidden layers of 10 nodes each.
            hidden_units=[10, 10],
            # The model must choose between 3 classes.
            n_classes=3)

        # Train the Model.
        classifier.train(
            input_fn=lambda: iris_data.train_input_fn(train_x, train_y,
                                              args.batch_size),
            steps=args.train_steps)

        # Evaluate the model.
        eval_result = classifier.evaluate(
            input_fn=lambda: iris_data.eval_input_fn(test_x, test_y,
                                             args.batch_size))

        print('\nTest set accuracy:         {accuracy:0.3f}\n'.format(**eval_result))

        # Generate predictions from the model
        expected = ['Setosa', 'Versicolor', 'Virginica']
        predict_x = {
            'SepalLength': [5.1, 5.9, 6.9],
            'SepalWidth': [3.3, 3.0, 3.1],
            'PetalLength': [1.7, 4.2, 5.4],
            'PetalWidth': [0.5, 1.5, 2.1],
        }

        predictions = classifier.predict(
            input_fn=lambda: iris_data.eval_input_fn(predict_x,
                                                     labels=None,
                                                     batch_size=args.batch_size))

        template = '\nPrediction is ""{}"" ({:.1f}%), expected ""{}""'

        for pred_dict, expec in zip(predictions, expected):
            class_id = pred_dict['class_ids'][0]
            probability = pred_dict['probabilities'][class_id]

            print(template.format(iris_data.SPECIES[class_id],
                          100 * probability, expec))


    if __name__ == '__main__':
        # tf.logging.set_verbosity(tf.logging.INFO)
        tf.app.run(main)
</code></pre>

<p>And this is the data file called <strong><code>iris_data.py</code></strong>:</p>

<pre><code>    import pandas as pd
    import tensorflow as tf

    TRAIN_URL = ""http://download.tensorflow.org/data/iris_training.csv""
    TEST_URL = ""http://download.tensorflow.org/data/iris_test.csv""

    CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',
                        'PetalLength', 'PetalWidth', 'Species']
    SPECIES = ['Setosa', 'Versicolor', 'Virginica']


    def maybe_download():
        train_path = tf.keras.utils.get_file(TRAIN_URL.split('/')[-1], TRAIN_URL)
        test_path = tf.keras.utils.get_file(TEST_URL.split('/')[-1], TEST_URL)

        return train_path, test_path


    def load_data(y_name='Species'):
        """"""Returns the iris dataset as (train_x, train_y), (test_x, test_y).""""""
        train_path, test_path = maybe_download()

        train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)
        train_x, train_y = train, train.pop(y_name)

        test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)
        test_x, test_y = test, test.pop(y_name)

        return (train_x, train_y), (test_x, test_y)


    def train_input_fn(features, labels, batch_size):
        """"""An input function for training""""""
        # Convert the inputs to a Dataset.
        dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))

        # Shuffle, repeat, and batch the examples.
        dataset = dataset.shuffle(1000).repeat().batch(batch_size)

        # Return the dataset.
        return dataset


    def eval_input_fn(features, labels, batch_size):
        """"""An input function for evaluation or prediction""""""
        features = dict(features)
        if labels is None:
            # No labels, use only features.
            inputs = features
        else:
            inputs = (features, labels)

        # Convert the inputs to a Dataset.
        dataset = tf.data.Dataset.from_tensor_slices(inputs)

        # Batch the examples
        assert batch_size is not None, ""batch_size must not be None""
        dataset = dataset.batch(batch_size)

        # Return the dataset.
        return dataset
</code></pre>

<p>** <strong>UPDATE</strong> **</p>

<p>Ok so I have found a seemingly very useful piece of code <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md"" rel=""nofollow noreferrer"">on this page</a>:</p>

<pre><code>    import tensorflow as tf

    img = tf.placeholder(name=""img"", dtype=tf.float32, shape=(1, 64, 64, 3))
    val = img + tf.constant([1., 2., 3.]) + tf.constant([1., 4., 4.])
    out = tf.identity(val, name=""out"")
    with tf.Session() as sess:
      tflite_model = tf.contrib.lite.toco_convert(sess.graph_def, [img], [out])
      open(""test.tflite"", ""wb"").write(tflite_model)
</code></pre>

<p>This little guy directly converts a simple model to a TensorFlow Lite Model. Now all I have to do is find a way to adapt this to the iris classification model. Any suggestions?</p>
","9863450","9863450","2018-06-05 11:12:00","How do I export a TensorFlow model as a .tflite file?","<python><tensorflow><machine-learning><pycharm><tensorflow-lite>","2","11","8754"
"50635114","2018-06-01 02:01:54","0","","<p>Here's the solution I came up with using <a href=""https://docs.python.org/3/library/itertools.html#itertools.dropwhile"" rel=""nofollow noreferrer""><code>dropwhile</code></a> and <a href=""https://docs.python.org/3/library/itertools.html#itertools.groupby"" rel=""nofollow noreferrer""><code>groupby</code></a></p>

<pre><code>from itertools import groupby, dropwhile

def spaces(iterable):
    it = dropwhile(lambda x: not x, iterable)
    grby = groupby(it, key=bool)
    try:
        k, g = next(grby)
    except StopIteration:
        return
    yield from g
    for k, g in grby:
        if k:
            yield ''
            yield from g

x = ["""", """", ""test"", """", """", ""not empty"", """", ""yes"", """"]
print(list(spaces(x)))
# ['test', '', 'not empty', '', 'yes']
</code></pre>
","6779307","","","0","776","Patrick Haugh","2016-08-31 14:38:46","36209","4654","2715","1256","50626817","50632478","2018-05-31 14:34:57","-7","78","<p>Let's say I have a Python list of strings like so:</p>

<pre><code>x = ["""", """", ""test"", """", """", ""not empty"", """", ""yes"", """"]
</code></pre>

<p>How can I remove:</p>

<ol>
<li>all leading empty strings</li>
<li>all trailing empty strings</li>
<li>all 'repeated' empty strings<br>
(i.e. reduce all internal sequences of empty space values to a single value)</li>
</ol>

<p><code>['test', '', 'not empty', '', 'yes']</code></p>
","7258898","9067615","2018-05-31 15:25:41","Remove extra empty strings?","<python><string><list>","2","4","427"
"50635122","2018-06-01 02:02:34","1","","<p>It is possible by using <code>cumcount</code></p>

<pre><code>df[df.groupby('Col1').cumcount().isin([1,2])]
Out[423]: 
  Col1  Col2  Col3
1    a     2     7
2    a     1     3
5    b     2     6
</code></pre>

<p>More information : </p>

<pre><code>df.groupby('Col1').cumcount()
Out[435]: 
0    0
1    1
2    2
3    3
4    0
5    1
dtype: int64
</code></pre>
","7964527","7964527","2018-06-01 02:09:38","4","362","WeNYoBen","2017-05-04 16:45:29","164847","15327","4764","689","50635053","50635122","2018-06-01 01:53:18","0","52","<p>I have a dataframe like this:</p>

<pre><code>Col1 | Col2 | Col3
 a   |   8  |  9
 a   |   3  |  7
 a   |   1  |  3
 a   |   0  |  8
 b   |   6  |  18
 b   |   2  |  6
</code></pre>

<p>I would like to drop everything but the 2nd and 3rd top value for Col2, by grouping Col1, assuming it's possible</p>

<p>Output desired:</p>

<pre><code>Col1 | Col2 | Col3
 a   |   3  |  7
 a   |   1  |  3
 b   |   2  |  6
</code></pre>
","5355343","5355343","2018-06-01 02:07:40","Groupby Pandas dataframe and drop values conditionally based on rank","<python><pandas>","1","0","426"
"50635139","2018-06-01 02:04:27","0","","<p>Well, i got the job done.</p>

<p>I had to use a form submit + redirect, cause i couldn't save the cookies in the right domain with ajax. I used <a href=""https://github.com/mgalante/jquery.redirect"" rel=""nofollow noreferrer"">jQuery.redirect</a> plugin so i don't have to create a form in a external website.</p>

<p>I created a custom authentication function and if the user has valid credentials, i redirect him to my home page (that part got the cookies working right).</p>

<p>I also could remove CORS settings, cause form submits aren't blocked in a cross domain request.</p>
","8658255","","","0","583","Felipe Menezes","2017-09-22 20:58:48","81","8","41","0","50519177","50635139","2018-05-24 22:42:05","0","33","<h3>Problem</h3>

<p>I need that people from one specific external website can authenticate in my website by sending a Ajax request. After that, they have to be redirected to my website home.</p>

<h3>What i'm trying</h3>

<p>I've created a custom authentication function and configured the CORS headers, so my request is fully working and i can authenticate from the external website.</p>

<p>The problem is: even if i can authenticate and get a OK response, when i redirect the user to my website, he doesn't have a active session.</p>

<h3>Why?</h3>

<p>I'm guessing that Django sets the session information in the browser cookies, and i'm not doing it.</p>

<h3>My question:</h3>

<p>What do i have to do, so i can keep a user authenticated after he authenticates from an external website?</p>

<p>I would appreciate any ideas.</p>
","8658255","8658255","2018-05-24 23:17:26","Authenticate users from external site by ajax","<python><django>","1","0","836"
"50635151","2018-06-01 02:06:13","0","","<p>If you are looking to remove any row that has <code>'N/A'</code> or <code>''</code> in the row then you can us a boolean index, just take the inverse of <code>isin()</code> e.g.:</p>

<pre><code>In []:
df[~df.isin(['N/A', '']).any(axis=1)]

Out[]:
   F  T  l
0  0  0  0
</code></pre>

<p>If you need to limit to just columns <code>'A', 'l'</code> then select them, e.g.:</p>

<pre><code>df[~df[['A', 'l']].isin(['N/A', '']).any(axis=1)]
</code></pre>

<p>You could also use a <code>dict</code> with <code>isin()</code> but that would only be useful if you had different values for the columns, e.g.:</p>

<pre><code>df[~df.isin({'A': ['N/A', ''], 'l': ['']}).any(axis=1)]
</code></pre>
","2750492","2750492","2018-06-01 02:14:29","0","689","AChampion","2013-09-05 11:19:58","22583","2373","75","181","50634988","50643908","2018-06-01 01:42:28","0","227","<p>I have dataframe in wich I have to drop row if some of values.</p>

<p>for instance,</p>

<pre><code>x not in ['N/A', ''] where x is columns
</code></pre>

<p>is there a way like, apply?</p>

<pre><code> df[x] = df[x].apply(lambda x: x.lower())
</code></pre>

<p>I am think in something like:</p>

<pre><code>df.drop.apply(lambda x: X not in ['N/A', ''])???
</code></pre>

<p>My DF</p>

<pre><code>     F   T   l
0    0   ""0""   ""0""
1    1   """"   ""1""
2    2   ""2""   """"
</code></pre>

<p>drop row if T == """" or l == """"</p>

<pre><code>     F   T   l
0    0   ""0""   ""0""
</code></pre>

<p>I could not use </p>

<pre><code>df.drop(df.T == """") since the condition ("""") depend on runtime data
</code></pre>
","1066110","1066110","2018-06-01 02:01:53","how to drop row base on lambda","<python><dataframe>","2","1","703"
"50635160","2018-06-01 02:06:58","1","","<pre><code>params = []
with open('Test_Tickers.txt') as f:
   for line in f:
       info = {}
       info['q'] = line.rstrip()
       info['x'] = ""NYSE""
       params.append(info)
       print(info)
</code></pre>

<p>I will work</p>

<p>you were updating only one dict object in list ,
to make multiple object you have to define <code>info = {}</code> inside the loop</p>
","7690443","7690443","2018-06-01 02:13:42","4","372","Ravi Bhushan","2017-03-10 13:12:17","380","66","9","3","50635116","50635160","2018-06-01 02:02:12","-1","56","<p>I have a weird problem, as I am trying to generate a list of dictionaries to pass them as a parameter to a function. Designing the input ""by hand"" looks like this:</p>

<pre><code>params = [
    # Amazon
    {
            'q': ""AMZN"",
            'x': ""NASDAQ"",
    },
            {
            'q': ""PIH"",
            'x': ""NASDAQ"",
    },
    {
            'q': ""AIR"",
            'x': ""NYSE"",
    },
    {
            'q': ""FCO"",
            'x': ""NYSEAMERICAN"",
    },
    {
            'q': ""7201"",
            'x': ""TYO"",
    }
].
</code></pre>

<p>I have tried to generate a similar list of dictionaries from a txt file containing a list of tickers (one per line) with the following code:</p>

<pre><code>info = {}
params = []
with open('Test_Tickers.txt') as f:
   for line in f:
       info['q'] = line.rstrip()
       info['x'] = ""NYSE""
       params.append(info)
       print(info)
</code></pre>

<p>The frustrating part is that while the print(info) returns the correct dictionaries </p>

<pre><code>{'q': 'ABB', 'x': 'NYSE'}
{'q': 'ABBV', 'x': 'NYSE'}
{'q': 'ABC', 'x': 'NYSE'}
{'q': 'ABEV', 'x': 'NYSE'}
...
{'q': 'IJS', 'x': 'NYSE'}
</code></pre>

<p>the params looks like this:</p>

<pre><code>[{'q': 'IJS', 'x': 'NYSE'}, {'q': 'IJS', 'x': 'NYSE'}, {'q': 'IJS', 'x':         'NYSE'}, {'q': 'IJS', 'x': 'NYSE'}, {'q': 'IJS', 'x': 'NYSE'}, {'q': 'IJS', 'x': 'NYSE'}, {'q': 'IJS', 'x': 'NYSE'}, {'q': 'IJS', 'x': 'NYSE'}, ... ]
</code></pre>

<p>How can I corrent the code so that the dictionaries contain all the tickers and not only the last one?</p>
","3612816","","","Value of a key in a Python dictinary is not updating","<python><list><dictionary>","1","4","1569"
"50635165","2018-06-01 02:07:39","1","","<p>You need call the mode function with <code>pd.Series.mode</code></p>

<pre><code>df.groupby(""first_name"")[""sex""].transform(pd.Series.mode)
Out[432]: 
0    m
1    m
2    f
3    m
4    f
Name: sex, dtype: object
</code></pre>
","7964527","","","10","227","WeNYoBen","2017-05-04 16:45:29","164847","15327","4764","689","50635073","50635165","2018-06-01 01:56:16","0","321","<p>I'm looking to fill in missing values of one column with the mode of the value from another column. Let's say this is our data set (borrowed from Chris Albon):</p>

<pre><code>import pandas as pd
import numpy as np

raw_data = {'first_name': ['Jake', 'Jake', 'Tina', 'Jake', 'Amy'], 
        'last_name': ['Miller', 'Smith', 'Ali', 'Milner', 'Cooze'], 
        'age': [42, np.nan, 36, 24, 73], 
        'sex': ['m', np.nan, 'f', 'm', 'f'], 
        'preTestScore': [4, np.nan, np.nan, 2, 3],
        'postTestScore': [25, np.nan, np.nan, 62, 70]}
df = pd.DataFrame(raw_data, columns = ['first_name', 'last_name', 'age', 'sex', 'preTestScore', 'postTestScore'])
df
</code></pre>

<p>I know we can fill in missing postTestScore with each sex's mean value of postTestScore with:</p>

<p><code>df[""postTestScore""].fillna(df.groupby(""sex"")[""postTestScore""].transform(""mean""), inplace=True)
df</code></p>

<p>But how would we fill in missing sex with each first name's mode value of sex (obviously this is not politically correct, but as an example this was an easy data set to use). So for this example the missing sex value would be 'm' because there are two Jake's with the value 'm'. If there were a Jake with value 'f' it would still pick 'm' as the mode value because 2 > 1. It would be nice if you could do: </p>

<p><code>df[""sex""].fillna(df.groupby(""first_name"")[""sex""].transform(""mode""), inplace=True)
df</code></p>

<p>I looked into value_counts and apply but couldn't find this specific case. My ultimate goal is to be able to look at one column and if that doesn't have a mode value then to look at another column for a mode value. </p>
","3737798","","","Python pandas fill in missing value of one variable with the mode of another variable","<python><pandas><dataframe><missing-data>","1","0","1647"
"50635198","2018-06-01 02:14:37","0","","<p>I suggest you either update Python Version > 3.0, so that the output for conda list shows something like this:</p>

<pre><code>(pip install pydicom)

pydicom                   1.0.2                     &lt;pip&gt;
python                    3.6.4                h6538335_1
</code></pre>

<p>Now import using:</p>

<pre><code>import pydicom #Preferable 
</code></pre>

<p>==========================================================</p>

<p>Or install dicom instead of pydicom using:</p>

<pre><code>(pip install pydicom-0.9.8)

pydicom                   0.9.8                    &lt;pip&gt;
python                    2.7.0                h6538335_1
</code></pre>

<p>And then, import using:</p>

<pre><code>import dicom
</code></pre>

<p>However, I strongly suggest that you install pydicom instead of dicom, since it is the upgraded version.</p>
","3705556","","","0","847","Vartika","2014-06-04 04:53:12","106","16","2","0","40075637","","2016-10-16 21:22:41","3","4428","<p>I have the following problem. 
I have tried to install the pydicom package in python 2.7 using the following command (windows, anaconda setup): </p>

<pre><code>conda install -c conda-forge pydicom
</code></pre>

<p>everything seems to work fine, the package seems to be installed.
I type </p>

<pre><code>conda list
</code></pre>

<p>and in the list I see</p>

<pre><code>pydicom                   0.9.8                     &lt;pip&gt;
</code></pre>

<p>I open spyder, or pycharm, type </p>

<pre><code>import pydicom
</code></pre>

<p>and I get </p>

<blockquote>
  <p>ImportError: No module named pydicom</p>
</blockquote>

<p>I have no idea what am I doing wrong. I went through <a href=""http://conda.pydata.org/docs/using/pkgs.html"" rel=""nofollow"">http://conda.pydata.org/docs/using/pkgs.html</a> and everything seems to be fine.</p>

<p>Please assist.</p>
","3206863","670206","2017-04-13 19:32:25","Anaconda and package installation (pydicom)","<python><packages><conda><pydicom>","2","4","865"
"50635201","2018-06-01 02:14:54","1","","<p>Your CSS file works well because the button inherits the bootstrap design,
the problem is all the inputs that you have don't have the class <code>form-control</code>. So to add these classes, you can do it with the widget</p>

<pre><code>class UserLoginForm(forms.Form):
    username = forms.CharField(max_length=120,
         widget=forms.TextInput(attrs={'class':'form-control',""placeholder"":""Username""}))
    password = forms.CharField(max_length=100,
          widget=forms.PasswordInput(attrs={'class':'form-control',""placeholder"":""Password""})) 
</code></pre>
","5644965","","","2","568","Lemayzeur","2015-12-05 21:02:14","5631","594","369","139","50635143","","2018-06-01 02:05:26","0","66","<p>My login page isn't looking like the way I want it to which is like this bootstrap signin page right here <a href=""https://getbootstrap.com/docs/4.0/examples/sign-in/"" rel=""nofollow noreferrer"">https://getbootstrap.com/docs/4.0/examples/sign-in/</a>. This is <a href=""https://i.stack.imgur.com/ZPLgT.jpg"" rel=""nofollow noreferrer"">what it looks like right now</a> </p>

<p>If you can help me out with this, it would mean <strong>the world to me</strong>.</p>

<p>Also if you think it's because I didn't link to a CSS page here's what my base template looks like and my structure:</p>

<pre><code>{% load staticfiles %}

&lt;!DOCTYPE html&gt;
&lt;html&gt;

&lt;head&gt;
    &lt;meta charset=""utf-8""&gt;
    &lt;meta http-equiv=""X-UA-Compatible"" content=""IE=edge""&gt;
    &lt;meta name=""viewport"" content=""width=device-width, initial-scale=1""&gt;

    &lt;title&gt;Accounts&lt;/title&gt;

    &lt;script src=""https://code.jquery.com/jquery-3.3.1.min.js""&gt;&lt;/script&gt;
    &lt;link rel=""stylesheet"" href=""https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css""&gt;
    &lt;script src=""https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js""&gt;&lt;/script&gt;
    &lt;link rel=""stylesheet"" type=""text/css"" href=""{% static 'accounts/signin.css' %}"" /&gt;
&lt;/head&gt;

&lt;body&gt;


    &lt;div class=""jumbotron""&gt;
        &lt;div class=""container""&gt;

            {% block main_content %} 
            {% endblock %} 
        &lt;/div&gt;
    &lt;/div&gt;

&lt;/body&gt;

&lt;/html&gt;
</code></pre>

<p>views.py</p>

<pre><code>from django.contrib.auth import (
      authenticate,
      get_user_model,
      login,
      logout,
 )
from django.shortcuts import render
from django.http import HttpResponse,HttpResponseRedirect
from .forms import UserLoginForm

# Create your views here.
def login_view(request):
    title = ""Login""
    form = UserLoginForm(request.POST or None)
    if form.is_valid():
        username = form.cleaned_data.get(""username"")
        password = form.cleaned_data.get('password')
        user = authenticate(request, username=username, password=password)      
        if user is not None:
          login(request, user)
          # or any other success page
        #   return HttpResponse(""Logged in"")
        return HttpResponseRedirect('accounts/home')
    return render(request, ""accounts/form.html"", {""form"":form, ""title"": title})
</code></pre>

<p>forms.py</p>

<pre><code>from django import forms
from django.contrib.auth import (
    authenticate,
    get_user_model,
    login,
    logout,

    )

User = get_user_model()

class UserLoginForm(forms.Form):
    username = forms.CharField()
    password = forms.CharField(widget=forms.PasswordInput) 

    class Meta: 
        model = User 
        fields = ('username', 'email', 'password')

    def clean(self,*args,**kwargs):
        username = self.cleaned_data.get(""username"")
        password = self.cleaned_data.get(""password"")
        user = authenticate(username=username,password=password)
        if not user:
            raise forms.ValidationError(""This user does not exist"")
        if not user.check_password(password):
            raise forms.ValidationError(""Incorrect Password"")
        if not user.is_active:
            raise forms.ValidationError(""This user is no longer active."")
        return super(UserLoginForm, self).clean(*args,**kwargs)
</code></pre>

<p>form.html (currently using this for login, open to use the same format for registration)</p>

<pre><code>{% extends 'accounts/base.html' %}
{% block main_content %}
    &lt;div class=""container""&gt;
        &lt;div class='col-sm-6 col-sm-offset-3'&gt;
        &lt;h1&gt;{{ title }}&lt;/h1&gt;
        &lt;form method='POST' action='' enctype='multipart/form-data'&gt;{% csrf_token %}
            {{ form }}
            &lt;input type='submit' class='btn btn-primary btn-block' value='{{ title }}' /&gt;
        &lt;/form&gt;
    &lt;/div&gt;
    &lt;/div&gt;
{% endblock %}
</code></pre>

<p><a href=""https://i.stack.imgur.com/y1kNv.jpg"" rel=""nofollow noreferrer"">What My Framework Looks Like</a></p>

<p>Thanks a lot</p>

<p>@Lemayzeur this is <a href=""https://i.stack.imgur.com/7gF2Z.jpg"" rel=""nofollow noreferrer"">what my site looks like now</a></p>

<p>And this is <a href=""https://i.stack.imgur.com/5yczD.jpg"" rel=""nofollow noreferrer"">what I want it to look like</a></p>

<p>-it comes pretty close but unfortunately it's not the same. Nonetheless, I'll keep it and thank you.</p>
","9820620","9820620","2018-06-01 02:42:46","BOOTSTRAP Sign in template not working on FORM","<python><css><django><twitter-bootstrap><forms>","1","2","4497"
"50635279","2018-06-01 02:27:05","3","","<p>Just using <code>repeat</code> with <code>reindex</code></p>

<pre><code>df.reindex(df.index.repeat(df.Col2)).drop('Col2',1)
Out[443]: 
  Col1  Col3
0    a     6
0    a     6
1    a     3
2    b     2
2    b     2
2    b     2
2    b     2
3    b     3
3    b     3
</code></pre>
","7964527","","","0","283","WeNYoBen","2017-05-04 16:45:29","164847","15327","4764","689","50635245","50635279","2018-06-01 02:21:52","-2","33","<p>I have a dataframe like this:</p>

<pre><code>Col1 | Col2 | Col3
 a   |   2  |  6
 a   |   1  |  3
 b   |   4  |  2
 b   |   2  |  3
</code></pre>

<p>Wish to expand it based on Col2 as a frequency. So Col2 means the combination of a-6 is repeated twice, like so:</p>

<pre><code>Col1 | Col3 |
 a   |   6  |
 a   |   6  |
 a   |   3  |
 b   |   2  |
 b   |   2  |
 b   |   2  |
 b   |   2  |
 b   |   3  |
 b   |   3  |
</code></pre>
","5355343","","","Expand dataframe based on values","<python><pandas>","1","1","437"
"50635306","2018-06-01 02:31:49","1","","<p>First, you <em>should</em> be using a dictionary or list to hold many similar structured dataframes and not flood your global environment with separate dataframes. Always use a container to organize yourself and set up to run bulk operations like <code>pd.concat</code> to build a master set. But be sure to assign dataframes to dictionary directly and not create separate objects.</p>

<p>As for the reason your dictionary dataframes do not update is you are not correctly assigning. Every instance of <code>df</code> needs to be replaced with <code>df[key]</code>. So, </p>

<pre><code>df[~(df['Productsize'] &lt;= 6)]
</code></pre>

<p>Would be replaced as</p>

<pre><code>df_dict[key][~(df_dict[key]['Productsize'] &lt;= 6)]
</code></pre>

<p>You lose no functionality of the dataframe when it is stored in a container, just referencing it changes. Therefore adjust accordingly:</p>

<pre><code>for k, v in df_dict.items():
    df_dict[k]['Productsize'] = df_dict[k]['product'].str.len()  
    df_dict[k] = df_dict[k][~(df_dict[k]['Productsize'] &lt;= 6)]
</code></pre>

<p>Alternatively, use the value item of dictionary loop, but reassign the temporary changes to current index as <a href=""https://stackoverflow.com/a/50634198/1422451"">@phi explains</a>.</p>

<pre><code>for k, v in df_dict.items():
    v['Productsize'] = v['product'].str.len()  
    v = v[~(v['Productsize'] &lt;= 6)]

    df_dict[k] = v
</code></pre>
","1422451","1422451","2018-06-01 10:57:07","3","1430","Parfait","2012-05-28 21:02:34","64441","5293","3714","87","50634023","50635306","2018-05-31 23:09:18","3","47","<p>I have a several <code>pandas</code> Data Frames stored in a dictionary:</p>

<pre><code>df1=pd.DataFrame({'product':['ajoijoft','bbhjbh','cser','sesrd','yfgjke','tfyfyf','drdrtjg'],'price':[1,2,3,4,5,6,7],'label':['h','i','j','k','L','n','m']})
df2=pd.DataFrame({'product':['ajyughjoijoft','bdrddbhjbh','rdtrdcser','sdtrdthddesrd','yawafgjke','tesrgsfyfyf','sresedrdrtjg'],'price':[1,2,3,4,5,6,7],'label':['h','i','j','k','L','n','m']})
df3=pd.DataFrame({'product':['joijoft','bdbhjbh','rdcser','sdhddesrd','wajke','yf','sresedrdrtjg'],'price':[1,2,3,4,5,6,7],'label':['h','i','j','k','L','n','m']})

df_dict = {""A"":df1,'B':df2, ""C"":df3}
</code></pre>

<p>I want to know the length of the each string in <code>product</code>, so I write as below.</p>

<pre><code>for i, ii in df_dict.items():
    ii['Productsize'] = ii['product'].str.len()
</code></pre>

<p>This worked and I could get the length for all ""product"".</p>

<p>Next, I want to remove rows that have a short <code>product</code> string length, that is: <code>Productsize &lt; 6</code></p>

<p>I tried to use this code:</p>

<pre><code>for i, ii in df_dict.items():
    ii=ii[~(ii['Productsize'] &lt;= 6)]
</code></pre>

<p>However, this did not work.
If I write individually (i.e. not in a loop) as below, it will work though.</p>

<pre><code>df1=df1[~(df1['Productsize'] &lt;= 6)]
</code></pre>

<p>Does anyone know what the problem might be?</p>

<p>I tried you guys suggested. Unfortunately, this does not work. Do you know why...? Here is the code.</p>

<pre><code>df1=pd.DataFrame({'product':['ajoijoft','bbhjbh','cser','sesrd','yfgjke','tfyfyf','drdrtjg'],'price':[1,2,3,4,5,6,7],'label':['h','i','j','k','L','n','m']})
df2=pd.DataFrame({'product':['ajyughjoijoft','bdrddbhjbh','rdtrdcser','sdtrdthddesrd','yawafgjke','tesrgsfyfyf','sresedrdrtjg'],'price':[1,2,3,4,5,6,7],'label':['h','i','j','k','L','n','m']})
df3=pd.DataFrame({'product':['joijoft','bdbhjbh','rdcser','sdhddesrd','wajke','yf','sresedrdrtjg'],'price':[1,2,3,4,5,6,7],'label':['h','i','j','k','L','n','m']})

df_dict = {""A"":df1,'B':df2, ""C"":df3}

for i, ii in df_dict.items():
    ii['Productsize'] = ii['product'].str.len()    

for i, ii in df_dict.items():
    df_dict[i] = ii[~(ii['Productsize'] &lt;= 6)]
</code></pre>
","7479634","8366499","2018-09-13 15:24:53","How to remove specific values sequentially in pandas dataframes?","<python><pandas><for-loop>","3","0","2264"
"50635322","2018-06-01 02:34:08","2","","<pre><code>aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId]' --output text | wc -l
</code></pre>

<p>If you wish to do it for a non-default region:</p>

<pre><code>aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId]' --region ap-southeast-2 --output text | wc -l
</code></pre>
","174777","","","0","332","John Rotenstein","2009-09-17 06:00:41","101063","8719","1880","29","50630942","50635322","2018-05-31 18:50:32","0","1634","<p>In shell script or Python, how can I find out how many EC2 instances are running and the count of the instances in each AWS region?</p>
","5630862","174777","2018-06-01 02:35:10","How can I find the number of EC2 instances?","<python><shell><amazon-web-services><amazon-ec2>","3","0","139"
"50635361","2018-06-01 02:38:53","0","","<p>You can use <code>re</code>:</p>

<pre><code>import re
lines = list(filter(None, content.split('\n')))
grouped = [[lines[i], lines[i+1]] for i in range(0, len(lines), 2)]
new_grouped = sorted(grouped, key=lambda x:int(re.findall('\d+', x[-1])[0]), reverse=True)[:3]
for a, b in new_grouped:
  print(f'{a}\n{b}')
</code></pre>

<p>Output:</p>

<pre><code>@SP_23RTY id=0
0387 pop
@TRIO_9078 id=0
0098 hench
@WRYE_LKP.CO id=0
0078 ffg
</code></pre>
","7326738","","","0","449","Ajax1234","2016-12-21 16:39:57","49079","3709","2930","360","50635294","50635361","2018-06-01 02:30:13","-2","27","<p>I have a python program where I print strings like</p>

<pre><code>@SP_YSNSB id=0
0054 log out
@SP_23RTY id=0
0387 pop
@TRIO_9078 id=0
0098 hench
@TRE_4657838.c id=0
0056 pop
@WRYE_LKP.CO id=0
0078 ffg
</code></pre>

<p>The ""@SP_23RTY"" is the header and ""0387 pop"" is the number of lines executed before it looped out.</p>

<p>How can I print the top 3 headers with the top 3 highest values of lines?</p>

<p>The output should be</p>

<pre><code>@SP_23RTY id=0
0387 pop
@TRIO_9078 id=0
0098 hench
@WRYE_LKP.CO id=0
0078 ffg
</code></pre>

<p>Any help would be appreciated.</p>
","9822184","7128479","2018-06-01 03:14:05","How can I get the top 3 strings with the highest value in this program?","<python>","1","2","580"
"50635389","2018-06-01 02:42:39","0","","<p>Consider directly  vectorize as you can run arithmetic operations across similar structured dataframes:</p>

<pre><code>(df1 + df2.reindex(labels=df1.index.values, fill_value=15)) / 2

#             June 2016  July 2016
# Flavor                          
# Vanilla          13.0       21.0
# Chocolate         8.5        7.5
# Strawberry       13.0       14.5
</code></pre>

<p>And for many dataframes in a list, consider <code>reduce</code>:</p>

<pre><code>from functools import reduce

df_list = [df1, df2]
new_df_list = [d.reindex(labels=df1.index.values, fill_value=15) for d in df_list]

reduce(lambda x,y: x + y, new_df_list) / len(new_df_list)

#             June 2016  July 2016
# Flavor                          
# Vanilla          13.0       21.0
# Chocolate         8.5        7.5
# Strawberry       13.0       14.5
</code></pre>

<p>Data</p>

<pre><code>import pandas as pd
from io import StringIO

txt = '''             
Flavor      ""June 2016""       ""July 2016""
Vanilla      17.0            23.0
Chocolate    7.0             12.0
Strawberry   11.0            14.0'''

df1 = pd.read_table(StringIO(txt), sep=""\s+"", index_col=0)


txt = '''             
Flavor      ""June 2016""       ""July 2016""
Vanilla      9.0            19.0
Chocolate    10.0           3.0'''

df2 = pd.read_table(StringIO(txt), sep=""\s+"", index_col=0)
</code></pre>
","1422451","1422451","2018-06-01 02:53:35","0","1354","Parfait","2012-05-28 21:02:34","64441","5293","3714","87","50634552","","2018-06-01 00:28:54","1","257","<p>I have several data frames that have months as the columns, and contain integer values. I am posting 2 for this example. </p>

<pre><code>df1 =     
             June 2016       July 2016
Flavor
Vanilla      17.0            23.0
Chocolate    7.0             12.0
Strawberry   11.0            14.0

df2 =        
             June 2016       July 2016
Flavor
Vanilla      9.0            19.0
Chocolate    10.0           3.0
</code></pre>

<p>How can I iterate through each dataframe and perform a calculation dependent on the row and column name of the dataframe when they have to match? For example, I want to calculate the average for Vanilla for July, which would be (23 + 19)/2. If a <code>Flavor</code> also does not exist in a data frame, then I would also like to assign a constant value (say 15 in this example) per month in that data frame. Would I append the data frames together then apply <code>.mean()</code>?</p>

<p>Thanks in advance, and sorry for any abruptness, I am currently on the go, traveling.</p>

<p>Thanks!</p>
","9343043","4492932","2018-06-01 00:30:16","Iterate through multiple dataframes and perform calculation based on specific column","<python><pandas><dataframe><average>","2","1","1039"
"50635390","2018-06-01 02:42:50","0","","<p>i think it has to do with the graph. accuracy is never updated as the only op you are calling that gets updated
change this code to </p>

<pre><code>with tf.Session() as sess:
   sess.run(init)
   for epoch in range(21):
    for batch in range(n_batch):
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
        sess.run([train_step,accuracy], feed_dict={x:batch_xs, y:batch_ys})
    acc = sess.run(accuracy, feed_dict = {x:mnist.test.images, y:mnist.test.labels})
    print(""Iter:"" + str(epoch) + "", Testing Accuray:"" + str(acc))
</code></pre>
","1524289","","","1","562","Eliethesaiyan","2012-07-13 18:03:24","1662","431","324","21","50634776","50637731","2018-06-01 01:09:06","0","362","<p>I am a novice of the tensorflow and python. I modified a sample tensorflow code by adding one hidden layer with 50 units, but the accuracy result turned to be wrong and it was not changed no matter how many times the model do training. I cannot find any problem with the code. The dataset is MNIST: </p>

<pre><code>import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets(""MNIST_data"", one_hot = True)

batch_size = 100
n_batch = mnist.train.num_examples // batch_size

x = tf.placeholder(tf.float32, [None, 784])
y = tf.placeholder(tf.float32, [None, 10])


W = tf.Variable(tf.zeros([784, 50]))
b = tf.Variable(tf.zeros([50]))

Wx_plus_b_L1 = tf.matmul(x,W) + b
L1 = tf.nn.relu(Wx_plus_b_L1)

W_2 = tf.Variable(tf.zeros([50, 10]))
b_2 = tf.Variable(tf.zeros([10]))

prediction = tf.nn.softmax(tf.matmul(L1, W_2) + b_2)


loss = tf.reduce_mean(tf.square(y - prediction))
train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)


correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1))

accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

init = tf.global_variables_initializer()


with tf.Session() as sess:
   sess.run(init)
   for epoch in range(21):
    for batch in range(n_batch):
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
        sess.run(train_step, feed_dict={x:batch_xs, y:batch_ys})
    acc = sess.run(accuracy, feed_dict = {x:mnist.test.images, y:mnist.test.labels})
    print(""Iter:"" + str(epoch) + "", Testing Accuray:"" + str(acc))
</code></pre>

<p>The output always be the same accuracy:
    <code>
Iter:0, Testing Accuray:0.1135
2018-05-31 18:05:21.039188: W tensorflow/core/framework/allocator.cc:101] Allocation of 31360000 exceeds 10% of system memory.
Iter:1, Testing Accuray:0.1135
2018-05-31 18:05:22.551525: W tensorflow/core/framework/allocator.cc:101] Allocation of 31360000 exceeds 10% of system memory.
Iter:2, Testing Accuray:0.1135
2018-05-31 18:05:24.070686: W tensorflow/core/framework/allocator.cc:101] Allocation of 31360000 exceeds 10% of system memory.
</code>
What's wrong in this code? Thank you~~</p>
","806088","","","Why the accuracy of the training model is not changed in the tensorflow code?","<python><tensorflow>","3","0","2167"
"50635400","2018-06-01 02:44:09","2","","<p>To directly create a dict with a key which is not a valid keyword, use the <code>{}</code> syntax like:</p>

<h3>Code:</h3>

<pre><code>d = {training_flower: 'a_value'}
</code></pre>

<h3>Test Code:</h3>

<pre><code>training_flower = 'a key'
d = {training_flower: 'a_value'}
print(d)
</code></pre>

<h2>Results:</h2>

<pre><code>{'a key': 'a_value'}
</code></pre>
","7311767","7311767","2018-06-01 02:56:54","0","367","Stephen Rauch","2016-12-18 02:06:51","33601","12784","4195","3857","50635368","50635400","2018-06-01 02:40:00","1","57","<p>I am trying to rephrase the implementation found <a href=""https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/"" rel=""nofollow noreferrer"">here</a>. This is what I have so far:</p>

<pre><code>import csv
import math
import random

training_set_ratio = 0.67

training_set = []
test_set = []


class IrisFlower:
    def __init__(self, petal_length, petal_width, sepal_length, sepal_width, flower_type):
        self.petal_length = petal_length
        self.petal_width = petal_width
        self.sepal_length = sepal_length
        self.sepal_width = sepal_width
        self.flower_type = flower_type

    def __hash__(self) -&gt; int:
        return hash((self.petal_length, self.petal_width, self.sepal_length, self.sepal_width))

    def __eq__(self, other):
        return (self.petal_length, self.petal_width, self.sepal_length, self.sepal_width) \
               == (other.petal_length, other.petal_width, other.sepal_length, other.sepal_width)


def load_data():
    with open('dataset.csv') as csvfile:
        rows = csv.reader(csvfile, delimiter=',')
        for row in rows:
            iris_flower = IrisFlower(float(row[0]), float(row[1]), float(row[2]), float(row[3]), row[4])
            if random.random() &lt; training_set_ratio:
                training_set.append(iris_flower)
            else:
                test_set.append(iris_flower)


def euclidean_distance(flower_one: IrisFlower, flower_two: IrisFlower):
    distance = 0.0
    distance = distance + math.pow(flower_one.petal_length - flower_two.petal_length, 2)
    distance = distance + math.pow(flower_one.petal_width - flower_two.petal_width, 2)
    distance = distance + math.pow(flower_one.sepal_length - flower_two.sepal_length, 2)
    distance = distance + math.pow(flower_one.sepal_width - flower_two.sepal_width, 2)
    return distance


def get_neighbors(test_flower: IrisFlower):
    distances = []
    for training_flower in training_set:
        dist = euclidean_distance(test_flower, training_flower)
        d = dict()
        d[training_flower] = dist
        print(d)
    return


load_data()
get_neighbors(test_set[0])
</code></pre>

<p>Currently, print statements in the following code block:</p>

<pre><code>def get_neighbors(test_flower: IrisFlower):
    distances = []
    for training_flower in training_set:
        dist = euclidean_distance(test_flower, training_flower)
        d = dict()
        d[training_flower] = dist
        print(d)
    return
</code></pre>

<p>will have outputs similar to</p>

<pre><code>{&lt;__main__.IrisFlower object at 0x107774fd0&gt;: 0.25999999999999945}
</code></pre>

<p>which is ok. But I do not want to create the dictionary first, and then append the key value, as in:</p>

<pre><code>d = dict()
d[training_flower] = dist
</code></pre>

<p>So this is what I am trying:</p>

<pre><code>d = dict(training_flower = dist)
</code></pre>

<p>However, it does not seem like the <code>dist</code> method is using the instance, but rather a String, because what I see printed is as follows:</p>

<pre><code>{'training_flower': 23.409999999999997}
{'training_flower': 16.689999999999998}
</code></pre>

<p>How do I create the dictionary by using the object as key in one statement?</p>
","1173112","7311767","2018-06-01 02:45:05","How can I initialize dictionary in Python where key is an object?","<python><python-3.5>","3","0","3267"
"50635427","2018-06-01 02:48:11","2","","<p>The idea behind <a href=""https://en.wikipedia.org/wiki/Multiprocessing"" rel=""nofollow noreferrer"">multiprocessing</a> is that you can take a problem that requires a lot of math to run, and split the workload between multiple computing systems.</p>

<p>This is often done within a single computer, but can also happen over a network of computers. In the case of python a ""multi-process"" is executed within a single computer.</p>

<p>The way this works is modern cpu's have several cores. Each core is like it's own processor, in that it can process a single thread at a time. </p>

<p>The reason cpu's are divided into cores is because it's hard to make a single core faster, but it's easy to add more cores, which in turn gives you more total processing power.</p>

<p>The problem with this is that each core can only execute a single thread at a time. So if your program is entirely single threaded it doesn't matter how many cores you have it will only run at the speed of the single core it's on.</p>

<p>Dividing your python script like you did above separates it into several threads that can run independently on different cores. Each core processes the task you give it and the final answer is combined and printed to the screen.</p>

<p>In your example there really is no beneficent to using multiprocessing because you aren't doing a significant amount of work to slow the program down, but say you had massive arrays that required expensive math to run, dividing that array into parts and distributing those parts to the different processes would make the overall program run faster.</p>
","5989485","5989485","2018-06-01 02:54:50","2","1601","YAHsaves","2016-02-27 07:31:27","687","230","58","1","50635378","50635427","2018-06-01 02:41:34","1","27","<p>I am learning <code>multiprocessing</code> from ""Introducing Python"", there's such a example to demonstrate the <code>multiprocessing</code></p>

<pre><code>import os
import multiprocessing as mp

def do_this(what):
    whoami(what)

def whoami(what):
    print(f""Process {os.getpid()} says: {what}."")

if __name__ == ""__main__"":
    whoami(""I'm the main program."")
    for i in range(4):
        p = mp.Process(target=do_this, args=(f""I'm function {i}"",))
        p.start()

def do_this(what):
    whoami(what)

def whoami(what):
    print(f""Process {os.getpid()} says: {what}."")

if __name__ == ""__main__"":
    whoami(""I'm the main program."")
    for i in range(4):
        do_this(f""I'm function {i}"")
</code></pre>

<p>Run it and come by:</p>

<pre><code>## -- End pasted text --
Process 2197 says: I'm the main program..
Process 2294 says: I'm function 1.
Process 2293 says: I'm function 0.
Process 2295 says: I'm function 2.
Process 2296 says: I'm function 3.
</code></pre>

<p>However, it's easily achieved by a single process:</p>

<pre><code>def do_this(what):
    whoami(what)

def whoami(what):
    print(f""Process {os.getpid()} says: {what}."")

if __name__ == ""__main__"":
    whoami(""I'm the main program."")
    for i in range(4):
        do_this(f""I'm function {i}"")
## -- End pasted text --
Process 2197 says: I'm the main program..
Process 2197 says: I'm function 0.
Process 2197 says: I'm function 1.
Process 2197 says: I'm function 2.
Process 2197 says: I'm function 3.
</code></pre>

<p>I try best to grasp the idea of <code>multiprocessing</code> and what's problem it solved if not introduced in. </p>

<p>In the above case, what's the extra benefits of <code>multiprocessing</code></p>
","7301792","","","The extra benefits of `multiprocessing` in a minimal demonstrating example","<python><multiprocessing>","1","0","1710"
"50635432","2018-06-01 02:49:04","0","","<p>You can use <code>regex</code> like this example:</p>

<pre><code>from bs4 import BeautifulSoup as bs

a = '''
&lt;li&gt;&lt;a **href=""/1/""**&gt;|&amp;lt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a accesskey=""p"" **href=""/1999/""** rel=""prev""&gt;&amp;lt; Prev&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""//c.xkcd.com/random/comic/""&gt;Random&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a accesskey=""n"" href=""#"" rel=""next""&gt;Next &amp;gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""/""&gt;&amp;gt;|&lt;/a&gt;&lt;/li&gt;
'''

soup = bs(a, 'html.parser')
nums = [k['**href'].strip('/') for k in soup.find_all('a', {'**href': re.compile(r'/\d+/')})]

print(nums)
</code></pre>

<p>Output:</p>

<pre><code>['1', '1999']
</code></pre>
","3926995","","","0","714","Chiheb Nexus","2014-08-10 12:58:13","5825","915","2336","8","50631800","","2018-05-31 19:52:25","1","43","<p>Want to parse out 1 &amp; 1999 from tags: </p>

<pre><code>'''
&lt;li&gt;&lt;a **href=""/1/""**&gt;|&amp;lt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a accesskey=""p"" **href=""/1999/""** rel=""prev""&gt;&amp;lt; Prev&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""//c.xkcd.com/random/comic/""&gt;Random&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a accesskey=""n"" href=""#"" rel=""next""&gt;Next &amp;gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=""/""&gt;&amp;gt;|&lt;/a&gt;&lt;/li&gt;
'''  
</code></pre>

<p>What I tried:
BeautifulSoup Code used to parse HTML:</p>

<pre><code>  for li in soup_obj.find_all('li'):
        for tag in li.find_all(re.compile(""\/[0-9]+\/"")):
            print(tag)
            print(tag['href'])
</code></pre>

<p>Steps:</p>

<p>1) Take out the (li)</p>

<p>2) Assert (href) is true</p>
","9782323","9782323","2018-06-01 02:34:03","BeautifulSoup - Parsing the file for numeric values","<python><python-3.x><web-scraping><beautifulsoup>","2","0","789"
"50635445","2018-06-01 02:51:22","1","","<p>I would recommend you to use two different serializers for creation purpose and others. See the below snippet,<br>
<strong>views.py</strong><br></p>

<pre><code>class RecipieView(viewsets.ModelViewSet):
    queryset = Recipe.objects.all()
    serializer_class = RecipeMainSerializer

    def get_serializer_class(self):
        if self.action == 'create':
            return RecipeCreateSerializer
        return RecipeMainSerializer
</code></pre>

<p><br>
<strong>serializer.py</strong><br></p>

<pre><code>class RecipeCreateSerializer(serializers.ModelSerializer):
    ingredients = serializers.ListField(write_only=True)

    class Meta:
        model = Recipe
        fields = '__all__'

    def create(self, validated_data):
        ingredients_data = validated_data.pop('ingredients')
        recipe = Recipe.objects.create(**validated_data)
        for ingredient in ingredients_data:
            ingredient, created = Ingredient.objects.get_or_create(name=ingredient)
            recipe.ingredients.add(ingredient)
        return recipe


class RecipeMainSerializer(serializers.ModelSerializer):
    ingredients = IngredientSerializer(many=True)

    class Meta:
        model = Recipe
        fields = '__all__'
</code></pre>
","8283848","","","1","1238","JPG","2017-07-10 12:56:09","1","2613","916","164","50625057","50635445","2018-05-31 13:04:33","1","693","<p>I am new to Python and Django. I am creating api using Django-Rest-Framework I want to serializer data that can accept json in below format:</p>

<pre><code>{
""ingredients"": [""Sugar"",""Egg""],
""name"": ""Cake"",
""description"": ""Dinner Food"",
""directions"": ""direction1""
}
</code></pre>

<p>However I am able to persist data in db with below format:</p>

<pre><code>{
""ingredients"": [{""name"":""Cake""},{""name"":""Egg""}],
""name"": ""Rice"",
""description"": ""Dinner Food"",
""directions"": ""direction1""
}
</code></pre>

<p>I am not sure how can I convert dictionary in to the set field. I am aware of <a href=""http://www.django-rest-framework.org/api-guide/fields/#listfield"" rel=""nofollow noreferrer"">List field</a> and <a href=""http://www.django-rest-framework.org/api-guide/serializers/#listserializer"" rel=""nofollow noreferrer"">list serialiser</a> but not sure how to use them.
Is it possible to do this using model serialiser? </p>

<p>Serializer.py</p>

<pre><code>class IngredientSerializer(serializers.ModelSerializer):

    class Meta:
        model = Ingredient
        fields = '__all__'


class RecipeSerializer(serializers.ModelSerializer):
    ingredients = IngredientSerializer(many=True)

        class Meta:
            model = Recipe
            fields = '__all__'

    def create(self, validated_data):
        ingredients_data = validated_data.pop('ingredients')
        print(ingredients_data)
        recipe = Recipe.objects.create(**validated_data)
        for ingredient in ingredients_data:
            ingredient, created = Ingredient.objects.get_or_create(name=ingredient['name'])
            recipe.ingredients.add(ingredient)
        return recipe
</code></pre>

<p>Model.py</p>

<pre><code>class Ingredient(models.Model):
    name = models.CharField(max_length=100)

    def __str__(self):
        return self.name


class Recipe(models.Model):
    name = models.CharField(max_length=100)
    description = models.TextField(blank=True, null=True)
    directions = models.TextField()
    ingredients = models.ManyToManyField(Ingredient)

    def __str__(self):
        return self.name
</code></pre>

<p>view.py</p>

<pre><code>class RecipieView(viewsets.ModelViewSet):
    queryset = Recipe.objects.all()
    serializer_class = RecipeSerializer


class IngredientView(viewsets.ModelViewSet):
    queryset = Ingredient.objects.all()
    serializer_class = IngredientSerializer  
</code></pre>
","9875979","8060120","2018-05-31 13:57:19","Nested Serializer for Many to Many","<python><django><django-models><django-rest-framework>","1","0","2405"
"50635468","2018-06-01 02:53:25","1","","<p>to initialize a dictionary with an object as a key, (edit: and the string in Stephen's example is an object anyway)</p>

<pre><code>class Flower:

    def __repr__(self):
        return 'i am flower'


flower1 = Flower()
d = {flower1: 4}
print(d)
</code></pre>

<p>outputs</p>

<pre><code>{i am flower: 4}
</code></pre>

<p>this is my first post here, and I know I'm late, sorry if it's a duplicate solution. just to show it works with an object.</p>

<p>would upvote Stephen's answer but I can't yet.</p>
","9878957","","","0","509","Jay Calamari","2018-06-01 02:37:50","335","32","205","2","50635368","50635400","2018-06-01 02:40:00","1","57","<p>I am trying to rephrase the implementation found <a href=""https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/"" rel=""nofollow noreferrer"">here</a>. This is what I have so far:</p>

<pre><code>import csv
import math
import random

training_set_ratio = 0.67

training_set = []
test_set = []


class IrisFlower:
    def __init__(self, petal_length, petal_width, sepal_length, sepal_width, flower_type):
        self.petal_length = petal_length
        self.petal_width = petal_width
        self.sepal_length = sepal_length
        self.sepal_width = sepal_width
        self.flower_type = flower_type

    def __hash__(self) -&gt; int:
        return hash((self.petal_length, self.petal_width, self.sepal_length, self.sepal_width))

    def __eq__(self, other):
        return (self.petal_length, self.petal_width, self.sepal_length, self.sepal_width) \
               == (other.petal_length, other.petal_width, other.sepal_length, other.sepal_width)


def load_data():
    with open('dataset.csv') as csvfile:
        rows = csv.reader(csvfile, delimiter=',')
        for row in rows:
            iris_flower = IrisFlower(float(row[0]), float(row[1]), float(row[2]), float(row[3]), row[4])
            if random.random() &lt; training_set_ratio:
                training_set.append(iris_flower)
            else:
                test_set.append(iris_flower)


def euclidean_distance(flower_one: IrisFlower, flower_two: IrisFlower):
    distance = 0.0
    distance = distance + math.pow(flower_one.petal_length - flower_two.petal_length, 2)
    distance = distance + math.pow(flower_one.petal_width - flower_two.petal_width, 2)
    distance = distance + math.pow(flower_one.sepal_length - flower_two.sepal_length, 2)
    distance = distance + math.pow(flower_one.sepal_width - flower_two.sepal_width, 2)
    return distance


def get_neighbors(test_flower: IrisFlower):
    distances = []
    for training_flower in training_set:
        dist = euclidean_distance(test_flower, training_flower)
        d = dict()
        d[training_flower] = dist
        print(d)
    return


load_data()
get_neighbors(test_set[0])
</code></pre>

<p>Currently, print statements in the following code block:</p>

<pre><code>def get_neighbors(test_flower: IrisFlower):
    distances = []
    for training_flower in training_set:
        dist = euclidean_distance(test_flower, training_flower)
        d = dict()
        d[training_flower] = dist
        print(d)
    return
</code></pre>

<p>will have outputs similar to</p>

<pre><code>{&lt;__main__.IrisFlower object at 0x107774fd0&gt;: 0.25999999999999945}
</code></pre>

<p>which is ok. But I do not want to create the dictionary first, and then append the key value, as in:</p>

<pre><code>d = dict()
d[training_flower] = dist
</code></pre>

<p>So this is what I am trying:</p>

<pre><code>d = dict(training_flower = dist)
</code></pre>

<p>However, it does not seem like the <code>dist</code> method is using the instance, but rather a String, because what I see printed is as follows:</p>

<pre><code>{'training_flower': 23.409999999999997}
{'training_flower': 16.689999999999998}
</code></pre>

<p>How do I create the dictionary by using the object as key in one statement?</p>
","1173112","7311767","2018-06-01 02:45:05","How can I initialize dictionary in Python where key is an object?","<python><python-3.5>","3","0","3267"
"50635488","2018-06-01 02:56:25","3","","<p>In your snippet, where you write <code>d = dict(training_flower=dist)</code>, ""training_flower"" is a keyword argument for dict function and not an object. It is equivalent to writing <code>d = {'training_flower': dist}</code>. The only way to create a dictionary with an object as a key is to use the latter syntax:</p>

<pre><code>d = {training_flower: dist}
</code></pre>
","9235852","9235852","2018-06-01 20:24:51","0","377","Maxim Kukhtenkov","2018-01-18 15:47:41","600","74","1914","0","50635368","50635400","2018-06-01 02:40:00","1","57","<p>I am trying to rephrase the implementation found <a href=""https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/"" rel=""nofollow noreferrer"">here</a>. This is what I have so far:</p>

<pre><code>import csv
import math
import random

training_set_ratio = 0.67

training_set = []
test_set = []


class IrisFlower:
    def __init__(self, petal_length, petal_width, sepal_length, sepal_width, flower_type):
        self.petal_length = petal_length
        self.petal_width = petal_width
        self.sepal_length = sepal_length
        self.sepal_width = sepal_width
        self.flower_type = flower_type

    def __hash__(self) -&gt; int:
        return hash((self.petal_length, self.petal_width, self.sepal_length, self.sepal_width))

    def __eq__(self, other):
        return (self.petal_length, self.petal_width, self.sepal_length, self.sepal_width) \
               == (other.petal_length, other.petal_width, other.sepal_length, other.sepal_width)


def load_data():
    with open('dataset.csv') as csvfile:
        rows = csv.reader(csvfile, delimiter=',')
        for row in rows:
            iris_flower = IrisFlower(float(row[0]), float(row[1]), float(row[2]), float(row[3]), row[4])
            if random.random() &lt; training_set_ratio:
                training_set.append(iris_flower)
            else:
                test_set.append(iris_flower)


def euclidean_distance(flower_one: IrisFlower, flower_two: IrisFlower):
    distance = 0.0
    distance = distance + math.pow(flower_one.petal_length - flower_two.petal_length, 2)
    distance = distance + math.pow(flower_one.petal_width - flower_two.petal_width, 2)
    distance = distance + math.pow(flower_one.sepal_length - flower_two.sepal_length, 2)
    distance = distance + math.pow(flower_one.sepal_width - flower_two.sepal_width, 2)
    return distance


def get_neighbors(test_flower: IrisFlower):
    distances = []
    for training_flower in training_set:
        dist = euclidean_distance(test_flower, training_flower)
        d = dict()
        d[training_flower] = dist
        print(d)
    return


load_data()
get_neighbors(test_set[0])
</code></pre>

<p>Currently, print statements in the following code block:</p>

<pre><code>def get_neighbors(test_flower: IrisFlower):
    distances = []
    for training_flower in training_set:
        dist = euclidean_distance(test_flower, training_flower)
        d = dict()
        d[training_flower] = dist
        print(d)
    return
</code></pre>

<p>will have outputs similar to</p>

<pre><code>{&lt;__main__.IrisFlower object at 0x107774fd0&gt;: 0.25999999999999945}
</code></pre>

<p>which is ok. But I do not want to create the dictionary first, and then append the key value, as in:</p>

<pre><code>d = dict()
d[training_flower] = dist
</code></pre>

<p>So this is what I am trying:</p>

<pre><code>d = dict(training_flower = dist)
</code></pre>

<p>However, it does not seem like the <code>dist</code> method is using the instance, but rather a String, because what I see printed is as follows:</p>

<pre><code>{'training_flower': 23.409999999999997}
{'training_flower': 16.689999999999998}
</code></pre>

<p>How do I create the dictionary by using the object as key in one statement?</p>
","1173112","7311767","2018-06-01 02:45:05","How can I initialize dictionary in Python where key is an object?","<python><python-3.5>","3","0","3267"
"50635582","2018-06-01 03:11:36","10","","<p>I am not sure why but the command below worked for me. </p>

<pre><code>pip install pyqt5
</code></pre>

<p>Of course I updated Anaconda and Navigator before running this command.</p>
","8235224","","","1","187","Regi Mathew","2017-06-30 04:38:24","672","94","1140","1","46335789","","2017-09-21 05:00:42","14","62182","<p>Anaconda navigator won't launch, I tried reinstalling it, that did not work either. anancondas' command prompt shows an error message. I've tried googling the answer, I guess I'm bad at it. 
<a href=""https://i.stack.imgur.com/KEjpf.png"" rel=""noreferrer"">this is what I see after opening anaconda prompt</a></p>

<p>p.s. I use spyder on it</p>
","7836367","","","Anaconda Navigator won't launch (windows 10)","<python><anaconda><spyder>","12","2","346"
"50635604","2018-06-01 03:14:43","1","","<p>edit: for a panda, pandas has <a href=""http://pandas.pydata.org/pandas-docs/version/0.21/generated/pandas.DataFrame.clip.html"" rel=""nofollow noreferrer"">DataFrame.clip</a>.</p>

<p>see <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.clip.html"" rel=""nofollow noreferrer"">numpy.clip.</a></p>

<pre><code>import numpy as np

thing = np.arange(-5, 6)  # [-5, 4, 3, 2, 1, 0, 1, 2, 3, 4, 5]

clipped_thing = np.clip(thing, -3, 3)

print(clipped_thing)
</code></pre>

<p>outputs</p>

<pre><code>[-3 -3 -3 -2 -1  0  1  2  3  3  3]
</code></pre>
","9878957","8747","2018-06-01 03:46:22","0","560","Jay Calamari","2018-06-01 02:37:50","335","32","205","2","50635540","","2018-06-01 03:04:17","-5","53","<p>I've tried but am not able to figure this one out. I have to change all the data that is greater than 3 to 3, and less than -3 to -3 in the dataframe below. </p>

<pre><code>np.random.seed(42)
randomdata = DataFrame(np.random.randn(400, 4))
</code></pre>

<p>I've tried for loops, .loc, .where and nothing seems to work. </p>
","9879011","8708364","2018-06-01 03:32:13","Set the values greater than x to x, and the values less than -x to -x","<python><pandas><random>","2","1","329"
"50635649","2018-06-01 03:20:48","1","","<p>Right now you've got a view, <code>login_view</code>, that works with your form, <code>UserLoginForm</code>, to accept some POST data and try to log a user in. You can reuse a good bit of this logic to make a view that creates a user. In fact, since Django defines a <code>UserCreationForm</code> for you, you don't even have to make one yourself (assuming you're using the default auth model): <a href=""https://docs.djangoproject.com/en/2.0/topics/auth/default/#django.contrib.auth.forms.UserCreationForm"" rel=""nofollow noreferrer"">https://docs.djangoproject.com/en/2.0/topics/auth/default/#django.contrib.auth.forms.UserCreationForm</a></p>

<p>Since this is for a school project I won't write the code for you, but think this through and you've got a good start:</p>

<ol>
<li>You need a view that feeds some POST data into a form - you know how to do that.</li>
<li>Your view needs to validate that form - you know how to do that, too. Since you're using Django's built-in form you'll also have its built-in validators.</li>
<li>You'll need to save the data from that form as a new user. This part will be new. If you use the built-in <code>UserCreationForm</code>, which is a <code>ModelForm</code>, you can use <code>form.save()</code>. You should read about <code>ModelForms</code> to understand why this is the case: <a href=""https://docs.djangoproject.com/en/2.0/topics/forms/modelforms/#django.forms.ModelForm"" rel=""nofollow noreferrer"">https://docs.djangoproject.com/en/2.0/topics/forms/modelforms/#django.forms.ModelForm</a> On the other hand, if you decide to write your own form, then you will need to access that form data and use it to create a new user manually. You know how to access your form data - you're doing it in the login view - so all you would need to do is use that data to create a user instead of log one in. Do not use <code>User.objects.create</code> to do this - use <code>User.objects.create_user</code>. Read this to understand why: <a href=""https://docs.djangoproject.com/en/2.0/ref/contrib/auth/#django.contrib.auth.models.UserManager.create_user"" rel=""nofollow noreferrer"">https://docs.djangoproject.com/en/2.0/ref/contrib/auth/#django.contrib.auth.models.UserManager.create_user</a></li>
<li>Now you have a new user, but your view still needs to return some kind of response. What should happen next? Render a welcome page, re-direct to a new location? Up to you, but look at your login view - you already know how to both render an arbitrary template with context and how to redirect to another view. You can do anything!</li>
</ol>

<p>I hope this serves as a jumping off point for you. Once you have read about <code>ModelForms</code>, you should have a good idea of how to implement an <code>edit</code> view like you described. Try it, then come back to SO and post specific questions with the problems you encounter along with the code you've tried. One hint, <code>instance</code> is your friend.</p>

<p>Lastly, your sign-in page looks funny because you're rendering the form with the plain <code>{{form}}</code> template tag. You can make it look slightly better (opinions vary) with <code>{{form.as_p}}</code> or <code>{{form.as_table}}</code>. To get it better than that, you'll either want to render each field individually (so that you can control their styling), or look into <code>django-crispy-forms</code>. </p>
","3500094","","","0","3373","souldeux","2014-04-05 00:39:28","2564","216","152","60","50633927","","2018-05-31 22:59:03","1","96","<p>I'm relatively new to Django but have spent countless hours this past week learning how to make webapps. I've bounced around from numerous tutorials promising to make a user registration, but they have all led my pages to crash and turn completely blank and white.</p>

<p>I'm now starting over once again, but this time I made an app COMPELETY devoted to user login/logout/registration. I have finally got my login to work, but that doesn't serve me any good since I don't have any user accounts.</p>

<p>I'm now just asking for a <strong><em>really simple</em></strong> way for me to make a user registration page to which it allows users to make an account in order to view the homepage and be able to edit stuff on it. I have seen multiple tutorials and <a href=""https://docs.djangoproject.com/en/dev/topics/auth/default/#module-django.contrib.auth.views"" rel=""nofollow noreferrer"">documentations</a>, and really am hoping that someone can give me a reliable answer to this. </p>

<p>Thanks in advance. </p>

<p>If it helps, this is what my code looks like so far: (it's also worth mentioning that I'm using Cloud9 IDE)</p>

<p>views.py</p>

<pre><code>from django.contrib.auth import (
      authenticate,
      get_user_model,
      login,
      logout,
 )
from django.shortcuts import render
from django.http import HttpResponse,HttpResponseRedirect
from .forms import UserLoginForm

# Create your views here.
def login_view(request):
    title = ""Login""
    form = UserLoginForm(request.POST or None)
    if form.is_valid():
        username = form.cleaned_data.get(""username"")
        password = form.cleaned_data.get('password')
        user = authenticate(request, username=username, password=password)      
        if user is not None:
          login(request, user)
          # or any other success page
        #   return HttpResponse(""Logged in"")
        return HttpResponseRedirect('accounts/home')
    return render(request, ""accounts/form.html"", {""form"":form, ""title"": title})
</code></pre>

<p>forms.py</p>

<pre><code>from django import forms
from django.contrib.auth import (
    authenticate,
    get_user_model,
    login,
    logout,

    )

User = get_user_model()

class UserLoginForm(forms.Form):
    username = forms.CharField()
    password = forms.CharField(widget=forms.PasswordInput) 

    class Meta: 
        model = User 
        fields = ('username', 'email', 'password')

    def clean(self,*args,**kwargs):
        username = self.cleaned_data.get(""username"")
        password = self.cleaned_data.get(""password"")
        user = authenticate(username=username,password=password)
        if not user:
            raise forms.ValidationError(""This user does not exist"")
        if not user.check_password(password):
            raise forms.ValidationError(""Incorrect Password"")
        if not user.is_active:
            raise forms.ValidationError(""This user is no longer active."")
        return super(UserLoginForm, self).clean(*args,**kwargs)
</code></pre>

<p>form.html (currently using this for login, open to use the same format for registration)</p>

<pre><code>{% extends 'accounts/base.html' %}
{% block main_content %}
    &lt;div class=""container""&gt;
        &lt;div class='col-sm-6 col-sm-offset-3'&gt;
        &lt;h1&gt;{{ title }}&lt;/h1&gt;
        &lt;form method='POST' action='' enctype='multipart/form-data'&gt;{% csrf_token %}
            {{ form }}
            &lt;input type='submit' class='btn btn-primary btn-block' value='{{ title }}' /&gt;
        &lt;/form&gt;
    &lt;/div&gt;
    &lt;/div&gt;
{% endblock %}
</code></pre>

<p>(side note, I know this isn't part of what I'm primarily asking for but my login page isn't looking like the way I want it to which is like this bootstrap signin page right here <a href=""https://getbootstrap.com/docs/4.0/examples/sign-in/"" rel=""nofollow noreferrer"">https://getbootstrap.com/docs/4.0/examples/sign-in/</a>. This is <a href=""https://i.stack.imgur.com/ZPLgT.jpg"" rel=""nofollow noreferrer"">what it looks like right now</a> </p>

<p>If you can help me out with this, it would mean <strong>the world to me</strong>. Thank you. Also if you think it's because I didn't link to a CSS page here's what my base template looks like and my structure:</p>

<pre><code>{% load staticfiles %}

&lt;!DOCTYPE html&gt;
&lt;html&gt;

&lt;head&gt;
    &lt;meta charset=""utf-8""&gt;
    &lt;meta http-equiv=""X-UA-Compatible"" content=""IE=edge""&gt;
    &lt;meta name=""viewport"" content=""width=device-width, initial-scale=1""&gt;

    &lt;title&gt;Accounts&lt;/title&gt;

    &lt;script src=""https://code.jquery.com/jquery-3.3.1.min.js""&gt;&lt;/script&gt;
    &lt;link rel=""stylesheet"" href=""https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css""&gt;
    &lt;script src=""https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js""&gt;&lt;/script&gt;
    &lt;link rel=""stylesheet"" type=""text/css"" href=""{% static 'accounts/signin.css' %}"" /&gt;
&lt;/head&gt;

&lt;body&gt;


    &lt;div class=""jumbotron""&gt;
        &lt;div class=""container""&gt;

            {% block main_content %} 
            {% endblock %} 
        &lt;/div&gt;
    &lt;/div&gt;

&lt;/body&gt;

&lt;/html&gt;
</code></pre>

<p><a href=""https://i.stack.imgur.com/y1kNv.jpg"" rel=""nofollow noreferrer"">What My Framework Looks Like</a></p>

<p>Sincerely appreciate whoever read this far.</p>
","9820620","9820620","2018-06-01 03:15:39","What's the SIMPLEST way to make a DJANGO Registration page?","<python><django><registration><cloud9-ide><cloud9>","1","5","5371"
"50635740","2018-06-01 03:34:32","1","","<p>works for me, python 3.6.4, anaconda 5.1.0 and matplotlib 2.2.2 in pycharm.</p>

<p>it shows this:
<a href=""https://i.stack.imgur.com/WNIA7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WNIA7.png"" alt=""your code shows this""></a></p>
","9878957","","","1","257","Jay Calamari","2018-06-01 02:37:50","335","32","205","2","50635610","50638840","2018-06-01 03:15:14","3","1303","<p>I'm trying to use matplotlib to display some 3d perlin noise. I have read that the <code>voxels</code> method from <code>Axes3DSubplot</code> could be used to simply display values. However, when I try and call <code>ax.voxels(voxels, facecolors=colors, edgecolor='k')</code>, it throws the exception <code>AttributeError: 'Axes3DSubplot' object has no attribute 'voxels'</code>. Here is my code:</p>

<pre><code>import noise
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

x, y, z = np.indices((8,8,8))
voxels = np.zeros((8,8,8), dtype=np.bool)

for xp in range(8):
    for yp in range(8):
        for zp in range(8):
            voxels[xp,yp,zp] = True if abs(noise.pnoise3(xp/8,yp/8,zp/8)) &gt; 0.5 else False

colors = np.empty(voxels.shape, dtype=object)
colors[voxels] = 'green'

fig = plt.figure()
ax = fig.gca(projection='3d')
ax.voxels(voxels, facecolors=colors, edgecolor='k')  #EXCEPTION


plt.show()
</code></pre>

<p>My python version is 3.6.2 (Anaconda 64-bit). My matplotlib version is 2.0.2. I have used both the ipynb (<code>module://backend_interagg</code>) and <code>Qt5Agg</code> backends, which both give the same problem. I'm running Windows 10.</p>
","1486262","","","'Axes3DSubplot' object has no attribute 'voxels'","<python><matplotlib><voxels>","2","0","1219"
"50635746","2018-06-01 03:35:15","0","","<p>Answer found here: </p>

<p><a href=""https://stackoverflow.com/questions/12451431/loading-and-parsing-a-json-file-with-multiple-json-objects-in-python"">Loading and parsing a JSON file with multiple JSON objects in Python</a></p>

<p>worked perfectly</p>
","4623489","4623489","2018-06-02 10:31:03","1","257","Bonzay","2015-03-02 12:58:57","230","55","19","5","50633581","","2018-05-31 22:18:56","2","887","<p>I have around 5 million tweets in the following format:</p>

<pre><code>{""created_at"":""Mon May 21 05:40:26 +0000 2018"",""id"":998438346987683840,""id_str"":""998438346987683840"",""text"":""sometext"",""display_text_range"":[0,0],""source"":""u003ca href=""someURL"" rel=""nofollow""u003eTwitter for iPhoneu003c/au003e"",""truncated"":false,""in_reply_to_status_id"":null,""in_reply_to_status_id_str"":null,""in_reply_to_user_id"":null,""in_reply_to_user_id_str"":null,""in_reply_to_screen_name"":null,""user"":{""id"":1062745482,""id_str"":""1062745482"",""name"":""u3074u30fc"",""screen_name"":""maga12171"",""location"":null,""url"":null,""description"":""u3068u3063u3066u3082u30aau30c8u30cau3067u3059 u706bu661fu4ebauff08uff0buff09 u7652u3057u306fu95a2u30b8u30e3u30cb u6c34u66dcu3069u3046u3067u3057u3087u3046"",""translator_type"":""none"",""protected"":false,""verified"":false,""followers_count"":4,""friends_count"":23,""listed_count"":0,""favourites_count"":977,""statuses_count"":238,""created_at"":""Sat Jan 05 11:09:11 +0000 2013"",""utc_offset"":32400,""time_zone"":""Tokyo"",""geo_enabled"":false,""lang"":""ja"",""contributors_enabled"":false,""is_translator"":false,""profile_background_color"":""000000"",""profile_background_image_url"":""someURL"",""profile_background_image_url_https"":""https://abs.twimg.com/images/themes/theme1/bg.png"",""profile_background_tile"":false,""profile_link_color"":""0066FF"",""profile_sidebar_border_color"":""000000"",""profile_sidebar_fill_color"":""000000"",""profile_text_color"":""000000"",""profile_use_background_image"":false,""profile_image_url"":""http://pbs.twimg.com/profile_images/874952211184222209/UZ8RcGuU_normal.jpg"",""profile_image_url_https"":""someURL"",""profile_banner_url"":""someURL"",""default_profile"":false,""default_profile_image"":false,""following"":null,""follow_request_sent"":null,""notifications"":null},""geo"":null,""coordinates"":null,""place"":null,""contributors"":null,""is_quote_status"":false,""quote_count"":0,""reply_count"":0,""retweet_count"":0,""favorite_count"":0,""entities"":{""hashtags"":[],""urls"":[],""user_mentions"":[],""symbols"":[],""media"":[{""id"":998438345532325888,""id_str"":""998438345532325888"",""indices"":[0,23],""media_url"":""someURL"",""media_url_https"":""someURL"",""url"":""someURL"",""display_url"":""pic.twitter.com/J1RJGazs8k"",""expanded_url"":""someURL"",""type"":""photo"",""sizes"":{""thumb"":{""w"":150,""h"":150,""resize"":""crop""},""large"":{""w"":750,""h"":1334,""resize"":""fit""},""small"":{""w"":382,""h"":680,""resize"":""fit""},""medium"":{""w"":675,""h"":1200,""resize"":""fit""}}}]},""extended_entities"":{""media"":[{""id"":998438345532325888,""id_str"":""998438345532325888"",""indices"":[0,23],""media_url"":""someURL"",""media_url_https"":""someURL"",""url"":""someURL"",""display_url"":""pic.twitter.com/J1RJGazs8k"",""expanded_url"":""someURL"",""type"":""photo"",""sizes"":{""thumb"":{""w"":150,""h"":150,""resize"":""crop""},""large"":{""w"":750,""h"":1334,""resize"":""fit""},""small"":{""w"":382,""h"":680,""resize"":""fit""},""medium"":{""w"":675,""h"":1200,""resize"":""fit""}}}]},""favorited"":false,""retweeted"":false,""possibly_sensitive"":false,""filter_level"":""low"",""lang"":""und"",""timestamp_ms"":""1526881226666""}
</code></pre>

<p>i have imported them in a mysqldb as text datatype and now i am trying to pull them row by row and clean them up so that i can keep only the data i need.</p>

<pre><code>import json
import MySQLdb.cursors

HOST = """"
USER = """"
PASSWD = """"
DB = """"

conn = MySQLdb.connect(host=HOST, port =3306, user=USER,passwd=PASSWD,db=DB , cursorclass= MySQLdb.cursors.SSCursor)

x = conn.cursor()

query = ""SELECT txt_column FROM txt_data limit 1""
x.execute(query)

for row in x:
    print(row)       
    res = json.loads(row[0])
</code></pre>

<p>when i run it i get : <code>ValueError: Expecting ',' delimiter: line 1 column 185 (char 184)</code> , That is a <code>""</code> character found at the <code>href=</code>"" of the attached json string. A few previous posts in stackoverflow recommend to use .replace() to get rid of the "" characters, but this will ruin the json format.</p>

<p>I think the problem is that python expects to find a format like ""attribute_name"":""data"". When it finds ""attribute_name"":""data ""data in quotes"" more data"" it throughs the error for the unexpected "" char</p>

<p>If i am right, is there anyway i can fix this?</p>

<p>Please note i had to modify the json sample attached, substituting all URLs with ""someURL"", stackoverflow doesn't allow urls. Thus you will not find the error at char 184. In the original data 184 is the first "" in href=""someURL""</p>
","4623489","","","ValueError: Expecting ',' delimiter, while trying to load string as json in python3","<python><json><python-3.x><file-io>","1","11","4338"
"50635759","2018-06-01 03:36:31","2","","<p>try this:</p>

<p><code>r'\-?\d{1,10}(?:\.\d{1,10})?'</code></p>

<p>use <code>(?:)?</code> to make fractional part optional.</p>

<p>for <code>r'\-?\d{0,10}\.?\d{1,10}'</code>, it is <code>\.?\d{1,10}</code> who matched <code>2</code>.</p>
","4201810","4201810","2018-06-01 03:46:39","1","244","georgexsh","2014-10-31 09:16:55","11761","486","324","95","50635699","50635759","2018-06-01 03:28:08","0","135","<p>I used regular expression in python2.7 to match the number in a string but I can't match a single number in my expression, here are my code</p>

<pre><code>import re
import cv2

s = '858  1790 -156.25 2'
re_matchData = re.compile(r'\-?\d{1,10}\.?\d{1,10}')
data = re.findall(re_matchData, s)
print data
</code></pre>

<p>and then print:</p>

<pre><code>['858', '1790', '-156.25']
</code></pre>

<p>but when I change expression from</p>

<pre><code>re_matchData = re.compile(r'\-?\d{1,10}\.?\d{1,10}')
</code></pre>

<p>to </p>

<pre><code>re_matchData = re.compile(r'\-?\d{0,10}\.?\d{1,10}')
</code></pre>

<p>then print:</p>

<pre><code>['858', '1790', '-156.25', '2']
</code></pre>

<p>is there any confuses between d{1, 10} and d{0,10} ?
If I did wrong, how to correct it ?
Thanks for checking my question !</p>
","5433656","","","Python regular expression match number in string","<python><regex>","3","0","818"
"50635769","2018-06-01 03:38:40","0","","<p>try this it work well and if the problem persist check your search regex :</p>

<pre><code>from sys import argv

log_file = """"
if len(argv) &gt; 0 :
    log_file = argv[1]
else :
    quit(""No log_file specified, exiting script."")

with open(log_file, encoding=""utf-8"") as infile:
    for line in infile:
        if name_to_check in line:

            username = re.search(r'(?&lt;=userName=)(.*)(?=,)', line)
            username = username.group()

            date = re.search(r'(?P&lt;date&gt;\d{8})\s+(?P&lt;time&gt;\d{9})\+(?P&lt;zone&gt;\d{4})', line)
            date = datetime.datetime.strptime(date.group('date'), ""%Y%m%d"").strftime(""%Y-%m-%d"")
            print(date)

            time = re.search(r'(?P&lt;date&gt;\d{8})\s+(?P&lt;time&gt;\d{9})\+(?P&lt;zone&gt;\d{4})', line)
            time = datetime.datetime.strptime(time.group('time'), ""%H%M%S%f"").strftime(""%H:%M:%S"")
            print(time)

            ip = re.search(r'(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])',line)

            with open(output_file, ""ab"", buffering=0) as outfile:
                outfile.write( (""{},{},{},{}\n"".format(username, date, time, ip)).encode() )
</code></pre>
","9768146","9768146","2018-06-02 05:33:50","3","1229","eden clarck","2018-05-10 01:29:08","74","13","1","0","50635062","50635769","2018-06-01 01:54:17","-1","75","<p>I have a log file which consists the capacity of 1TB. I am uncertain that how to run this python script in the command line. I use the sys library but still my csv data is not added.</p>

<p>Below is my python code. </p>

<pre><code>import re
import sys
from csv import writer
import datetime
log_file = '/Users/kiya/Desktop/mysql/ipscan/ip.txt'
output_file = '/Users/kiya/Desktop/mysql/ipscan/output.csv'

try:
    ip_file =sys.argv[1]
except Exception:
    print(""usage: pythone3 {} [ip file]"".format(sys.argv[0]))
    sys.exit()

name_to_check = 'MBX_AUTHENTICATION_FAILED'

with open(log_file,encoding=""utf-8"") as infile:
    for line in infile:
        if name_to_check in line:
            username = re.search(r'(?&lt;=userName=)(.*)(?=,)', line)
            username = username.group()

            ip = re.search(r'(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])',line)
            ip = ip.group()

            with open(output_file, 'a') as outfile:
                outfile.write('{username},{ip}\n'.format(username=username, ip=ip))
</code></pre>
","9262171","9262171","2018-06-04 01:20:48","How can I execute 1TB log file python script in command line","<python><sys>","1","9","1117"
"50635801","2018-06-01 03:43:01","1","","<p>The first <code>\d{1,10}</code> matches from 1 to 10 digits, and the second <code>\d{1,10}</code> also matches from 1 to 10 digits. In order for them both to match, you need at least 2 digits in your number, with an optional <code>.</code> between them.</p>

<p>You should make the entire fraction optional, not just the <code>.</code>.</p>

<pre><code>r'\-?\d{1,10}(?:\.\d{1,10})?'
</code></pre>
","1491895","","","1","400","Barmar","2012-06-29 18:12:29","477375","68451","6422","3351","50635699","50635759","2018-06-01 03:28:08","0","135","<p>I used regular expression in python2.7 to match the number in a string but I can't match a single number in my expression, here are my code</p>

<pre><code>import re
import cv2

s = '858  1790 -156.25 2'
re_matchData = re.compile(r'\-?\d{1,10}\.?\d{1,10}')
data = re.findall(re_matchData, s)
print data
</code></pre>

<p>and then print:</p>

<pre><code>['858', '1790', '-156.25']
</code></pre>

<p>but when I change expression from</p>

<pre><code>re_matchData = re.compile(r'\-?\d{1,10}\.?\d{1,10}')
</code></pre>

<p>to </p>

<pre><code>re_matchData = re.compile(r'\-?\d{0,10}\.?\d{1,10}')
</code></pre>

<p>then print:</p>

<pre><code>['858', '1790', '-156.25', '2']
</code></pre>

<p>is there any confuses between d{1, 10} and d{0,10} ?
If I did wrong, how to correct it ?
Thanks for checking my question !</p>
","5433656","","","Python regular expression match number in string","<python><regex>","3","0","818"
"50635825","2018-06-01 03:47:18","1","","<p>Try Visual Studio Code, it comes with everything you need for Python development, including a feature to launch and test your program from the application.</p>

<p>It's free and open source, you can pick it up <a href=""https://code.visualstudio.com"" rel=""nofollow noreferrer"">here</a> :)</p>
","9852277","","","2","295","Jeffery","2018-05-26 19:41:35","11","4","0","0","50635811","","2018-06-01 03:45:02","0","105","<p>I have been using IDLE to program in python for the time being, and it is starting to get tedious to launch it from terminal.  I have looked online to try to find a solution for this but haven't found out how to launch it in a typical Mac like way from spotlight (I have already tried putting it into the applications folder).</p>

<p>I am also open to any other suggestions for any better IDE's that work the same way as IDLE, with its own built in compiler.</p>
","9836259","8708364","2018-06-01 03:47:25","Launching python from idle, or better IDE options","<python><macos><compilation><python-idle>","2","2","467"
"50635846","2018-06-01 03:50:07","1","","<p>Create a dictionary to hold <a href=""https://docs.python.org/3/library/operator.html"" rel=""nofollow noreferrer"">functions that correspond to <code>'conditions'</code></a></p>

<pre><code>import operator, functools
operations = {'AND':operator.and_, 'OR':operator.or_, 'XOR':operator.xor}
</code></pre>

<p>Write a recursive function that will recurse when <code>'conditions'</code> is a key in a rule, otherwise iterate over the rules and accumulate <code>'decisions'</code> in a list.  Use <a href=""https://docs.python.org/3/library/functools.html#functools.reduce"" rel=""nofollow noreferrer""><code>functools.reduce</code></a> to apply the <code>condition</code> to the decisions.</p>

<pre><code>def f(d):
    func = operations.get(d['condition'], None)
    if func is None:
        return
    decisions = []
    for rule in d['rules']:
        if 'condition' in rule:
            decision = f(rule)
        else:
            decision = rule['decision']
        decisions.append(decision)
    return functools.reduce(func, decisions)
</code></pre>

<hr>

<p><code>if func is None: return</code> was meant to be the base case but I'm not so sure it's needed - if that happens the dict is messed up and it probably should raise a <code>ValueError</code>  I think this has an implicit base case (if there is such a thing) - it relies on the <code>for rule in d['rules']:</code> loop to run out of items.</p>

<hr>

<p>If the conditions limited to 'AND' and 'OR' you can use <code>all</code> and <code>any</code>.</p>

<pre><code>ops = {'AND':all, 'OR':any}
def g(d):
    func = ops.get(d['condition'], None)
    if func is None:
        return
    decisions = []
    for rule in d['rules']:
        if 'condition' in rule:
            decision = f(rule)
        else:
            decision = rule['decision']
        decisions.append(decision)
    return func(decisions)
</code></pre>
","2823755","2823755","2018-06-01 04:18:06","1","1885","wwii","2013-09-27 14:02:10","12942","2554","818","1086","50635554","50635846","2018-06-01 03:06:30","0","153","<p>I have a dict structure that appears like the following:</p>

<pre><code>{
""condition"": ""AND"",
""rules"": [
    {
        ""id"": ""monitor_category"",
        ""field"": ""monitor_category"",
        ""type"": ""string"",
        ""input"": ""select"",
        ""operator"": ""equal"",
        ""value"": ""Competition"",
        ""decision"": True
    },
    {
        ""id"": ""monitor_tag"",
        ""field"": ""monitor_tag"",
        ""type"": ""string"",
        ""input"": ""text"",
        ""operator"": ""equal"",
        ""value"": ""PassiveTotal"",
        ""decision"": True
    },
    {
        ""condition"": ""OR"",
        ""rules"": [
            {
                ""id"": ""article_tag"",
                ""field"": ""article_tag"",
                ""type"": ""string"",
                ""input"": ""text"",
                ""operator"": ""contains"",
                ""value"": ""Attack"",
                ""decision"": False
            },
            {
                ""id"": ""article_tag"",
                ""field"": ""article_tag"",
                ""type"": ""string"",
                ""input"": ""text"",
                ""operator"": ""contains"",
                ""value"": ""Hunt"",
                ""decision"": True
            },
            {
                ""id"": ""article_tag"",
                ""field"": ""article_tag"",
                ""type"": ""string"",
                ""input"": ""text"",
                ""operator"": ""contains"",
                ""value"": ""Threat"",
                ""decision"": False
            }
        ]
    },
    {
        ""id"": ""monitor_tag"",
        ""field"": ""monitor_tag"",
        ""type"": ""string"",
        ""input"": ""text"",
        ""operator"": ""equal"",
        ""value"": ""Analysis"",
        ""decision"": False
    }
]
</code></pre>

<p>}</p>

<p>For each rule, I derive a decision and attach it to the policy rule. I do that via a simple recursive walk of the dict. In the above sample policy, the boolean logic equates to the following:</p>

<pre><code>(True and True and (False or True or False) and False)
</code></pre>

<p>I'd like to have a function that takes this policy in and is able to derive the boolean logic in order to return the final evaluation. I know a depth-search first approach is liable to be the direction here, but am struggling with how to maintain the boolean state and know which level I am at within the structure.</p>
","1257332","1257332","2018-06-01 03:51:41","Evaluate boolean logic using DFS in python dict","<python><recursion><depth-first-search><boolean-logic>","1","3","2294"
"50635852","2018-06-01 03:50:42","2","","<p>This seems to work:</p>

<pre><code>params = {'a': 'aaa', 'b': 'bbb', 'c': 'ccc', 'd': 'ddd-hoge'}

print(""1: {a} 2: {b} 3: {c} 4: {d}"".format(**{
    k: v.split('-')[0] for k, v in params.items()}))
</code></pre>

<p>This does the processing on the dict, and then formats for print.</p>

<h3>Results:</h3>

<pre><code>1: aaa 2: bbb 3: ccc 4: ddd
</code></pre>
","7311767","","","0","364","Stephen Rauch","2016-12-18 02:06:51","33601","12784","4195","3857","50635826","","2018-06-01 03:47:18","2","256","<p>Suppose I have the following dictionary.</p>

<pre><code>params = {'a': 'aaa', 'b': 'bbb', 'c': 'ccc', 'd': 'ddd-hoge'}
</code></pre>

<p>When printing using the following script,</p>

<pre><code>&gt;&gt;&gt; print(""1: {a} 2: {b} 3: {c} 4: {d}.split('-')[0]"".format(**params))
</code></pre>

<p>the output will be</p>

<pre><code>""1: aaa 2: bbb 3: ccc 4: ddd-hoge.split('-')[0]""
</code></pre>

<p>How should I modify the script to get the same output as the following?</p>

<pre><code>""1:aaa 2:bbb 3:ccc 4:ddd""
</code></pre>
","2159158","7311767","2018-06-01 03:55:43","Python: How to format a string with split function?","<python><python-2.7>","3","2","528"
"50635869","2018-06-01 03:53:13","1","","<p>I prefer Pycharm IDE. It is a JetBrain Product
<a href=""https://www.jetbrains.com/pycharm/"" rel=""nofollow noreferrer"">https://www.jetbrains.com/pycharm/</a></p>
","9861733","","","0","164","jerrycheng","2018-05-29 02:33:42","159","17","4","2","50635811","","2018-06-01 03:45:02","0","105","<p>I have been using IDLE to program in python for the time being, and it is starting to get tedious to launch it from terminal.  I have looked online to try to find a solution for this but haven't found out how to launch it in a typical Mac like way from spotlight (I have already tried putting it into the applications folder).</p>

<p>I am also open to any other suggestions for any better IDE's that work the same way as IDLE, with its own built in compiler.</p>
","9836259","8708364","2018-06-01 03:47:25","Launching python from idle, or better IDE options","<python><macos><compilation><python-idle>","2","2","467"
"50635886","2018-06-01 03:56:41","1","","<p>Just simply split afterward:</p>

<pre><code>print(""1: {a} 2: {b} 3: {c} 4: {d}"".format(**params).split('-')[0])
</code></pre>
","9861733","","","2","130","jerrycheng","2018-05-29 02:33:42","159","17","4","2","50635826","","2018-06-01 03:47:18","2","256","<p>Suppose I have the following dictionary.</p>

<pre><code>params = {'a': 'aaa', 'b': 'bbb', 'c': 'ccc', 'd': 'ddd-hoge'}
</code></pre>

<p>When printing using the following script,</p>

<pre><code>&gt;&gt;&gt; print(""1: {a} 2: {b} 3: {c} 4: {d}.split('-')[0]"".format(**params))
</code></pre>

<p>the output will be</p>

<pre><code>""1: aaa 2: bbb 3: ccc 4: ddd-hoge.split('-')[0]""
</code></pre>

<p>How should I modify the script to get the same output as the following?</p>

<pre><code>""1:aaa 2:bbb 3:ccc 4:ddd""
</code></pre>
","2159158","7311767","2018-06-01 03:55:43","Python: How to format a string with split function?","<python><python-2.7>","3","2","528"
"50635888","2018-06-01 03:56:43","1","","<p>Have you tried installing <code>pathlib2</code> with <code>pip</code> or <code>pip3</code>? Please try</p>

<pre><code>pip install pathlib2
</code></pre>

<p>if you're using Python2, and </p>

<pre><code>pip3 install pathlib2
</code></pre>

<p>if you're using Python3. However, If <code>pip</code> is not found, then try installing it with</p>

<pre><code>apt-get install python-pip python3-pip
</code></pre>
","3903685","","","1","412","srakrn","2014-08-03 09:58:42","96","16","7","0","50635854","","2018-06-01 03:51:25","3","5902","<p>I've been working on getting Google Assistant working on my Raspberry Pi 3.
It is working but I'm having problems getting this specific step to work:
<a href=""https://developers.google.com/assistant/sdk/guides/library/python/extend/handle-device-commands"" rel=""nofollow noreferrer"">https://developers.google.com/assistant/sdk/guides/library/python/extend/handle-device-commands</a></p>

<p>This step covers sending an on/off command to the Pi to turn a LED bulb on or off.
I have confirmed the Bread board and LED is setup correctly because I can turn the LED on or off via a python script.</p>

<p>However, after following the steps in that page and trying to run the following command ""python hotword.py --device_model_id my-model"" 
(which is actually:  python hotword.py --device_model_id assistantraspi-1d671-pigooglev2-8n98u3)
I get the following error:
ImportError: No module named pathlib2</p>

<p>I am including a copy of that file (hotword.py)</p>

<pre><code>#!/usr/bin/env python

# Copyright (C) 2017 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


from __future__ import print_function

import argparse
import json
import os.path
import pathlib2 as pathlib
import RPi.GPIO as GPIO

import google.oauth2.credentials

from google.assistant.library import Assistant
from google.assistant.library.event import EventType
from google.assistant.library.file_helpers import existing_file
from google.assistant.library.device_helpers import register_device

try:
    FileNotFoundError
except NameError:
    FileNotFoundError = IOError


WARNING_NOT_REGISTERED = """"""
    This device is not registered. This means you will not be able to use
    Device Actions or see your device in Assistant Settings. In order to
    register this device follow instructions at:

    https://developers.google.com/assistant/sdk/guides/library/python/embed/register-device
""""""


def process_event(event):
    """"""Pretty prints events.

    Prints all events that occur with two spaces between each new
    conversation and a single space between turns of a conversation.

    Args:
        event(event.Event): The current event to process.
    """"""
    if event.type == EventType.ON_CONVERSATION_TURN_STARTED:
        print()

    print(event)

    if (event.type == EventType.ON_CONVERSATION_TURN_FINISHED and
            event.args and not event.args['with_follow_on_turn']):
        print()
    if event.type == EventType.ON_DEVICE_ACTION:
        for command, params in event.actions:
            print('Do command', command, 'with params', str(params))
            if command == ""action.devices.commands.OnOff"":
                if params['on']:
                    print('Turning the LED on.')
                    GPIO.output(25, 1)
                else:
                    print('Turning the LED off.')
                    GPIO.output(25, 0)

def main():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.RawTextHelpFormatter)
    parser.add_argument('--device-model-id', '--device_model_id', type=str,
                        metavar='DEVICE_MODEL_ID', required=False,
                        help='the device model ID registered with Google')
    parser.add_argument('--project-id', '--project_id', type=str,
                        metavar='PROJECT_ID', required=False,
                        help='the project ID used to register this device')
    parser.add_argument('--device-config', type=str,
                        metavar='DEVICE_CONFIG_FILE',
                        default=os.path.join(
                            os.path.expanduser('~/.config'),
                            'googlesamples-assistant',
                            'device_config_library.json'
                        ),
                        help='path to store and read device configuration')
    parser.add_argument('--credentials', type=existing_file,
                        metavar='OAUTH2_CREDENTIALS_FILE',
                        default=os.path.join(
                            os.path.expanduser('~/.config'),
                            'google-oauthlib-tool',
                            'credentials.json'
                        ),
                        help='path to store and read OAuth2 credentials')
    parser.add_argument('-v', '--version', action='version',
                        version='%(prog)s ' + Assistant.__version_str__())

    args = parser.parse_args()
    with open(args.credentials, 'r') as f:
        credentials = google.oauth2.credentials.Credentials(token=None,
                                                            **json.load(f))

    device_model_id = None
    last_device_id = None
    try:
        with open(args.device_config) as f:
            device_config = json.load(f)
            device_model_id = device_config['model_id']
            last_device_id = device_config.get('last_device_id', None)
    except FileNotFoundError:
        pass

    if not args.device_model_id and not device_model_id:
        raise Exception('Missing --device-model-id option')

    # Re-register if ""device_model_id"" is given by the user and it differs
    # from what we previously registered with.
    should_register = (
        args.device_model_id and args.device_model_id != device_model_id)

    device_model_id = args.device_model_id or device_model_id

    with Assistant(credentials, device_model_id) as assistant:
        events = assistant.start()

        device_id = assistant.device_id
        print('device_model_id:', device_model_id)
        print('device_id:', device_id + '\n')
        GPIO.setmode(GPIO.BCM)
        GPIO.setup(25, GPIO.OUT, initial=GPIO.LOW)

        # Re-register if ""device_id"" is different from the last ""device_id"":
        if should_register or (device_id != last_device_id):
            if args.project_id:
                register_device(args.project_id, credentials,
                                device_model_id, device_id)
                pathlib.Path(os.path.dirname(args.device_config)).mkdir(
                    exist_ok=True)
                with open(args.device_config, 'w') as f:
                    json.dump({
                        'last_device_id': device_id,
                        'model_id': device_model_id,
                    }, f)
            else:
                print(WARNING_NOT_REGISTERED)

        for event in events:
            process_event(event)


if __name__ == '__main__':
    main()
</code></pre>
","9584310","1000551","2018-08-08 11:01:26","No module named pathlib2","<python><json><argparse><google-assistant-sdk><pathlib>","4","1","6933"
"50635898","2018-06-01 03:58:23","0","","<p>An operation of that complexity won't be handled by the formatting mini language. You would have to do the transformation externally.</p>

<p>One way would be to make a copy of the dictionary and update the particular value you want:</p>

<pre><code>p = params.copy()
p['d'] = params['d'].split('-')[0]
print(""1: {a} 2: {b} 3: {c} 4: {d}"".format(**p))
</code></pre>

<p>You could of course modify <code>params</code> in place as well. A more elegant solution might be to apply the transformation to all the values:</p>

<pre><code>print(""1: {a} 2: {b} 3: {c} 4: {d}"".format(**{k: v.split('-')[0] for k, v in params.items()}))
</code></pre>
","2988730","","","0","643","Mad Physicist","2013-11-13 16:56:40","47060","10150","10088","2033","50635826","","2018-06-01 03:47:18","2","256","<p>Suppose I have the following dictionary.</p>

<pre><code>params = {'a': 'aaa', 'b': 'bbb', 'c': 'ccc', 'd': 'ddd-hoge'}
</code></pre>

<p>When printing using the following script,</p>

<pre><code>&gt;&gt;&gt; print(""1: {a} 2: {b} 3: {c} 4: {d}.split('-')[0]"".format(**params))
</code></pre>

<p>the output will be</p>

<pre><code>""1: aaa 2: bbb 3: ccc 4: ddd-hoge.split('-')[0]""
</code></pre>

<p>How should I modify the script to get the same output as the following?</p>

<pre><code>""1:aaa 2:bbb 3:ccc 4:ddd""
</code></pre>
","2159158","7311767","2018-06-01 03:55:43","Python: How to format a string with split function?","<python><python-2.7>","3","2","528"
"50635912","2018-06-01 04:00:22","1","","<p>If you want to reverse a list, do as follows:</p>

<pre><code># Solution 1
alist = random.sample(range(1, 100), 5)
otherlist = list(reversed(alist))

# Solution 2 (My favourite)
otherlist = alist[::-1]

# Solution 3
otherlist = alist[:]
otherlist.reverse()

# Solution 4
otherlist = []
for i in range(len(alist)-1, -1, -1):
    otherlist.append(alist[i])

# Solution 5
counter = count2 = 0
alist, templist = [], []
while counter&lt;5:
    alist.append(random.randint(5,10))
    print(alist[counter])
    counter += 1

while counter &gt; 0:
    counter -= 1
    templist.append(alist[counter])
    print(alist[counter])
    print(""Real"",templist[count2])    
    count2 += 1
</code></pre>
","9586338","9586338","2018-06-01 04:15:51","5","691","Waket Zheng","2018-04-02 13:50:59","1227","101","88","7","50635777","50635912","2018-06-01 03:39:49","-1","45","<p>Write a list of 5 numbers then another list with the values reversed... </p>

<p>Solution 1 works. Solution 2, not so much. </p>

<pre><code>counter = 0
count2 = 0
list = []
otherlist = []

import random

#Solution 1
list = random.sample(range(1, 100), 5)
otherlist = list.reverse    

#Solution 2
while counter&lt;5:
    list.append(random.randint(5,10))
    print(list[counter])
    counter = counter + 1

counter = counter - 1

while counter &gt; (-1):
    templist.append(list[counter])
    print(list[counter])
    print(""Real"",templist[count2])
    counter = counter - 1
    count2 = count2 + 1
</code></pre>
","3042850","","","Write a list then reverse it returning","<python>","1","4","618"
"50635979","2018-06-01 04:09:27","0","","<p>I would rather do as follows:</p>

<pre><code>import re
s = '858  1790 -156.25 2'
re_matchData = re.compile(r'\-?\d{1,10}\.?\d{0,10}')
data = re_matchData.findall(s)
print data
</code></pre>

<p>Output:</p>

<pre><code>['858', '1790', '-156.25', '2']
</code></pre>
","9586338","","","1","268","Waket Zheng","2018-04-02 13:50:59","1227","101","88","7","50635699","50635759","2018-06-01 03:28:08","0","135","<p>I used regular expression in python2.7 to match the number in a string but I can't match a single number in my expression, here are my code</p>

<pre><code>import re
import cv2

s = '858  1790 -156.25 2'
re_matchData = re.compile(r'\-?\d{1,10}\.?\d{1,10}')
data = re.findall(re_matchData, s)
print data
</code></pre>

<p>and then print:</p>

<pre><code>['858', '1790', '-156.25']
</code></pre>

<p>but when I change expression from</p>

<pre><code>re_matchData = re.compile(r'\-?\d{1,10}\.?\d{1,10}')
</code></pre>

<p>to </p>

<pre><code>re_matchData = re.compile(r'\-?\d{0,10}\.?\d{1,10}')
</code></pre>

<p>then print:</p>

<pre><code>['858', '1790', '-156.25', '2']
</code></pre>

<p>is there any confuses between d{1, 10} and d{0,10} ?
If I did wrong, how to correct it ?
Thanks for checking my question !</p>
","5433656","","","Python regular expression match number in string","<python><regex>","3","0","818"
"50635991","2018-06-01 04:10:53","1","","<p>Calling <code>client.recv(1024)</code> will block and return 1 to 1024 bytes <em>unless</em> the client disconnects, in which case it will return an empty bytestring:</p>

<pre><code>def handle_client(client):
    while True:
        message = client.recv(BUFFER_SIZE)

        if not message:
            # client disconnected
            break

        ...
</code></pre>
","464744","","","0","376","Blender","2010-10-02 17:47:48","223387","26068","2924","740","50635835","50635991","2018-06-01 03:48:09","0","62","<p>I'm trying to achieve interprocess communication with socket. I have this code which acts as a socket server. When a message is received, the server broadcasts all the message to the other conntected clients.</p>

<pre><code>from socket import AF_INET, gethostname, socket, SOCK_STREAM
import _thread as thread

clients = []

HOST = gethostname()
PORT = 33000
BUFFER_SIZE = 1024
ADDR = (HOST, PORT)

s = socket(AF_INET, SOCK_STREAM)
s.bind(ADDR)

def accept_client():
    while True:
        client, _ = s.accept()
        clients.append(client)
        print(""Client accepted."")
        thread.start_new_thread(handle_client, (client, ))

def handle_client(client):
    while True:
        message = client.recv(BUFFER_SIZE)
        print(""Incoming message: {}"".format(message.decode('utf-8')))
        broadcast(message)

def broadcast(message):
    print(""Broadcasting..."")
    for c in clients:
        print(""Broadcasting to {}"".format(c))
        c.send(message)

if __name__ == '__main__':
    s.listen()
    print(""Listening..."")
    thread.start_new_thread(accept_client, ())
    try:
        while True:
            pass
    except KeyboardInterrupt:
        print(""Exited."")
        exit(0)
</code></pre>

<p>And here is a class for my client. </p>

<pre><code>from socket import AF_INET, gethostname, socket, SOCK_STREAM
import _thread as thread

class SocketClient:
    buffer_size = 1024

    def __init__(self, host=gethostname(), port=33000):
        self.host = host
        self.port = port
        self.buffer_size = 1024
        self.s = socket(AF_INET, SOCK_STREAM)
        self.s.connect((self.host, self.port))

    def bind_callback(self, callback, args=()):
        thread.start_new_thread(self.receive, (callback, args))

    def receive(self, callback, args):
        while True:
            try:
                msg = self.s.recv(self.buffer_size).decode('utf-8')
                callback(msg, *args)
            except OSError as err:
                self.close()
                raise err
            except NameError:
                raise err

    def send(self, msg):
        try:
            self.s.send(msg.encode('utf-8'))
        except:
            self.close()

    def close(self):
        self.send('{clientClose}')
        self.s.close()
</code></pre>

<p>In other script which I've create the object of<code>SocketClient</code>, <strong>there is an infinite amount of incoming socket after the script terminates</strong> (by Ctrl+D in Python). The server script (which prints an attempt when an incoming socket is received) shows this repeatedly:</p>

<pre><code>Incoming message: 
Broadcasting...
Broadcasting to &lt;socket.socket fd=4, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.1.1', 33000), raddr=('127.0.0.1', 44126)&gt;
Broadcasting to &lt;socket.socket fd=5, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.1.1', 33000), raddr=('127.0.0.1', 44276)&gt;
</code></pre>

<p>Observe that there appears to be no incoming messages.</p>

<p>My assumption is that I've failed to close the socket connection properly (before exiting). How can this be achieved? Thank you in advance.</p>
","3903685","","","Python socket still keeps incoming even after client terminates","<python><sockets>","1","0","3203"
"50636040","2018-06-01 04:18:49","1","","<p>While you iterating your loop, you remove item from orderer_list which cause index error
Try this:</p>

<pre><code>def remove_items_from_list(ordered_list, temp):
    list_ = [x for x in orderer_list if x not in temp]
    return list_
</code></pre>
","9861733","","","0","252","jerrycheng","2018-05-29 02:33:42","159","17","4","2","50635960","50636165","2018-06-01 04:07:21","1","900","<p>I am curious. How can I correctly iterate through a list, compare two values and delete the duplicate if it exists. </p>

<p>Here I created a nested for loop:</p>

<pre><code>my_list =  [ 1, 2, 3, 4, 5 ]
temp = [1, 5, 6]

def remove_items_from_list(ordered_list, temp):
    # Removes all values, found in items_to_remove list, from my_list
        for j in range(0, len(temp)):
                for i in range(0, len(ordered_list)):
                        if ordered_list[i] == temp[j]:
                                ordered_list.remove(ordered_list[i])
</code></pre>

<p>But when I execute my my code I get an error:</p>

<pre><code>  File ""./lab3f.py"", line 15, in remove_items_from_list
    if ordered_list[i] == items_to_remove[j]:
</code></pre>

<p>can anyone explain why?</p>

<p>This question, wanted to me compare two lists with one another, and these lists have two different lengths. If an item in list a matched a value in list b, we wanted then to delete it from list a. </p>
","8729657","8729657","2018-06-01 16:09:03","Iterate through a list, compare values and remove duplicate - Python","<python><python-3.x>","4","4","993"
"50636042","2018-06-01 04:19:03","0","","<p>To get the model output, without training, like you're doing in the TF code, the following code should work. Indeed, you need an <code>Input</code> layer, and to hook each layer to the previous one, and a <code>Model</code> as well:</p>

<pre><code>import numpy as np
from keras.models import Model
from keras.layers import Dropout, GRU, Input

x = np.random.randn(10, 5, 3)

inputs = Input(shape=(5, 3))
gru_layer = GRU(2, return_sequences=True)(inputs)
gru_layer = Dropout(0.5)(gru_layer)

model = Model(inputs=inputs, outputs=gru_layer)

output = model.predict(x)
</code></pre>
","1098884","","","1","584","antishok","2011-12-14 23:33:47","2826","351","281","14","50631867","","2018-05-31 19:56:15","0","649","<p>I would like to apply dropout to the outputs from an RNN. For example, in Tensorflow 1.8.0, I could do this:</p>

<pre><code>import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

x = tf.random_uniform((10, 5, 3))

gru_cell1 = tf.contrib.rnn.GRUCell(2)
gru_cell1 = tf.contrib.rnn.DropoutWrapper(gru_cell1, output_keep_prob=0.5)
cell = tf.contrib.rnn.MultiRNNCell([gru_cell1])
init_state = cell.zero_state(10, tf.float32)

cell_output, _ = tf.nn.dynamic_rnn(cell, x,
                                   initial_state=init_state, time_major=False)
cell_output
</code></pre>

<p>How can I achieve the same thing using the Keras API?</p>

<p>I have thought of the following two ways but they were unsuccessful:</p>

<pre><code>import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

# Attempt 1
x = tf.random_uniform((10, 5, 3))

gru_layer = tf.keras.layers.GRU(2, return_sequences=True, input_shape=(10, 5, 3))
gru_layer = tf.keras.layers.Dropout(0.5)(gru_layer)

# Gives the following error:
# ValueError: Attempt to convert a value (&lt;tensorflow.python.keras._impl.keras.layers.recurrent.GRU object
#  at 0x000001C520681F60&gt;) with an unsupported type (&lt;class 'tensorflow.python.keras._impl.keras.layers.recurrent.GRU'&gt;) 
# to a Tensor.

# Attempt 2
x = tf.random_uniform((10, 5, 3))

gru_layer = tf.keras.layers.GRU(2, return_sequences=True, input_shape=(10, 5, 3))
gru_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Dropout(0.4))(gru_layer)

# Gives the following error:
# ValueError: as_list() is not defined on an unknown TensorShape.
</code></pre>
","3775778","3775778","2018-06-02 06:06:40","How to apply dropout to the outputs of an RNN in TensorFlow Eager using the Keras API?","<python><tensorflow><keras><recurrent-neural-network><dropout>","1","5","1650"
"50636053","2018-06-01 04:20:34","-1","","<p>You can't remove an items from the list you are iterating over. You can create a copy of the array and remove items from it. </p>
","8887062","","","0","133","Johnny Chia","2017-11-04 21:15:45","72","12","9","0","50635960","50636165","2018-06-01 04:07:21","1","900","<p>I am curious. How can I correctly iterate through a list, compare two values and delete the duplicate if it exists. </p>

<p>Here I created a nested for loop:</p>

<pre><code>my_list =  [ 1, 2, 3, 4, 5 ]
temp = [1, 5, 6]

def remove_items_from_list(ordered_list, temp):
    # Removes all values, found in items_to_remove list, from my_list
        for j in range(0, len(temp)):
                for i in range(0, len(ordered_list)):
                        if ordered_list[i] == temp[j]:
                                ordered_list.remove(ordered_list[i])
</code></pre>

<p>But when I execute my my code I get an error:</p>

<pre><code>  File ""./lab3f.py"", line 15, in remove_items_from_list
    if ordered_list[i] == items_to_remove[j]:
</code></pre>

<p>can anyone explain why?</p>

<p>This question, wanted to me compare two lists with one another, and these lists have two different lengths. If an item in list a matched a value in list b, we wanted then to delete it from list a. </p>
","8729657","8729657","2018-06-01 16:09:03","Iterate through a list, compare values and remove duplicate - Python","<python><python-3.x>","4","4","993"
"50636082","2018-06-01 04:24:14","1","","<p>Try adding the to data frames together then use the pandas <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html?highlight=apply"" rel=""nofollow noreferrer""><code>apply</code></a> function then add a <code>lambda</code> in it then divide <code>x</code> with two:</p>

<pre><code>import pandas as pd
df1 = pd.DataFrame({'A': [2,2]})
df2 = pd.DataFrame({'A': [4,4]})
print((df1+df2).apply(lambda x: x/2))
</code></pre>

<p>Output:</p>

<pre><code>   A
0  3.0
1  3.0
</code></pre>

<p><em>Note: this is just with a dummy data frame</em></p>
","8708364","8708364","2018-06-01 04:28:22","4","577","U10-Forward","2017-10-02 12:38:41","29180","4315","2153","1036","50636010","50636298","2018-06-01 04:14:09","2","93","<p>I have a unique requirement , where i need mean of common columns (per row) from two dataframes.</p>

<p>I can not think of a pythonic way of doing this. I know i can loop through two data frames and find common columns  and then get mean of rows where key matches.</p>

<p>Assuming I have below Data Frames:
DF1:</p>

<pre><code>Key A   B   C   D   E
K1  2   3   4   5   8
K2  2   3   4   5   8
K3  2   3   4   5   8
K4  2   3   4   5   8
</code></pre>

<p>DF2:</p>

<pre><code>Key A   B   C   D
K1  4   7   4   7
K2  4   7   4   7
K3  4   7   4   7
K4  4   7   4   7
</code></pre>

<p>The result DF should be the mean values of the two DF , each column per row where Key matches. 
ResultDF:</p>

<pre><code> Key    A   B   C   D
    K1  3   5   4   6
    K2  3   5   4   6
    K3  3   5   4   6
    K4  3   5   4   6
</code></pre>

<p>I know i should put sample code here , but i can not think of any logic for achieving this till now. </p>
","1718956","","","Pandas Mean Across Two Data Frames on Similar Columns only","<python><pandas>","2","0","946"
"50636084","2018-06-01 04:24:19","1","","<p>try this,</p>

<pre><code>crit=[{""ip_src"": ""192.168.84.129"", ""ip_dst"": ""192.168.84.128"", ""label"": 1},{""ip_src"": ""192.168.1.101"", ""dst_port"": 19305, ""label"": 4}]

dictionary={}
for dic in crit:
    dictionary[dic['ip_src']]=dic['label']
df['label']=df['ip_src'].map(dictionary).fillna(0)
</code></pre>

<p>Input:</p>

<pre><code>           ip_src          ip_dst  ip_proto  frame_time_delta  payload_size  \
0  192.168.84.129  192.168.84.128      17.0          0.000000         172.0   
1     31.13.94.53   192.168.1.101      17.0          0.006656         176.0   
2   192.168.1.101     31.13.94.53      17.0          0.012948          72.0   

   src_port  dst_port  flow_dir  
0   52165.0   35456.0         1  
1   40002.0   52165.0         0  
2   52165.0   19305.0         1
</code></pre>

<p>Output:</p>

<pre><code>           ip_src          ip_dst  ip_proto  frame_time_delta  payload_size  \
0  192.168.84.129  192.168.84.128      17.0          0.000000         172.0   
1     31.13.94.53   192.168.1.101      17.0          0.006656         176.0   
2   192.168.1.101     31.13.94.53      17.0          0.012948          72.0   

   src_port  dst_port  flow_dir  label  
0   52165.0   35456.0         1    1.0  
1   40002.0   52165.0         0    0.0  
2   52165.0   19305.0         1    4.0 
</code></pre>

<p>Edit 1:</p>

<pre><code>l_crit = [{""ip_src"": ""192.168.84.129"", ""ip_dst"": ""192.168.84.128"", ""label"": 1},
          {""ip_src"": ""192.168.1.100"", ""ip_dst"": ""192.168.1.105"", ""dst_port"": 9999, ""label"": 1},
          {""ip_src"": ""192.168.1.101"", ""ip_dst"": ""104.44.195.76"", ""label"": 2},
          {""ip_src"": ""192.168.1.101"", ""ip_dst"": ""31.13.94.53"", ""ip_proto"": 17, ""label"": 3},
          {""ip_src"": ""192.168.1.101"", ""dst_port"": 19305, ""label"": 4}]


temp=pd.DataFrame()

l=[]
v=[]
for dic in l_crit:
    l.append(dic['ip_src'])
    v.append(dic['label'])
temp['ip_src']=l
temp['label']=v

df=pd.merge(df,temp,how='left',on=['ip_src'])
df['label']=df['label'].fillna(0)
</code></pre>

<p>Input:</p>

<pre><code>          ip_src          ip_dst  ip_proto  frame_time_delta  payload_size  \
0  192.168.84.129  192.168.84.128      17.0          0.000000         172.0   
1     31.13.94.53   192.168.1.101      17.0          0.006656         176.0   
2   192.168.1.101     31.13.94.53      17.0          0.012948          72.0   

   src_port  dst_port  flow_dir  
0   52165.0   35456.0         1  
1   40002.0   52165.0         0  
2   52165.0   19305.0         1
</code></pre>

<p>Output:</p>

<pre><code>           ip_src          ip_dst  ip_proto  frame_time_delta  payload_size  \
0  192.168.84.129  192.168.84.128      17.0          0.000000         172.0   
1     31.13.94.53   192.168.1.101      17.0          0.006656         176.0   
2   192.168.1.101     31.13.94.53      17.0          0.012948          72.0   
3   192.168.1.101     31.13.94.53      17.0          0.012948          72.0   
4   192.168.1.101     31.13.94.53      17.0          0.012948          72.0   

   src_port  dst_port  flow_dir  label  
0   52165.0   35456.0         1    1.0  
1   40002.0   52165.0         0    0.0  
2   52165.0   19305.0         1    2.0  
3   52165.0   19305.0         1    3.0  
4   52165.0   19305.0         1    4.0 
</code></pre>
","4684861","4684861","2018-06-01 16:49:37","8","3250","Mohamed Thasin ah","2015-03-18 10:49:38","4803","833","1351","258","50635844","50636084","2018-06-01 03:49:34","0","56","<p>This question could be a little tricky...</p>

<p>I have a function that labels a dataframe based on some values in its columns. The function receives as parameter, a dataframe and a dictionary. This dictionary has key-value pairs that indicate the columns(key) and the value that it have to have to be labeled with certain number. For example:</p>

<pre><code>{""ip_src"": ""192.168.84.129"", ""ip_dst"": ""192.168.84.128"", ""label"": 1}
</code></pre>

<p>when the column ""ip_src"" of the dataframe have the value ""192.168.84.129"" and the column ""ip_dst"" have the value ""192.168.84.128"", that rows have to be labeled whit a '1'.
The thing is that those conditions may vary, so I want to generalize the code, so I could pass several other conditions as:</p>

<pre><code>{""ip_src"": ""192.168.1.101"", ""dst_port"": 19305, ""label"": 4}
</code></pre>

<p>and so on.</p>

<p>I started with:</p>

<pre><code>def labeling(df, crit):
    for dic in crit:
        lbl = dic[""label""]
        del dic[""label""]
        conds = []
        pairs = len(dic)
        for key in dic:
            conds.append((df[key] == dic[key])) 
</code></pre>

<p>But I get stuck in the last line, because I can't figure how to concatenate the conditions and then apply them as: <code>df[conds] = lbl</code></p>

<p>Thanks!</p>

<p><strong>Edit:</strong></p>

<p><em>Input:</em></p>

<pre><code>   index         ip_src         ip_dst  ip_proto  frame_time_delta  \
0      0  192.168.84.129 192.168.84.128      17.0          0.000000   
1      1    31.13.94.53  192.168.1.101      17.0          0.006656   
2      2  192.168.1.101    31.13.94.53      17.0          0.012948   

   payload_size  src_port  dst_port  flow_dir  
0         172.0   52165.0   40002.0         1  
1         176.0   40002.0   52165.0         0  
2         172.0   52165.0   19305.0         1 
</code></pre>

<p><em>Output:</em></p>

<pre><code>       ip_src         ip_dst       ip_proto  frame_time_delta  \
0  192.168.84.129 192.168.84.128     17.0          0.000000   
1    31.13.94.53  192.168.1.101      17.0          0.006656   
2  192.168.1.101    31.13.94.53      17.0          0.012948   

   payload_size  src_port  dst_port  flow_dir   label
0         172.0   52165.0   35456.0         1    1 
1         176.0   40002.0   52165.0         0    0
2         172.0   52165.0   19305.0         1    4
</code></pre>

<p>Possible cases:</p>

<pre><code>l_crit = [{""ip_src"": ""192.168.84.129"", ""ip_dst"": ""192.168.84.128"", ""label"": 1},
          {""ip_src"": ""192.168.1.100"", ""ip_dst"": ""192.168.1.105"", ""dst_port"": 9999, ""label"": 1},
          {""ip_src"": ""192.168.1.101"", ""ip_dst"": ""104.44.195.76"", ""label"": 2},
          {""ip_src"": ""192.168.1.101"", ""ip_dst"": ""31.13.94.53"", ""ip_proto"": 17, ""label"": 3},
          {""ip_src"": ""192.168.1.101"", ""dst_port"": 19305, ""label"": 4}]
</code></pre>
","9164661","9164661","2018-06-01 16:07:32","Concatenate conditions","<python><pandas><concatenation><conditional-statements>","1","0","2821"
"50636105","2018-06-01 04:28:36","0","","<p>Traverse through alternate lines and write current and next line if current line does not contain ""city=seattle"". </p>

<pre><code>#read in my text file
file = open(""phonebook.txt"",""r"")
allLines = file.readlines()
file.close()

allLines = list(filter(None, allLines))

#rewrite my text file now
file = open(""phonebook.txt"",""w"")

#rewrite allLines into my text file but skip lines that contain seattle
for x in range(0, len(allLines), 2):
  if ""city=seattle"" not in allLines[x].casefold():
    file.write(allLines[x])
    try:
        file.write(allLines[x+1])
    except IndexError:
        print('end')
file.close()
</code></pre>
","8472377","","","0","634","Austin","2017-08-16 11:54:23","18860","1780","141","2099","50635963","50636105","2018-06-01 04:07:28","1","32","<p>For example, if I have a .txt file with basic telephone book information and I want to remove lines that contain something like ""city=Seattle"" from the said txt file. However, some names may contain a few extra lines or a note after the entry, which I want gone as well. Such as the following:</p>

<blockquote>
  <p>Name=John Doe, Address= 25 Main St, City=Denver,Phone= 1-310-999-9999</p>
  
  <p>Note: John has a dog</p>
  
  <p>Name=Jane Doe, Address= 12 Richard Rd, City=Seattle, Phone=
  1-310-999-9999</p>
  
  <p>Note: Jane is allergic to nuts</p>
  
  <p>Name=Michael Smith, Address= 25 California BLVD, City=Los Angeles, Phone=
  1-310-999-9999</p>
</blockquote>

<p>After the script.py runs (and removing listings from seattle)</p>

<blockquote>
  <p>Name=John Doe, Address= 25 California BLVD, City=Denver, Phone=
  1-310-999-9999</p>
  
  <p>Note: John has a dog</p>
  
  <p>Name=Michael Smith, Address= 25 California BLVD, City=Los Angeles, Phone=
  1-310-999-9999*</p>
</blockquote>

<p>This is what I have, but I don't know how to make it to deal with some names that may have note or log entries I want deleted</p>

<pre><code>#read in my text file
file = open(""phonebook.txt"",""r"")
allLines = file.readlines()
file.close()
#rewrite my text file now
file = open(""phonebook.txt"",""w"")

#rewrite allLines into my text file but skip lines that contain seattle
for line in allLines:
  if ""seattle"" not in line:
    file.write(line)
</code></pre>
","7212344","","","How to read a text file and delete it and associated lines, if it contains a certain string value?","<python><python-3.x>","1","1","1460"
"50636112","2018-06-01 04:29:02","1","","<p>Here is an approach with functions that separate each of the tasks:</p>

<p>You first need to build a data structure that associates a name to the values so they can later be placed on the grid.<br>
Then you must build a grid of the correct size.<br>
Then place the labels at the place indicated by the values.<br>
And finally assemble a representation of the grid that can be printed.</p>

<p>It would probably be more suitable to build a class to encapsulate the work, but maybe it is more clear, as a first step, that way.</p>

<pre><code>def make_grid(items):
    max_col = max(x for x, _ in items) + 1
    max_row = max(y for _, y in items) + 1
    return [[' ' for dummy_col in range(max_col)] for dummy_row in range(max_row)]


def place_on_grid(grid, val, key):
    col, row = val
    grid[row][col] = key


def repr_grid(grid):
    return '\n'.join('|' + '|'.join(sub + ['']) for sub in grid)


def place_items_on_grid(data):
    grid = make_grid(data.values())
    for k, v in data.items():
        place_on_grid(grid, v, k)
    print(repr_grid(grid))


p = (0,0)
v = (2,3)
t = (5,4)
data = {k: val for k, val in zip(('p', 'v', 't'), (p, v, t))}

place_items_on_grid(data) 
</code></pre>

<h3>output:</h3>

<pre><code>|p| | | | | |
| | | | | | |
| | | | | | |
| | |v| | | |
| | | | | |t|
</code></pre>
","2875563","2875563","2018-06-01 04:46:07","0","1315","Reblochon Masque","2013-10-13 07:06:56","23302","2751","885","2671","50635956","","2018-06-01 04:06:31","-1","82","<p>So I want to take various coordinates with variables assigned to them and place them into a grid.</p>

<p>For example:</p>

<pre><code>p = (0,0)
v = (2,3)
t = (5,4)
</code></pre>

<p>Would output:</p>

<pre><code>|p| | | | | |
| | | | | | |
| | | | | | |
| | |v| | | |
| | | | | |t|
</code></pre>

<p>So obviously I could use print and go line by line but is there a way to 1. Do this more efficiently and 2. Make my grid adaptable for bigger coordinates?</p>
","9644112","","","Outputting coordinates onto a grid python","<python><python-3.x>","1","3","463"
"50636123","2018-06-01 04:31:07","1","","<pre><code># Creating a data frame with 4 variables and 400 observations
np.random.seed(42)
randomdata = pd.DataFrame(np.random.randn(400, 4))
randomdata.head()

        0           1           2          3
0    0.496714   -0.138264    0.647689    1.523030
1   -0.234153   -0.234137    1.579213    0.767435
2   -0.469474    0.542560   -0.463418   -0.465730
3    0.241962   -1.913280   -1.724918   -0.562288
4   -1.012831    0.314247   -0.908024   -1.412304

# Cap and floor for one variable
randomdata[0].clip(lower=-0.5, upper=0.5)

# Cap and floor entire dataframe
clean_df = randomdata.clip(lower=-0.5, upper=0.5)
clean_df.head()

       0            1           2          3
0    0.496714   -0.138264    0.500000    0.50000
1   -0.234153   -0.234137    0.500000    0.50000
2   -0.469474    0.500000   -0.463418   -0.46573
3    0.241962   -0.500000   -0.500000   -0.50000
4   -0.500000    0.314247   -0.500000   -0.50000
</code></pre>
","7624057","","","1","938","Ram","2017-02-26 05:25:03","95","10","18","0","50635540","","2018-06-01 03:04:17","-5","53","<p>I've tried but am not able to figure this one out. I have to change all the data that is greater than 3 to 3, and less than -3 to -3 in the dataframe below. </p>

<pre><code>np.random.seed(42)
randomdata = DataFrame(np.random.randn(400, 4))
</code></pre>

<p>I've tried for loops, .loc, .where and nothing seems to work. </p>
","9879011","8708364","2018-06-01 03:32:13","Set the values greater than x to x, and the values less than -x to -x","<python><pandas><random>","2","1","329"
"50636138","2018-06-01 04:32:54","10","","<p>You also need to add the static directory to your <code>urls.py</code> file. Add the following:</p>

<pre><code>from django.conf import settings
from django.conf.urls.static import static

urlpatterns = [
    ...
] + static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)
</code></pre>
","5728276","","","0","298","Colton Hicks","2015-12-29 19:42:34","791","62","85","1","39291223","39291292","2016-09-02 11:26:25","10","9817","<p>I just deployed my first Django app on Heroku but I notice that it doesn't have any CSS like when I runserver on the local machine. I know there's something wrong with static files but I don't understand much about it even when I already read <a href=""https://docs.djangoproject.com/en/1.10/howto/static-files/#serving-static-files-in-development"" rel=""noreferrer"">the docs</a>. I can do</p>

<p><code>python3 manage.py collectstatic</code></p>

<p>to create a static folder but I don't know where to put it and how to change the DIRS in settings.py. I really need some help to get rid of it.</p>

<p><a href=""https://i.stack.imgur.com/1NtBa.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1NtBa.png"" alt=""root directory""></a></p>

<p>settings.py:</p>

<pre><code>DEBUG = True

INSTALLED_APPS = [
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'household_management',
]

TEMPLATES = [
    {
        'BACKEND': 'django.template.backends.django.DjangoTemplates',
        'DIRS': [os.path.join(BASE_DIR, 'templates')],
        'APP_DIRS': True,
        'OPTIONS': {
            'context_processors': [
                'django.template.context_processors.debug',
                'django.template.context_processors.request',
                'django.contrib.auth.context_processors.auth',
                'django.contrib.messages.context_processors.messages',
            ],
        },
    },
]

STATIC_ROOT = 'static'
STATIC_URL = '/static/'
</code></pre>

<p>heroku logs:</p>

<pre><code>2016-09-02T10:42:09.461124+00:00 heroku[router]: at=info method=GET path=""/"" host=peaceful-earth-63194.herokuapp.com request_id=33fc071d-344c-47e7-8721-919ba6d5df65 fwd=""14.191.217.103"" dyno=web.1 connect=2ms service=53ms status=302 bytes=400
2016-09-02T10:42:09.760323+00:00 heroku[router]: at=info method=GET path=""/admin/login/?next=/"" host=peaceful-earth-63194.herokuapp.com request_id=c050edcd-02d9-4c39-88ba-8a16be692843 fwd=""14.191.217.103"" dyno=web.1 connect=1ms service=45ms status=200 bytes=2184
2016-09-02T10:42:10.037370+00:00 heroku[router]: at=info method=GET path=""/static/admin/css/login.css"" host=peaceful-earth-63194.herokuapp.com request_id=ec43016a-09b7-499f-a84b-b8024577b717 fwd=""14.191.217.103"" dyno=web.1 connect=2ms service=9ms status=404 bytes=4569
2016-09-02T10:42:10.047224+00:00 heroku[router]: at=info method=GET path=""/static/admin/css/base.css"" host=peaceful-earth-63194.herokuapp.com request_id=6570ee02-3b78-44f4-9ab9-0e80b706ea40 fwd=""14.191.217.103"" dyno=web.1 connect=1ms service=16ms status=404 bytes=4566
2016-09-02T10:42:10.030726+00:00 app[web.1]: Not Found: /static/admin/css/login.css
2016-09-02T10:42:10.043743+00:00 app[web.1]: Not Found: /static/admin/css/base.css
2016-09-02T10:48:56.593180+00:00 heroku[api]: Deploy d1d39dc by huyvohcmc@gmail.com
2016-09-02T10:48:56.593290+00:00 heroku[api]: Release v21 created by huyvohcmc@gmail.com
2016-09-02T10:48:56.803122+00:00 heroku[slug-compiler]: Slug compilation started
2016-09-02T10:48:56.803127+00:00 heroku[slug-compiler]: Slug compilation finished
2016-09-02T10:48:56.893962+00:00 heroku[web.1]: Restarting
2016-09-02T10:48:56.894722+00:00 heroku[web.1]: State changed from up to starting
2016-09-02T10:48:59.681267+00:00 heroku[web.1]: Stopping all processes with SIGTERM
2016-09-02T10:49:00.418357+00:00 app[web.1]: [2016-09-02 17:49:00 +0000] [9] [INFO] Worker exiting (pid: 9)
2016-09-02T10:49:00.418377+00:00 app[web.1]: [2016-09-02 17:49:00 +0000] [10] [INFO] Worker exiting (pid: 10)
2016-09-02T10:49:00.418393+00:00 app[web.1]: [2016-09-02 10:49:00 +0000] [3] [INFO] Handling signal: term
2016-09-02T10:49:00.477684+00:00 app[web.1]: [2016-09-02 10:49:00 +0000] [3] [INFO] Shutting down: Master
2016-09-02T10:49:00.594623+00:00 heroku[web.1]: Process exited with status 0
2016-09-02T10:49:00.607775+00:00 heroku[web.1]: Starting process with command `gunicorn assignment.wsgi --log-file -`
2016-09-02T10:49:02.911936+00:00 app[web.1]: [2016-09-02 10:49:02 +0000] [3] [INFO] Starting gunicorn 19.6.0
2016-09-02T10:49:02.912529+00:00 app[web.1]: [2016-09-02 10:49:02 +0000] [3] [INFO] Listening at: http://0.0.0.0:18162 (3)
2016-09-02T10:49:02.917427+00:00 app[web.1]: [2016-09-02 10:49:02 +0000] [9] [INFO] Booting worker with pid: 9
2016-09-02T10:49:02.912655+00:00 app[web.1]: [2016-09-02 10:49:02 +0000] [3] [INFO] Using worker: sync
2016-09-02T10:49:02.980208+00:00 app[web.1]: [2016-09-02 10:49:02 +0000] [10] [INFO] Booting worker with pid: 10
2016-09-02T10:49:04.228057+00:00 heroku[web.1]: State changed from starting to up
2016-09-02T10:53:41.572630+00:00 heroku[router]: at=info method=GET path=""/"" host=peaceful-earth-63194.herokuapp.com request_id=68c0b216-2084-46c8-9be5-b7e5aacaa590 fwd=""14.191.217.103"" dyno=web.1 connect=0ms service=42ms status=302 bytes=400
2016-09-02T10:53:41.880217+00:00 heroku[router]: at=info method=GET path=""/admin/login/?next=/"" host=peaceful-earth-63194.herokuapp.com request_id=17b91dc2-ba06-482c-8af0-e7b015fe2077 fwd=""14.191.217.103"" dyno=web.1 connect=0ms service=41ms status=200 bytes=2184
2016-09-02T10:53:42.156295+00:00 heroku[router]: at=info method=GET path=""/static/admin/css/base.css"" host=peaceful-earth-63194.herokuapp.com request_id=40dec62d-8c4a-4af6-8e0f-8053fe8379b9 fwd=""14.191.217.103"" dyno=web.1 connect=0ms service=9ms status=404 bytes=4566
2016-09-02T10:53:42.157491+00:00 heroku[router]: at=info method=GET path=""/static/admin/css/login.css"" host=peaceful-earth-63194.herokuapp.com request_id=3a29f200-c185-4344-a6e1-5af35e5d120e fwd=""14.191.217.103"" dyno=web.1 connect=0ms service=17ms status=404 bytes=4569
2016-09-02T10:53:42.164162+00:00 app[web.1]: Not Found: /static/admin/css/base.css
2016-09-02T10:53:42.177480+00:00 app[web.1]: Not Found: /static/admin/css/login.css
2016-09-02T11:01:19.031353+00:00 heroku[api]: Deploy 2beb15a by huyvohcmc@gmail.com
2016-09-02T11:01:19.031444+00:00 heroku[api]: Release v22 created by huyvohcmc@gmail.com
2016-09-02T11:01:19.262522+00:00 heroku[slug-compiler]: Slug compilation started
2016-09-02T11:01:19.262528+00:00 heroku[slug-compiler]: Slug compilation finished
2016-09-02T11:01:19.426837+00:00 heroku[web.1]: Restarting
2016-09-02T11:01:19.427455+00:00 heroku[web.1]: State changed from up to starting
2016-09-02T11:01:22.141325+00:00 heroku[web.1]: Stopping all processes with SIGTERM
2016-09-02T11:01:22.545379+00:00 heroku[web.1]: Starting process with command `gunicorn assignment.wsgi --log-file -`
2016-09-02T11:01:22.754067+00:00 app[web.1]: [2016-09-02 18:01:22 +0000] [9] [INFO] Worker exiting (pid: 9)
2016-09-02T11:01:22.754077+00:00 app[web.1]: [2016-09-02 18:01:22 +0000] [10] [INFO] Worker exiting (pid: 10)
2016-09-02T11:01:22.757599+00:00 app[web.1]: [2016-09-02 11:01:22 +0000] [3] [INFO] Handling signal: term
2016-09-02T11:01:22.763197+00:00 app[web.1]: [2016-09-02 11:01:22 +0000] [3] [INFO] Shutting down: Master
2016-09-02T11:01:22.880977+00:00 heroku[web.1]: Process exited with status 0
2016-09-02T11:01:24.628348+00:00 app[web.1]: [2016-09-02 11:01:24 +0000] [3] [INFO] Starting gunicorn 19.6.0
2016-09-02T11:01:24.628921+00:00 app[web.1]: [2016-09-02 11:01:24 +0000] [3] [INFO] Listening at: http://0.0.0.0:34235 (3)
2016-09-02T11:01:24.629075+00:00 app[web.1]: [2016-09-02 11:01:24 +0000] [3] [INFO] Using worker: sync
2016-09-02T11:01:24.636198+00:00 app[web.1]: [2016-09-02 11:01:24 +0000] [9] [INFO] Booting worker with pid: 9
2016-09-02T11:01:24.722355+00:00 app[web.1]: [2016-09-02 11:01:24 +0000] [10] [INFO] Booting worker with pid: 10
2016-09-02T11:01:26.271435+00:00 heroku[web.1]: State changed from starting to up
2016-09-02T11:01:27.930795+00:00 heroku[router]: at=info method=GET path=""/"" host=peaceful-earth-63194.herokuapp.com request_id=a844ef4b-a2d1-44fe-af0e-09c76cb0e034 fwd=""14.191.217.103"" dyno=web.1 connect=0ms service=46ms status=302 bytes=400
2016-09-02T11:01:28.363163+00:00 heroku[router]: at=info method=GET path=""/admin/login/?next=/"" host=peaceful-earth-63194.herokuapp.com request_id=31c0823a-466f-4363-b550-3c81681305f5 fwd=""14.191.217.103"" dyno=web.1 connect=0ms service=171ms status=200 bytes=2184
2016-09-02T11:01:28.716801+00:00 heroku[router]: at=info method=GET path=""/static/admin/css/base.css"" host=peaceful-earth-63194.herokuapp.com request_id=2d1b8bb2-9ab3-49f7-b557-a54eed996547 fwd=""14.191.217.103"" dyno=web.1 connect=0ms service=8ms status=404 bytes=4566
2016-09-02T11:01:28.693936+00:00 heroku[router]: at=info method=GET path=""/static/admin/css/login.css"" host=peaceful-earth-63194.herokuapp.com request_id=24aa1eed-aa87-4854-ab35-1604e8393b9d fwd=""14.191.217.103"" dyno=web.1 connect=0ms service=18ms status=404 bytes=4569
2016-09-02T11:01:28.681948+00:00 app[web.1]: Not Found: /static/admin/css/base.css
2016-09-02T11:01:28.692958+00:00 app[web.1]: Not Found: /static/admin/css/login.css
2016-09-02T11:12:43.686922+00:00 heroku[api]: Deploy 63085e6 by huyvohcmc@gmail.com
2016-09-02T11:12:43.687037+00:00 heroku[api]: Release v23 created by huyvohcmc@gmail.com
2016-09-02T11:12:43.951987+00:00 heroku[slug-compiler]: Slug compilation started
2016-09-02T11:12:43.951998+00:00 heroku[slug-compiler]: Slug compilation finished
2016-09-02T11:12:43.926959+00:00 heroku[web.1]: Restarting
2016-09-02T11:12:43.929107+00:00 heroku[web.1]: State changed from up to starting
2016-09-02T11:12:46.931285+00:00 heroku[web.1]: Starting process with command `gunicorn assignment.wsgi --log-file -`
2016-09-02T11:12:47.860591+00:00 heroku[web.1]: Stopping all processes with SIGTERM
2016-09-02T11:12:48.729601+00:00 app[web.1]: [2016-09-02 18:12:48 +0000] [10] [INFO] Worker exiting (pid: 10)
2016-09-02T11:12:48.729617+00:00 app[web.1]: [2016-09-02 18:12:48 +0000] [9] [INFO] Worker exiting (pid: 9)
2016-09-02T11:12:48.729623+00:00 app[web.1]: [2016-09-02 11:12:48 +0000] [3] [INFO] Handling signal: term
2016-09-02T11:12:48.775112+00:00 app[web.1]: [2016-09-02 11:12:48 +0000] [3] [INFO] Shutting down: Master
2016-09-02T11:12:48.890301+00:00 heroku[web.1]: Process exited with status 0
2016-09-02T11:12:48.839674+00:00 app[web.1]: [2016-09-02 11:12:48 +0000] [3] [INFO] Starting gunicorn 19.6.0
2016-09-02T11:12:48.840093+00:00 app[web.1]: [2016-09-02 11:12:48 +0000] [3] [INFO] Listening at: http://0.0.0.0:20001 (3)
2016-09-02T11:12:48.840166+00:00 app[web.1]: [2016-09-02 11:12:48 +0000] [3] [INFO] Using worker: sync
2016-09-02T11:12:48.843687+00:00 app[web.1]: [2016-09-02 11:12:48 +0000] [9] [INFO] Booting worker with pid: 9
2016-09-02T11:12:48.939210+00:00 app[web.1]: [2016-09-02 11:12:48 +0000] [10] [INFO] Booting worker with pid: 10
2016-09-02T11:12:50.565750+00:00 heroku[web.1]: State changed from starting to up
2016-09-02T11:13:00.439745+00:00 heroku[router]: at=info method=GET path=""/"" host=peaceful-earth-63194.herokuapp.com request_id=c30b47e6-fbb8-4412-9242-5fe37217026a fwd=""14.191.217.103"" dyno=web.1 connect=0ms service=49ms status=400 bytes=199
2016-09-02T11:14:01.686661+00:00 heroku[api]: Deploy c149525 by huyvohcmc@gmail.com
2016-09-02T11:14:01.686965+00:00 heroku[api]: Release v24 created by huyvohcmc@gmail.com
2016-09-02T11:14:02.189063+00:00 heroku[slug-compiler]: Slug compilation started
2016-09-02T11:14:02.189073+00:00 heroku[slug-compiler]: Slug compilation finished
2016-09-02T11:14:02.466456+00:00 heroku[web.1]: Restarting
2016-09-02T11:14:02.467005+00:00 heroku[web.1]: State changed from up to starting
2016-09-02T11:14:04.713176+00:00 heroku[web.1]: Stopping all processes with SIGTERM
2016-09-02T11:14:05.259388+00:00 app[web.1]: [2016-09-02 18:14:05 +0000] [10] [INFO] Worker exiting (pid: 10)
2016-09-02T11:14:05.260345+00:00 app[web.1]: [2016-09-02 11:14:05 +0000] [3] [INFO] Handling signal: term
2016-09-02T11:14:05.265937+00:00 app[web.1]: [2016-09-02 18:14:05 +0000] [9] [INFO] Worker exiting (pid: 9)
2016-09-02T11:14:05.317647+00:00 app[web.1]: [2016-09-02 11:14:05 +0000] [3] [INFO] Shutting down: Master
2016-09-02T11:14:05.411311+00:00 heroku[web.1]: Process exited with status 0
2016-09-02T11:14:06.581314+00:00 heroku[web.1]: Starting process with command `gunicorn assignment.wsgi --log-file -`
2016-09-02T11:14:10.282506+00:00 heroku[web.1]: State changed from starting to up
2016-09-02T11:14:10.187781+00:00 app[web.1]: [2016-09-02 11:14:10 +0000] [3] [INFO] Starting gunicorn 19.6.0
2016-09-02T11:14:10.188490+00:00 app[web.1]: [2016-09-02 11:14:10 +0000] [3] [INFO] Listening at: http://0.0.0.0:27446 (3)
2016-09-02T11:14:10.188627+00:00 app[web.1]: [2016-09-02 11:14:10 +0000] [3] [INFO] Using worker: sync
2016-09-02T11:14:10.211822+00:00 app[web.1]: [2016-09-02 11:14:10 +0000] [9] [INFO] Booting worker with pid: 9
2016-09-02T11:14:10.231978+00:00 app[web.1]: [2016-09-02 11:14:10 +0000] [10] [INFO] Booting worker with pid: 10
2016-09-02T11:14:29.714607+00:00 heroku[router]: at=info method=GET path=""/"" host=peaceful-earth-63194.herokuapp.com request_id=947ed6b9-b48a-48b1-8860-36846248acea fwd=""14.191.217.103"" dyno=web.1 connect=0ms service=153ms status=302 bytes=400
2016-09-02T11:14:30.522664+00:00 heroku[router]: at=info method=GET path=""/admin/login/?next=/"" host=peaceful-earth-63194.herokuapp.com request_id=b74c55bf-913c-4e0d-8d16-2b1f4f0cea13 fwd=""14.191.217.103"" dyno=web.1 connect=0ms service=561ms status=200 bytes=2184
2016-09-02T11:14:30.879732+00:00 heroku[router]: at=info method=GET path=""/static/admin/css/base.css"" host=peaceful-earth-63194.herokuapp.com request_id=769f989a-f051-4a89-a079-1d6acea3c185 fwd=""14.191.217.103"" dyno=web.1 connect=0ms service=86ms status=404 bytes=4566
2016-09-02T11:14:30.865971+00:00 heroku[router]: at=info method=GET path=""/static/admin/css/login.css"" host=peaceful-earth-63194.herokuapp.com request_id=b271b831-a4fb-4bdb-9f6a-e4d66297db88 fwd=""14.191.217.103"" dyno=web.1 connect=0ms service=75ms status=404 bytes=4569
2016-09-02T11:14:30.865501+00:00 app[web.1]: Not Found: /static/admin/css/login.css
2016-09-02T11:14:30.871110+00:00 app[web.1]: Not Found: /static/admin/css/base.css
</code></pre>
","","","","Django: Not Found static/admin/css","<python><django><heroku><deployment>","3","1","14054"
"50636165","2018-06-01 04:36:41","3","","<p>You actually can remove items from a list while iterating over it but <em>do read</em> links by @ReblochonMasque.</p>

<p>Here is one way of removing duplicates:</p>

<pre><code>def remove_items_from_list(ordered_list, temp):
    n = len(ordered_list)
    for i in range(n - 1, -1, -1):
        if ordered_list[i] in temp:
            del ordered_list[i]      
</code></pre>

<p>Then </p>

<pre><code>&gt;&gt;&gt; remove_items_from_list(my_list, temp)
&gt;&gt;&gt; print(my_list)
[2, 3, 4]
</code></pre>

<p>However, one of the easiest ways of solving your problem is to use sets:</p>

<pre><code>list(set(my_list) - set(temp))
</code></pre>

<p>When using this approach, order of items in the resulting list may be arbitrary. Also, this will create a <em>new</em> list instead of modifying an existing list object. If order is important - use list comprehension:</p>

<pre><code>[v for v in my_list if v not in temp]
</code></pre>
","8033585","","","1","935","AGN Gazer","2017-05-18 21:39:33","6041","460","707","281","50635960","50636165","2018-06-01 04:07:21","1","900","<p>I am curious. How can I correctly iterate through a list, compare two values and delete the duplicate if it exists. </p>

<p>Here I created a nested for loop:</p>

<pre><code>my_list =  [ 1, 2, 3, 4, 5 ]
temp = [1, 5, 6]

def remove_items_from_list(ordered_list, temp):
    # Removes all values, found in items_to_remove list, from my_list
        for j in range(0, len(temp)):
                for i in range(0, len(ordered_list)):
                        if ordered_list[i] == temp[j]:
                                ordered_list.remove(ordered_list[i])
</code></pre>

<p>But when I execute my my code I get an error:</p>

<pre><code>  File ""./lab3f.py"", line 15, in remove_items_from_list
    if ordered_list[i] == items_to_remove[j]:
</code></pre>

<p>can anyone explain why?</p>

<p>This question, wanted to me compare two lists with one another, and these lists have two different lengths. If an item in list a matched a value in list b, we wanted then to delete it from list a. </p>
","8729657","8729657","2018-06-01 16:09:03","Iterate through a list, compare values and remove duplicate - Python","<python><python-3.x>","4","4","993"
"50636175","2018-06-01 04:37:42","1","","<p>It's because <code>super(sub_class, instance).method()</code> means call the method <code>method</code> of the <em>parent</em> of <code>sub_class</code> on the instance <code>instance</code>. Since the parent of <code>B</code> is <code>A</code>, the result makes sense.</p>

<p>What you want is <code>super(C, self).bar()</code>.</p>
","532978","","","0","337","JCOC611","2010-12-06 22:59:19","13703","1712","1407","166","50636131","50636175","2018-06-01 04:31:50","0","17","<p>When I tried to call the <code>bar()</code> of <code>class B</code> from <code>class C</code>, which is a direct subclass of <code>B</code>, it turned out that the <code>bar()</code> of <code>class A</code> was called. But I explicitly required that the <code>B</code> version should be used. How can the method be resolved to that of <code>A</code>?</p>

<pre><code>class A(object):
    def bar(self):
        print('bar from A')

class B(A):
    def bar(self):
        print('bar from B')

class C(B):
    def bar(self):
        super(B, self).bar()

c = C()
# It should print ""bar from B""
c.bar()
# But actually it prints ""bar from A""
</code></pre>
","8039762","","","super() of python2.7 skips direct parents?","<python><inheritance><super>","1","0","655"
"50636179","2018-06-01 04:38:31","1","","<p>Easiest fix:</p>

<pre><code>for i, x in enumerate(zip(list1, list2)):
    a = list1[i*2]
    b = list1[i*2 + 1]
    print a, b
</code></pre>

<p>Output:</p>

<pre><code>a1 a2
a3 a4
a5 a6
</code></pre>
","9861733","","","1","205","jerrycheng","2018-05-29 02:33:42","159","17","4","2","50636133","50636179","2018-06-01 04:32:29","0","77","<p>I have below lists</p>

<pre><code>list1 = ['a1', 'a2', 'a3', 'a4', 'a5', 'a6']
list2 = [1, 2, 3]
</code></pre>

<p>Code:</p>

<pre><code>&gt;&gt;&gt; for i, x in enumerate(zip(list1, list2)):
...     a = list1[i]
...     b = list1[i + 1]
...     print a, b
...
</code></pre>

<p>output:</p>

<pre><code>a1 a2
a2 a3
a3 a4
</code></pre>

<p>Expected output:</p>

<pre><code>a1 a2
a3 a4
a5 a6
</code></pre>

<p>Please help</p>
","7874910","","","Python for loop to select elements in list","<python><python-2.7>","1","1","428"
"50636183","2018-06-01 04:38:58","2","","<p>If your cost doesn't decrease with ReLu activation, it seems like your network is stuck in the region where the input of ReLu is negative, so its output is a constant zero, and no graident flows back - the neuron is dead. </p>

<p>You can tackle this problem by using leaky ReLu instead of simple ReLu. You should also start training biases. With ReLu, it is recommended to initialize biases with small positive values, to avoid this dead neuron problem.</p>

<p>For some problems, it would also help to decrease learning rate and make the network deeper. Maybe, you would like to make learning rate adjustable, e.g. if the cost does not decrease, multiply LR by 0.5.</p>

<p>With leaky ReLu, trainable biases, and some refactoring, your model could look like this:</p>

<pre><code>import numpy as np
trng_input = np.random.uniform(size=(1000, 7))
trng_output = np.column_stack([np.sin(trng_input).sum(axis=1), np.cos(trng_input).sum(axis=1)])

LEAK = 0.0001

def relu(x):
    return x * (x &gt; 0) + LEAK * x * (x &lt; 0)

def reluprime(x):
    return (x&gt;0).astype(x.dtype) + LEAK * (x&lt;0).astype(x.dtype)


class Neural_Net():
    def __init__(self, data_input, data_output):
        self.data_input = data_input
        self.trng_output = trng_output
        self.nodes = np.array([7, 10, 2])
        self.LR = 0.00001
        self.weightinit()
        self.training(2000, self.LR)

    def weightinit(self):
        self.weights = [np.random.uniform(-1, 1, size=self.nodes[i:(i+2)]) for i in range(len(self.nodes) - 1)]
        self.biases = [np.random.uniform(0, 1, size=self.nodes[i+1]) for i in range(len(self.nodes) - 1)]

    def forward(self, data):
        self.Z = []
        self.A = [np.array(data)]
        for layer in range(len(self.weights)):
            self.Z.append(np.dot(self.A[layer], self.weights[layer]) + self.biases[layer])
            self.A.append(relu(self.Z[layer]))
        self.output = self.A[-1]
        return self.output

    def costFunction(self):
        self.totalcost = 0.5*np.sum((self.trng_output-self.output)**2, axis=0)
        return self.totalcost

    def costFunctionPrime(self):
        self.forward(self.data_input)
        self.delta = [[] for x in range(len(self.weights))]
        self.DcostDw = [[] for x in range(len(self.weights))]
        self.DcostDb = [[] for x in range(len(self.weights))]
        for layer in reversed(range(len(self.weights))):
            Zprime = reluprime(self.Z[layer])
            if layer == len(self.weights)-1:
                self.delta[layer] = np.multiply(-(self.trng_output-self.output), Zprime)
            else:
                self.delta[layer] = np.dot(self.delta[layer+1], self.weights[layer+1].T) * Zprime
            self.DcostDw[layer] = np.dot(self.A[layer].T, self.delta[layer])
            self.DcostDb[layer] = np.sum(self.delta[layer], axis=0)

    def backprop(self, LR):
        for layer in range(len(self.weights)):
            self.weights[layer] -= self.DcostDw[layer] * LR
            self.biases[layer] -= self.DcostDb[layer] * LR

    def training(self, iteration, LR):
        for i in range(iteration):
            self.costFunctionPrime()
            self.backprop(LR)
            if (i/100.0) == (i/100):
                print(self.costFunction())
        print(sum(self.costFunction())/len(self.costFunction()))

NN = Neural_Net(trng_input, trng_output)
</code></pre>
","6498293","6498293","2018-06-02 08:56:29","2","3397","David Dale","2016-06-22 09:04:21","5537","467","164","10","50534429","50636183","2018-05-25 17:43:24","2","225","<p>I made a FC neural network with numpy based on the video's of welch's lab but when I try to train it I seem to have exploding gradients at launch, which is weird, I will put down the whole code which is testable in python 3+. only costfunctionprime seem to break the gradient descent stuff going but I have no idea what is happening. Can someone smarter than me help?</p>

<p>EDIT: the trng_input and trng_output are not the one I use, I use a big dataset</p>

<pre><code>import numpy as np
import random

trng_input = [[random.random() for _ in range(7)] for _ in range(100)]
trng_output = [[random.random() for _ in range(2)] for _ in range(100)]

def relu(x):
    return x * (x &gt; 0)

def reluprime(x):
    return (x&gt;0).astype(x.dtype)


class Neural_Net():
    def __init__(self, data_input, data_output):
        self.data_input = data_input
        self.trng_output = trng_output
        self.bias = 0
        self.nodes = np.array([7, 2])
        self.LR = 0.01
        self.weightinit()
        self.training(1000, self.LR)

    def randomweight(self, n):
        output = []
        for i in range(n):
            output.append(random.uniform(-1,1))
        return output

    def weightinit(self):
        self.weights = []
        for n in range(len(self.nodes)-1):
            temp = []
            for _ in range(self.nodes[n]+self.bias):
                temp.append(self.randomweight(self.nodes[n+1]))
            self.weights.append(temp)
        self.weights = [np.array(tuple(self.weights[i])) for i in range(len(self.weights))]


    def forward(self, data):
        self.Z = []
        self.A = [np.array(data)]

        for layer in range(len(self.weights)):
            self.Z.append(np.dot(self.A[layer], self.weights[layer]))
            self.A.append(relu(self.Z[layer]))

        self.output = self.A[-1]
        return self.output

    def costFunction(self):
        self.totalcost = 0.5*sum((self.trng_output-self.output)**2)
        return self.totalcost

    def costFunctionPrime(self):
        self.forward(self.data_input)
        self.delta = [[] for x in range(len(self.weights))]
        self.DcostDw = [[] for x in range(len(self.weights))]

        for layer in reversed(range(len(self.weights))):
            Zprime = reluprime(self.Z[layer])
            if layer == len(self.weights)-1:
                self.delta[layer] = np.multiply(-(self.trng_output-self.output), Zprime)
            else:
                self.delta[layer] = np.dot(self.delta[layer+1], self.weights[layer+1].T) * Zprime
            self.DcostDw[layer] = np.dot(self.A[layer].T, self.delta[layer])

        return self.DcostDw

    def backprop(self, LR):
        self.DcostDw = (np.array(self.DcostDw)*LR).tolist()
        self.weights = (np.array(self.weights) - np.array(self.DcostDw)).tolist()

    def training(self, iteration, LR):
        for i in range(iteration):
            self.costFunctionPrime()
            self.backprop(LR)
            if (i/1000.0) == (i/1000):
                print(self.costFunction())
        print(sum(self.costFunction())/len(self.costFunction()))

NN = Neural_Net(trng_input, trng_output)
</code></pre>

<p>as asked, this is the expected result (result I got using the sigmoid activation function):</p>

<p><a href=""https://i.stack.imgur.com/en6ty.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/en6ty.jpg"" alt=""""></a></p>

<p>as you can see, the numbers are going down and thus the network is training.</p>

<p>this is the result using the relu activation function:</p>

<p><a href=""https://i.stack.imgur.com/wQrQq.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wQrQq.jpg"" alt=""""></a></p>

<p>Here, the network is stuck and isnt getting trained, it never gets trained using the relu activation function and would like to understand why</p>
","7974205","7974205","2018-05-31 23:24:42","backpropagation trouble; getting higher and higher total cost up until its infinity","<python><numpy><neural-network><backpropagation>","2","2","3839"
"50636247","2018-06-01 04:47:21","2","","<p>Set <code>no</code> as the index for <code>words</code> and then iterate over <code>sentences</code> using a list comprehension:</p>

<pre><code>v = words.set_index('no')['word']
sentences = [
    ' '.join(v.loc[i:j]) for i, j in zip(sentences['start'], sentences['stop'])
]
</code></pre>

<p></p>

<p>Or index agnostic:</p>

<pre><code>v = words['word'].tolist()
sentences = [
    ' '.join(v[i - 1:j - 1] for i, j in zip(sentences['start'], sentences['stop'])
]
</code></pre>

<p></p>

<pre><code>['cat in hat', 'the dog', 'in love ! &lt;3']
</code></pre>

<p>Saving to a file should be straightforward from here:</p>

<pre><code>with open('file.txt', 'w') as f:
    for sent in sentences:
        f.write(sent + '\n')
        f.write('***\n')
</code></pre>
","4909087","4909087","2018-06-01 05:07:34","4","762","cs95","2015-05-17 14:31:31","173545","54178","8947","7377","50636192","50636328","2018-06-01 04:40:19","3","79","<p>I have 2 data frames.
I need to read in values from one data frame based on values from another</p>

<p>words:</p>

<pre><code>words = pd.DataFrame()
words['no'] = [1,2,3,4,5,6,7,8,9]
words['word'] = ['cat', 'in', 'hat', 'the', 'dog', 'in', 'love', '!', '&lt;3']
words
</code></pre>

<p>Sentences:</p>

<pre><code>sentences =  pd.DataFrame()
sentences['no'] =[1,2,3]
sentences['start'] = [1, 4, 6]
sentences['stop'] = [3, 5, 9]
sentences
</code></pre>

<p>the desired output is in to a text file:</p>

<pre><code>cat in hat
***
the dog
***
in love ! &lt;3
</code></pre>

<p>however i cant get past this step, i have tried running the following code:</p>

<p>for x in sentances:
    print(words['word'][words['no'].between(sentences['start'], sentences['stop'], inclusive = True)</p>

<p>but am returned with this error</p>

<pre><code> File ""&lt;ipython-input-16-ae3f5333be66&gt;"", line 3
    print(words['word'][words['no'].between(sentences['start'], sentences['stop'], inclusive = True)
                                                                                                    ^
SyntaxError: unexpected EOF while parsing
</code></pre>
","9849954","","","reading date from a data frame based on conditions in a different data frame","<python><pandas>","2","1","1151"
"50636280","2018-06-01 04:51:51","0","","<p>Here is a quick walkthrough of how plotly online works!</p>

<p>First create an account in the <a href=""https://plot.ly/"" rel=""nofollow noreferrer"">plotly official site</a></p>

<p>Then after logging in, go to <strong>Settings -> API Key -> Regenerate</strong>, this will give you a new API key. From the page, you need to get your <strong>""User Name"" and ""API key""</strong></p>

<p>I am not sure which editor you are using, but make the following change and the graph will get generated.</p>

<p><strong>Before:</strong></p>

<pre><code>import plotly.plotly as py
</code></pre>

<p><strong>After:</strong></p>

<pre><code>import plotly.plotly as py
plotly.tools.set_credentials_file(username='&lt;&lt;Your username goes here&gt;&gt;', api_key='&lt;&lt;Your API key goes here&gt;&gt;')
</code></pre>

<p>Please let me know if this solves your issue too!</p>
","5924562","","","1","861","Naren Murali","2016-02-14 07:57:05","10576","999","2009","19","50633829","","2018-05-31 22:47:24","0","428","<p>I'd like to quickly visualise some 3D volumetric data (just a regular 3d tensor).
Are there any generic packages that will automatically make these types of plots? Ideally I'd like a slider that would allow me to slice through the volume.</p>

<p>Something like this will be great but somehow I can't get the example to work (trying to walk through the example results in ""Authentication credentials were not provided."" and it looks a bit complicated):
<a href=""https://plot.ly/python/visualizing-mri-volume-slices/"" rel=""nofollow noreferrer"">https://plot.ly/python/visualizing-mri-volume-slices/</a></p>
","3538339","","","Generic 3D Volumetric Plot in Python","<python><data-visualization><plotly>","1","1","608"
"50636285","2018-06-01 04:52:11","0","","<p>You have the NameError because you are referencing <code>url</code> in <code>urls.py</code> but haven't imported it. add the following line to <code>urls.py</code><br></p>

<pre><code>from django.conf.urls import url
</code></pre>
","8283848","","","0","234","JPG","2017-07-10 12:56:09","1","2613","916","164","50636257","50636302","2018-06-01 04:48:46","0","1099","<p>I am trying to get TinyMCE working in Django. Here is what I did:</p>

<ul>
<li>Using this package as a reference: <a href=""https://github.com/romanvm/django-tinymce4-lite"" rel=""nofollow noreferrer"">django-tinymce4-lite</a></li>
<li>Successfully ran <code>pip install django-tinymce4-lite</code>; package installs fine</li>
<li>Added tinymce to INSTALLED_APPS in settings.py </li>
</ul>

<p>Then here it gets tricky:</p>

<pre><code>Add tinymce.urls to urls.py for your project:

urlpatterns = [
    ...
    url(r'^tinymce/', include('tinymce.urls')),
    ...
]
</code></pre>

<p>When I do this, I get this error:</p>

<pre><code>url(r'^tinymce/', include('tinymce.urls')),  
NameError: name 'url' is not defined
</code></pre>

<p>I have tried the following:</p>

<ul>
<li>Restarting django</li>
<li>Instead of placing this in my project's urls.py I have tried my app's urls.py</li>
<li>I have tried to convert this to ""<em>path('tinymce/', include('tinymce.urls')),</em>"" because all other entries use 'path' and not 'url', but that didn't work either (ModuleNotFoundError: No module named 'tinymce.urls)</li>
<li>I have tried <a href=""https://github.com/aljosa/django-tinymce"" rel=""nofollow noreferrer"">another tinymce plugin</a> </li>
</ul>

<p>None of this helped. Any suggestions? </p>

<p><strong>UPDATE</strong></p>

<p>As per the suggestions, I updated url to path. Now I have a new error:</p>

<pre><code>ModuleNotFoundError: No module named 'tinymce.urls'
</code></pre>

<p>Here is my urls.py:</p>

<pre><code>from django.urls import include, path
from django.contrib import admin

from django.conf import settings
from django.conf.urls.static import static

urlpatterns = [
    path('', include('core.urls')),
    path('tinymce/', include('tinymce.urls')),
]

if settings.DEBUG:
    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)

urlpatterns += static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)
</code></pre>

<p>This error made me doubt if I had installed the plugin correctly. But it seems I have:</p>

<pre><code>pip install django-tinymce4-lite
Requirement already satisfied: django-tinymce4-lite in /usr/local/lib/python3.6/site-packages
Requirement already satisfied: Django&gt;=1.8.0 in /usr/local/lib/python3.6/site-packages (from django-tinymce4-lite)
Requirement already satisfied: jsmin in /usr/local/lib/python3.6/site-packages (from django-tinymce4-lite)
Requirement already satisfied: pytz in /usr/local/lib/python3.6/site-packages (from Django&gt;=1.8.0-&gt;django-tinymce4-lite)
</code></pre>
","","","2018-06-01 04:56:57","Django and TinyMCE: NameError: name 'url' is not defined","<python><django><tinymce>","3","1","2568"
"50636298","2018-06-01 04:53:16","2","","<p>Use <code>DataFrame.add</code> using <code>Key</code> as the indexes:</p>

<pre><code>df1.set_index('Key').add(df2.set_index('Key')).dropna(axis=1) / 2

     A  B  C  D
Key            
K1   3  5  4  6
K2   3  5  4  6
K3   3  5  4  6
K4   3  5  4  6
</code></pre>

<hr>

<p>Alternative with <code>concat</code> + <code>groupby</code>.</p>

<pre><code>pd.concat([df1, df2], axis=0).dropna(axis=1).groupby('Key').mean()

     A  B  C  D
Key            
K1   3  5  4  6
K2   3  5  4  6
K3   3  5  4  6
K4   3  5  4  6
</code></pre>
","4909087","","","0","531","cs95","2015-05-17 14:31:31","173545","54178","8947","7377","50636010","50636298","2018-06-01 04:14:09","2","93","<p>I have a unique requirement , where i need mean of common columns (per row) from two dataframes.</p>

<p>I can not think of a pythonic way of doing this. I know i can loop through two data frames and find common columns  and then get mean of rows where key matches.</p>

<p>Assuming I have below Data Frames:
DF1:</p>

<pre><code>Key A   B   C   D   E
K1  2   3   4   5   8
K2  2   3   4   5   8
K3  2   3   4   5   8
K4  2   3   4   5   8
</code></pre>

<p>DF2:</p>

<pre><code>Key A   B   C   D
K1  4   7   4   7
K2  4   7   4   7
K3  4   7   4   7
K4  4   7   4   7
</code></pre>

<p>The result DF should be the mean values of the two DF , each column per row where Key matches. 
ResultDF:</p>

<pre><code> Key    A   B   C   D
    K1  3   5   4   6
    K2  3   5   4   6
    K3  3   5   4   6
    K4  3   5   4   6
</code></pre>

<p>I know i should put sample code here , but i can not think of any logic for achieving this till now. </p>
","1718956","","","Pandas Mean Across Two Data Frames on Similar Columns only","<python><pandas>","2","0","946"
"50636302","2018-06-01 04:53:32","0","","<p>Since you are using django 2.0 you should use <code>path</code> instead of <code>url</code>:</p>

<pre><code>from django.urls import path

urlpatterns = [
    ...
    path('tinymce/', include('tinymce.urls')),
    ...
]
</code></pre>

<p>You can find more details <a href=""https://docs.djangoproject.com/en/2.0/topics/http/urls/"" rel=""nofollow noreferrer"">here</a>.</p>
","641249","","","5","373","neverwalkaloner","2011-03-02 13:25:37","28058","1244","1934","0","50636257","50636302","2018-06-01 04:48:46","0","1099","<p>I am trying to get TinyMCE working in Django. Here is what I did:</p>

<ul>
<li>Using this package as a reference: <a href=""https://github.com/romanvm/django-tinymce4-lite"" rel=""nofollow noreferrer"">django-tinymce4-lite</a></li>
<li>Successfully ran <code>pip install django-tinymce4-lite</code>; package installs fine</li>
<li>Added tinymce to INSTALLED_APPS in settings.py </li>
</ul>

<p>Then here it gets tricky:</p>

<pre><code>Add tinymce.urls to urls.py for your project:

urlpatterns = [
    ...
    url(r'^tinymce/', include('tinymce.urls')),
    ...
]
</code></pre>

<p>When I do this, I get this error:</p>

<pre><code>url(r'^tinymce/', include('tinymce.urls')),  
NameError: name 'url' is not defined
</code></pre>

<p>I have tried the following:</p>

<ul>
<li>Restarting django</li>
<li>Instead of placing this in my project's urls.py I have tried my app's urls.py</li>
<li>I have tried to convert this to ""<em>path('tinymce/', include('tinymce.urls')),</em>"" because all other entries use 'path' and not 'url', but that didn't work either (ModuleNotFoundError: No module named 'tinymce.urls)</li>
<li>I have tried <a href=""https://github.com/aljosa/django-tinymce"" rel=""nofollow noreferrer"">another tinymce plugin</a> </li>
</ul>

<p>None of this helped. Any suggestions? </p>

<p><strong>UPDATE</strong></p>

<p>As per the suggestions, I updated url to path. Now I have a new error:</p>

<pre><code>ModuleNotFoundError: No module named 'tinymce.urls'
</code></pre>

<p>Here is my urls.py:</p>

<pre><code>from django.urls import include, path
from django.contrib import admin

from django.conf import settings
from django.conf.urls.static import static

urlpatterns = [
    path('', include('core.urls')),
    path('tinymce/', include('tinymce.urls')),
]

if settings.DEBUG:
    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)

urlpatterns += static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)
</code></pre>

<p>This error made me doubt if I had installed the plugin correctly. But it seems I have:</p>

<pre><code>pip install django-tinymce4-lite
Requirement already satisfied: django-tinymce4-lite in /usr/local/lib/python3.6/site-packages
Requirement already satisfied: Django&gt;=1.8.0 in /usr/local/lib/python3.6/site-packages (from django-tinymce4-lite)
Requirement already satisfied: jsmin in /usr/local/lib/python3.6/site-packages (from django-tinymce4-lite)
Requirement already satisfied: pytz in /usr/local/lib/python3.6/site-packages (from Django&gt;=1.8.0-&gt;django-tinymce4-lite)
</code></pre>
","","","2018-06-01 04:56:57","Django and TinyMCE: NameError: name 'url' is not defined","<python><django><tinymce>","3","1","2568"
"50636320","2018-06-01 04:55:09","1","","<p>Yes, you must have to install Service Pack 1 in order to make it installed.
or you have to continue your stuff with Python 2.7.</p>

<p>Download Service Pack 1 from the link suggested by @jerrycheng</p>
","5870824","","","0","206","Parth Patel","2016-02-02 04:34:38","722","198","556","1","50575512","","2018-05-29 02:26:14","1","2614","<p>I am unable to install Python 3.6 on Windows 7 Professional Edition 32 bits the install dialog points me to the log file whose content is as follows:</p>

<pre><code>[1398:1318][2018-05-30T01:15:25]i001: Burn v3.10.3.3007, Windows v6.1 (Build 7600: Service Pack 0), path: C:\Users\sos\AppData\Local\Temp\{948239C0-AFB0-400B-8609-44F78A4DD47A}\.cr\python-3.6.5.exe
[1398:1318][2018-05-30T01:15:25]i000: Initializing string variable 'ActionLikeInstalling' to value 'Installing'
[1398:1318][2018-05-30T01:15:25]i000: Initializing string variable 'ActionLikeInstallation' to value 'Setup'
[1398:1318][2018-05-30T01:15:25]i000: Initializing string variable 'ShortVersion' to value '3.6'
[1398:1318][2018-05-30T01:15:25]i000: Initializing numeric variable 'ShortVersionNoDot' to value '36'
[1398:1318][2018-05-30T01:15:25]i000: Initializing string variable 'WinVer' to value '3.6-32'
[1398:1318][2018-05-30T01:15:25]i000: Initializing string variable 'WinVerNoDot' to value '36-32'
[1398:1318][2018-05-30T01:15:25]i000: Initializing numeric variable 'InstallAllUsers' to value '0'
[1398:1318][2018-05-30T01:15:25]i000: Initializing numeric variable 'InstallLauncherAllUsers' to value '1'
[1398:1318][2018-05-30T01:15:25]i000: Initializing string variable 'TargetDir' to value ''
[1398:1318][2018-05-30T01:15:25]i000: Initializing string variable 'DefaultAllUsersTargetDir' to value '[ProgramFilesFolder]Python[WinVerNoDot]'
[1398:1318][2018-05-30T01:15:25]i000: Initializing string variable 'TargetPlatform' to value 'x86'
[1398:1318][2018-05-30T01:15:25]i000: Initializing string variable 'DefaultJustForMeTargetDir' to value '[LocalAppDataFolder]Programs\Python\Python[WinVerNoDot]'
[1398:1318][2018-05-30T01:15:25]i000: Initializing string variable 'OptionalFeaturesRegistryKey' to value 'Software\Python\PythonCore\[WinVer]\InstalledFeatures'
[1398:1318][2018-05-30T01:15:25]i000: Initializing string variable 'TargetDirRegistryKey' to value 'Software\Python\PythonCore\[WinVer]\InstallPath'
[1398:1318][2018-05-30T01:15:25]i000: Initializing string variable 'DefaultCustomTargetDir' to value ''
[1398:1318][2018-05-30T01:15:25]i000: Initializing string variable 'InstallAllUsersState' to value 'enabled'
[1398:1318][2018-05-30T01:15:25]i000: Initializing string variable 'InstallLauncherAllUsersState' to value 'enabled'
[1398:1318][2018-05-30T01:15:25]i000: Initializing string variable 'CustomInstallLauncherAllUsersState' to value '[InstallLauncherAllUsersState]'
[1398:1318][2018-05-30T01:15:25]i000: Initializing string variable 'TargetDirState' to value 'enabled'
[1398:1318][2018-05-30T01:15:25]i000: Initializing string variable 'CustomBrowseButtonState' to value 'enabled'
[1398:1318][2018-05-30T01:15:25]i000: Initializing numeric variable 'Include_core' to value '1'
[1398:1318][2018-05-30T01:15:25]i000: Initializing numeric variable 'Include_exe' to value '1'
[1398:1318][2018-05-30T01:15:25]i000: Initializing numeric variable 'Include_dev' to value '1'
[1398:1318][2018-05-30T01:15:25]i000: Initializing numeric variable 'Include_lib' to value '1'
[1398:1318][2018-05-30T01:15:25]i000: Initializing numeric variable 'Include_test' to value '1'
[1398:1318][2018-05-30T01:15:25]i000: Initializing numeric variable 'Include_doc' to value '1'
[1398:1318][2018-05-30T01:15:25]i000: Initializing numeric variable 'Include_tools' to value '1'
[1398:1318][2018-05-30T01:15:25]i000: Initializing numeric variable 'Include_tcltk' to value '1'
[1398:1318][2018-05-30T01:15:25]i000: Initializing numeric variable 'Include_pip' to value '1'
[1398:1318][2018-05-30T01:15:25]i000: Initializing numeric variable 'Include_launcher' to value '1'
[1398:1318][2018-05-30T01:15:25]i000: Initializing string variable 'Include_launcherState' to value 'enabled'
[1398:1318][2018-05-30T01:15:25]i000: Initializing numeric variable 'Include_symbols' to value '0'
[1398:1318][2018-05-30T01:15:25]i000: Initializing numeric variable 'Include_debug' to value '0'
[1398:1318][2018-05-30T01:15:25]i000: Initializing numeric variable 'LauncherOnly' to value '0'
[1398:1318][2018-05-30T01:15:25]i000: Initializing numeric variable 'DetectedLauncher' to value '0'
[1398:1318][2018-05-30T01:15:25]i000: Initializing numeric variable 'DetectedOldLauncher' to value '0'
[1398:1318][2018-05-30T01:15:25]i000: Initializing numeric variable 'AssociateFiles' to value '1'
[1398:1318][2018-05-30T01:15:25]i000: Initializing numeric variable 'Shortcuts' to value '1'
[1398:1318][2018-05-30T01:15:25]i000: Initializing numeric variable 'PrependPath' to value '0'
[1398:1318][2018-05-30T01:15:25]i000: Initializing numeric variable 'CompileAll' to value '0'
[1398:1318][2018-05-30T01:15:25]i000: Initializing numeric variable 'SimpleInstall' to value '0'
[1398:1318][2018-05-30T01:15:25]i000: Initializing string variable 'SimpleInstallDescription' to value ''
[1398:1318][2018-05-30T01:15:25]i009: Command Line: '-burn.clean.room=C:\Users\sos\Downloads\python-3.6.5.exe -burn.filehandle.attached=160 -burn.filehandle.self=168'
[1398:1318][2018-05-30T01:15:25]i000: Setting string variable 'WixBundleOriginalSource' to value 'C:\Users\sos\Downloads\python-3.6.5.exe'
[1398:1318][2018-05-30T01:15:25]i000: Setting string variable 'WixBundleOriginalSourceFolder' to value 'C:\Users\sos\Downloads\'
[1398:1318][2018-05-30T01:15:25]i000: Setting string variable 'WixBundleLog' to value 'C:\Users\sos\AppData\Local\Temp\Python 3.6.5 (32-bit)_20180530011525.log'
[1398:1318][2018-05-30T01:15:25]i000: Setting string variable 'WixBundleName' to value 'Python 3.6.5 (32-bit)'
[1398:1318][2018-05-30T01:15:25]i000: Setting string variable 'WixBundleManufacturer' to value 'Python Software Foundation'
[1398:1318][2018-05-30T01:15:25]i000: Setting numeric variable 'CRTInstalled' to value 0
[1398:0874][2018-05-30T01:15:25]i000: Did not find C:\Users\sos\Downloads\unattend.xml
[1398:0874][2018-05-30T01:15:26]i000: Setting string variable 'ActionLikeInstalling' to value 'Installing'
[1398:0874][2018-05-30T01:15:26]i000: Setting string variable 'ActionLikeInstallation' to value 'Setup'
[1398:0874][2018-05-30T01:15:26]i000: Setting version variable 'WixBundleFileVersion' to value '3.6.5150.0'
[1398:0874][2018-05-30T01:15:26]e000: Detected Windows 7 RTM
[1398:0874][2018-05-30T01:15:26]e000: Service Pack 1 is required to continue installation
</code></pre>

<p>How can I solve this error ? Should I install a different version of Windows in order to make this work ? </p>
","9861536","","","Issues while installing Python 3.6 on Windows 7 : Service Pack 1 is required to continue installation","<python><windows-7><python-3.6>","2","0","6439"
"50636328","2018-06-01 04:55:59","1","","<p>one way to solve this,</p>

<pre><code>res=pd.DataFrame()
res['s']=sentences.apply(lambda x: ' '.join(words.iloc[(x['start']-1):(x['stop'])]['word']),axis=1)
res.to_csv('a.txt',index=False,header=False,line_terminator='\n***\n')
</code></pre>
","4684861","4684861","2018-06-01 05:19:11","2","246","Mohamed Thasin ah","2015-03-18 10:49:38","4803","833","1351","258","50636192","50636328","2018-06-01 04:40:19","3","79","<p>I have 2 data frames.
I need to read in values from one data frame based on values from another</p>

<p>words:</p>

<pre><code>words = pd.DataFrame()
words['no'] = [1,2,3,4,5,6,7,8,9]
words['word'] = ['cat', 'in', 'hat', 'the', 'dog', 'in', 'love', '!', '&lt;3']
words
</code></pre>

<p>Sentences:</p>

<pre><code>sentences =  pd.DataFrame()
sentences['no'] =[1,2,3]
sentences['start'] = [1, 4, 6]
sentences['stop'] = [3, 5, 9]
sentences
</code></pre>

<p>the desired output is in to a text file:</p>

<pre><code>cat in hat
***
the dog
***
in love ! &lt;3
</code></pre>

<p>however i cant get past this step, i have tried running the following code:</p>

<p>for x in sentances:
    print(words['word'][words['no'].between(sentences['start'], sentences['stop'], inclusive = True)</p>

<p>but am returned with this error</p>

<pre><code> File ""&lt;ipython-input-16-ae3f5333be66&gt;"", line 3
    print(words['word'][words['no'].between(sentences['start'], sentences['stop'], inclusive = True)
                                                                                                    ^
SyntaxError: unexpected EOF while parsing
</code></pre>
","9849954","","","reading date from a data frame based on conditions in a different data frame","<python><pandas>","2","1","1151"
"50636357","2018-06-01 04:59:23","0","","<p>I figured out that it had to do with security issues within my terminal settings. Simple fix by using an IDE.</p>
","4671846","","","0","117","Carbon","2015-03-14 22:25:58","73","21","198","0","50633716","","2018-05-31 22:34:25","0","158","<p>I have installed ODBC and done pip install teradata, but </p>

<p>After trying to connect to teradata session, I get this error:</p>

<pre><code>teradata.api.DatabaseError: (0, u'[HY000] [Teradata][ODBC Teradata Driver] Could not find security entry point')
</code></pre>

<p>Here is what my python script looks like:</p>

<pre><code> import teradata


udaExec = teradata.UdaExec(appName = 'table1', version='1.0', logConsole = False)

session = udaExec.connect(method = ""odbc"", system = ""db1"", username = ""user1"", password = ""pass1"", driver = ""Teradata"")
</code></pre>

<p>Any idea of what I am doing wrong? Thanks!</p>
","4671846","","","Trouble connecting to Teradata with teradata python module","<python><odbc><teradata><pyodbc>","1","0","624"
"50636358","2018-06-01 04:59:31","1","","<p>You can't <em>prevent</em> the module being assigned under its real name.  After all, the following has to set attributes <code>foo</code> <em>and</em> <code>bar</code> on the package module object:</p>

<pre><code># pkg/__init__.py
from .foo import bar
</code></pre>

<p>You can of course <code>del</code> the name after it's been added (by <code>import</code>ing it):</p>

<pre><code># pkg/__init__.py
from . import foo as bar
del foo
</code></pre>

<p>But watch out: it leads to strange situations like</p>

<pre><code>&gt;&gt;&gt; import pkg.foo
&gt;&gt;&gt; from pkg.foo import a
&gt;&gt;&gt; pkg.foo
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
AttributeError: module 'pkg' has no attribute 'foo'
&gt;&gt;&gt; import pkg.bar
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ImportError: No module named 'pkg.bar'
&gt;&gt;&gt; pkg.bar.a is a
True
&gt;&gt;&gt; from pkg.bar import a
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ImportError: No module named 'pkg.bar'
</code></pre>

<p>Of course this doesn't matter so much if the <em>status</em> of <code>pkg.bar</code> as a module is considered an implementation detail, so that no one will be issuing <code>import</code>s like these.  It also matters less if you're <em>adding</em> an alias without suppressing the true name.   (In your case, why not call <code>lex_c89.py</code> just <code>c89.py</code>?  The whole package is a lexer anyway&hellip;)   Even then, such hiding precludes the performance benefit of importing only the modules that you need, since the user can't indicate what they need.</p>
","8586227","","","1","1681","Davis Herring","2017-09-10 03:32:04","13348","1378","835","85","50633897","50636358","2018-05-31 22:55:53","1","68","<p>I have the following directory structure:</p>

<pre><code>project/
  \__ module/
        \__ __init__.py
        \__ stuff.py
</code></pre>

<p><br/>
The <code>__init__.py</code> file looks like this:</p>

<pre><code>from . import stuff as othername
</code></pre>

<p><br/>
However, when I open up the python interactive interpreter and import the module, <code>module</code>, and call <code>dir()</code> on the module, I get the following result:</p>

<pre><code>&gt;&gt;&gt; dir(module)
['__builtins__',
 '__cached__',
 ...
 'othername',
 'stuff']
</code></pre>

<p>As you can see, the name of the file, <code>stuff</code> (minus the .py extension), is still present.</p>

<p><br/>
Without simply changing the name of <code>stuff.py</code> to <code>othername.py</code>, how would I import <code>stuff</code> as <code>othername</code>, without also import <code>stuff</code> as <code>stuff</code>?</p>

<p><br/>
Also, on a sidenote, what's the best way to provide an alias for the same module?</p>

<p>Is this how is supposed to be done...</p>

<pre><code>from . import stuff as othername
aliasname = othername
</code></pre>

<p>...or is there another way that is considered the ""correct"" way to do it?</p>

<p><br/></p>

<h2><strong>UPDATE</strong></h2>

<p>I tried setting <code>__all__</code> manually within my <code>__init__.py</code> file, but the name of the file itself is still being included in the import.</p>

<p><code>__init__.py:</code></p>

<pre><code>from . import stuff as othername
from . import stuff as aliasname

__all__ = [ 'othername', 'aliasname' ]
</code></pre>

<p><br/>
I've managed to get the following to work, but I don't know if it would be considered ""good practice"" or if it would even provide consistent behavior:</p>

<p><code>__init__.py:</code></p>

<pre><code>from . import stuff as othername
from . import stuff as aliasname

del stuff
</code></pre>
","6637939","6637939","2018-05-31 23:26:49","How to stop Python 'import' from importing the filename as well?","<python><python-import>","1","14","1893"
"50636373","2018-06-01 05:01:33","1","","<p>Unfortunately, no.  Reading in files and operating on the lines read (such as json parsing or computation) is a CPU-bound operation, so there's no clever asyncio tactics to speed it up.  In <em>theory</em> one could utilize multiprocessing and multiple cores to read and process in parallel, but having multiple threads reading the same file is bound to cause major problems.  Because your file is so large, storing it all in memory and then parallelizing the computation is also going to be difficult.</p>

<p>Your best bet would be to head this problem off at the pass by partitioning the data (if possible) into multiple files, which could then open up safer doors to parallelism with multiple cores.  Sorry there isn't a better answer AFAIK.</p>
","3874512","","","0","753","BowlingHawk95","2014-07-24 19:52:40","613","36","49","3","50636059","","2018-06-01 04:21:38","2","1227","<p>I have a large file almost <code>20GB</code>, more than <strong>20 mln</strong> lines and each line represents separate serialized <strong>JSON</strong>.</p>

<p>Reading file <code>line by line</code> as a regular <code>loop</code> and performing manipulation on line data <strong>takes a lot of time</strong>. </p>

<p>Is there any <code>state of art</code> approach or <code>best practices</code> for reading large files in <strong>parallel</strong> with <strong>smaller chunks</strong> in order to make processing faster?</p>

<p>I'm using Python 3.6.X</p>
","4517646","","","How to read / process large files in parallel with Python","<python><multithreading><python-3.x>","2","3","563"
"50636381","2018-06-01 05:02:24","0","","<p>Assuming</p>

<pre><code>brackets=""a|b|c|d|e""
</code></pre>

<p>you could do</p>

<pre><code>[brackets[0:x] for x in range(3, len(brackets), 2)]
</code></pre>

<p>which gives</p>

<pre><code>['a|b', 'a|b|c', 'a|b|c|d']
</code></pre>
","5375464","5375464","2018-06-01 05:08:39","0","236","J...S","2015-09-25 08:46:34","4021","623","389","61","50636323","50636381","2018-06-01 04:55:41","-2","27","<pre><code>exp = a | b | c | d
brackets = set('[(()())]')

def evaluate():
        for i in brackets[:]:
            arr[i] = ('''1st 2 exp''')
            print(arr)
            i = i + 1
</code></pre>

<p>an expression is given <code>a|b|c|d|e</code>. Then there is an array where we have to store the 1st 2 values in <code>a[0]</code> and then the <code>(a[0]|c)</code> is stored in <code>a[1]</code>.
How can we attain this?</p>
","8839696","8839696","2018-06-01 05:47:11","evaluation of an expression recursively using array and stack","<python><arrays><python-3.x><stack>","1","1","433"
"50636437","2018-06-01 05:09:53","3","","<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>map</code></a> by <code>dict</code> created with <code>zip</code>:</p>

<pre><code>L1 = ['excellent', 'good', 'average', 'bad', 'very bad']
L2 = ['EX', 'G', 'Avg', 'B', 'VB']

df['col'] = df['col'].map(dict(zip(L1, L2)))
</code></pre>

<p><strong>Detail</strong>:</p>

<pre><code>print (dict(zip(L1, L2)))
{'excellent': 'EX', 'good': 'G', 'average': 'Avg', 'bad': 'B', 'very bad': 'VB'}
</code></pre>
","2901002","2901002","2018-06-01 05:20:09","2","527","jezrael","2013-10-20 20:27:26","427380","89269","18260","743","50636424","50636437","2018-06-01 05:08:10","-2","23","<p>A column in my pandas dataframe has one of the following values [excellent, good, average, bad, very bad]. I want to create a column in the same dataframe with each of <strong><em>[excellent, good, average, bad, very bad]</em></strong> being converted to <strong><em>[EX, G, Avg, B, VB]</em></strong> respectively.</p>

<p>I searched for it but most of the answers are explained using scikit-learn which I am not quite familiar with. Is there a way it can be done using pandas?</p>
","7484120","","","creating an encoded column of a column in pandas","<python><pandas><dataframe>","1","3","485"
"50636507","2018-06-01 05:16:59","1","","<p>Your code is equivalent to <code>itertools.product</code>:</p>

<pre><code>print(list(itertools.product(*ranges)))
</code></pre>
","8472377","","","1","132","Austin","2017-08-16 11:54:23","18860","1780","141","2099","50636404","50636507","2018-06-01 05:05:10","-1","291","<p>I want to create a code that can iterate over a dynamic number (N) of nested loops each with different range.
For example:</p>

<pre><code>N=3 
ranges=[[-3, -2, -1, 0, 1, 2, 3],
 [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5],
  [-3, -2, -1, 0, 1, 2, 3]]

for x in ranges[0]:
    for y in ranges[1]:
        for z in range[2]:
            variable=[x, y, z]
</code></pre>

<p>Im new to python. As I went over similar questions posted here I have the understanding that this can be done with recursion or itertools. However, none of the answers posted solve this problem for a different range at each level.
The closest posted question similar to mine was <a href=""https://stackoverflow.com/questions/49306528/variable-number-of-nested-for-loops-with-fixed-range"">Variable number of nested for loops with fixed range</a> . However, the answer posted by user633183 is coded in python 3.X and I am coding in python 2.7 so I couldn't implement it as some of its code does not work on python 2.7. 
Can you please help me to code this problem. Thanks!</p>
","9879210","9879210","2018-06-01 05:19:36","Python: Dynamic nested for loops each with different range","<python><loops><dynamic><nested-loops>","2","2","1045"
"50636525","2018-06-01 05:19:14","0","","<p>Thanks a lot, shumuels! I had not specified the key <strong>message=</strong> and that is why it was not working. I will go through the source code on Github in the future if I don't find answers in the documentation. </p>

<pre><code>from wtforms_components import Unique

  class SignupForm():
        email = EmailField('Email', validators=[Unique(User.email, get_session=lambda: db.session, message='THIS IS MY NEW ERROR MESSAGE.')])
</code></pre>
","6140832","","","0","455","jollycat","2016-03-31 15:30:05","32","12","3","0","50633747","","2018-05-31 22:38:21","0","437","<p>When using the <code>Unique()</code> validator of wtforms_components (with Flask and SQLAlchemy), the default error message that shows up in the form is ""Already exists"". How can I change this error message as easy as I can change the error message of the <code>DataRequired()</code> validator?</p>

<hr>

<p>The error message of the <code>DataRequired()</code> validator can be changed like this:</p>

<pre><code>from wtforms.validators import DataRequired


class SignupForm():
    email = EmailField('Email', validators=[DataRequired('THIS IS MY NEW ERROR MESSAGE.')])
</code></pre>

<p>This changes the default error message from ""This is required."" to ""THIS IS MY NEW ERROR MESSAGE."" I am looking for a similarly simple and elegant solution for changing the default error message of the <code>Unique()</code> validator of wtforms_components</p>

<hr>

<p>I was hoping that there is something like the following. However, it does not work:</p>

<pre><code>from wtforms_components import Unique

  class SignupForm():
        email = EmailField('Email', validators=[Unique(User.email, get_session=lambda: db.session, 'THIS IS MY NEW ERROR MESSAGE.')])
</code></pre>

<p>The way, the validator is normally works (with the default error message) is as follows:</p>

<pre><code>from wtforms_components import Unique

  class SignupForm():
        email = EmailField('Email', validators=[Unique(User.email, get_session=lambda: db.session)])
</code></pre>

<hr>

<p>I am using the following packages:</p>

<pre><code>Flask==0.10.1
Flask-SQLAlchemy==2.1
Flask-WTF==0.9.5
WTForms-Components==0.9.7
</code></pre>

<hr>

<p>I did not find a solution in the WTForms-Alchemy documentation (<a href=""http://wtforms-alchemy.readthedocs.io/en/latest/validators.html"" rel=""nofollow noreferrer"">http://wtforms-alchemy.readthedocs.io/en/latest/validators.html</a>). Also, I did not find a solution in the WTForms-Components documentation (<a href=""http://wtforms-components.readthedocs.io/en/latest/#unique-validator"" rel=""nofollow noreferrer"">http://wtforms-components.readthedocs.io/en/latest/#unique-validator</a>). There was no solution in this forum either.</p>

<p>Thanks a lot for your hints in advance.</p>
","6140832","9530790","2018-06-01 00:15:25","How to change the error message of the Unique() validator of wtforms_components?","<python><flask><flask-sqlalchemy><flask-wtforms>","2","0","2204"
"50636543","2018-06-01 05:21:20","1","","<p>So, if I am understanding your question correctly, you want the values being iterated over to be <code>[-3, -5, -3], [-2, -4, -2]...</code>.  This can be accomplished easily with zip function built into python:</p>

<pre><code>for x in zip(*ranges):
    # Do something with x
</code></pre>

<p>x will take on a tuple of all the first values, then a tuple of all the second values, etc, stopping when the shortest list ends.  Using this <code>*</code> splat notation avoids even having to know about the number of lists being combined.</p>
","3874512","","","0","542","BowlingHawk95","2014-07-24 19:52:40","613","36","49","3","50636404","50636507","2018-06-01 05:05:10","-1","291","<p>I want to create a code that can iterate over a dynamic number (N) of nested loops each with different range.
For example:</p>

<pre><code>N=3 
ranges=[[-3, -2, -1, 0, 1, 2, 3],
 [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5],
  [-3, -2, -1, 0, 1, 2, 3]]

for x in ranges[0]:
    for y in ranges[1]:
        for z in range[2]:
            variable=[x, y, z]
</code></pre>

<p>Im new to python. As I went over similar questions posted here I have the understanding that this can be done with recursion or itertools. However, none of the answers posted solve this problem for a different range at each level.
The closest posted question similar to mine was <a href=""https://stackoverflow.com/questions/49306528/variable-number-of-nested-for-loops-with-fixed-range"">Variable number of nested for loops with fixed range</a> . However, the answer posted by user633183 is coded in python 3.X and I am coding in python 2.7 so I couldn't implement it as some of its code does not work on python 2.7. 
Can you please help me to code this problem. Thanks!</p>
","9879210","9879210","2018-06-01 05:19:36","Python: Dynamic nested for loops each with different range","<python><loops><dynamic><nested-loops>","2","2","1045"
"50636561","2018-06-01 05:22:51","1","","<p>There are several possibilites, but first profile your code in find the bottlenecks. Maybe your processing does some slows things which can be speed up - which would be vastly preferable to multiprocessing.
If that does not help, you could try:</p>

<ol>
<li><p>Use another file format. Reading serialized json from text is not the fastest operation in the world. So you could store your data (for example in hdf5) which could speed up processing.</p></li>
<li><p>Implement multiple worker processes which can read portions of the file (worker1 reads lines  0 - 1million, worker2 1million - 2million etc). You can orchestrate that with joblib or celery, depending on your needs. Integrating the results is the challenge, there you have to see what your needs are (map-reduce style?). This is more difficult in python due to no real threading than in other languages, so maybe you could switch the language for that.</p></li>
</ol>
","1393543","","","0","934","Christian Sauer","2012-05-14 10:54:06","6100","849","318","45","50636059","","2018-06-01 04:21:38","2","1227","<p>I have a large file almost <code>20GB</code>, more than <strong>20 mln</strong> lines and each line represents separate serialized <strong>JSON</strong>.</p>

<p>Reading file <code>line by line</code> as a regular <code>loop</code> and performing manipulation on line data <strong>takes a lot of time</strong>. </p>

<p>Is there any <code>state of art</code> approach or <code>best practices</code> for reading large files in <strong>parallel</strong> with <strong>smaller chunks</strong> in order to make processing faster?</p>

<p>I'm using Python 3.6.X</p>
","4517646","","","How to read / process large files in parallel with Python","<python><multithreading><python-3.x>","2","3","563"
"50636595","2018-06-01 05:26:28","1","","<p>Just replace </p>

<pre><code>EC.visibility_of_element_located
</code></pre>

<p>with</p>

<pre><code>EC.presence_of_element_located
</code></pre>

<p>to be able to handle required checkbox</p>
","4549554","","","1","197","Andersson","2015-02-10 08:27:34","40727","5512","1461","1828","50636109","50636595","2018-06-01 04:28:50","1","53","<p>I am practicing some Selenium on the following website:</p>

<p>www.automationpractice.com</p>

<p>I have a couple basic tests I have started below:</p>

<pre><code>import unittest
from webdriver import Driver
from values import strings
from pageobjects.homescreen import Homescreen


class TestHomeScreen(unittest.TestCase):
    @classmethod
    def setUp(self):
        self.driver = Driver()
        self.driver.navigate(strings.base_url)

    def test_home_screen_components(self):
        home_screen = Homescreen(self.driver)
        home_screen.logo_present()

    def test_choose_dress(self):
        home_screen = Homescreen(self.driver)
        home_screen.choose_dress()

    @classmethod
    def tearDown(self):
        self.driver.instance.quit()
</code></pre>

<p>Those tests are being read from the following:</p>

<pre><code>from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from values import strings


class Homescreen:

    def __init__(self, driver):
        self.driver = driver

    def logo_present(self):
        self.logo = WebDriverWait(self.driver.instance, 10).until(
            EC.visibility_of_element_located((
                By.ID, ""header_logo"")))
        assert self.logo.is_displayed()

    def choose_dress(self):
        self.dresses = WebDriverWait(self.driver.instance, 5).until(
            EC.visibility_of_element_located((
                By.XPATH, '//*[@id=""block_top_menu""]/ul/li[2]/a')))
        self.dresses.click()
        self.casual_dresses = WebDriverWait(self.driver.instance, 10).until(
            EC.visibility_of_element_located((
                By.XPATH,'//input[@type=""checkbox"" and @id=""layered_category_9""]')))
</code></pre>

<p>the test_home_screen_components passes fine, but the test_choose_dress fails.  I have narrowed it down it is failing on the final XPATH, which is a checkbox for ""casual dresses"". It can't be found.  I have confirmed in Chrome that this XPATH is valid:</p>

<pre><code>self.casual_dresses = WebDriverWait(self.driver.instance, 10).until(
     EC.visibility_of_element_located((
         By.XPATH,'//input[@type=""checkbox"" and @id=""layered_category_9""]')))
</code></pre>

<p>on the following page:
<a href=""http://automationpractice.com/index.php?id_category=8&amp;controller=category#/categories-casual_dresses"" rel=""nofollow noreferrer"">http://automationpractice.com/index.php?id_category=8&amp;controller=category#/categories-casual_dresses</a></p>

<p>So I am not sure what the problem is.  Maybe I am missing something because it is embedded?</p>

<p>Also I know I need to add some Try/Except to my code eventually as well, I am just starting out with this stuff.</p>
","4439019","","","Difficulty narrowing down XPath of checkbox","<python><selenium><xpath>","3","0","2831"
"50636613","2018-06-01 05:28:15","0","","<p>It should be</p>

<pre><code>for i,item in enumerate(fileList):
    fileList[i] = item.replace(',', """").replace('.', """")
</code></pre>

<p>Without enumerate,</p>

<pre><code>for i in range(len(fileList)):
    fileList[i] = fileList[i].replace(',', """").replace('.', """")
</code></pre>
","5371551","5371551","2018-06-01 05:46:29","7","286","Shivam Singh","2015-09-24 10:03:46","1366","96","33","0","50636598","50636613","2018-06-01 05:26:33","-3","63","<p>I'm trying to figure out why the .replace function in python isn't functioning correctly. I have spent the entire day yesterday searching for an answer but alas have not found one.</p>

<p>I'm trying to open and read a file, copy it into a list, count the number of lines in the list and remove all the punctuation (ie , . ! ? etc). I can do everything except remove the punctuation (and I must use the .replace function instead of importing a module).</p>

<pre><code>with open('Small_text_file.txt', 'r') as myFile:        #adding lines from file to list
contents = myFile.readlines()
fileList= []
# punctuation = ['(', ')', '?', ':', ';', ',', '.', '!', '/', '""', ""'""]
for i in contents:
    fileList.append(i.rstrip())

print('The Statistics are:\n','Number of lines:', len(fileList)) #first part of question



for item in fileList:
    fileList = item.replace(',', """")
    fileList = item.replace('.', """")

print(fileList)
</code></pre>

<p>The ""Small text file"" is:</p>

<p><em>Lorem ipsum dolor sit amet, consectetur adipiscing elit.
Vivamus condimentum sagittis lacus? laoreet luctus ligula laoreet ut.
Vestibulum ullamcorper accumsan velit vel vehicula?
Proin tempor lacus arcu. Nunc at elit condimentum, semper nisi et, condimentum mi.
In venenatis blandit nibh at sollicitudin. Vestibulum dapibus mauris at orci maximus pellentesque.
Nullam id elementum ipsum. Suspendisse</em></p>

<p><strong>Running the code returns the following:</strong></p>

<p>The Statistics are:</p>

<p>Number of lines: 6</p>

<p>Nullam id elementum ipsum Suspendisse</p>

<p>So the code DOES remove the comma and period characters but it also removes the preceding 5 lines of the text and only prints the very last line. What am I doing wrong here?</p>
","9879377","","","Python .replace() function not working correctly","<python><python-3.x>","2","1","1745"
"50636615","2018-06-01 05:28:22","1","","<p>Please scroll the element <strong>casual dresses</strong> into view and then check the presence of the element</p>

<pre><code>driver.execute_script(""arguments[0].scrollIntoView();"", self.casual_dresses)
</code></pre>
","6496071","","","0","221","Monika","2016-06-21 20:14:25","619","176","0","5","50636109","50636595","2018-06-01 04:28:50","1","53","<p>I am practicing some Selenium on the following website:</p>

<p>www.automationpractice.com</p>

<p>I have a couple basic tests I have started below:</p>

<pre><code>import unittest
from webdriver import Driver
from values import strings
from pageobjects.homescreen import Homescreen


class TestHomeScreen(unittest.TestCase):
    @classmethod
    def setUp(self):
        self.driver = Driver()
        self.driver.navigate(strings.base_url)

    def test_home_screen_components(self):
        home_screen = Homescreen(self.driver)
        home_screen.logo_present()

    def test_choose_dress(self):
        home_screen = Homescreen(self.driver)
        home_screen.choose_dress()

    @classmethod
    def tearDown(self):
        self.driver.instance.quit()
</code></pre>

<p>Those tests are being read from the following:</p>

<pre><code>from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from values import strings


class Homescreen:

    def __init__(self, driver):
        self.driver = driver

    def logo_present(self):
        self.logo = WebDriverWait(self.driver.instance, 10).until(
            EC.visibility_of_element_located((
                By.ID, ""header_logo"")))
        assert self.logo.is_displayed()

    def choose_dress(self):
        self.dresses = WebDriverWait(self.driver.instance, 5).until(
            EC.visibility_of_element_located((
                By.XPATH, '//*[@id=""block_top_menu""]/ul/li[2]/a')))
        self.dresses.click()
        self.casual_dresses = WebDriverWait(self.driver.instance, 10).until(
            EC.visibility_of_element_located((
                By.XPATH,'//input[@type=""checkbox"" and @id=""layered_category_9""]')))
</code></pre>

<p>the test_home_screen_components passes fine, but the test_choose_dress fails.  I have narrowed it down it is failing on the final XPATH, which is a checkbox for ""casual dresses"". It can't be found.  I have confirmed in Chrome that this XPATH is valid:</p>

<pre><code>self.casual_dresses = WebDriverWait(self.driver.instance, 10).until(
     EC.visibility_of_element_located((
         By.XPATH,'//input[@type=""checkbox"" and @id=""layered_category_9""]')))
</code></pre>

<p>on the following page:
<a href=""http://automationpractice.com/index.php?id_category=8&amp;controller=category#/categories-casual_dresses"" rel=""nofollow noreferrer"">http://automationpractice.com/index.php?id_category=8&amp;controller=category#/categories-casual_dresses</a></p>

<p>So I am not sure what the problem is.  Maybe I am missing something because it is embedded?</p>

<p>Also I know I need to add some Try/Except to my code eventually as well, I am just starting out with this stuff.</p>
","4439019","","","Difficulty narrowing down XPath of checkbox","<python><selenium><xpath>","3","0","2831"
"50636617","2018-06-01 05:29:05","1","","<p>Not sure if I understood your question completely. Let me attempt the solution basis my understanding of the question</p>

<p>You have a variable to which a string is assigned</p>

<pre><code>name = ""V01""
</code></pre>

<p>You want to assign a data frame to the string</p>

<pre><code>my_dict = {}
my_dict[name] = pd.DataFrame({""Name"" : [""John"",""Simon""], ""Age"" : [21,30]})
</code></pre>

<p>Now you want to call that data frame without explicitly calling the string but the variable to which it is assigned</p>

<pre><code>print(my_dict[name])

   Age  Name
0  21   John
1  30   Simon
</code></pre>

<p>Edit 1 :</p>

<pre><code># Creating the data frame ""df1"" and the list of names ""L""
df = pd.DataFrame({""name"" : [""V01"",""V02"",""V03"",""V01"",""V02"",""V03""], ""Age"" : [21,30,21,23,45,32]})
L = [""V01"", ""V02"", ""V03""]
print(df)
    Age name
0   21  V01
1   30  V02
2   21  V03
3   23  V01
4   45  V02
5   32  V03

# Sub setting the data frame by names from ""L"" and storing it in a dictionary as a key-value pair
my_dict = {}
for name in L :
    my_dict[name] = df[df['name'] == name]

# Printing the data frame for ""V01""
my_dict[""V01""]
Output :
    Age name
0   21  V01
3   23  V01
</code></pre>
","7624057","7624057","2018-06-02 10:37:48","3","1190","Ram","2017-02-26 05:25:03","95","10","18","0","50636379","","2018-06-01 05:02:20","-2","852","<p>I have a list of string like <code>L = [""V01"", ""V02"", ""V03""]</code> and I want to convert each element in L to a variable, to which I can assign a pd.DataFrame. </p>

<p><code>
    df_list = []
    for name in L:
        df_temp = df[df['name'] == name]
        df_list.append(df_temp)
    dd = dict(zip(L, df_list))
    globals().update(dd)
</code></p>

<p>At this point <code>V01</code>, <code>V02</code>, and <code>V03</code> are recognized as variables and when keyed in, the outputs are the respective pd.DataFrame's. </p>

<p>Is there a way for me to reference the new variables <code>V01</code>, <code>V02</code>, and <code>V03</code> without first printing them out? </p>
","8544512","8544512","2018-06-01 16:17:29","python - Convert a string to variable name, to which I can assign a list or pandas.DataFrame","<python><string><pandas><variables><type-conversion>","1","0","683"
"50636663","2018-06-01 05:34:05","2","","<p>If use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.DataFrameGroupBy.resample.html"" rel=""nofollow noreferrer""><code>DataFrameGroupBy.resample</code></a> is necessary <code>DatetimeIndex</code> set before <code>groupby</code>, also <code>apply</code> is not necessary, faster is <code>resample</code> <code>sum</code>, then divide final <code>Series</code> by 10 and then use <code>np.sqrt</code>:</p>

<pre><code>df = df.set_index([""date""]).sort_index() 

df1 = (np.sqrt(df.groupby(""id"")[""val""]
                 .resample(rule = ""M"", closed = ""right"", label = ""right"")
                 .sum()
                 .div(10)))

print (df1)
id  date      
1   2000-01-31    4.582576
2   2000-01-31    4.743416
Name: val, dtype: float64
</code></pre>
","2901002","2901002","2018-06-01 05:42:06","8","784","jezrael","2013-10-20 20:27:26","427380","89269","18260","743","50636635","50636663","2018-06-01 05:30:55","0","549","<p>I have a df that has a <code>MultiIndex</code> of <code>(id, date)</code> and I would like to do 2 things:</p>

<ol>
<li><p>convert the <code>DateTimeIndex</code> named <code>date</code> to a <code>PeriodIndex</code> within each <code>id</code> group</p></li>
<li><p><code>resample</code> the frequency of the <code>PeriodIndex</code> to monthly from daily</p></li>
</ol>

<p>My current (non-working) method is to (even before converting to <code>PeriodIndex</code>):</p>

<pre><code>df = pd.DataFrame(data = {""val"": np.arange(30), 
                          ""id"": np.tile([1,2], 15),
                          ""date"": np.repeat(pd.date_range(start = ""2000-01-01"", periods = 15, name=""date""), 2)
                         })

df = df.set_index([""id"", ""date""]).sort_index() 
df.groupby(""id"")[""val""].resample(rule = ""M"", closed = ""right"", label = ""right"").apply(lambda x: np.sqrt(sum(x)/10))
</code></pre>

<p>This raises:</p>

<pre><code>TypeError: Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'MultiIndex'
</code></pre>

<p>What's the right way to do the whole procedure? I'm a bit confused about how to think about <code>groupby</code>: my mental model is that anything that follows a <code>groupby</code> operation will only receive the subframe corresponding to that group (ie the <code>MultiIndex</code> becomes a single index of just <code>date</code> within that particular group). Is this not correct?</p>
","9814693","","","resample a pandas df within each group","<python><python-3.x><pandas>","1","0","1455"
"50636681","2018-06-01 05:35:43","0","","<p>As noted above, this <em>will not</em> be feasible for any more than the number of letters countable on your two hands.  There's just too many possibilities to check.  But if you were to try this, here's how the code would look.</p>

<pre><code>letters = ['a', 'b', 'c']

def powerset(letters):
    output = [set()]
    for x in letters:
        output.extend([y.union({x}) for y in output])
    return output

for subset in powerset(letters):
    for potential_word in map(''.join, itertools.permutations(list(subset))):
        # Check if potential_word is a word
</code></pre>

<p>This won't try words with duplicate letters (that would be another layer of insanity), but it will try all possible potential words that could be formed by a subset of the letters you give in any order.</p>

<p>[edit] Just realized you requested a recursive solution.  Dunno if that's required or not, but the powerset function could be changed to be recursive.  I think that would make it uglier and harder to comprehend, though.</p>
","3874512","","","0","1022","BowlingHawk95","2014-07-24 19:52:40","613","36","49","3","50633762","50648943","2018-05-31 22:39:31","2","154","<p>given letters: example of letters</p>

<pre><code>letters = 'hutfb' 
</code></pre>

<p>I am given a file with a list of words.</p>

<p>I need to write a recursive function that allows me to check all possibilities the letters can make. If the possibility is in the list of words from the file, I need to print that specific word. </p>

<p>so for letters given</p>

<p>they can create the words:</p>

<ul>
<li>a</li>
<li>cat</li>
<li>ac</li>
<li>act</li>
<li>cab</li>
</ul>

<p>and so on and on </p>

<p>each combination the letters make I need to check the file to see if its a valid word. if it is I need to print them. </p>

<p>I don't know how start to write this function.</p>
","9849592","9849592","2018-06-02 20:12:48","recursive function for wordsearch","<python><python-3.x><file><recursion>","4","4","684"
"50636692","2018-06-01 05:36:52","8","","<ol>
<li>Check if any other container is running, If yes, do: <code>docker-compose down</code></li>
<li><p>If VPN is connected, then disconnect it and try again to up docker container:</p>

<pre><code>docker-compose up -d container_name
</code></pre></li>
</ol>
","9799849","404623","2018-06-01 05:57:37","0","262","Nandini Chaurasiya","2018-05-16 11:37:27","81","1","1","0","43720339","43721103","2017-05-01 13:59:29","94","51405","<p>I have a directory <code>apkmirror-scraper-compose</code> with the following structure:</p>

<pre><code>.
├── docker-compose.yml
├── privoxy
│   ├── config
│   └── Dockerfile
├── scraper
│   ├── Dockerfile
│   ├── newnym.py
│   └── requirements.txt
└── tor
    └── Dockerfile
</code></pre>

<p>I'm trying to run the following <code>docker-compose.yml</code>:</p>

<pre><code>version: '3'

services:
  privoxy:
    build: ./privoxy
    ports:
      - ""8118:8118""
    links:
      - tor

  tor:
    build:
      context: ./tor
      args:
        password: """"
    ports:
      - ""9050:9050""
      - ""9051:9051""

  scraper:
    build: ./scraper
    links:
      - tor
      - privoxy
</code></pre>

<p>where the <code>Dockerfile</code> for <code>tor</code> is</p>

<pre><code>FROM alpine:latest
EXPOSE 9050 9051
ARG password
RUN apk --update add tor
RUN echo ""ControlPort 9051"" &gt;&gt; /etc/tor/torrc
RUN echo ""HashedControlPassword $(tor --quiet --hash-password $password)"" &gt;&gt; /etc/tor/torrc
CMD [""tor""]
</code></pre>

<p>that for <code>privoxy</code> is</p>

<pre><code>FROM alpine:latest
EXPOSE 8118
RUN apk --update add privoxy
COPY config /etc/privoxy/config
CMD [""privoxy"", ""--no-daemon""]
</code></pre>

<p>where <code>config</code> consists of the two lines</p>

<pre><code>listen-address 0.0.0.0:8118
forward-socks5 / tor:9050 .
</code></pre>

<p>and the <code>Dockerfile</code> for <code>scraper</code> is</p>

<pre><code>FROM python:2.7-alpine
ADD . /scraper
WORKDIR /scraper
RUN pip install -r requirements.txt
CMD [""python"", ""newnym.py""]
</code></pre>

<p>where <code>requirements.txt</code> contains the single line <code>requests</code>. Finally, the program <code>newnym.py</code> is designed to simply test whether changing the IP address using Tor is working:</p>

<pre><code>from time import sleep, time

import requests as req
import telnetlib


def get_ip():
    IPECHO_ENDPOINT = 'http://ipecho.net/plain'
    HTTP_PROXY = 'http://privoxy:8118'
    return req.get(IPECHO_ENDPOINT, proxies={'http': HTTP_PROXY}).text


def request_ip_change():
    tn = telnetlib.Telnet('tor', 9051)
    tn.read_until(""Escape character is '^]'."", 2)
    tn.write('AUTHENTICATE """"\r\n')
    tn.read_until(""250 OK"", 2)
    tn.write(""signal NEWNYM\r\n"")
    tn.read_until(""250 OK"", 2)
    tn.write(""quit\r\n"")
    tn.close()


if __name__ == '__main__':
    dts = []
    try:
        while True:
            ip = get_ip()
            t0 = time()
            request_ip_change()
            while True:
                new_ip = get_ip()
                if new_ip == ip:
                    sleep(1)
                else:
                    break
            dt = time() - t0
            dts.append(dt)
            print(""{} -&gt; {} in ~{}s"".format(ip, new_ip, int(dt)))
    except KeyboardInterrupt:
        print(""Stopping..."")
        print(""Average: {}"".format(sum(dts) / len(dts)))
</code></pre>

<p>The <code>docker-compose build</code> builds successfully, but if I try <code>docker-compose up</code>, I get the following error message:</p>

<pre><code>Creating network ""apkmirrorscrapercompose_default"" with the default driver
ERROR: could not find an available, non-overlapping IPv4 address pool among the defaults to assign to the network
</code></pre>

<p>I tried searching for help on this error message, but couldn't find any. What is causing this error?</p>
","995862","","","Docker ""ERROR: could not find an available, non-overlapping IPv4 address pool among the defaults to assign to the network""","<python><docker><docker-compose>","13","13","3379"
"50636709","2018-06-01 05:37:57","2","","<p>Use <a href=""https://docs.python.org/3/library/functions.html#enumerate"" rel=""nofollow noreferrer""><code>enumerate</code></a>:</p>

<pre><code>for x, item in enumerate(fileList): 
    fileList[x] = item.replace(',', """").replace('.', """") 
</code></pre>

<p><em>Note</em>: <code>item.replace()</code> returns replaced string which you need to store in the right index of list. <code>enumerate</code> helps you keep track of index while iterating through the list.</p>
","8472377","8472377","2018-06-01 05:43:21","4","469","Austin","2017-08-16 11:54:23","18860","1780","141","2099","50636598","50636613","2018-06-01 05:26:33","-3","63","<p>I'm trying to figure out why the .replace function in python isn't functioning correctly. I have spent the entire day yesterday searching for an answer but alas have not found one.</p>

<p>I'm trying to open and read a file, copy it into a list, count the number of lines in the list and remove all the punctuation (ie , . ! ? etc). I can do everything except remove the punctuation (and I must use the .replace function instead of importing a module).</p>

<pre><code>with open('Small_text_file.txt', 'r') as myFile:        #adding lines from file to list
contents = myFile.readlines()
fileList= []
# punctuation = ['(', ')', '?', ':', ';', ',', '.', '!', '/', '""', ""'""]
for i in contents:
    fileList.append(i.rstrip())

print('The Statistics are:\n','Number of lines:', len(fileList)) #first part of question



for item in fileList:
    fileList = item.replace(',', """")
    fileList = item.replace('.', """")

print(fileList)
</code></pre>

<p>The ""Small text file"" is:</p>

<p><em>Lorem ipsum dolor sit amet, consectetur adipiscing elit.
Vivamus condimentum sagittis lacus? laoreet luctus ligula laoreet ut.
Vestibulum ullamcorper accumsan velit vel vehicula?
Proin tempor lacus arcu. Nunc at elit condimentum, semper nisi et, condimentum mi.
In venenatis blandit nibh at sollicitudin. Vestibulum dapibus mauris at orci maximus pellentesque.
Nullam id elementum ipsum. Suspendisse</em></p>

<p><strong>Running the code returns the following:</strong></p>

<p>The Statistics are:</p>

<p>Number of lines: 6</p>

<p>Nullam id elementum ipsum Suspendisse</p>

<p>So the code DOES remove the comma and period characters but it also removes the preceding 5 lines of the text and only prints the very last line. What am I doing wrong here?</p>
","9879377","","","Python .replace() function not working correctly","<python><python-3.x>","2","1","1745"
"50636754","2018-06-01 05:42:44","0","","<p>Use broadcasted numpy comparison on the underlying arrays for performance.</p>

<pre><code>df['count_greaterandequal'] = (df.r_no.values &gt;= df_2.r_no[:, None]).sum(0)

df
   r_no   user  value cam_id  count_greaterandequal
0     1    sam     76     ab                      3
1     1    sam     76    abc                      3
2     1    sam      7     ab                      3
3     2    sam      8     ab                      7
4     3  peter      8     ab                     10
5     1   jack      2   abcd                      3
6     1   jack     29   abcd                      3
7     1   Kris      2   abcd                      3
8     2  peter      8     ab                      7
</code></pre>
","4909087","","","2","711","cs95","2015-05-17 14:31:31","173545","54178","8947","7377","50636711","50636754","2018-06-01 05:38:14","-1","497","<p>Consider i have two pandas dataframe</p>

<pre><code>df = pd.DataFrame()
df['r_no'] = [1,1,1,2,3,1,1,1,2]
df['user'] = ['sam','sam','sam','sam','peter','jack','jack','Kris','peter']
df['value'] = [76,76,7,8,8,2,29,2,8]
df['cam_id'] = ['ab','abc','ab','ab','ab','abcd','abcd','abcd','ab']

df_2 = pd.DataFrame()
df_2['r_no'] = [1,3,2,2,4,1,1,3,2,5,7,2,8,9,3]
</code></pre>

<p></p>

<pre><code>df
   r_no   user  value cam_id
0     1    sam     76     ab
1     1    sam     76    abc
2     1    sam      7     ab
3     2    sam      8     ab
4     3  peter      8     ab
5     1   jack      2   abcd
6     1   jack     29   abcd
7     1   Kris      2   abcd
8     2  peter      8     ab

df_2 
    r_no
0      1
1      3
2      2
3      2
4      4
5      1
6      1
7      3
8      2
9      5
10     7
11     2
12     8
13     9
14     3
</code></pre>

<p>Expected Output
A new column in df dataframe </p>

<pre><code>df['count_greaterandequal']
0     3
1     3
2     3
3     7
4    10
5     3
6     3
7     3
8     7
</code></pre>

<p>to calculate the number for 
<strong>r_no</strong> in <em>df---dataframe</em> has been <strong>equal to or greater than</strong> in 
<em>df_2---dataframe</em></p>
","9879419","4909087","2018-12-28 05:15:21","Compare each value of a Series against every other value from another Series in pandas","<python><pandas><numpy><dataframe>","1","0","1201"
"50636755","2018-06-01 05:42:56","7","","<p>You should use the <code>round</code> function and then cast to integer type. However, do not use a second argument to the <code>round</code> function. By using 2 there it will round to 2 decimal places, the <code>cast</code> to integer will then <strong>round down</strong> to the nearest number.</p>

<p>Instead use:</p>

<pre><code>df2 = df.withColumn(""col4"", func.round(df[""col3""]).cast('integer'))
</code></pre>
","7579547","7579547","2018-06-01 05:51:49","3","420","Shaido","2017-02-17 08:20:41","16179","1879","3497","636","50636311","50636755","2018-06-01 04:54:37","5","5398","<p>I have a data frame in PySpark like below. </p>

<pre><code>import pyspark.sql.functions as func

df = sqlContext.createDataFrame(
        [(0.0, 0.2, 3.45631),
         (0.4, 1.4, 2.82945),
         (0.5, 1.9, 7.76261),
         (0.6, 0.9, 2.76790),
         (1.2, 1.0, 9.87984)],
         [""col1"", ""col2"", ""col3""])

df.show()
+----+----+-------+ 
|col1|col2|   col3|
+----+----+-------+
| 0.0| 0.2|3.45631| 
| 0.4| 1.4|2.82945|
| 0.5| 1.9|7.76261| 
| 0.6| 0.9| 2.7679| 
| 1.2| 1.0|9.87984| 
+----+----+-------+

# round 'col3' in a new column:
df2 = df.withColumn(""col4"", func.round(df[""col3""], 2))
df2.show()

+----+----+-------+----+
|col1|col2|   col3|col4|
+----+----+-------+----+
| 0.0| 0.2|3.45631|3.46|
| 0.4| 1.4|2.82945|2.83|
| 0.5| 1.9|7.76261|7.76|
| 0.6| 0.9| 2.7679|2.77|
| 1.2| 1.0|9.87984|9.88|
+----+----+-------+----+
</code></pre>

<p>In the above data frame <code>col4</code> is <code>double</code>. Now I want to convert <code>col4</code> as <code>Integer</code></p>

<pre><code>df2 = df.withColumn(""col4"", func.round(df[""col3""], 2).cast('integer'))

+----+----+-------+----+
|col1|col2|   col3|col4|
+----+----+-------+----+
| 0.0| 0.2|3.45631|   3|
| 0.4| 1.4|2.82945|   2|
| 0.5| 1.9|7.76261|   7|
| 0.6| 0.9| 2.7679|   2|
| 1.2| 1.0|9.87984|   9|
+----+----+-------+----+
</code></pre>

<p>But I want to round the <code>col4</code> values to nearest</p>

<p><code>expected result</code></p>

<pre><code>+----+----+-------+----+
|col1|col2|   col3|col4|
+----+----+-------+----+
| 0.0| 0.2|3.45631|   3|
| 0.4| 1.4|2.82945|   3|
| 0.5| 1.9|7.76261|   8|
| 0.6| 0.9| 2.7679|   3|
| 1.2| 1.0|9.87984|  10|
+----+----+-------+----+
</code></pre>

<p>How can I do that?</p>
","8151392","7579547","2018-09-11 01:57:09","Round double values and cast as integers","<python><apache-spark><pyspark><apache-spark-sql><rounding>","1","0","1699"
"50636769","2018-06-01 05:44:01","0","","<p>cross_val_score and GridSearchCV will first split the data, train the model on the train data only and then score on test data.</p>

<p>Here you are training on the full data, and then scoring on test data. Hence you dont match the results of <code>cross_val_score</code>.</p>

<p>Instead of this:</p>

<pre><code>lm=lr.fit(X,y)
</code></pre>

<p>Try this:</p>

<pre><code>lm=lr.fit(X_train, y_train)
</code></pre>

<p>Same for pipeline:</p>

<p>Instead of <code>p=pipe.fit(X,y)</code>, do this:</p>

<pre><code>p=pipe.fit(X_train, y_train)
</code></pre>

<p>You can look at my answers for more description:- </p>

<ul>
<li><a href=""https://stackoverflow.com/a/42364900/3374996"">https://stackoverflow.com/a/42364900/3374996</a></li>
<li><a href=""https://stackoverflow.com/a/42230764/3374996"">https://stackoverflow.com/a/42230764/3374996</a></li>
</ul>
","3374996","","","4","855","Vivek Kumar","2014-03-03 13:07:13","20443","4119","856","5581","50629219","50636769","2018-05-31 16:50:39","2","1319","<p>I am new to python and I have been trying to figure out how gridsearchCV and cross_val_score work.</p>

<p>Finding odds results a set up a sort of validation experiment, but still I do not understand what I am doing wrong.</p>

<p>To try to simplify I am using gridsearchCV is the simplest possible way and try to validate and understand what is happening:</p>

<p>Here it is:</p>

<pre><code>import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer
from sklearn.feature_selection import SelectKBest, f_regression, RFECV
from sklearn.decomposition import PCA
from sklearn.linear_model import RidgeCV,Ridge, LinearRegression
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.model_selection import GridSearchCV,KFold,TimeSeriesSplit,PredefinedSplit,cross_val_score
from sklearn.metrics import mean_squared_error,make_scorer,r2_score,mean_absolute_error,mean_squared_error
from math import sqrt
</code></pre>

<p>I create a cross validation object (for gridsearchCV and cross_val_score) and a train/test dataset  for pipeline and simple linear regression. I have checked that the two dataset are identical:</p>

<pre><code>train_indices = np.full((15,), -1, dtype=int)
test_indices = np.full((6,), 0, dtype=int)
test_fold = np.append(train_indices, test_indices)
kf = PredefinedSplit(test_fold)

for train_index, test_index in kf.split(X):
    print('TRAIN:', train_index, 'TEST:', test_index)
    X_train_kf = X[train_index]
    X_test_kf = X[test_index]

train_data = list(range(0,15))
test_data = list(range(15,21))

X_train, y_train=X[train_data,:],y[train_data]
X_test, y_test=X[test_data,:],y[test_data]
</code></pre>

<p>Here is what I do:</p>

<p>instantiate a simple linear model and use it with the manual set of data</p>

<pre><code>lr=LinearRegression()
lm=lr.fit(X,y)
lmscore_train=lm.score(X_train,y_train) 
</code></pre>

<p>->r2=0.4686662249071524</p>

<pre><code>lmscore_test=lm.score(X_test,y_test)
</code></pre>

<p>->r2 0.6264021467338086</p>

<p>now I try do do the exact same things using a pipeline:</p>

<pre><code>pipe_steps = ([('est', LinearRegression())])
pipe=Pipeline(pipe_steps)
p=pipe.fit(X,y)
pscore_train=p.score(X_train,y_train) 
</code></pre>

<p>->r2=0.4686662249071524</p>

<pre><code>pscore_test=p.score(X_test,y_test)
</code></pre>

<p>->r2 0.6264021467338086</p>

<p>LinearRegression and pipeline matches perfectly</p>

<p>Now I try to do the same by using cross_val_score using the predefined split kf </p>

<pre><code>cv_scores = cross_val_score(lm, X, y, cv=kf)  
</code></pre>

<p>->r2 = -1.234474757883921470e+01?!?! (this is supposed to be the test score)</p>

<p>Now let's try gridsearchCV</p>

<pre><code>scoring = {'r_squared':'r2'}
grid_parameters = [{}] 
gridsearch=GridSearchCV(p, grid_parameters, verbose=3,cv=kf,scoring=scoring,return_train_score='true',refit='r_squared')
gs=gridsearch.fit(X,y)
results=gs.cv_results_
</code></pre>

<p>from cv_results_ I get once again
<code>-&gt;mean_test_r_squared-&gt;r2-&gt;-1.234474757883921292e+01</code></p>

<p>So cross_val_score and gridsearch in the end match one another, but the score is totally off and different from what should be.</p>

<p>Will you please help me out solving this puzzle?</p>
","9876932","9758922","2018-05-31 17:52:57","How does cross_val_score and gridsearchCV works?","<python><cross-validation>","1","1","3281"
"50636838","2018-06-01 05:49:28","0","","<p>Python is mainly a scripting language unlike Java. Java is a Object Oriented Language. So There are some differentiations when coneverting from java code to python code. But python also supports class concept. In your case the working code is, </p>

<pre><code>class ConfigurationManager:

    def read(self):
        try:
            readIn = open('C:/Users/george/Desktop/hello.txt', 'r')
            # readIn = open('./resources/my.properties','r')
            print(readIn.read())
        except Exception as e:
             print('except:', e)
        finally:
            if readIn:
                readIn.close()


con = ConfigurationManager()
con.read()
</code></pre>

<p>But this is not the best python code for your requirement. My opinion is first of all you should learn python basics instead of converting java code into python. </p>
","7764309","","","0","850","Sajith Herath","2017-03-24 20:58:22","465","73","288","14","50636025","","2018-06-01 04:16:23","0","67","<p>I am new to python and start to convert a piece of Java code into python.</p>

<p>I am taking advice:<a href=""https://www.geeksforgeeks.org/g-fact-34-class-or-static-variables-in-python/"" rel=""nofollow noreferrer"">https://www.geeksforgeeks.org/g-fact-34-class-or-static-variables-in-python/</a> to imitate Java static block in python</p>

<pre><code>class ConfigurationManager:
    try:
        readIn = open('C:/Users/george/Desktop/hello.txt','r')
        # readIn = open('./resources/my.properties','r')
    except Exception as e:
        print('except:',e)
    finally:
        if readIn:
            readIn.close()
    def read(self):
        print(readIn.read())


con = ConfigurationManager()
con.read()
</code></pre>

<p>and got :</p>

<blockquote>
  <p>D:\Python27\python.exe D:/laotang/session/conf/ConfigurationManager.py
  Traceback (most recent call last):   File
  ""D:/laotang/session/conf/ConfigurationManager.py"", line 18, in
  
      con.read()   File ""D:/laotang/session/conf/ConfigurationManager.py"", line 14, in read
      print(readIn.read()) NameError: global name 'readIn' is not defined</p>
</blockquote>

<p>the original Java code：</p>

<pre><code>static {
        try {
            InputStream in = ConfigurationManager.class
                    .getClassLoader().getResourceAsStream(""my.properties""); 

            prop.load(in);  
        } catch (Exception e) {
            e.printStackTrace();  
        }
    }
</code></pre>

<p>someone please kindly fix it,thanks</p>
","9798983","","","python global variable/java python translate","<java><python>","1","1","1503"
"50636909","2018-06-01 05:55:55","0","","<p>I guess you are running into an encoding issue, from the documentation,</p>

<p><a href=""http://passlib.readthedocs.io/en/stable/narr/hash-tutorial.html#hashing"" rel=""nofollow noreferrer"">http://passlib.readthedocs.io/en/stable/narr/hash-tutorial.html#hashing</a></p>

<blockquote>
  <p>Use PasswordHash.hash() to hash a password. This call takes care of <strong>unicode encoding</strong>....</p>
</blockquote>

<pre><code>from passlib.hash import pbkdf2_sha256

hash = pbkdf2_sha256.hash(""$password"")
pbkdf2_sha256.verify(""$password"", hash)
# True
</code></pre>
","4237254","","","0","566","BcK","2014-11-10 21:01:26","1519","137","63","23","50636805","","2018-06-01 05:46:54","0","72","<p>I'm using passlib==1.7.1 with the following import:</p>

<pre><code>from passlib.apps import custom_app_context as pwd_context
</code></pre>

<p>Then hashing the password with the following:</p>

<p><code>pwd_context.encrypt(password)</code></p>

<p>I then verify with:</p>

<pre><code>pwd_context.verify(password, self.password_hash)
</code></pre>

<p>This is fine, but verification fails with certain characters.  e.g. ""£"" or ""$"".</p>

<p>Does anyone know why this would be the case please?</p>

<p>Thank you!</p>

<hr>

<p>Update:</p>

<p>Thank you all very much.  Armed with this info I investigated a bit more and it seems that the problem is not passlib but sits somewhere between angular4 where I send a base64 authorisation header to the flask app.  </p>

<p>I'm currently using the following to do this:</p>

<pre><code>let headers: Headers = new Headers({
        'Content-Type': 'application/json',
        'Authorization': 'Basic ' + btoa(userLoginRequest.username + ':' + userLoginRequest.password)
    });
</code></pre>

<p>I have read a lot today about unescape (and it's depreciation in favour of decodeURI()).  I have also read a lot about support for unicode in base64 encoding.  I tried a number of combinations of these things and it made no difference.  I am now really rather confused!</p>

<p>To test what's going on I do the following.  In angular4 I execute the following:</p>

<pre><code>let encodedString = btoa('doug:Tree£9')
console.log(encodedString)
console.log(atob(encodedString))
</code></pre>

<p>As expected, this prints the following to the console.  </p>

<pre><code>ZG91ZzpUcmVlozk=
doug:Tree£9
</code></pre>

<p>So it's clearly ok encoding and decoding.</p>

<p>Doing the same process in Python...</p>

<pre><code>import base64
encoded = base64.b64encode('doug:Tree£9')
print encoded
print base64.b64decode(encoded)
</code></pre>

<p>I get the the following in terminal.</p>

<pre><code>ZG91ZzpUcmVlwqM5
doug:Tree£9
</code></pre>

<p>I note that ""ZG91ZzpUcmVlozk="" and ""ZG91ZzpUcmVlwqM5"" are not the same.  However, both methods are working within their own languages.</p>

<p>If I put the ""ZG91ZzpUcmVlozk="" encoded string from javascript into python and decode it as follows...</p>

<pre><code>import base64
print base64.b64decode(""ZG91ZzpUcmVlozk="")
</code></pre>

<p>I get:</p>

<pre><code>doug:Tree�9
</code></pre>

<p>Note that the £ character has now been mashed.</p>

<p>Other Unicode characters fail too.</p>

<p>So I think the question is how to I encode the Authorisation header so that python correctly recognises the £ character, and any other character users choose for their passwords?</p>

<p>Thanks so much!</p>

<hr>

<p>Edit: Resolved!</p>

<p>I found this <a href=""https://stackoverflow.com/questions/30106476/using-javascripts-atob-to-decode-base64-doesnt-properly-decode-utf-8-strings"">Using Javascript&#39;s atob to decode base64 doesn&#39;t properly decode utf-8 strings</a> which goes into some detail.  I resolved it by using the following approach recommended by @brandonscript.</p>

<pre><code>b64EncodeUnicode(str) : string{
    return btoa(encodeURIComponent(str).replace(/%([0-9A-F]{2})/g, function(match, p1) {
        return String.fromCharCode(parseInt(p1, 16))
    }))
}
</code></pre>

<p>Works perfectly!  Phew!</p>
","7412939","7412939","2018-06-05 03:39:42","Verifying a password with £ or $ characters failing using passlib","<python><passlib>","2","1","3295"
"50636918","2018-06-01 05:57:02","1","","<p>As per the <em>url</em> <code>http://automationpractice.com/index.php?id_category=8&amp;controller=category#/categories-casual_dresses</code> moving forward as you are trying to invoke <code>click()</code> on the element so instead of <code>visibility_of_element_located()</code>  method you need to use <a href=""https://seleniumhq.github.io/selenium/docs/api/py/webdriver_support/selenium.webdriver.support.expected_conditions.html#selenium.webdriver.support.expected_conditions.element_to_be_clickable"" rel=""nofollow noreferrer""><strong><code>element_to_be_clickable()</code></strong></a> method as follows:</p>

<pre><code>self.casual_dresses = WebDriverWait(self.driver.instance, 10).until(EC.element_to_be_clickable((By.XPATH,""//span[@class='checked']/input[@class='checkbox' and @id='layered_category_9']"")))
</code></pre>
","7429447","","","1","832","DebanjanB","2017-01-17 08:59:30","63154","13103","3455","2612","50636109","50636595","2018-06-01 04:28:50","1","53","<p>I am practicing some Selenium on the following website:</p>

<p>www.automationpractice.com</p>

<p>I have a couple basic tests I have started below:</p>

<pre><code>import unittest
from webdriver import Driver
from values import strings
from pageobjects.homescreen import Homescreen


class TestHomeScreen(unittest.TestCase):
    @classmethod
    def setUp(self):
        self.driver = Driver()
        self.driver.navigate(strings.base_url)

    def test_home_screen_components(self):
        home_screen = Homescreen(self.driver)
        home_screen.logo_present()

    def test_choose_dress(self):
        home_screen = Homescreen(self.driver)
        home_screen.choose_dress()

    @classmethod
    def tearDown(self):
        self.driver.instance.quit()
</code></pre>

<p>Those tests are being read from the following:</p>

<pre><code>from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from values import strings


class Homescreen:

    def __init__(self, driver):
        self.driver = driver

    def logo_present(self):
        self.logo = WebDriverWait(self.driver.instance, 10).until(
            EC.visibility_of_element_located((
                By.ID, ""header_logo"")))
        assert self.logo.is_displayed()

    def choose_dress(self):
        self.dresses = WebDriverWait(self.driver.instance, 5).until(
            EC.visibility_of_element_located((
                By.XPATH, '//*[@id=""block_top_menu""]/ul/li[2]/a')))
        self.dresses.click()
        self.casual_dresses = WebDriverWait(self.driver.instance, 10).until(
            EC.visibility_of_element_located((
                By.XPATH,'//input[@type=""checkbox"" and @id=""layered_category_9""]')))
</code></pre>

<p>the test_home_screen_components passes fine, but the test_choose_dress fails.  I have narrowed it down it is failing on the final XPATH, which is a checkbox for ""casual dresses"". It can't be found.  I have confirmed in Chrome that this XPATH is valid:</p>

<pre><code>self.casual_dresses = WebDriverWait(self.driver.instance, 10).until(
     EC.visibility_of_element_located((
         By.XPATH,'//input[@type=""checkbox"" and @id=""layered_category_9""]')))
</code></pre>

<p>on the following page:
<a href=""http://automationpractice.com/index.php?id_category=8&amp;controller=category#/categories-casual_dresses"" rel=""nofollow noreferrer"">http://automationpractice.com/index.php?id_category=8&amp;controller=category#/categories-casual_dresses</a></p>

<p>So I am not sure what the problem is.  Maybe I am missing something because it is embedded?</p>

<p>Also I know I need to add some Try/Except to my code eventually as well, I am just starting out with this stuff.</p>
","4439019","","","Difficulty narrowing down XPath of checkbox","<python><selenium><xpath>","3","0","2831"
"50637035","2018-06-01 06:08:34","3","","<p>In stars/app.py</p>

<pre><code>from django.apps import AppConfig


class RequestsConfig(AppConfig):
    name = 'stars'
    verbose_name = ""Star of India""
</code></pre>

<p>in  stars/<strong>init</strong>.py</p>

<pre><code>default_app_config = 'stars.apps.RequestsConfig'
</code></pre>

<p>To access the app name in custom menu methods you can try to get that from</p>

<pre><code>model._meta.app_config.verbose_name
</code></pre>

<p>Check the Django Doc for reference <a href=""https://docs.djangoproject.com/en/1.11/ref/applications/#for-application-users"" rel=""nofollow noreferrer"">https://docs.djangoproject.com/en/1.11/ref/applications/#for-application-users</a></p>
","1556933","59303","2019-05-19 15:20:02","0","676","Akhilraj N S","2012-02-14 05:37:13","7057","361","56","1","26972625","","2014-11-17 12:21:27","11","16760","<p>I created <code>'frontend'</code> application using <code>./manage.py startproject frontend</code></p>

<p>But for some reason I just want to change the app name in the Django admin to display <code>'Your Home Page'</code> instead of <code>'frontend'</code>.</p>

<p>How to do that?</p>

<p>Update:
Here is the little more detail: </p>

<pre><code>#settings.py
INSTALLED_APPS = (
    'frontend',
)
</code></pre>

<p>and </p>

<pre><code>#frontend/models.py
class Newpage(models.Model):
    #field here
class Oldpage(models.Model):
    #field here
</code></pre>
","","6622817","2017-05-23 05:00:57","How to change app name in Django admin?","<python><django>","7","1","564"
"50637073","2018-06-01 06:11:26","2","","<p>There is a concept I think you have perhaps overlooked when configuring the sftp part, This is <code>ChrootDirectory</code>. </p>

<p>A <code>Chroot</code> in Unix world is a way to execute a command or an environment inside a system directory, so this directory appears the root of the system you're into. This is primary used as security feature because there is no way to escape this chroot. For instance imagine you have a path <code>/opt/server/ftp/users/</code> and a ftp daemon is <code>chroot</code>ed in <code>/opt/server/ftp/</code> a client will see the <code>users</code> directory when he will do a <code>ls -al</code> and it will be impossible to access files on the system like <code>/etc/</code></p>

<p>So this problem has nothing to do with the Paramiko code per-se but with the sftp configuration you set and the comprehension of what is a Chroot environment.</p>

<p><code>ChrootDirectory</code> in you setup define the sftp user will be dropped into this directory when connection it created AND that he'll be impossible to see the full path of the system when it is logged, so when you upload the files you don't have to <code>chdir /var/www/html/reports</code> because you can't see this directory. Considering you set <code>ChrootDirectory /var/www/html/reports</code> </p>

<p>Check first the <code>ChrootDirectory</code> value you set, if you put <code>/var/sftp/</code> but you want to access the system path (not the chroot one) <code>/var/www/html/reports/</code> this is wrong. Correct to <code>/var/www/html/reports/</code> seems legit, then change your code to </p>

<pre><code>sftp.put(base_dir + '\\report', '.' + host_name, confirm = False)
</code></pre>

<p>the character <code>.</code> as second parameter means the current directory</p>
","338011","338011","2018-06-01 06:36:20","1","1778","Baptiste Mille-Mathias","2010-05-11 07:46:22","1272","390","454","438","50635938","50637073","2018-06-01 04:03:35","-1","367","<p>I have been trying to automate SFTP transfer from a Windows client via a python script to a CentOS machine running an Apache server. I have created a user account on the CentOS server that can only access SFTP, similar to the instructions listed here: <a href=""https://www.digitalocean.com/community/tutorials/how-to-enable-sftp-without-shell-access-on-centos-7"" rel=""nofollow noreferrer"">https://www.digitalocean.com/community/tutorials/how-to-enable-sftp-without-shell-access-on-centos-7</a></p>

<p>I then used the following code in an attempt to transfer the file</p>

<pre><code>transport.connect(username = username, password = password)
sftp = paramiko.SFTPClient.from_transport(transport)
sftp.put(base_dir + '\\report', '/var/www/html/reports/' + host_name, confirm = False)
</code></pre>

<p>However this results in the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""noschedule_make_report.py"", line 74, in &lt;module&gt;
    main()
  File ""noschedule_make_report.py"", line 62, in main
    sftp.chdir('/var/www/html/reports')
  File ""C:\Python27\lib\site-packages\paramiko\sftp_client.py"", line 626, in chdir
    if not stat.S_ISDIR(self.stat(path).st_mode):
  File ""C:\Python27\lib\site-packages\paramiko\sftp_client.py"", line 460, in stat
    t, msg = self._request(CMD_STAT, path)
  File ""C:\Python27\lib\site-packages\paramiko\sftp_client.py"", line 780, in _request
    return self._read_response(num)
  File ""C:\Python27\lib\site-packages\paramiko\sftp_client.py"", line 832, in _read_response
    self._convert_status(msg)
  File ""C:\Python27\lib\site-packages\paramiko\sftp_client.py"", line 861, in _convert_status
    raise IOError(errno.ENOENT, text)
IOError: [Errno 2] No such file
</code></pre>

<p>This code worked when I didn't set the restrictions on the upload user account as described in the Digital Ocean post, and instead had much more liberal permissions and shell login. Is there a way for me to have both the locked out login for the upload user and to use the Paramiko funcitonality? </p>

<p>Please note that using a <code>sftp.chdir('/var/www/html/reports')</code> command before the <code>put</code> command produced the same error, occurring at the <code>chdir</code> line instead. </p>

<p>Also I understand that similar questions have been asked (<a href=""https://stackoverflow.com/questions/15481934/ioerror-errno-2-no-such-file-paramiko-put"">IOError: [Errno 2] No such file - Paramiko put()</a>), but I am specifically asking if I can relegate these two sets of functionality.   </p>
","2278859","","","Paramiko failing due due to file permissions","<python><centos><sftp><paramiko>","1","4","2556"
"50637125","2018-06-01 06:15:35","0","","<p>I found a solution by instantiating the DateTime instance again after parsing:</p>

<pre><code>dt = pendulum.parse('2017-10-29 02:30:00', tz='Europe/Berlin')
dt = pendulum.datetime(
    dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second, dt.microsecond,
    tz='Europe/Berlin', dst_rule=pendulum.PRE_TRANSITION
)
</code></pre>

<p>Output now is <code>'2017-10-29T02:30:00+02:00'</code></p>
","3355198","","","0","399","cbergmiller","2014-02-26 09:43:04","163","15","1","2","50621108","","2018-05-31 09:34:29","0","45","<p>I have to parse time-series data from CSV files in an application.
The timestamps sometimes are in locale time with DST transitions.
I want to use python-pendulum for the parsing of the date strings.
Is it possible to specify the <code>dst_rule</code> when calling the <code>pendulum.parse</code> method? It seems to be ignored:</p>

<pre><code>&gt;&gt;&gt; str(pendulum.parse('2017-10-29 02:30:00', tz='Europe/Berlin', dst_rule=pendulum.PRE_TRANSITION))
'2017-10-29T02:30:00+01:00'
&gt;&gt;&gt; str(pendulum.parse('2017-10-29 02:30:00', tz='Europe/Berlin', dst_rule=pendulum.POST_TRANSITION))
'2017-10-29T02:30:00+01:00'
</code></pre>

<p>I need the first string to be '2017-10-29T02:30:00+<strong>02</strong>:00' in my application.
Is there a way to achieve this?</p>
","3355198","3355198","2018-05-31 10:18:40","Specify dst_rule for ambiguous dates in pendulum.parse","<python><dst><pendulum>","1","0","773"
"50637168","2018-06-01 06:18:49","0","","<p>I've lost 2 nights in this, I finally solved it using this solution: <a href=""http://blog.macuyiko.com/post/2016/fixing-flask-url_for-when-behind-mod_proxy.html"" rel=""nofollow noreferrer"">http://blog.macuyiko.com/post/2016/fixing-flask-url_for-when-behind-mod_proxy.html</a>, my proxy settings:</p>

<pre><code>ProxyPass /crm http://localhost:5013/
ProxyPassReverse /crm http://localhost:5013/
</code></pre>

<p>And the ReverseProxy class worked perfectly, the trick was the ""script_name"" that it's not originally included in the snippet from here: <a href=""http://flask.pocoo.org/snippets/35/"" rel=""nofollow noreferrer"">http://flask.pocoo.org/snippets/35/</a> I guess the snipped from flask page works with ngnix and this is for apache2 proxy. Hope this helps if someone comes here again with the same issue (future me included)</p>
","465977","","","0","837","Cross","2010-10-04 15:29:56","1106","65","126","3","22312014","22338517","2014-03-10 21:46:01","12","7629","<p>I am having a problem with the redirect(url_for) function in my flask app.</p>

<p>Any redirect(url_for(""index"")) line redirects the application from <strong><em>domain.com/app</em></strong> to <strong><em>ip-addr/app</em></strong>, where the <strong><em>ip-addr</em></strong> is my own client machines ip, not the server's. </p>

<p>This has gotten me very confused, and I don't know where exactly the issue occurs, as it only happens on the server and not on any local testing.</p>

<p><strong>Details:</strong></p>

<p>I am using the reverse proxy setup found here <a href=""http://flask.pocoo.org/snippets/35/"" rel=""noreferrer"">http://flask.pocoo.org/snippets/35/</a>.
My nginx config is setup like so</p>

<pre><code>location /app {
                proxy_set_header X-Script-Name /app;
                proxy_set_header Host $http_host;
                proxy_set_header X-Forwarded-Host $proxy_add_x_forwarded_for;
                proxy_redirect off;
                proxy_set_header X-Real-IP $remote_addr;
                proxy_set_header X-Scheme $scheme;
                proxy_connect_timeout 60;
                proxy_read_timeout 60;
                proxy_pass http://localhost:8000/;
        }
</code></pre>

<p>I have gunicorn running my flask app as a upstart task.
Any hints?</p>

<p>EDIT:</p>

<p>So I dug around a bit and found that the this git report had similar issues, <a href=""https://github.com/omab/django-social-auth/issues/157"" rel=""noreferrer"">https://github.com/omab/django-social-auth/issues/157</a>.</p>

<p><em>Nginx - Gunicorn serving to Nginx via localhost (127.0.0.1:1234). Unfortunately, when I authenticate with social platforms, the redirect URL social-auth is sending them to is 127.0.0.1:1234/twitter/complete, which obviously can't be resolved by the client's browser.</em></p>

<p>It seems my Flask app is not getting the memo to update its redirect routes.</p>
","430031","430031","2014-03-11 17:49:24","Flask redirect(url_for) error with gunricorn + nginx","<python><nginx><flask><reverse-proxy><gunicorn>","5","1","1904"
"50637240","2018-06-01 06:24:30","2","","<p>you can use <a href=""https://docs.python.org/3/library/functions.html#min"" rel=""nofollow noreferrer"">min</a> function for O(n) complexity</p>

<pre><code>min(my_randoms)
</code></pre>
","7773888","7773888","2018-06-01 06:43:42","1","187","raviraja","2017-03-27 11:43:57","660","171","29","18","50637158","","2018-06-01 06:18:15","0","75","<p>This function generates a list of 10 random numbers. I need to modify this function to select and print the smallest number from the randomly generated list Please help, I am having a very hard time with it:</p>

<pre><code>import random
my_randoms=[]
for i in range (10):
    my_randoms.append(random.randrange(1,101,1))

print (my_randoms)
</code></pre>
","9879485","1222951","2018-06-01 06:20:21","Python write a function that selects the smallest number from a list of 10 randomly generated numbers","<python><python-3.x>","3","1","359"
"50637261","2018-06-01 06:26:24","1","","<p><code>cnn_utils</code> here is just a personal library for the course, not a public module. You cannot install it with <code>pip install</code>.</p>

<p>You need to find the source of <code>cnn_utils.py</code> then download that to your computer, then upload it to Colab.</p>
","6729010","","","1","279","Korakot Chaovavanich","2016-08-18 04:09:58","7208","610","754","8","50635315","50637261","2018-06-01 02:32:57","0","993","<p>I'm trying to run a copy of a notebook from my Coursera class (which is in Jupyter) in Colab and getting an error:</p>

<pre><code>ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-6-1ee14f68a167&gt; in &lt;module&gt;()
      8 import tensorflow as tf
      9 from tensorflow.python.framework import ops
---&gt; 10 from cnn_utils import *
     11 
     12 get_ipython().magic('matplotlib inline')

ModuleNotFoundError: No module named 'cnn_utils'
</code></pre>

<p>i tried doing <code>!pip install</code> but it yelled at me again:</p>

<pre><code>Could not find a version that satisfies the requirement cnn_utils (from versions: )
No matching distribution found for cnn_utils
</code></pre>

<p>anyone know how to fix it?</p>
","4414359","","","cnn_utils module missing from Google Colab","<python><jupyter-notebook><google-colaboratory>","1","0","773"
"50637271","2018-06-01 06:27:19","0","","<p>I would use numpy</p>

<pre><code>import numpy as np

np.random.randint(1, 101, 10).min()
</code></pre>

<p>Short explanation:</p>

<pre><code>np.random.randint(1, 101, 10)
</code></pre>

<p>will print something like this</p>

<pre><code>array([91, 57, 53, 26, 95,  9, 31, 47, 29, 78])
</code></pre>

<p>and then you just access the minimum value with <code>min()</code>.</p>

<p>In your implementation you would just do</p>

<pre><code>min(my_randoms)
</code></pre>

<p>as @Austin mentioned in the comments</p>
","1534017","","","0","515","Cleb","2012-07-18 07:51:03","13486","1563","2133","554","50637158","","2018-06-01 06:18:15","0","75","<p>This function generates a list of 10 random numbers. I need to modify this function to select and print the smallest number from the randomly generated list Please help, I am having a very hard time with it:</p>

<pre><code>import random
my_randoms=[]
for i in range (10):
    my_randoms.append(random.randrange(1,101,1))

print (my_randoms)
</code></pre>
","9879485","1222951","2018-06-01 06:20:21","Python write a function that selects the smallest number from a list of 10 randomly generated numbers","<python><python-3.x>","3","1","359"
"50637297","2018-06-01 06:28:55","2","","<p>Append each name to a list, then print the list. And use string formatting to put an appropriate number in the prompt.</p>

<pre><code>friendList = []
Friends = int(input(""Please enter number of friends"")
for i in range(Friends):
    Name = input(""Please enter friend number %d: "" % (i+1))
    friendList.append(Name)
print(friendList)
</code></pre>
","1491895","","","0","353","Barmar","2012-06-29 18:12:29","477375","68451","6422","3351","50637234","","2018-06-01 06:24:08","0","43","<p>How would I loop this code to make it so that the user inputs the number of friends they have, their names, and in the end, the program will be able to output the information? This is what I have so far, but I believe it's incorrect. </p>

<pre><code>Friends = int(input(""Please enter number of friends"")
for i in range(Friends):
    Name = input(""Please enter friend number 1:"")
</code></pre>
","9879581","9879581","2018-06-01 06:26:25","How would I loop this on Python?","<python>","4","3","397"
"50637321","2018-06-01 06:30:21","0","","<p>Here a try using list comprehension:</p>

<pre><code>Friends = int(input(""Please enter number of friends :""))
Names = [input(""Please enter friend number {}:"".format(i)) for i in range(1,Friends+1)]
print(Names)
</code></pre>
","8393004","8393004","2018-06-01 06:35:26","0","228","Vikas Damodar","2017-07-31 09:22:41","2264","383","137","99","50637234","","2018-06-01 06:24:08","0","43","<p>How would I loop this code to make it so that the user inputs the number of friends they have, their names, and in the end, the program will be able to output the information? This is what I have so far, but I believe it's incorrect. </p>

<pre><code>Friends = int(input(""Please enter number of friends"")
for i in range(Friends):
    Name = input(""Please enter friend number 1:"")
</code></pre>
","9879581","9879581","2018-06-01 06:26:25","How would I loop this on Python?","<python>","4","3","397"
"50637324","2018-06-01 06:30:28","0","","<pre><code>unwantedRows = []
Rows = []

for index, row in enumerate(ws1.iter_rows(max_col = 50), start = 1):
  sublist = []
  for cell in row:
    sublist.append(cell.value)

  if sublist not in Rows:
    Rows.append((sublist))
  else:
    unwantedRows.append(index)
</code></pre>
","2798027","","","1","281","The Decks of Bicycle","2013-09-20 06:38:39","73","17","25","0","50624409","50637324","2018-05-31 12:31:45","3","137","<p>I want to find the index of all duplicate rows in an excel file and add them to a list which will be handled later. </p>

<pre><code>unwantedRows = []
Row = []
item = """"

for index, row in enumerate(ws1.iter_rows(max_col = 50), start = 1):
  for cell in row:
    if cell.value:
      item += cell.value
  if item in Row:
    unwantedRows.append(index)
  else:
    Row.append(item)
</code></pre>

<p>However this fails to work. It only indexes rows that are completely empty. How do I fix this? </p>
","2632595","","","Find index of duplicate rows in Openpyxl","<python><excel><openpyxl>","1","1","502"
"50637361","2018-06-01 06:32:17","1","","<p>You can catch the exception in case it's not set in the dict : </p>

<pre><code>try:
    training_data = results[""train""]
    testing_data = results[""test""]
except KeyError as ex:
    print(""Missing key in dictionary : {}"".format(ex))
</code></pre>

<p>It's more pythonic to ask for forgiveness than permission.</p>
","2711210","","","1","319","Chuk Ultima","2013-08-23 13:32:03","762","85","527","8","50637311","","2018-06-01 06:29:42","-3","302","<p>Below is my code.</p>

<pre><code>print(""Preprocessing data..."")
with multiprocessing.Manager() as manager:

    results = manager.dict()

    preprocess_training = Process(target=preprocess, args=(
        results, ""data\\train.csv"", False, ""train"", min_occurrences, train_data_file_name,))

    preprocess_testing = Process(target=preprocess, args=(
        results, ""data\\test.csv"", True, ""test"", min_occurrences, test_data_file_name,))

    preprocess_training.start()
    preprocess_testing.start()
    print(""Multiple processes started..."")

    preprocess_testing.join()
    print(""Preprocessed testing data..."")

    preprocess_training.join()
    print(""Preprocessed training data..."")

    training_data = results[""train""]
    testing_data = results[""test""]

    print(""Data preprocessed &amp; cached..."")
</code></pre>

<p>I am getting following error</p>

<blockquote>
  <p>File ""C:\Users\Samad\Anaconda3\lib\multiprocessing\managers.py"",
  line 772, in _callmethod
      raise convert_to_error(kind, result)</p>
  
  <p>KeyError: 'train'</p>
</blockquote>

<p>How it can be resolves?</p>
","9879619","1324033","2018-06-01 06:31:50","Python Programming error","<python><keyerror>","1","3","1105"
"50637374","2018-06-01 06:32:55","0","","<p>The problem is that when you split the string, whitespaces are removed, so when you are looking for a trailing whitespace in your regex expression, your search fails.</p>

<p>In your case,</p>

<pre><code>line = ""R_d_10763 VDDI_1007 Group_BGA_BGA_VDD_INT_AD26 3.711438e+000""
conn1, conn2, res = line.split()[1:]

print(conn2) # notice that trailing whitespace is gone
# 'Group_BGA_BGA_VDD_INT_AD26'

re.search(r'(A?[A-Z][0-9]{1,2})(?:_SINK|\s)', conn2)
# no match
</code></pre>

<hr>

<p>Here is a working version..</p>

<pre><code>import re

lines = ['R_d_10763     VDDI_1007                             Group_BGA_BGA_VDD_INT_AD26                  3.711438e+000',
         'R_d_10771     VDDI_1007                             VDDI_1012                                   1.337785e+001',
         'R_d_8607      Group_BGA_BGA_VSS_AH6                 Group_BGA_BGA_VSS_AJ43                      3.777161e+000',
         'R_d_6585      Group_BGA_BGA_AVDD085_MIPI_DPHY_Y36   Group_BGA_BGA_AVDD085_MIPI_DPHY_W35_SINK_   3.860682e-003',
         'R_d_69804     Xm4s4s2_M_DPDATA0_580                 Group_BGA_BGA_XMIPI_DSI_D0_P_V38_SINK_      2.668494e-001',
         'R_d_69668     XCP_ANT_SW1_383                       Group_BGA_BGA_XCP_ANT_SW1_N5_SINK_ 5.037550e-001',
         'R_d_13        AVDD085_MIPI_DPHY_540                 Group_BGA_BGA_AVDD085_MIPI_DPHY_Y36 2.854267e-002',]


for line in lines:
    conn1, conn2, res = line.split()[1:]
    match1 = re.search(r'(?&lt;=_)[A-Z]{1,2}\d{1,2}(?=_SINK|$)', conn1)
    match2 = re.search(r'(?&lt;=_)[A-Z]{1,2}\d{1,2}(?=_SINK|$)', conn2)
    try:
        conn1 = match1.group()
    except AttributeError:
        pass
    try:
        conn2 = match2.group()
    except AttributeError:
        pass
    print(f'{conn1:40} -- {conn2:40} -- {res:40}')
</code></pre>

<p>Output: </p>

<pre><code>Connection 1: VDDI_1007  Connection 2: AD26  Resistance: 3.711438e+000
Connection 1: VDDI_1007  Connection 2: VDDI_1012  Resistance: 1.337785e+001
Connection 1: AH6  Connection 2: AJ43  Resistance: 3.777161e+000
Connection 1: Y36  Connection 2: W35  Resistance: 3.860682e-003
Connection 1: Xm4s4s2_M_DPDATA0_580  Connection 2: V38  Resistance: 2.668494e-001
Connection 1: XCP_ANT_SW1_383  Connection 2: N5  Resistance: 5.037550e-001
Connection 1: AVDD085_MIPI_DPHY_540  Connection 2: Y36  Resistance: 2.854267e-002
</code></pre>
","4237254","4237254","2018-06-01 07:06:53","1","2373","BcK","2014-11-10 21:01:26","1519","137","63","23","50636902","50637638","2018-06-01 05:55:19","0","59","<p>I am at a loss on why this isn't matching all the occurances. I even put it in a RE tool online and it should match. I am running on Linux with 3.6.</p>

<p>It only matches the search when it is followed by the '_SINK' but not when followed by '\s'.</p>

<p>Here is a snippet of my search text and RE:</p>

<pre><code>import re

lines = ['R_d_10763 VDDI_1007 Group_BGA_BGA_VDD_INT_AD26 3.711438e+000',
         'R_d_10771 VDDI_1007 VDDI_1012 1.337785e+001',
         'R_d_8607 Group_BGA_BGA_VSS_AH6 Group_BGA_BGA_VSS_AJ43 3.777161e+000',
         'R_d_6585 Group_BGA_BGA_AVDD085_MIPI_DPHY_Y36 Group_BGA_BGA_AVDD085_MIPI_DPHY_W35_SINK_ 3.860682e-003',
         'R_d_69804 Xm4s4s2_M_DPDATA0_580 Group_BGA_BGA_XMIPI_DSI_D0_P_V38_SINK_ 2.668494e-001',
         'R_d_69668 XCP_ANT_SW1_383 Group_BGA_BGA_XCP_ANT_SW1_N5_SINK_ 5.037550e-001',
         'R_d_13 AVDD085_MIPI_DPHY_540 Group_BGA_BGA_AVDD085_MIPI_DPHY_Y36 2.854267e-002',]


for line in lines:
    conn1, conn2, res = line.split()[1:]
    match1 = re.search(r'(A?[A-Z][0-9]{1,2})(?:_SINK|\s)', conn1)
    match2 = re.search(r'(A?[A-Z][0-9]{1,2})(?:_SINK|\s)', conn2)
    try:
        conn1 = match1.group(1)
    except AttributeError:
        pass
    try:
        conn2 = match2.group(1)
    except AttributeError:
        pass
    print(f'Connection 1: {conn1}  Connection 2: {conn2}  Resistance: {res}')
</code></pre>

<p>The output I get is:</p>

<pre><code>Connection 1: VDDI_1007  Connection 2: Group_BGA_BGA_VDD_INT_AD26  Resistance: 3.711438
Connection 1: VDDI_1007  Connection 2: VDDI_1012  Resistance: 13.37785
Connection 1: Group_BGA_BGA_VSS_AH6  Connection 2: Group_BGA_BGA_VSS_AJ43  Resistance: 3.777161
Connection 1: Group_BGA_BGA_AVDD085_MIPI_DPHY_Y36  Connection 2: W35  Resistance: 0.003860682
Connection 1: Xm4s4s2_M_DPDATA0_580  Connection 2: V38  Resistance: 0.2668494
Connection 1: XCP_ANT_SW1_383  Connection 2: N5  Resistance: 0.5037550
Connection 1: AVDD085_MIPI_DPHY_540  Connection 2: Group_BGA_BGA_AVDD085_MIPI_DPHY_Y36  Resistance: 0.02854267
</code></pre>

<p>But it should be</p>

<pre><code>Connection 1: VDDI_1007  Connection 2: AD26  Resistance: 3.711438
Connection 1: VDDI_1007  Connection 2: VDDI_1012  Resistance: 13.37785
Connection 1: AH6  Connection 2: AJ43  Resistance: 3.777161
Connection 1: Y36  Connection 2: W35  Resistance: 0.003860682
Connection 1: Xm4s4s2_M_DPDATA0_580  Connection 2: V38  Resistance: 0.2668494
Connection 1: XCP_ANT_SW1_383  Connection 2: N5  Resistance: 0.5037550
Connection 1: AVDD085_MIPI_DPHY_540  Connection 2: Y36  Resistance: 0.02854267
</code></pre>

<p>Thanks,</p>
","9143296","4237254","2018-06-01 10:20:43","regular expression not matching white space","<python><regex><python-3.x>","2","3","2596"
"50637382","2018-06-01 06:33:22","1","","<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>map</code></a> by dictionary and if some another values out of <code>dict</code> add <code>fillna</code>:</p>

<pre><code>a = pd.DataFrame({'outcome':['correct','correct','false', 'val']})
print (a)
   outcome
0  correct
1  correct
2    false
3      val

d = {'correct':'false', 'false':'correct'}
a['outcome'] = a['outcome'].map(d).fillna(a['outcome'])
print (a)
   outcome
0    false
1    false
2  correct
3      val
</code></pre>
","2901002","","","0","559","jezrael","2013-10-20 20:27:26","427380","89269","18260","743","50637310","","2018-06-01 06:29:41","-1","19","<p>I just want to switch correct to false and false to correct in my panda data frame, doing what I have written below changes everything to correct. How do I fix this?</p>

<p>a.loc[(a[""outcome""] == 'correct') 'outcome'] = 'false' and                                                                                                      a.loc[(a[""outcome""] == 'false'), 'outcome'] = 'correct'</p>
","9457956","","","Python : switching values in panda dataframe","<python><pandas>","1","0","397"
"50637397","2018-06-01 06:34:33","0","","<p>Loop using the number of friends, and store the name for each of them:</p>

<pre><code>friend_count = int(input(""Please enter number of friends: ""))
friend_list = []
for friend_index in range(friend_count):
    name = input(""Please enter friend number {}: "".format(friend_index + 1))
    friend_list.append(name)

print(friend_list)
</code></pre>
","7663649","","","0","350","Pierre","2017-03-06 00:24:56","658","36","318","6","50637234","","2018-06-01 06:24:08","0","43","<p>How would I loop this code to make it so that the user inputs the number of friends they have, their names, and in the end, the program will be able to output the information? This is what I have so far, but I believe it's incorrect. </p>

<pre><code>Friends = int(input(""Please enter number of friends"")
for i in range(Friends):
    Name = input(""Please enter friend number 1:"")
</code></pre>
","9879581","9879581","2018-06-01 06:26:25","How would I loop this on Python?","<python>","4","3","397"
"50637432","2018-06-01 06:36:52","0","","<p>You can use raw_input, from the documentation</p>

<blockquote>
  <p>If the prompt argument is present, it is written to standard output without a trailing newline. The function then reads a line from input, converts it to a string (stripping a trailing newline), and returns that. When EOF is read, EOFError is raised.</p>
</blockquote>

<p>Code</p>

<pre><code>name_array = list()
num_friends = raw_input(""Please enter number of friends:"")
print 'Enter Name(s): '
for i in range(int(num_friends)):
    n = raw_input(""Name :"")
    name_array.append((n))
print 'Names: ',name_array
</code></pre>
","9174418","","","0","599","Harshit","2018-01-04 19:08:23","1","3","0","0","50637234","","2018-06-01 06:24:08","0","43","<p>How would I loop this code to make it so that the user inputs the number of friends they have, their names, and in the end, the program will be able to output the information? This is what I have so far, but I believe it's incorrect. </p>

<pre><code>Friends = int(input(""Please enter number of friends"")
for i in range(Friends):
    Name = input(""Please enter friend number 1:"")
</code></pre>
","9879581","9879581","2018-06-01 06:26:25","How would I loop this on Python?","<python>","4","3","397"
"50637461","2018-06-01 06:38:50","0","","<p>It turns out that what I am trying to do is not achievable. </p>

<p><strong>The first problem</strong></p>

<p>I found in <a href=""https://groups.google.com/forum/#!topic/parallelssh/LGs64wcIssY"" rel=""nofollow noreferrer"">this post</a> that all commands are in their own channel. That means that even if <code>su</code> would be successful it wouldn't affect the second command. The author of the post recommends running</p>

<pre><code>su -c whoami - root
</code></pre>

<p><strong>The second problem</strong></p>

<p>I managed to debug the problem even further by changing <code>host_output.stdout</code> to <code>host_output.stderr</code> It turned out that I receive an error which previously was not being shown on the terminal:</p>

<pre><code>standard in must be a tty
</code></pre>

<p>Possible solutions to this problem are <a href=""https://serverfault.com/questions/397031/run-script-as-another-user-from-a-root-script-with-no-tty-stdin"">here</a> . They didn't work for me but might work for you. </p>

<p>For me workaround was to allow on all my hosts root login. And then in parallel-ssh I log in as a root already with all the rights in place.</p>
","2390219","","","0","1165","kukis","2013-05-16 13:43:15","2939","464","219","56","50585828","50637461","2018-05-29 13:38:53","0","445","<p>I want to log in to two hosts using <a href=""https://parallel-ssh.readthedocs.io/en/1.3.1/index.html"" rel=""nofollow noreferrer"">parallel-ssh</a> and execute <code>su</code> command. Then I want to confirm that I am the root user by printing out <code>whoami</code></p>

<p>Code:</p>

<pre><code>hosts = ['myHost1', 'myHost2']
client = ParallelSSHClient(hosts, user='myUser', password='myPassword')

output = client.run_command('su')

for host in output:
    stdin = output[host].stdin
    stdin.write('rootPassword\n')
    stdin.flush()

client.join(output)

output = client.run_command('whoami')

for host, host_output in output.items():
    for line in host_output.stdout:
        print(""Host [%s] - %s"" % (host, line))
</code></pre>

<p>Result:</p>

<pre><code>Host [myHost1] - myUser
Host [myHost2] - myUser
</code></pre>

<p>Obviously, I expect root in the output. <a href=""https://parallel-ssh.readthedocs.io/en/1.3.1/advanced.html#run-command-features-and-options"" rel=""nofollow noreferrer"">I am following the documentation.</a> </p>

<p>I've tried using all different line endings instead of <code>\n</code> and nothing has changed. 
How can I execute <code>su</code> command using <code>parallel-ssh</code>?</p>
","2390219","","","How to execute 'su' command using parallel-ssh","<python><linux><stdin><root><parallel-ssh>","2","0","1224"
"50637466","2018-06-01 06:39:18","1","","<p>I believe need:</p>

<pre><code>df = pd.concat([df1, df2]).drop_duplicates(subset=['ID'], keep='last').sort_values('ID')
print (df)
    ID  A
0  ID1  5
1  ID2  1
2  ID3  8
3  ID4  8
3  ID5  7
4  ID6  8
5  ID7  9
</code></pre>

<p><strong>Explanation</strong>:</p>

<p>First <a href=""http://pands.pydata.org/pandas-docs/stable/generated/pandas.concat.html"" rel=""nofollow noreferrer""><code>concat</code></a> both <code>DataFrame</code> together:</p>

<pre><code>print (pd.concat([df1, df2]))
    ID  A
0  ID1  5
1  ID2  6
2  ID3  7
3  ID4  8
0  ID1  5
1  ID2  1
2  ID3  8
3  ID5  7
4  ID6  8
5  ID7  9
</code></pre>

<p>Because same <code>ID</code> are created, remove duplicates by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html"" rel=""nofollow noreferrer""><code>drop_duplicates</code></a> with keep only last value:</p>

<pre><code>print (pd.concat([df1, df2]).drop_duplicates(subset=['ID'], keep='last'))
    ID  A
3  ID4  8
0  ID1  5
1  ID2  1
2  ID3  8
3  ID5  7
4  ID6  8
5  ID7  9
</code></pre>

<p>And last sort by <code>ID</code> by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html"" rel=""nofollow noreferrer""><code>sort_values</code></a>.</p>
","2901002","2901002","2018-06-01 06:58:04","3","1254","jezrael","2013-10-20 20:27:26","427380","89269","18260","743","50637420","50637466","2018-06-01 06:36:14","1","30","<p>I have a column 'A' in two dataframes, say df1 and df2. </p>

<p>df1:</p>

<pre><code>|  ID  | A |  |
|------|---|--|
| ID1  | 5 |  |
| ID2  | 6 |  |
| ID3  | 7 |  |
| ID4  | 8 |  |
</code></pre>

<p>df2: </p>

<pre><code>|  ID  | A |  |
|------|---|--|
| ID1  | 5 |  |
| ID2  | 1 |  |
| ID3  | 8 |  |
| ID5  | 7 |  |
| ID6  | 8 |  |
| ID7  | 9 |  |
</code></pre>

<p>Required updated df1: </p>

<pre><code>|  ID  | A |  |
|------|---|--|
| ID1  | 5 |  |
| ID2  | 1 |  |
| ID3  | 8 |  |
| ID4  | 8 |  |
| ID5  | 7 |  |
| ID6  | 8 |  |
| ID7  | 9 |  |
</code></pre>

<p>I want to update the column 'A' in df1 with values from df2 if the ID is in df2,  else I want to keep the same value in df1. Moreover, if there are new IDs in df2, I want to add the new values in df1.</p>

<p>I have seen the documentation of pd.DataFrame.update it does update the values from the df2 to df1 but it does not adds the new values to df1.  Any help will be appreciated. Thanks in advance. </p>
","9863102","","","updating and merging a column in pandas dataframe","<python><pandas><dataframe><algorithmic-trading>","2","0","979"
"50637472","2018-06-01 06:39:31","2","","<p>I suggest you print the matrix you create:</p>

<pre><code>#           [,1]      [,2]       [,3]
#[1,] -0.4442772 0.2783537 -0.3190043
#[2,] -0.1844143 0.2599441  0.2062185
</code></pre>

<p>That's not the same as your Wolfram Alpha link shows.</p>

<p>You get the same result as in python and Mathematica if you create the same matrix:</p>

<pre><code>hidden &lt;- matrix(c(-0.4442772, -0.1844143, 0.2783537, 0.2599441, -0.3190043, 0.2062185), nrow=2, ncol=3, 
            byrow = TRUE) #fill matrix by row instead of default by column 
#           [,1]       [,2]      [,3]
#[1,] -0.4442772 -0.1844143 0.2783537
#[2,]  0.2599441 -0.3190043 0.2062185
Xp &lt;- c(0, 1, 1)
hidden %*% Xp
#           [,1]
#[1,]  0.0939394
#[2,] -0.1127858
</code></pre>
","1412059","","","2","754","Roland","2012-05-23 08:09:10","106397","13644","2913","6104","50637376","50637472","2018-06-01 06:32:58","0","50","<p>I am transcripting a multi-layer perceptron algorithm from R program to python, however I am facing with a problem, the matrix multiplication has given me a different results:</p>

<p>R language:</p>

<pre><code>&gt; hidden &lt;- matrix(c(-0.4442772, -0.1844143, 0.2783537, 0.2599441, -0.3190043, 0.2062185), nrow=2, ncol=3) 
&gt; Xp &lt;- c(0, 1, 1)
&gt; hidden %*% Xp

           [,1]
[1,] -0.0406506
[2,]  0.4661626
</code></pre>

<p>Python:</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; Xp = np.array([0, 1, 1])
&gt;&gt;&gt; hidden = np.asmatrix([[-0.4442772, -0.1844143, 0.2783537], [0.2599441, -0.3190043, 0.2062185]])
&gt;&gt;&gt; np.dot(hidden, nq)

matrix([[ 0.0939394, -0.1127858]])
</code></pre>

<p>Using the <a href=""https://www.wolframalpha.com/input/?i=%7B%7B-0.4442772,%20-0.1844143,%20%200.2783537%7D,%20%7B0.2599441,%20-0.3190043,%20%200.2062185%7D%7D*%7B0,1,%201%7D"" rel=""nofollow noreferrer"">Wolfram</a>, Python would be right, but even that it is not converging to the right result of algorithm. I searched for its difference between both languages in this aspects and did not find nothing. Besides that, I applied another matrix multiplication to test and both results was equal:</p>

<p>R language:</p>

<pre><code>&gt; m1 &lt;- matrix(c(2,3,4,7,0,1), nrow = 3, ncol=2)
&gt; m2 &lt;- matrix(c(1,6,4,3,6,9), nrow = 2, ncol=3)
&gt; m1 %*% m2

     [,1] [,2] [,3]
[1,]   44   29   75
[2,]    3   12   18
[3,]   10   19   33
</code></pre>

<p>Python:</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; m1 = np.matrix([[2,7], [3,0], [4,1]])
&gt;&gt;&gt; m2 = np.matrix([[1,4,6], [6,3,9]])
&gt;&gt;&gt; np.dot(m1,m2)

matrix([[44, 29, 75],
        [ 3, 12, 18],
        [10, 19, 33]])
</code></pre>

<p>What is the difference in this case? And how can I reach this results in Python?</p>
","6373949","","","Different results in matrix multiplication using Python and R language","<python><r><numpy><matrix>","1","0","1833"
"50637484","2018-06-01 06:40:20","0","","<p>Use <code>str.zfill(2)</code> to pad your date pieces with 2 leading zeros:</p>

<pre><code>date = [2, 5, 2018]
text = ""%s/%s/%s"" % tuple([str(date_part).zfill(2) for date_part in date])
print(text) # Outputs: 02/05/2018
</code></pre>
","7663649","","","0","238","Pierre","2017-03-06 00:24:56","658","36","318","6","50637431","","2018-06-01 06:36:50","0","57","<pre><code>date = [2, 5, 2018]
text = ""%s/%s/%s"" % tuple(date)
print(text)
</code></pre>

<p>It gives result <code>2/5/2018</code>.How to convert it like <code>02/05/2018</code></p>
","9304462","1222951","2018-06-01 06:37:48","Python convert tuple into string","<python><python-2.7>","3","0","182"
"50637498","2018-06-01 06:41:01","2","","<pre><code>text = ""{:02d}/{:02d}/{:d}"".format(*date) 
</code></pre>
","8033585","","","0","68","AGN Gazer","2017-05-18 21:39:33","6041","460","707","281","50637431","","2018-06-01 06:36:50","0","57","<pre><code>date = [2, 5, 2018]
text = ""%s/%s/%s"" % tuple(date)
print(text)
</code></pre>

<p>It gives result <code>2/5/2018</code>.How to convert it like <code>02/05/2018</code></p>
","9304462","1222951","2018-06-01 06:37:48","Python convert tuple into string","<python><python-2.7>","3","0","182"
"50637516","2018-06-01 06:42:02","0","","<p>First for loop in pandas is best avoid if some vectorized solution exist.</p>

<p>I think <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer""><code>merge</code></a> with left join is necessary, parameter <code>on</code> should be omit if only <code>col1</code> is same in both <code>DataFrame</code>s:</p>

<pre><code>df3 = df2.merge(df1, how='left')
print (df3)
  col1  col2  col3
0    a   1.0   2.0
1    f   NaN   NaN
2    c   NaN   NaN
3    d   4.0   4.0
</code></pre>
","2901002","2901002","2018-06-01 06:52:32","2","541","jezrael","2013-10-20 20:27:26","427380","89269","18260","743","50637465","50637516","2018-06-01 06:39:12","1","35","<p>I want python to perform updating of values next to a value found in both dataframes (somewhat similar to VLOOKUP in MS Excel). So, for</p>

<pre><code>import pandas as pd
df1 = pd.DataFrame(data = {'col1':['a', 'b', 'd'], 'col2': [1, 2, 4], 'col3': [2, 3, 4]})
df2 = pd.DataFrame(data = {'col1':['a', 'f', 'c', 'd']})
In [3]: df1
Out[3]: 
        col1    col2    col3
      0  a        1       2
      1  b        2       3
      2  d        4       4

In [4]: df2
Out[4]: 
        col1    
      0  a        
      1  f        
      2  c       
      3  d
</code></pre>

<p>Outcome must be the following:</p>

<pre><code>In [6]: df3 = *somecode*
        df3
Out[6]: 
        col1    col2    col3
      0  a        1       2
      1  f                
      2  c                
      3  d        4       4
</code></pre>

<p>The main part is that I want some sort of ""for loop"" to do this. </p>

<p>So, for instance python searches for first value in col1 in df2, finds it in df1, and updates col2 and col3 respectivly, then moves forward.</p>
","9879398","","","for loop for searching value in dataframe and updating values next to it","<python><pandas><for-loop><dataframe>","2","0","1049"
"50637519","2018-06-01 06:42:11","0","","<p>first find the duplicated elements and then remove them from the original list.</p>

<pre><code>dup_list = [item for item in temp if item in my_list]

for ele in dup_list:
    my_list.remove(ele)
</code></pre>

<p><a href=""https://www.tutorialspoint.com/python/list_remove.htm"" rel=""nofollow noreferrer"">remove() source</a></p>
","6468916","","","0","331","Mohan Babu","2016-06-15 10:09:53","301","50","79","0","50635960","50636165","2018-06-01 04:07:21","1","900","<p>I am curious. How can I correctly iterate through a list, compare two values and delete the duplicate if it exists. </p>

<p>Here I created a nested for loop:</p>

<pre><code>my_list =  [ 1, 2, 3, 4, 5 ]
temp = [1, 5, 6]

def remove_items_from_list(ordered_list, temp):
    # Removes all values, found in items_to_remove list, from my_list
        for j in range(0, len(temp)):
                for i in range(0, len(ordered_list)):
                        if ordered_list[i] == temp[j]:
                                ordered_list.remove(ordered_list[i])
</code></pre>

<p>But when I execute my my code I get an error:</p>

<pre><code>  File ""./lab3f.py"", line 15, in remove_items_from_list
    if ordered_list[i] == items_to_remove[j]:
</code></pre>

<p>can anyone explain why?</p>

<p>This question, wanted to me compare two lists with one another, and these lists have two different lengths. If an item in list a matched a value in list b, we wanted then to delete it from list a. </p>
","8729657","8729657","2018-06-01 16:09:03","Iterate through a list, compare values and remove duplicate - Python","<python><python-3.x>","4","4","993"
"50637530","2018-06-01 06:42:42","0","","<p>try this,</p>

<p>Simple left join will solve your problem,</p>

<pre><code>pd.merge(df2,df1,how='left',on=['col1'])

  col1  col2  col3
0    a   1.0   2.0
1    f   NaN   NaN
2    c   NaN   NaN
3    d   4.0   4.0
</code></pre>
","4684861","","","0","230","Mohamed Thasin ah","2015-03-18 10:49:38","4803","833","1351","258","50637465","50637516","2018-06-01 06:39:12","1","35","<p>I want python to perform updating of values next to a value found in both dataframes (somewhat similar to VLOOKUP in MS Excel). So, for</p>

<pre><code>import pandas as pd
df1 = pd.DataFrame(data = {'col1':['a', 'b', 'd'], 'col2': [1, 2, 4], 'col3': [2, 3, 4]})
df2 = pd.DataFrame(data = {'col1':['a', 'f', 'c', 'd']})
In [3]: df1
Out[3]: 
        col1    col2    col3
      0  a        1       2
      1  b        2       3
      2  d        4       4

In [4]: df2
Out[4]: 
        col1    
      0  a        
      1  f        
      2  c       
      3  d
</code></pre>

<p>Outcome must be the following:</p>

<pre><code>In [6]: df3 = *somecode*
        df3
Out[6]: 
        col1    col2    col3
      0  a        1       2
      1  f                
      2  c                
      3  d        4       4
</code></pre>

<p>The main part is that I want some sort of ""for loop"" to do this. </p>

<p>So, for instance python searches for first value in col1 in df2, finds it in df1, and updates col2 and col3 respectivly, then moves forward.</p>
","9879398","","","for loop for searching value in dataframe and updating values next to it","<python><pandas><for-loop><dataframe>","2","0","1049"
"50637546","2018-06-01 06:44:07","0","","<p>Inside the app, there will be a directory for migrations where the django will store your model changes as migrations files.</p>

<p>To see the query you have to use the command <code>python manage.py sqlmigrate {your_app_name;{eg :polls}} {migrations_file_name:{eg:0001_init.py}}</code></p>

<p>command: </p>

<pre><code>python manage.py sqlmigrate polls 0001
</code></pre>

<p>Hope this will help </p>
","8750147","2995527","2018-06-01 07:15:57","0","407","Dipu Muraleedharan","2017-10-10 05:46:22","13","4","0","0","26816915","","2014-11-08 12:02:50","0","1999","<p>I am just learning Django, I have created a model inside an app Book</p>

<pre><code>from django.db import models

class Book(models.Model):
    title = models.CharField(max_length=100)
    author = models.CharField(max_length=50)
    read = models.CharField(max_length=50)
</code></pre>

<p>Now I want to generate it's correspondence SQL sentence with the help of</p>

<pre><code>python manage.py sql books
</code></pre>

<p>But it's showing me error </p>

<blockquote>
  <p>CommandError: App 'Books' has migrations. Only the sqlmigrate and sqlflush commands can be used when an app has migrations.</p>
</blockquote>

<p>I have used <code>makemigrartion</code> and <code>migrate</code> command it's showing no migration is remaining.</p>

<p>Can anyone have any idea regarding to this error?</p>
","3886602","396300","2014-11-08 13:55:55","Django Migration-SQL Server","<python><django><django-forms>","3","1","800"
"50637601","2018-06-01 06:47:43","0","","<pre><code>date = [2, 5, 2018]
text = ""{:0&gt;2}/{:0&gt;2}/{}"".format(*date)
print(text)
</code></pre>

<p>to learn more about using <code>format</code>, read: <a href=""https://pyformat.info/"" rel=""nofollow noreferrer"">https://pyformat.info/</a></p>
","4350517","","","0","250","Sebastian Loehner","2014-12-11 15:00:53","877","43","21","6","50637431","","2018-06-01 06:36:50","0","57","<pre><code>date = [2, 5, 2018]
text = ""%s/%s/%s"" % tuple(date)
print(text)
</code></pre>

<p>It gives result <code>2/5/2018</code>.How to convert it like <code>02/05/2018</code></p>
","9304462","1222951","2018-06-01 06:37:48","Python convert tuple into string","<python><python-2.7>","3","0","182"
"50637626","2018-06-01 06:49:20","1","","<p>In the future, before performing any email campaign, clean your mailing list with MailboxValidator. MailboxValidator has integrated with Mailgun to easily import your list, clean it of invalid emails and then update Mailgun automatically.</p>

<p><a href=""https://www.mailboxvalidator.com/resources/articles/how-to-import-email-list-from-mailgun/"" rel=""nofollow noreferrer"">https://www.mailboxvalidator.com/resources/articles/how-to-import-email-list-from-mailgun/</a></p>
","6647585","","","0","476","Vlam","2016-07-28 03:13:18","1033","110","13","1","43990078","","2017-05-15 23:03:06","0","284","<p>I have setup a mailing list via Mailgun, newsletter@example.com. When I send a message to this address with my Mailgun API key, the message is delivered to all of the members of the mailing list. </p>

<p><strong>Core Issue:</strong> Mailgun also attempts to deliver the message to newsletter@example.com itself, which is an email address that doesn't exist, resulting in the below error in our logs each day</p>

<pre><code>Failed: morningalert@example.com → newsletter@example.com 'Morning Report: 2017-05-12' Not delivering to previously bounced address
</code></pre>

<p>I could simply ignore this, but it does put a skew on our analytics and makes it difficult for us to notice meaningful errors since it shows a bounce every time. How can I address this so we do not cause this bounce error when sending to our mailing list? My (poorly written) Python code is below. Thank you!</p>

<pre><code>def standard_message(key, to, from_email, from_name, subject, body, delivery_time=False, replyto=False):
    url = ""https://api.mailgun.net/v3/tradedefender.com/messages""
    auth = (""api"", key)
    data =  {""from"": from_name + "" &lt;"" + from_email + ""&gt;"",
            ""to"": to,
            ""subject"": subject,
            ""text"": body}

    if(delivery_time != False):
        data[""o:deliverytime""] = delivery_time

    if(replyto != False):
        data[""h:Reply-To""] = replyto

    response = requests.post(url, auth=auth, data=data)

    if(""200"" in str(response)):
        return(True)
    else:
        raise Exception(response)
</code></pre>
","3418425","","","Mailgun Mailing List Address Bounced","<python><python-2.7><python-3.x><mailgun>","2","0","1555"
"50637632","2018-06-01 06:50:04","0","","<p>For readabilities sake, it might be better to do</p>

<pre><code>if isinstance(accents['ac'], str):
    pass #insert what you want to happen here when it is a string
else:
    for(ac in accents['ac']):
        pass #insert what you want to happen here when it is a list
</code></pre>
","9861056","","","0","287","Abe Binder","2018-05-28 21:51:11","31","2","0","0","50637413","50638382","2018-06-01 06:35:21","0","38","<p>Currently i am getting information from an xml file. 
If an xml tag has more then one children it will return as a list inside that tag, however if that xml tag has only 1 child it will return not as a list and only as a regular string.</p>

<p>My question is: is there a better way to iterate through this tag? if it is a list, iterate through the list length amount of times, but if it is a string only iterate once?</p>

<p>This is my current approach:</p>

<pre><code> #check if tag is a list, if not then make a list with empty slot at end
 if not isinstance(accents['ac'], list):
      accents['ac'] = list((accents['ac'], {}))

 #loop through guaranteed list
 for ac in accents['ac']: #this line throws error if not list object!

      #if the empty slot added is encountered at end, break out of loop
      if bool(ac) == False:
            break
</code></pre>

<p>any ideas on how to make this cleaner or more professional is appreciated.</p>
","5859663","","","Proper way to iterate through a list which may not be a list","<python><python-3.x><list><iteration>","3","3","955"
"50637638","2018-06-01 06:50:07","0","","<p>Just replace <code>\s</code> by <code>\b</code>:</p>

<pre><code>import re

lines = [
    ""R_d_10763 VDDI_1007 Group_BGA_BGA_VDD_INT_AD26 3.711438e+000"",
    ""R_d_10771 VDDI_1007 VDDI_1012 1.337785e+001"",
    ""R_d_8607 Group_BGA_BGA_VSS_AH6 Group_BGA_BGA_VSS_AJ43 3.777161e+000"",
    ""R_d_6585 Group_BGA_BGA_AVDD085_MIPI_DPHY_Y36 Group_BGA_BGA_AVDD085_MIPI_DPHY_W35_SINK_ 3.860682e-003"",
    ""R_d_69804 Xm4s4s2_M_DPDATA0_580 Group_BGA_BGA_XMIPI_DSI_D0_P_V38_SINK_ 2.668494e-001"",
    ""R_d_69668 XCP_ANT_SW1_383 Group_BGA_BGA_XCP_ANT_SW1_N5_SINK_ 5.037550e-001"",
    ""R_d_13 AVDD085_MIPI_DPHY_540 Group_BGA_BGA_AVDD085_MIPI_DPHY_Y36 2.854267e-002"",
]


for line in lines:
    conn1, conn2, res = line.split()[1:]
    match1 = re.search(r""(A?[A-Z][0-9]{1,2})(?:_SINK|\b)"", conn1)
    match2 = re.search(r""(A?[A-Z][0-9]{1,2})(?:_SINK|\b)"", conn2)
    try:
        conn1 = match1.group(1)
    except AttributeError:
        pass
    try:
        conn2 = match2.group(1)
    except AttributeError:
        pass
    print(f""Connection 1: {conn1}  Connection 2: {conn2}  Resistance: {res}"")
</code></pre>

<p>Output:</p>

<pre><code>Connection 1: VDDI_1007  Connection 2: AD26  Resistance: 3.711438e+000
Connection 1: VDDI_1007  Connection 2: VDDI_1012  Resistance: 1.337785e+001
Connection 1: AH6  Connection 2: AJ43  Resistance: 3.777161e+000
Connection 1: Y36  Connection 2: W35  Resistance: 3.860682e-003
Connection 1: Xm4s4s2_M_DPDATA0_580  Connection 2: V38  Resistance: 2.668494e-001
Connection 1: XCP_ANT_SW1_383  Connection 2: N5  Resistance: 5.037550e-001
Connection 1: AVDD085_MIPI_DPHY_540  Connection 2: Y36  Resistance: 2.854267e-002
</code></pre>
","9586338","","","1","1658","Waket Zheng","2018-04-02 13:50:59","1227","101","88","7","50636902","50637638","2018-06-01 05:55:19","0","59","<p>I am at a loss on why this isn't matching all the occurances. I even put it in a RE tool online and it should match. I am running on Linux with 3.6.</p>

<p>It only matches the search when it is followed by the '_SINK' but not when followed by '\s'.</p>

<p>Here is a snippet of my search text and RE:</p>

<pre><code>import re

lines = ['R_d_10763 VDDI_1007 Group_BGA_BGA_VDD_INT_AD26 3.711438e+000',
         'R_d_10771 VDDI_1007 VDDI_1012 1.337785e+001',
         'R_d_8607 Group_BGA_BGA_VSS_AH6 Group_BGA_BGA_VSS_AJ43 3.777161e+000',
         'R_d_6585 Group_BGA_BGA_AVDD085_MIPI_DPHY_Y36 Group_BGA_BGA_AVDD085_MIPI_DPHY_W35_SINK_ 3.860682e-003',
         'R_d_69804 Xm4s4s2_M_DPDATA0_580 Group_BGA_BGA_XMIPI_DSI_D0_P_V38_SINK_ 2.668494e-001',
         'R_d_69668 XCP_ANT_SW1_383 Group_BGA_BGA_XCP_ANT_SW1_N5_SINK_ 5.037550e-001',
         'R_d_13 AVDD085_MIPI_DPHY_540 Group_BGA_BGA_AVDD085_MIPI_DPHY_Y36 2.854267e-002',]


for line in lines:
    conn1, conn2, res = line.split()[1:]
    match1 = re.search(r'(A?[A-Z][0-9]{1,2})(?:_SINK|\s)', conn1)
    match2 = re.search(r'(A?[A-Z][0-9]{1,2})(?:_SINK|\s)', conn2)
    try:
        conn1 = match1.group(1)
    except AttributeError:
        pass
    try:
        conn2 = match2.group(1)
    except AttributeError:
        pass
    print(f'Connection 1: {conn1}  Connection 2: {conn2}  Resistance: {res}')
</code></pre>

<p>The output I get is:</p>

<pre><code>Connection 1: VDDI_1007  Connection 2: Group_BGA_BGA_VDD_INT_AD26  Resistance: 3.711438
Connection 1: VDDI_1007  Connection 2: VDDI_1012  Resistance: 13.37785
Connection 1: Group_BGA_BGA_VSS_AH6  Connection 2: Group_BGA_BGA_VSS_AJ43  Resistance: 3.777161
Connection 1: Group_BGA_BGA_AVDD085_MIPI_DPHY_Y36  Connection 2: W35  Resistance: 0.003860682
Connection 1: Xm4s4s2_M_DPDATA0_580  Connection 2: V38  Resistance: 0.2668494
Connection 1: XCP_ANT_SW1_383  Connection 2: N5  Resistance: 0.5037550
Connection 1: AVDD085_MIPI_DPHY_540  Connection 2: Group_BGA_BGA_AVDD085_MIPI_DPHY_Y36  Resistance: 0.02854267
</code></pre>

<p>But it should be</p>

<pre><code>Connection 1: VDDI_1007  Connection 2: AD26  Resistance: 3.711438
Connection 1: VDDI_1007  Connection 2: VDDI_1012  Resistance: 13.37785
Connection 1: AH6  Connection 2: AJ43  Resistance: 3.777161
Connection 1: Y36  Connection 2: W35  Resistance: 0.003860682
Connection 1: Xm4s4s2_M_DPDATA0_580  Connection 2: V38  Resistance: 0.2668494
Connection 1: XCP_ANT_SW1_383  Connection 2: N5  Resistance: 0.5037550
Connection 1: AVDD085_MIPI_DPHY_540  Connection 2: Y36  Resistance: 0.02854267
</code></pre>

<p>Thanks,</p>
","9143296","4237254","2018-06-01 10:20:43","regular expression not matching white space","<python><regex><python-3.x>","2","3","2596"
"50637667","2018-06-01 06:51:54","0","","<p>here's a version of joinfoo that gives what you want:</p>

<pre><code>def empty(item):  # added this function
   if item is None:
      return True
   else:
      return not any(item)


def joinfoo(items):
   if len(items) == 1:
      return items[0]

   result = []
   active = None
   y_last = None  # added this
   for x, y in zip(items[0], joinfoo(items[1:])):
      active = x if x else active
      if not empty(y_last) and empty(y):  # added this if statement
         active = None
      y_last = y  # added this
      if type(y) is tuple:
         result.append((active, y[0], y[1]))
      else:
         result.append((active, y))

   return result
</code></pre>

<p>Every time the y entry switches back to None, you want ""active"" to switch back to None too.</p>

<p>btw, as it's written joinfoo doesn't work for joining any more than 3 lists. if you need it to,</p>

<p>replace <code>result.append((active, y[0], y[1]))</code> with <code>result.append((active, *y))</code>.</p>
","9878957","9878957","2018-06-01 08:54:40","5","992","Jay Calamari","2018-06-01 02:37:50","335","32","205","2","50636877","50638769","2018-06-01 05:52:42","1","62","<p>I am parsing a file where labels are defined as below, with hierarchies represented by using new lines</p>

<pre><code>+--------------------+--------------------+--------------------+
| L1 - A             |                    |                    |
|                    |  L2 - B            |                    |
|                    |                    |  L3 - C            |
|                    |                    |                    |
| L1 - D             |                    |                    |
|                    |  L2 - E            |                    |
|                    |                    |  L3 - F            |
+--------------------+--------------------+--------------------+
</code></pre>

<p>I represent the above as:</p>

<pre><code>labels = [
   ['A', None, None, None, 'D', None, None],
   [None, 'B', None, None, None, 'E', None],
   [None, None, 'C', None, None, None, 'F']
]
</code></pre>

<p>I tried</p>

<pre><code>def joinfoo(items):
   if len(items) == 1:
      return items[0]

   result = []
   active = None
   for x, y in zip(items[0], joinfoo(items[1:])):
      active = x if x else active
      if type(y) is tuple:
         result.append((active, y[0], y[1]))
      else:
         result.append((active, y))

   return result
</code></pre>

<p>I wanted </p>

<pre><code>[
   ('A', None, None), ('A', 'B', None), ('A', 'B', 'C'),
   (None, None, None),
   ('D', None, None), ('D', 'E', None), ('D', 'E', 'F')
]
</code></pre>

<p>and got this</p>

<pre><code>[
   ('A', None, None), ('A', 'B', None), ('A', 'B', 'C'),
   ('A', 'B', None),
   ('D', 'B', None), ('D', 'E', None), ('D', 'E', 'F')
]
</code></pre>

<p>Suggestions on how to fix <code>joinfoo()</code> to achieve the desired result? Solution needs to support a variable number of columns.</p>

<p>It should be something like <code>for x, y in zip(joinfoo(items[:-1]), items[-1]):</code> instead of <code>for x, y in zip(items[0], joinfoo(items[1:])):</code> to go in the right direction...?</p>

<p>Edit:
The original list of lists may have wrongly implied a pattern to the hierarchy. There is no defined pattern. Number of columns are also variable. A better test case maybe.. </p>

<pre><code>+--------------+--------------+--------------+
|   L1 - A     |              |              |    = A
|              |    L2 - B    |              |    = A - B
|              |              |    L3 - C    |    = A - B - C
|              |              |    L3 - D    |    = A - B - D
|              |    L2 - E    |              |    = A - E
|              |              |              |    =   
|   L1 - F     |              |              |    = F
|              |    L2 - G    |              |    = F - G
|              |              |    L3 - H    |    = F - G - H
+--------------+--------------+--------------+

labels = [
   ['A', None, None, None, None, None, 'F', None, None],
   [None, 'B', None, None, 'E', None, None, 'G', None],
   [None, None, 'C', 'D', None, None, None, None, 'H']
]
</code></pre>
","2193381","2193381","2018-06-01 09:12:30","Denormalizing hierarchy in list of lists","<python><python-2.7>","3","2","3017"
"50637731","2018-06-01 06:56:59","0","","<p>The reason is I initialize all weights and bias to zero. If that so, all of the output of the neurons will be the same. The back propagation behavior of all neurons within the same layer is the same - the same gradient, weight update is the same.This is clearly an unacceptable result.</p>
","806088","","","0","293","xiao su","2011-06-20 05:48:59","69","111","35","0","50634776","50637731","2018-06-01 01:09:06","0","362","<p>I am a novice of the tensorflow and python. I modified a sample tensorflow code by adding one hidden layer with 50 units, but the accuracy result turned to be wrong and it was not changed no matter how many times the model do training. I cannot find any problem with the code. The dataset is MNIST: </p>

<pre><code>import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets(""MNIST_data"", one_hot = True)

batch_size = 100
n_batch = mnist.train.num_examples // batch_size

x = tf.placeholder(tf.float32, [None, 784])
y = tf.placeholder(tf.float32, [None, 10])


W = tf.Variable(tf.zeros([784, 50]))
b = tf.Variable(tf.zeros([50]))

Wx_plus_b_L1 = tf.matmul(x,W) + b
L1 = tf.nn.relu(Wx_plus_b_L1)

W_2 = tf.Variable(tf.zeros([50, 10]))
b_2 = tf.Variable(tf.zeros([10]))

prediction = tf.nn.softmax(tf.matmul(L1, W_2) + b_2)


loss = tf.reduce_mean(tf.square(y - prediction))
train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)


correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1))

accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

init = tf.global_variables_initializer()


with tf.Session() as sess:
   sess.run(init)
   for epoch in range(21):
    for batch in range(n_batch):
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
        sess.run(train_step, feed_dict={x:batch_xs, y:batch_ys})
    acc = sess.run(accuracy, feed_dict = {x:mnist.test.images, y:mnist.test.labels})
    print(""Iter:"" + str(epoch) + "", Testing Accuray:"" + str(acc))
</code></pre>

<p>The output always be the same accuracy:
    <code>
Iter:0, Testing Accuray:0.1135
2018-05-31 18:05:21.039188: W tensorflow/core/framework/allocator.cc:101] Allocation of 31360000 exceeds 10% of system memory.
Iter:1, Testing Accuray:0.1135
2018-05-31 18:05:22.551525: W tensorflow/core/framework/allocator.cc:101] Allocation of 31360000 exceeds 10% of system memory.
Iter:2, Testing Accuray:0.1135
2018-05-31 18:05:24.070686: W tensorflow/core/framework/allocator.cc:101] Allocation of 31360000 exceeds 10% of system memory.
</code>
What's wrong in this code? Thank you~~</p>
","806088","","","Why the accuracy of the training model is not changed in the tensorflow code?","<python><tensorflow>","3","0","2167"
"50637801","2018-06-01 07:01:45","0","","<p>If you don't want to import <code>Counter</code>, use a dictionary to keep track of the employee count per department:</p>

<pre><code>employee_list = [{
    'name': 'John',
    'empID': '102',
    'dpt': 'tech',
    'title': 'programmer',
    'salary': '75'
}, {
    'name': 'Jane',
    'empID': '202',
    'dpt': 'tech',
    'title': 'programmer',
    'salary': '80'
}, {
    'name': 'Joe',
    'empID': '303',
    'dpt': 'accounting',
    'title': 'accountant',
    'salary': '85'
}]

department_to_count = dict()
for employee in employee_list:
    department = employee[""dpt""]

    if department not in department_to_count:
        department_to_count[department] = 0

    department_to_count[department] += 1

for department, employee_count in department_to_count.items():
    print(""Department {} has {} employees"".format(department,
                                                  employee_count))
</code></pre>
","7663649","7663649","2018-06-01 07:07:19","1","924","Pierre","2017-03-06 00:24:56","658","36","318","6","50637701","50637801","2018-06-01 06:54:48","1","572","<p>python is  new language for me so this question might sound simple, but if someone can point me in the right direction, I would appreciate it! I created a dictionary call employees and it holds some value about them:</p>

<p>I'm trying to read how many people are in each department, for example: tech-2, accounting-1. </p>

<p>I have something like this, but it prints out blank.</p>

<pre><code>  def main():
   employees= {'name': 'John', 'empID': '102', 'dpt': 'tech', 'title': 
   'programmer', 'salary': '75'}
   {'name': 'Jane', 'empID': '202', 'dpt': 'tech', 'title': 'programmer', 
   'salary': '80'}
   {'name': 'Joe', 'empID': '303', 'dpt': 'accounting', 'title': 
   'accountant', 'salary': '85'}
    for item in employees:
    dic = employees[item]
    if dic['dpt'[0]]==dic['dpt'[1]]:
        duplicate += 1
        print(""there are ""+ duplicate)
    else:
        print(""There are no duplicate"")
</code></pre>
","2774460","1222951","2018-06-01 06:55:39","Counting values in an employee dictionary database in python","<python><python-3.x><dictionary><counter><key-value>","4","0","928"
"50637809","2018-06-01 07:02:10","2","","<p>Use <a href=""https://docs.python.org/3/library/collections.html#collections.Counter"" rel=""nofollow noreferrer""><code>collections.Counter</code></a>:</p>

<pre><code>from collections import Counter

employees = [{'name': 'John', 'empID': '102', 'dpt': 'tech', 'title': 'programmer', 'salary': '75'},
             {'name': 'Jane', 'empID': '202', 'dpt': 'tech', 'title': 'programmer', 'salary': '80'},
             {'name': 'Joe', 'empID': '303', 'dpt': 'accounting', 'title': 'accountant', 'salary': '85'}]

dpts = [x['dpt'] for x in employees]
print(Counter(dpts))

# Counter({'tech': 2, 'accounting': 1})
</code></pre>
","8472377","","","3","623","Austin","2017-08-16 11:54:23","18860","1780","141","2099","50637701","50637801","2018-06-01 06:54:48","1","572","<p>python is  new language for me so this question might sound simple, but if someone can point me in the right direction, I would appreciate it! I created a dictionary call employees and it holds some value about them:</p>

<p>I'm trying to read how many people are in each department, for example: tech-2, accounting-1. </p>

<p>I have something like this, but it prints out blank.</p>

<pre><code>  def main():
   employees= {'name': 'John', 'empID': '102', 'dpt': 'tech', 'title': 
   'programmer', 'salary': '75'}
   {'name': 'Jane', 'empID': '202', 'dpt': 'tech', 'title': 'programmer', 
   'salary': '80'}
   {'name': 'Joe', 'empID': '303', 'dpt': 'accounting', 'title': 
   'accountant', 'salary': '85'}
    for item in employees:
    dic = employees[item]
    if dic['dpt'[0]]==dic['dpt'[1]]:
        duplicate += 1
        print(""there are ""+ duplicate)
    else:
        print(""There are no duplicate"")
</code></pre>
","2774460","1222951","2018-06-01 06:55:39","Counting values in an employee dictionary database in python","<python><python-3.x><dictionary><counter><key-value>","4","0","928"
"50637888","2018-06-01 07:07:35","0","","<p>You need <code>name</code> attributes in your html inputs if you want the browser to send data fur them. This is nothing to do with Django.</p>
","104349","","","2","147","Daniel Roseman","2009-05-10 12:36:13","489411","52610","12851","10717","50637081","","2018-06-01 06:12:20","0","48","<p>Right now I'm stuck between two things: Either I have a login form that works but isn't exactly what you call visually appealing and one that looks flawless but doesn't do anything. (I know quite the situation)</p>

<p><a href=""https://i.stack.imgur.com/OJqCN.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OJqCN.jpg"" alt=""If you look at this image right here you&#39;ll know exactly what I&#39;m talking about:""></a></p>

<p>As shown the one up top is clearly quite the looker (sarcasm*), but I want the hideous one at the bottom to work.</p>

<p>For the for up top, I just used <code>{{ form }}</code> tag and have at one point my code looked like this: </p>

<pre><code>&lt;form method='POST' action='' enctype='multipart/form-data'&gt;{% csrf_token %}

    {{ form.username.label_tag }}
    {{ form.username }}
    {{ form.password.label_tag }}
    {{ form.password }}

   &lt;input type='submit' class='btn btn-primary btn-block' value='{{ title }}' /&gt;
&lt;/form&gt;
</code></pre>

<p>But again this doesn't give me the look I want. As of right now, to get the non-functional (but pretty) form look at the bottom I'm using the code for the <a href=""https://getbootstrap.com/docs/4.0/examples/sign-in/"" rel=""nofollow noreferrer"">bootstrap template signin</a> (signin.css included in my base template). This is what my form.html looks like right now: </p>

<pre><code>{% extends 'accounts/base.html' %}
{% block main_content %}
    &lt;div class=""container""&gt;
        &lt;div class='col-sm-6 col-sm-offset-3'&gt;
        &lt;form method='POST' action='' enctype='multipart/form-data' class=""form-signin""&gt;{% csrf_token %}
            {{ form}}
            &lt;h2 class=""h3 mb-3 font-weight-normal""&gt;Please Sign In&lt;/h2&gt;
            &lt;input type=""username"" id=""inputUsername"" class=""form-control"" placeholder=""Username"" required autofocus&gt;
            &lt;input type=""password"" id=""inputPassword"" class=""form-control"" placeholder=""Password"" required&gt;
            &lt;input type='submit' class='""btn btn-lg btn-primary btn-block' value='{{ title }}' /&gt;
        &lt;/form&gt;
        &lt;/div&gt;
    &lt;/div&gt;
{% endblock %}
</code></pre>

<p>I have seen another StackOverflow post <a href=""https://stackoverflow.com/questions/38496439/django-fields-assign-to-html-fields"">such as this one</a> and tried installing <code>django-widget-tweaks</code> but since I'm using cloud9-ide it didn't work </p>

<p><a href=""https://i.stack.imgur.com/2aO79.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2aO79.jpg"" alt=""cloud9-ide it didn&#39;t work""></a></p>

<p>So please if there's a way where I could either somehow implement the form tags in my Html or successfully download django-tweaks, please let me know and I would greatly appreciate it.</p>

<p>Thanks in advance.</p>
","9820620","5689074","2018-06-01 10:19:28","Is there a way to link input fields from HTML to the ACTUAL form fields used in DJANGO","<python><html><django><twitter-bootstrap>","2","4","2840"
"50637892","2018-06-01 07:07:47","1","","<p>For your first question, it looks like unclear. You should first figure out where is the bottleneck.</p>

<p>If I understand properly, every 1 second your extracting thread add 700 tasks into queue, and then every work thread just take one of these tasks and send a request to a remote server. Am I right?</p>

<p>In this case, does the network cause slow down? Could you confirm it? If you think <code>GIL</code> leads to poor performance when using more threads, does that mean there is some calculation bottleneck in your program?</p>

<p>And then for your second question, <code>gevent</code> will help you a lot if the slow down is caused by network.</p>
","5588279","","","2","663","Sraw","2015-11-21 05:17:37","9865","931","197","101","50637772","50637892","2018-06-01 06:59:59","0","45","<p>I make a python program that periodically (interval of 1 secs) grabs some data from ~700 files and queries to a server with the grabbed data. For a single query response time is about 2 ~ 3 msecs usually, but for some case it could take up to 200 msecs. The program consists of:</p>

<ol>
<li><p>One extracting thread: every 1 sec iterates over 700 files, grabs data then dispatches grabbed data to a shared queue of a so called query pool.</p></li>
<li><p>A pool of N threads: each thread picks data from the shared queue and sends query to the server.</p></li>
</ol>

<p>With N being 4 the program shows the best performance. If I increase N being 8 then the performance degrades significantly. I guess this is because of GIL of python.</p>

<p>Most of time the program works well but if there is a query that takes much time (due to late responding from the server) then the whole followed queries are dramatically affected.</p>

<p>I really want to take advantage of threading so I have been looking at <code>gevent</code> but not sure if it may help.</p>

<p>My questions:</p>

<ol>
<li><p>How does my current design look like? Is there a better design for it?</p></li>
<li><p>Will <code>gevent</code> help in this problem?</p></li>
</ol>
","2589553","2589553","2018-06-01 07:18:35","How can I improve performance of my multi-threaded python program?","<python><python-2.7><gevent><gil>","2","1","1247"
"50637905","2018-06-01 07:08:26","0","","<p>The key message here is <code>Coordinator node timed out waiting for replica nodes' responses</code> - this means that timeout happens inside Cassandra itself. In your case, request doesn't go directly to the node that owns the data, but to one of the nodes that acts as <code>Coordinator</code>, and this node then re-sends request to the node that owns the data, and doesn't get answer from it during allocated time.</p>

<p>I don't remember precisely, but Python driver should use so-called <code>TokenAware</code> load balancing policy that will make driver to send requests directly to nodes that own the data - you need to check that you use this policy.</p>

<p>Also, you need to check logs on the Cassandra nodes to find why they are timeout, and if necessary, tune timeouts on these nodes.</p>
","18627","","","5","806","Alex Ott","2008-09-19 07:53:08","32618","3898","2731","31","50630585","50637905","2018-05-31 18:25:59","1","265","<p>Every time I try to get query using the cassandra python driver, I will receive such an exception:</p>

<pre><code>**File ""something.py"", line 32, in &lt;module&gt;
    rows = session.execute('some query execution', timeout=None)
  File ""C:\Anaconda2\lib\site-packages\cassandra\cluster.py"", line 2141, in execute
    return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile, paging_state).result()
  File ""C:\Anaconda2\lib\site-packages\cassandra\cluster.py"", line 4033, in result
    raise self._final_exception
cassandra.ReadTimeout: Error from server: code=1200 [Coordinator node timed out waiting for replica nodes' responses] message=""Operation timed out - received only 0 responses."" info={'received_responses': 0, 'required_responses': 1, 'consistency': 'LOCAL_ONE'}**
</code></pre>

<p>To avoid this exception, I have already tried to set the default timeout to none, like:</p>

<pre><code>cluster.default_timeout = None
session.default_timeout = None
session.execute('some query execution', timeout=None)
</code></pre>

<p>However, they never really change the Readtimeout period.</p>

<p>One thing to notice is that this query command doesn't take too long of a time when I execute it in Squrriel, about 1.5 seconds.</p>

<p>Does anyone know how to solve this problem? Thank you!</p>
","8436348","","","Python cassandra driver Readtimeout","<python><cassandra><cql3><cassandra-driver>","1","3","1336"
"50637911","2018-06-01 07:08:41","0","","<p>suppposing that i want to add them to a list, i would check if its a nested tag first then add them.</p>

<pre><code> tag_list=[]    
 if(len(accents['ac'])&gt;1):
   for tag in accents['ac']:
       tag_list.append(tag)
 else:
   tag_list.append(tag)
</code></pre>
","1524289","","","0","269","Eliethesaiyan","2012-07-13 18:03:24","1662","431","324","21","50637413","50638382","2018-06-01 06:35:21","0","38","<p>Currently i am getting information from an xml file. 
If an xml tag has more then one children it will return as a list inside that tag, however if that xml tag has only 1 child it will return not as a list and only as a regular string.</p>

<p>My question is: is there a better way to iterate through this tag? if it is a list, iterate through the list length amount of times, but if it is a string only iterate once?</p>

<p>This is my current approach:</p>

<pre><code> #check if tag is a list, if not then make a list with empty slot at end
 if not isinstance(accents['ac'], list):
      accents['ac'] = list((accents['ac'], {}))

 #loop through guaranteed list
 for ac in accents['ac']: #this line throws error if not list object!

      #if the empty slot added is encountered at end, break out of loop
      if bool(ac) == False:
            break
</code></pre>

<p>any ideas on how to make this cleaner or more professional is appreciated.</p>
","5859663","","","Proper way to iterate through a list which may not be a list","<python><python-3.x><list><iteration>","3","3","955"
"50637935","2018-06-01 07:10:54","0","","<p>This sounds to me like some sort of homework assignment. If you need to make minimum number of changes to the existing function and you need to <em>find minimum yourself</em> (without calling built-in functions), then I would suggest:</p>

<pre><code>In [14]: import random
    ...: my_randoms=[]
    ...: minv = None
    ...: for i in range (10):
    ...:     my_randoms.append(random.randrange(1,101,1))
    ...:     if minv is None:
    ...:         minv = my_randoms[-1]
    ...:     elif my_randoms[-1] &lt; minv:
    ...:         minv = my_randoms[-1]
    ...: 
    ...: print (my_randoms)
    ...: print(minv)
    ...: 
[94, 24, 79, 75, 12, 57, 39, 37, 7, 96]
7
</code></pre>

<p>Even better, but at the expense of modifying/restructuring function's code:</p>

<pre><code>In [15]: import random
    ...: minv = random.randrange(1,101,1)
    ...: my_randoms = [minv]
    ...: for i in range (9):
    ...:     v = random.randrange(1,101,1)
    ...:     my_randoms.append(v)
    ...:     if v &lt; minv:
    ...:         minv = v
    ...: 
    ...: print (my_randoms)
    ...: print(minv)
    ...: 
[67, 33, 2, 83, 66, 93, 51, 57, 43, 55]
2
</code></pre>

<p>If you are allowed to use built-in functions, then simply call <code>min(my_randoms)</code> as suggested by <a href=""https://stackoverflow.com/questions/50637158/python-write-a-function-that-selects-the-smallest-number-from-a-list-of-10-rando/50637935#comment88284469_50637240"">@Aran-Frey</a>.</p>
","8033585","","","0","1464","AGN Gazer","2017-05-18 21:39:33","6041","460","707","281","50637158","","2018-06-01 06:18:15","0","75","<p>This function generates a list of 10 random numbers. I need to modify this function to select and print the smallest number from the randomly generated list Please help, I am having a very hard time with it:</p>

<pre><code>import random
my_randoms=[]
for i in range (10):
    my_randoms.append(random.randrange(1,101,1))

print (my_randoms)
</code></pre>
","9879485","1222951","2018-06-01 06:20:21","Python write a function that selects the smallest number from a list of 10 randomly generated numbers","<python><python-3.x>","3","1","359"
"50637979","2018-06-01 07:13:56","0","","<p>When we select a text we tkinter trigger(fire) 3 events, which are ButtonPress, Motion and ButtonRelease all these 3 events call a event handler fucntion.
The function run select_range(start, end) method which highlight selected text.</p>

<p>To disable this we have to hanlde ButtonPress and Motion events together and call select_clear method on the widget.</p>

<p>If you handle the events and call select_clear method it work but not properly, the text will be highlighted and when another event occured highligh color will be cleared.
This happend because of execution order of events. That's mean you have to tell to tk handle your event after default event handler. We can change order of events execution with bindtags and bin_class methods</p>

<p>example:
</p>

<pre><code>from tkinter import *
from tkinter import ttk


def on_select(event):
    event.widget.select_clear()  # clear selected text.


root = Tk()

name_entry = ttk.Entry(root)
name_entry.pack()

# with PostEvent tag, on_select the event hanlde after default event handler
name_entry.bindtags((str(name_entry), ""TEntry"", ""PostEvent"", ""."", ""all""))
name_entry.bind_class(""PostEvent"", ""&lt;ButtonPress-1&gt;&lt;Motion&gt;"", on_select)


root.mainloop()
</code></pre>
","9114103","","","0","1243","saeedghollami","2017-12-18 15:22:24","6","8","0","0","26208695","","2014-10-06 00:38:33","3","876","<p>How would that be done?
I have been unable to find it on here or with Google.</p>

<pre><code>#Refrences
from tkinter import *

class Interface:

    def __init__(self,stage):

        topLeft = Frame(stage,bg='black')
        test = Entry(topLeft,bg='#2a2a2a',fg='white',insertontime=0)
        test.config(insertbackground='white', exportselection=0)
        test.grid()
        topLeft.grid(row=0,column=0)

def launch():
    window = Tk()
    lobby = Interface(window)
    window.mainloop()

launch()
</code></pre>
","2625730","2864740","2014-10-06 00:41:03","How to disable selecting text in a Python/Tk program?","<python><user-interface><text><tkinter>","4","1","522"
"50637985","2018-06-01 07:14:14","1","","<p>Well, your path used in <code>find</code>is obviously wrong.
I would suggest, you reread the documentation, especially about the <a href=""https://docs.python.org/2/library/xml.etree.elementtree.html#supported-xpath-syntax"" rel=""nofollow noreferrer"">supported XPATH syntax</a></p>

<pre><code>import xml.etree.ElementTree as ET
tree = ET.parse(xml_file)
root = tree.getroot() # element: jmeterTestPlan
</code></pre>

<p>while this works for me</p>

<pre><code>print(root.find('./hashTree/hashTree/ThreadGroup/longProp[@name=""ThreadGroup.end_time""]').text)
</code></pre>

<p>It might be better to use something like this.
Especially if you want to replace multiple values:</p>

<pre><code>names_and_values = {
    ""WebDriverConfig.maximize_browser"" : ""true"",
    ""WebDriverConfig.reset_per_iteration"": ""true"",
    # ...
}

for name, value in names_and_values.items():
    element = next(root.iterfind(f'.//*[@name=""{name}""]'))
    element.text = value

print(ET.tostring(root)) 
</code></pre>
","4350517","","","0","994","Sebastian Loehner","2014-12-11 15:00:53","877","43","21","6","50617931","50637985","2018-05-31 06:25:52","1","27","<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;jmeterTestPlan version=""1.2"" properties=""3.2"" jmeter=""3.3 r1808647""&gt;
  &lt;hashTree&gt;
    &lt;TestPlan guiclass=""TestPlanGui"" testclass=""TestPlan"" testname=""Test Plan"" enabled=""true""&gt;
      &lt;stringProp name=""TestPlan.comments""&gt;&lt;/stringProp&gt;
      &lt;boolProp name=""TestPlan.functional_mode""&gt;false&lt;/boolProp&gt;
      &lt;boolProp name=""TestPlan.serialize_threadgroups""&gt;false&lt;/boolProp&gt;
      &lt;elementProp name=""TestPlan.user_defined_variables"" elementType=""Arguments"" guiclass=""ArgumentsPanel"" testclass=""Arguments"" testname=""User Defined Variables"" enabled=""true""&gt;
        &lt;collectionProp name=""Arguments.arguments""/&gt;
      &lt;/elementProp&gt;
      &lt;stringProp name=""TestPlan.user_define_classpath""&gt;&lt;/stringProp&gt;
    &lt;/TestPlan&gt;
    &lt;hashTree&gt;
      &lt;ThreadGroup guiclass=""ThreadGroupGui"" testclass=""ThreadGroup"" testname=""Thread Group"" enabled=""true""&gt;
        &lt;stringProp name=""ThreadGroup.on_sample_error""&gt;continue&lt;/stringProp&gt;
        &lt;elementProp name=""ThreadGroup.main_controller"" elementType=""LoopController"" guiclass=""LoopControlPanel"" testclass=""LoopController"" testname=""Loop Controller"" enabled=""true""&gt;
          &lt;boolProp name=""LoopController.continue_forever""&gt;false&lt;/boolProp&gt;
          &lt;stringProp name=""LoopController.loops""&gt;1&lt;/stringProp&gt;
        &lt;/elementProp&gt;
        &lt;stringProp name=""ThreadGroup.num_threads""&gt;1&lt;/stringProp&gt;
        &lt;stringProp name=""ThreadGroup.ramp_time""&gt;1&lt;/stringProp&gt;
        &lt;longProp name=""ThreadGroup.start_time""&gt;1522132570000&lt;/longProp&gt;
        &lt;longProp name=""ThreadGroup.end_time""&gt;1522132570000&lt;/longProp&gt;
        &lt;boolProp name=""ThreadGroup.scheduler""&gt;false&lt;/boolProp&gt;
        &lt;stringProp name=""ThreadGroup.duration""&gt;&lt;/stringProp&gt;
        &lt;stringProp name=""ThreadGroup.delay""&gt;&lt;/stringProp&gt;
      &lt;/ThreadGroup&gt;
      &lt;hashTree&gt;
        &lt;com.googlecode.jmeter.plugins.webdriver.config.ChromeDriverConfig guiclass=""com.googlecode.jmeter.plugins.webdriver.config.gui.ChromeDriverConfigGui"" testclass=""com.googlecode.jmeter.plugins.webdriver.config.ChromeDriverConfig"" testname=""jp@gc - Chrome Driver Config"" enabled=""true""&gt;
          &lt;stringProp name=""WebDriverConfig.proxy_type""&gt;SYSTEM&lt;/stringProp&gt;
          &lt;stringProp name=""WebDriverConfig.proxy_pac_url""&gt;&lt;/stringProp&gt;
          &lt;stringProp name=""WebDriverConfig.http_host""&gt;&lt;/stringProp&gt;
          &lt;intProp name=""WebDriverConfig.http_port""&gt;8080&lt;/intProp&gt;
          &lt;boolProp name=""WebDriverConfig.use_http_for_all_protocols""&gt;true&lt;/boolProp&gt;
          &lt;stringProp name=""WebDriverConfig.https_host""&gt;&lt;/stringProp&gt;
          &lt;intProp name=""WebDriverConfig.https_port""&gt;8080&lt;/intProp&gt;
          &lt;stringProp name=""WebDriverConfig.ftp_host""&gt;&lt;/stringProp&gt;
          &lt;intProp name=""WebDriverConfig.ftp_port""&gt;8080&lt;/intProp&gt;
          &lt;stringProp name=""WebDriverConfig.socks_host""&gt;&lt;/stringProp&gt;
          &lt;intProp name=""WebDriverConfig.socks_port""&gt;8080&lt;/intProp&gt;
          &lt;stringProp name=""WebDriverConfig.no_proxy""&gt;localhost&lt;/stringProp&gt;
          &lt;boolProp name=""WebDriverConfig.maximize_browser""&gt;false&lt;/boolProp&gt;
          &lt;boolProp name=""WebDriverConfig.reset_per_iteration""&gt;false&lt;/boolProp&gt;
          &lt;boolProp name=""WebDriverConfig.dev_mode""&gt;false&lt;/boolProp&gt;
          &lt;stringProp name=""ChromeDriverConfig.chromedriver_path""&gt;chromedriver.exe&lt;/stringProp&gt;
          &lt;boolProp name=""ChromeDriverConfig.android_enabled""&gt;false&lt;/boolProp&gt;
        &lt;/com.googlecode.jmeter.plugins.webdriver.config.ChromeDriverConfig&gt;
        &lt;hashTree/&gt;
        &lt;com.googlecode.jmeter.plugins.webdriver.sampler.WebDriverSampler guiclass=""com.googlecode.jmeter.plugins.webdriver.sampler.gui.WebDriverSamplerGui"" testclass=""com.googlecode.jmeter.plugins.webdriver.sampler.WebDriverSampler"" testname=""jp@gc - WebDriver Sampler"" enabled=""true""&gt;
          &lt;stringProp name=""WebDriverSampler.script""&gt;import org.openqa.selenium.support.ui.WebDriverWait;&lt;/stringProp&gt;
          &lt;stringProp name=""WebDriverSampler.parameters""&gt;&lt;/stringProp&gt;
          &lt;stringProp name=""WebDriverSampler.language""&gt;beanshell&lt;/stringProp&gt;
        &lt;/com.googlecode.jmeter.plugins.webdriver.sampler.WebDriverSampler&gt;
        &lt;hashTree/&gt;
      &lt;/hashTree&gt;
    &lt;/hashTree&gt;
    &lt;WorkBench guiclass=""WorkBenchGui"" testclass=""WorkBench"" testname=""WorkBench"" enabled=""true""&gt;
      &lt;boolProp name=""WorkBench.save""&gt;true&lt;/boolProp&gt;
    &lt;/WorkBench&gt;
    &lt;hashTree/&gt;
  &lt;/hashTree&gt;
&lt;/jmeterTestPlan&gt;
</code></pre>

<p>Above is my first part of XML file. where I need to change
from there I need to edit the line of</p>

<pre><code> &lt;boolProp name=""WebDriverConfig.reset_per_iteration""&gt;false&lt;/boolProp&gt;
</code></pre>

<p>I have tried many tried using many types of code like </p>

<pre><code>def cache_it2(datafile):
    tree = et.parse(datafile)
    tree.find('jmeterTestPlan/hashTree/hashTree/ThreadGroup/longProp/ThreadGroup.end_time').text = 'false'
    tree.write(datafile)
</code></pre>

<p>it does not work for me sinse it gives</p>

<blockquote>
  <p>AttributeError: 'NoneType' object has no attribute 'text'</p>
</blockquote>

<p>and the same error I found with other codes also, Please let me know if there are any work around for this.</p>
","8746415","8746415","2018-05-31 07:09:50","How do I parse XML file which is under multiple hierarchy?","<python><xml-parsing><hierarchy>","1","2","5702"
"50637991","2018-06-01 07:14:24","0","","<p><strong>Try this:</strong> </p>

<pre><code>def main():
    duplicate = 0
    employees = [
        {'name': 'John', 'empID': '102', 'dpt': 'tech', 'title': 'programmer', 'salary': '75'},
        {'name': 'Jane', 'empID': '202', 'dpt': 'tech', 'title': 'programmer', 'salary': '80'},
        {'name': 'Joe', 'empID': '303', 'dpt': 'accounting', 'title': 'accountant', 'salary': '85'}
    ]
    tech_dep = 0
    accounting_dep = 0
    for item in employees:
        if item['dpt'] == 'tech':
            tech_dep += 1
        elif item['dpt'] == 'accounting':
            accounting_dep += 1

    print('Number of Employees from tech dep are : ', tech_dep)
    print('Number of Employees from accounting dep are : ', accounting_dep)


main()
</code></pre>

<p><strong>It will give you output like this:</strong></p>

<pre><code>Number of Employees from tech dep are :  2
Number of Employees from accounting dep are :  1
</code></pre>

<p><strong>Note:</strong></p>

<p>It is a simple answer to your question because as you said that you are new to python so i just thought to give you simple answer! Also note that the way you create dictionary in your question is wrong you can either make dictionary of dictionary or list of dictionaries. Moreover, the answer @Austin just gave you is also simple and it shows the use of counter.</p>

<p>Hope this will help you! :) </p>
","9400024","","","1","1375","Abdullah Ahmed Ghaznavi","2018-02-23 05:56:43","989","186","218","610","50637701","50637801","2018-06-01 06:54:48","1","572","<p>python is  new language for me so this question might sound simple, but if someone can point me in the right direction, I would appreciate it! I created a dictionary call employees and it holds some value about them:</p>

<p>I'm trying to read how many people are in each department, for example: tech-2, accounting-1. </p>

<p>I have something like this, but it prints out blank.</p>

<pre><code>  def main():
   employees= {'name': 'John', 'empID': '102', 'dpt': 'tech', 'title': 
   'programmer', 'salary': '75'}
   {'name': 'Jane', 'empID': '202', 'dpt': 'tech', 'title': 'programmer', 
   'salary': '80'}
   {'name': 'Joe', 'empID': '303', 'dpt': 'accounting', 'title': 
   'accountant', 'salary': '85'}
    for item in employees:
    dic = employees[item]
    if dic['dpt'[0]]==dic['dpt'[1]]:
        duplicate += 1
        print(""there are ""+ duplicate)
    else:
        print(""There are no duplicate"")
</code></pre>
","2774460","1222951","2018-06-01 06:55:39","Counting values in an employee dictionary database in python","<python><python-3.x><dictionary><counter><key-value>","4","0","928"
"50637994","2018-06-01 07:14:29","2","","<p>Use <code>list</code> comprehension with <code>split</code> first values of lists and then add <code>strip</code> for remove trailing whitespaces:</p>

<pre><code>df = pd.DataFrame([[y.strip() for y in x[0].split(',') + [x[1]]] for x in dataInList])
print (df)
                           0    1       2       3          4
0  '2018-05-15 15:35:57\t\n'  'A'  'xtre'  'retle  105.0 (C)
1  '2018-05-15 15:35:57\t\n'  'A'  'xtre'  'retla    0 (s*C)
2  '2018-05-15 15:35:57\t\n'  'A'  'xtre'  'retla    0 (s*C)
</code></pre>

<p>EDIT:</p>

<p>Problem is there are some list with no lenght 2, so need filter it:</p>

<pre><code>dataInList = [[""'2018-05-15 15:35:57\t\n', 'A', 'xtre','retle "",' 105.0 (C)\n'],
 [""'2018-05-15 15:35:57\t\n', 'A', 'xtre','retla "",' 0 (s*C)\n'],
 [""'2018-05-15 15:35:57\t\n', 'A', 'xtre','retla"",' 0 (s*C)\n'],
 [ ""aaa""]]

df = pd.DataFrame([[y.strip() for y in x[0].split(',') + [x[1]]] for x in dataInList if len(x) == 2])
print (df)
                           0    1       2       3          4
0  '2018-05-15 15:35:57\t\n'  'A'  'xtre'  'retle  105.0 (C)
1  '2018-05-15 15:35:57\t\n'  'A'  'xtre'  'retla    0 (s*C)
2  '2018-05-15 15:35:57\t\n'  'A'  'xtre'  'retla    0 (s*C)
</code></pre>
","2901002","2901002","2018-06-01 11:29:59","3","1219","jezrael","2013-10-20 20:27:26","427380","89269","18260","743","50637934","50637994","2018-06-01 07:10:45","1","42","<p>I'm having troubles making a list into a dataframe. </p>

<p>dataInList=</p>

<pre><code> [""'2018-05-15 15:35:57\t\n', 'A', 'xtre','retle "",' 105.0 (C)\n'],
 [""'2018-05-15 15:35:57\t\n', 'A', 'xtre','retla "",' 0 (s*C)\n'],
 [""'2018-05-15 15:35:57\t\n', 'A', 'xtre','retla"",' 0 (s*C)\n'],
 [""'2018-05-15 15:35:57\t\n', 'A', 'xtre','retke"",' 0 (s)\n'],
 [""'2018-05-15 15:35:57\t\n', 'A', 'xtre','retds"",' 0 (s)\n'],
 [""'2018-05-15 15:35:57\t\n', 'A', 'xtre','rewr"",' 0 (s)\n'],
 [""'2018-05-15 15:35:57\t\n', 'A', 'xtre','sdff"",' 0 (s)\n']
</code></pre>

<p>df = pd.DataFrame(dataInList) only recognizes two of the sample points as columns producing this:</p>

<pre><code>                                                   0             1
1  '2018-05-15 15:35:57\t\n', 'A', 'xtre','..............'   101.5 (C)\n
2  '2018-05-15 15:35:57\t\n', 'A', 'xtre','..............'   105.0 (C)\n
3  '2018-05-15 15:35:57\t\n', 'A', 'xtre','..............'   118.0 (C)\n
4  '2018-05-15 15:35:57\t\n', 'A', 'xtre','..............'   110.0 (C)\n
5  '2018-05-15 15:35:57\t\n', 'A', 'xtre','..............'   110.0 (C)\n
</code></pre>

<p>How should I proceed? </p>

<p>Thanks on beforehand!</p>
","9063798","","","Making specific list into a dataframe","<python><pandas>","1","1","1179"
"50638001","2018-06-01 07:14:50","-2","","<p>I got it, the trick is to create the first euclidean list inside the first for loop, and then deleting the list after appending it to the complete euclidean list</p>

<pre><code>import math
euclidean = 0

euclidean_list_complete = []

test1 = [[0.0, 0.0, 0.0, 152.0, 12.29], [0.0, 0.0, 0.357, 245.0, 10.4], [0.0, 0.0, 0.10, 200.0, 11.0]]

test2 = [[0.0, 0.0, 0.0, 72.0, 12.9], [0.0, 0.0, 0.0, 80.0, 11.3]]

for i in range(len(test2)):
    euclidean_list = []
    for j in range(len(test1)):
        for k in range(len(test1[0])):
            euclidean += pow((test2[i][k]-test1[j][k]),2)      
        euclidean_list.append(math.sqrt(euclidean))
        euclidean = 0
        euclidean_list.sort(reverse=True)
    euclidean_list_complete.append(euclidean_list)
    del euclidean_list

print euclidean_list_complete
</code></pre>
","8126411","","","0","832","Iqbal Pratama","2017-06-07 14:56:22","53","23","3","0","50637446","50639386","2018-06-01 06:37:49","0","3288","<p>I'm writing a simple program to compute the euclidean distances between multiple lists using python. This is the code I have so fat</p>

<pre><code>import math
euclidean = 0
euclidean_list = []
euclidean_list_complete = []

test1 = [[0.0, 0.0, 0.0, 152.0, 12.29], [0.0, 0.0, 0.357, 245.0, 10.4], [0.0, 0.0, 0.10, 200.0, 11.0]]

test2 = [[0.0, 0.0, 0.0, 72.0, 12.9], [0.0, 0.0, 0.0, 80.0, 11.3]]

for i in range(len(test2)):
    for j in range(len(test1)):
        for k in range(len(test1[0])):
            euclidean += pow((test2[i][k]-test1[j][k]),2)

        euclidean_list.append(math.sqrt(euclidean))
        euclidean = 0

    euclidean_list_complete.append(euclidean_list)


print euclidean_list_complete
</code></pre>

<p>my problem with this code is it doesn't print the output i want properly. The output should be 
<code>[[80.0023, 173.018, 128.014], [72.006, 165.002, 120.000]]</code> </p>

<p>but instead, it prints</p>

<p><code>[[80.00232559119766, 173.01843095173416, 128.01413984400315, 72.00680592832875, 165.0028407300917, 120.00041666594329], [80.00232559119766, 173.01843095173416, 128.01413984400315, 72.00680592832875, 165.0028407300917, 120.00041666594329]]</code></p>

<p>I'm guessing it has something to do with the loop. What should I do to fix it? By the way, I don't want to use numpy or scipy for studying purposes</p>

<p>If it's unclear, I want to calculate the distance between lists on test2 to each lists on test1</p>
","8126411","8126411","2018-06-01 07:05:52","Computing euclidean distance with multiple list in python","<python><list><euclidean-distance>","4","6","1456"
"50638028","2018-06-01 07:16:38","0","","<p>try this,</p>

<pre><code>string['res'] = string.apply(lambda x: ' '.join(globals()[x['location']].iloc[(x['start']-1):(x['stop'])]['w']),axis=1)
print string['res'].values.tolist()
</code></pre>

<p>output:</p>

<pre><code>['i am', 'in love', 'with you', 'i wish', 'python is', 'my friend', 'the ship', 'has', 'set sail']
</code></pre>

<p>For further Result (Adding Boundary):</p>

<pre><code>string['res'] = string.apply(lambda x: ' '.join(globals()[x['location']].iloc[(x['start']-1):(x['stop'])]['w']),axis=1)
string.loc[~string['part'].duplicated(keep='last'),'flag']='boundry'

l=list(string['res'].values)
b = list(np.where(string['flag'].values == 'boundry')[0])
[l.insert(ind+i,'boundary') for i,ind in enumerate(b,1)]
print l
</code></pre>

<p>Output:</p>

<pre><code>['i am', 'in love', 'with you', 'i wish', 'python is', 'my friend', 'boundary', 'the ship', 'has', 'set sail', 'boundary']
</code></pre>
","4684861","4684861","2018-06-01 09:32:19","10","919","Mohamed Thasin ah","2015-03-18 10:49:38","4803","833","1351","258","50637763","","2018-06-01 06:59:13","-1","42","<p>Hello I am trying to construct sentences, the words come from the first 3 data frames</p>

<pre><code>df1=pd.DataFrame()
    df1['w']=['i', 'am', 'python', 'is', 'set', 'sail']
    df1['n'] = [1,2,3,4,5,6]
    df2 =pd.DataFrame()
    df2['w']=['i', 'wish', 'in', 'love', 'has' ]
    df2['n'] =[1,2,3,4,5]
    df3 = pd.DataFrame()
    df3['w']=['the', 'ship', 'with', 'you', 'my', 'friend']
    df3['n']=[1,2,3,4,5,6]
</code></pre>

<p>the parts are defined here, and locations for words of each sentence and boundary  </p>

<pre><code>string= pd.DataFrame()
string['location'] = ['df1', 'df2', 'df3', 'df2', 'df1', 'df3', 'df3', 'df2', 'df1']
string['start'] = [1, 3, 3, 1, 3, 5, 1, 5, 5]
string['stop'] = [2 , 4, 4, 1, 4, 6, 2, 5, 6]
string['sentence] = [1,1,1,2,2,2,3,3,3]
string['part'] = [1, 1, 1, 1, 1, 1, 2, 2, 2]
</code></pre>

<p>the desired output is</p>

<pre><code>i am in love with you 
i wish pyhton is my fried
**boundry**
the ship has set sail
**boundry**
</code></pre>

<p>the code I have tried is, I have found this o far it seems to do most of what I want but I cut figure out how to Mae it work with multiple tables, and get the order I am after.</p>

<pre><code>x = df1.set_index('n')['w']
sent = [
    ' '.join(x.loc[i:j]) for i, j in zip(string['start'], string['stop'])
]

sent
</code></pre>

<p>the output I get is</p>

<pre><code>['i am',
 'python is',
 'python is',
 'i',
 'python is',
 'set sail',
 'i am',
 'set',
 'set sail']
</code></pre>
","8106322","8106322","2018-06-01 09:01:03","gathering information form multiple data frames based on a reference data frame","<python><pandas>","1","0","1472"
"50638040","2018-06-01 07:17:19","2","","<p>Yes it will:</p>

<p>you will have to wait for an iPython 3.7 distribution, and install it. In all likeliness, it will be published shortly after the python 3.7 final version, which is due around mid of June</p>
","2875563","2875563","2018-06-01 07:23:00","15","215","Reblochon Masque","2013-10-13 07:06:56","23302","2751","885","2671","50637990","50638040","2018-06-01 07:14:22","1","1448","<p>I'm a super newb, so forgive me if my question seems silly. I messed around a little bit with Python back in the day when Python 3.x didn't even exist, and I'm looking to do some web scraping, so I decided to re-familiarize myself with Python in order to use Scrapy. However, in the Scrapy documentation it suggests installing Anaconda, which I'm in the process of doing, but the only two downloads on the Anaconda website are: <code>Python 3.6 version *</code> and <code>Python 3.6 version *</code> (and I can't seem to find any information pertaining to <code>*</code>).</p>

<p>Thanks</p>
","7543162","7543162","2018-06-01 07:26:17","Will Anaconda work with Python 3.7?","<python><python-3.x><anaconda>","1","0","595"
"50638057","2018-06-01 07:18:07","0","","<p><code>-&gt;</code> generally means ""returns"". In your case, your <code>rect()</code> function returns a <code>Rect</code> instance.</p>
","7663649","","","0","139","Pierre","2017-03-06 00:24:56","658","36","318","6","50637965","50638057","2018-06-01 07:13:02","-2","52","<p>this is a part from the document of pygame, I don't know what does ""->"" mean.</p>

<p><img src=""https://i.stack.imgur.com/4PsW1.png"" alt=""""></p>
","9879787","1324033","2018-06-01 07:19:24","what does ""->"" mean in ""rect(Surface, color, Rect, width=0) -> Rect""","<python>","1","0","148"
"50638091","2018-06-01 07:20:40","2","","<p>Spark connector is optimized because it parallelize processing and reading/inserting data into nodes that are owns the data.  You may get better throughput by using Cassandra Spark Connector, but this will require more resources.</p>

<p>Talking about your task - 300000 inserts/minute is 5000/second, and this is not very big number frankly speaking - you can increase throughput by putting different optimizations:</p>

<ul>
<li>Using <a href=""https://datastax.github.io/python-driver/getting_started.html#asynchronous-queries"" rel=""nofollow noreferrer"">asynchronous calls</a> to submit requests. You only need to make sure that you submit more requests that could be handled by one connection (but you can also increase this number - I'm not sure how to do it in Python, but please check <a href=""https://docs.datastax.com/en/developer/java-driver/3.5/manual/pooling/#simultaneous-requests-per-connection"" rel=""nofollow noreferrer"">Java driver doc</a> to get an idea).</li>
<li>use correct consistency level (<code>LOCAL_ONE</code> should give you very good performance)</li>
<li>use correct <a href=""https://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.load_balancing_policy"" rel=""nofollow noreferrer"">load balancing policy</a></li>
<li>you can run several copies of your script in parallel, making sure that they are all in the same Kafka consumer group.</li>
</ul>
","18627","","","2","1415","Alex Ott","2008-09-19 07:53:08","32618","3898","2731","31","50635752","","2018-06-01 03:35:48","1","550","<p>Can anyone please explain the internal working of spark when reading data from one table and writing it to another in cassandra.</p>

<p>Here is my use case:</p>

<p>I am ingesting data coming in from an IOT platform into cassandra through a kafka topic. I have a small python script that parses each message from kafka to get the tablename it belongs to, prepares a query and writes it to cassandra using datastax's cassandra-driver for python. With that script I am able to ingest around <strong>300000 records per min</strong> into cassandra. However my incoming data rate is <strong>510000 records per minute</strong> so kafka consumer lag keeps on increasing.</p>

<p>Python script is already making concurrent calls to cassandra. If I increase the number of python executors, cassandra-driver starts failing because cassandra nodes become unavailable to it. I am assumin there is a limit of cassandra calls per sec that I am hitting there. Here is the error message that I get:</p>

<pre><code>ERROR Operation failed: ('Unable to complete the operation against any hosts', {&lt;Host: 10.128.1.3 datacenter1&gt;: ConnectionException('Pool is shutdown',), &lt;Host: 10.128.1.1 datacenter1&gt;: ConnectionException('Pool is shutdown',)})""
</code></pre>

<p>Recently, I ran a pyspark job to copy data from a couple of columns in one table to another. The table had around 168 million records in it. Pyspark job completed in around 5 hours. So it processed over <strong>550000 records per min</strong>. </p>

<p>Here is the pyspark code I am using:</p>

<pre><code>df = spark.read\
    .format(""org.apache.spark.sql.cassandra"")\
    .options(table=sourcetable, keyspace=sourcekeyspace)\
    .load().cache()

df.createOrReplaceTempView(""data"")

query = (""select dev_id,datetime,DATE_FORMAT(datetime,'yyyy-MM-dd') as day, "" + field + "" as value  from data  "" )

vgDF = spark.sql(query)
vgDF.show(50)
vgDF.write\
    .format(""org.apache.spark.sql.cassandra"")\
    .mode('append')\
    .options(table=newtable, keyspace=newkeyspace)\
    .save()
</code></pre>

<p><strong>Versions:</strong></p>

<ul>
<li>Cassandra 3.9.</li>
<li>Spark 2.1.0.</li>
<li>Datastax's spark-cassandra-connector 2.0.1</li>
<li>Scala version 2.11</li>
</ul>

<p><strong>Cluster:</strong></p>

<ul>
<li>Spark setup with 3 workers and 1 master node.</li>
<li>3 worker nodes also have a cassandra cluster installed. (each cassandra node with one spark worker node)</li>
<li>Each worker was allowed 10 GB ram and 3 cores.</li>
</ul>

<p>So I am wondering:</p>

<ul>
<li><p>Does spark read all the data from cassandra first and then writes it to the new table or is there some kind of optimization in spark cassandra connector that allows it to move the data around cassandra tables without reading all the records?</p></li>
<li><p>If I replace my python script with a spark streaming job in which I parse the packet to get the table name for cassandra, will that help me ingest data more quickly into cassandra?</p></li>
</ul>
","7878496","","","How does spark copy data between cassandra tables?","<python><apache-spark><cassandra><pyspark><cassandra-3.0>","1","0","2998"
"50638126","2018-06-01 07:22:46","1","","<p>Binet's formula for the nth Fibonacci number is as follows:</p>

<pre><code>def binet(n):
    phi = (1 + 5**.5) / 2
    psi = (1 - 5**.5) / 2
    return (phi**n - psi**n) / 5**.5
</code></pre>

<p>This formula is mathematically exact, but in practice it is subject to floating point error. The term psi**n converges rapidly to zero as n increases, so it can be omitted when n is large. This yields your approximate formula.</p>

<p>Binet's formula is very fast. On my machine, it computes the 1000th Fibonacci number in about 400 nanoseconds.</p>

<pre><code>In [21]: %timeit binet(1000)
426 ns ± 24.3 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
</code></pre>

<p>The binomial sum formula for Fibonacci numbers is very interesting. It says that Fibonacci numbers are sums along shallow diagonals of Pascal's triangle, as shown in this figure.
<a href=""https://i.stack.imgur.com/OJBIB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OJBIB.png"" alt=""Fibonacci numbers in Pascal&#39;s triangle""></a></p>

<p>This formula works because each diagonal is the sum of the two previous diagonals, just as every term in the Fibonacci sequence is the sum of the two previous terms. For example, the ninth and tenth diagonals can be added to obtain the eleventh diagonal.</p>

<pre><code>    1 +  7 + 15 + 10 + 1 = 34
1 + 8 + 21 + 20 +  5     = 55
-----------------------------
1 + 9 + 28 + 35 + 15 + 1 = 89
</code></pre>

<p>However, this formula is not fast at all. It <em>seems</em> fast because computers can perform millions of calculations per second. My machine needs 84 ms to calculate the 1000th Fibonacci number using your code. This is 200,000 times longer than it takes using Binet's formula.</p>

<pre><code> In [22]: %timeit fr(1001, 2)
 84 ms ± 875 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
</code></pre>
","677398","677398","2018-06-01 07:35:32","0","1861","Dave Radcliffe","2011-03-25 20:32:49","213","28","26","1","50622088","50638126","2018-05-31 10:25:52","-4","519","<p>I have seen  Fibonacci has direct formula with this <code>(Phi^n)/√5</code> 
while I am getting results in same time but accurate result not approximate with something I managed to write:</p>

<pre><code>for r = 0 to 2 Sum [(n-r)!/((n-2r)!r!)] 
</code></pre>

<p>(<code>!</code> is the symbol for <em>factorial</em> ):</p>

<pre><code>def fr(n, p):
    # (n-r)!/((n-2r)!r!)
    r = int(n / p)
    n_f = 0
    for j in range(1, r + 1):
        t_f = 1
        r_f = factorial(j)
        i = (n - j)

        while i &gt; (n - (2 * j)):
            t_f = t_f * i
            i = i - 1

        n_f = n_f + t_f / r_f

    return n_f + 1

def factorial(n):
    if n == 0:
        return 1
    else:
        return n * factorial(n - 1)
</code></pre>

<p>so for 12 we can do <code>fr(11, 2)</code>
also <code>(Phi12)/√5 = 144.0013888754932</code> rounds to <code>Fib(12) =144</code></p>

<p>I don't understand why <code>(n-r)!/((n-2r)!r!)</code> is fast </p>
","2130479","2130479","2018-06-01 05:56:25","Fibonacci direct calculation formula","<python><math><fibonacci>","1","10","956"
"50638127","2018-06-01 07:22:51","0","","<p>You may also use <code>arccos</code> from the module <code>numpy</code></p>

<pre><code>&gt;&gt;&gt; import numpy
&gt;&gt;&gt; numpy.arccos(0.5)
1.0471975511965979
</code></pre>

<p><strong>WARNING</strong>: For scalars the <code>numpy.arccos()</code> function is much slower (~ 10x) than <code>math.acos</code>.  <a href=""https://stackoverflow.com/questions/35183787/in-python-is-math-acos-faster-than-numpy-arccos-for-scalars"">See post here</a></p>

<p>Nevertheless, the <code>numpy.arccos()</code> is suitable for sequences, while <code>math.acos</code> is not. :)</p>

<pre><code>&gt;&gt;&gt; numpy.arccos([0.5, 0.3])
array([ 1.04719755,  1.26610367])
</code></pre>

<p>but</p>

<pre><code>&gt;&gt;&gt; math.acos([0.5, 0.3])
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
TypeError: a float is required
</code></pre>
","4428520","","","0","862","loved.by.Jesus","2015-01-07 11:00:38","433","108","553","3","6745464","6745479","2011-07-19 10:05:28","36","68749","<p>Apologies if this is straight forward, but I have not found any help in the python manual or google.</p>

<p>I am trying to find the inverse cosine for a value using python.</p>

<p>i.e. cos-1(x)</p>

<p>Does anyone know how to do this?</p>

<p>Thanks</p>
","846396","","","Inverse Cosine in Python","<python><math><trigonometry>","7","0","259"
"50638143","2018-06-01 07:23:42","0","","<p>One easy way is that you upload your CSV into a Mailgun list and then use MailboxValidator to import the list and perform the verification.</p>

<p><a href=""https://www.mailboxvalidator.com/resources/articles/how-to-import-email-list-from-mailgun/"" rel=""nofollow noreferrer"">https://www.mailboxvalidator.com/resources/articles/how-to-import-email-list-from-mailgun/</a></p>

<p>This is the most practical method if you are planning to use Mailgun to send out your emails later on.</p>

<p>But if you are just looking for an email verification API, try the MailboxValidator free API.</p>

<p>Free API key: <a href=""https://www.mailboxvalidator.com/pay/9"" rel=""nofollow noreferrer"">https://www.mailboxvalidator.com/pay/9</a></p>

<p>API documentation: <a href=""https://www.mailboxvalidator.com/api-single-validation"" rel=""nofollow noreferrer"">https://www.mailboxvalidator.com/api-single-validation</a></p>

<p>You get 300 free verification every 30 days.</p>

<h2>Sample Python code for calling the API</h2>

<pre><code>import httplib
import urllib
import hashlib

p = { 'key': 'Enter_License_Key', 'format': 'json', 'email': 'Enter_Email' }

conn = httplib.HTTPConnection(""api.mailboxvalidator.com"")
conn.request(""GET"", ""/v1/validation/single?"" + urllib.urlencode(p))
res = conn.getresponse()
print res.read()
</code></pre>
","6647585","","","0","1326","Vlam","2016-07-28 03:13:18","1033","110","13","1","49482011","","2018-03-25 23:22:31","0","409","<p>Mailgun has an email validator that they have a sample response for but I don't know how to use it. I want to verify that ""is_valid"" is true or false and clean my email list of bad emails. I get Response [401] when I print the function call. How do I ask it if is_valid is false?</p>

<pre><code>def get_validate(email):
        return requests.get(
            ""https://api.mailgun.net/v3/address/validate"",
            auth=(""api"", ""key""),
            params={""address"": email})


with open('emails.csv', 'r') as file:
    reader = csv.reader(file)
    for i in reader:
        s = ''
        try:
            print(i[0])
            s = s + i[0]
            print(get_validate(s))
        except IndexError:
            pass
</code></pre>

<p>The sample response is: </p>

<pre><code>{
    ""address"": ""foo@mailgun.net"",
    ""did_you_mean"": null,
    ""is_disposable_address"": false,
    ""is_role_address"": false,
    ""is_valid"": true,
    ""parts"": {
        ""display_name"": null,
        ""domain"": ""mailgun.net"",
        ""local_part"": ""foo""
    }
}
</code></pre>
","5310500","1848654","2018-03-25 23:25:39","Verify if an email exists using mailgun and python","<python><email><mailgun>","3","0","1068"
"50638192","2018-06-01 07:26:41","4","","<p>I believe need <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.DataFrameGroupBy.diff.html"" rel=""nofollow noreferrer""><code>DataFrameGroupBy.diff</code></a>:</p>

<pre><code>df['new'] = df.groupby('user')['time'].diff()
print (df)
  user  time  new
0    F     0  NaN
1    T     0  NaN
2    T     0  0.0
3    T     1  1.0
4    B     1  NaN
5    K     2  NaN
6    J     2  NaN
7    T     3  2.0
8    J     4  2.0
9    B     4  3.0
</code></pre>
","2901002","2901002","2018-06-01 07:35:26","7","480","jezrael","2013-10-20 20:27:26","427380","89269","18260","743","50637942","50638192","2018-06-01 07:11:17","1","87","<p>This is part of a larger project, but I've broken my problem down into steps, so here's the first step. Take a Pandas dataframe, like this:</p>

<pre><code>index | user   time     
---------------------
 0      F       0   
 1      T       0   
 2      T       0   
 3      T       1   
 4      B       1 
 5      K       2 
 6      J       2 
 7      T       3 
 8      J       4 
 9      B       4 
</code></pre>

<p>For each unique user, can I extract the difference between the values in column ""time,"" but with some conditions?</p>

<p>So, for example, there are two instances of user J, and the ""time"" difference between these two instances is 2. Can I extract the difference, 2, between these two rows? Then if that user appears again, extract the difference between that row and the previous appearance of that user in the dataframe?</p>
","1298833","","","In a Pandas dataframe, how can I extract the difference between the values on separate rows within the same column, conditional on a second column?","<python><pandas><slice>","2","1","849"
"50638239","2018-06-01 07:29:36","0","","<p>You can bring the element to the visible part of the screen by using one of the following methods.</p>

<ol>
<li><p>Using <code>driver.execute_script(""arguments[0].scrollIntoView();"", element)</code>
you can read more about <code>scrollIntoView()</code> method <a href=""https://developer.mozilla.org/en-US/docs/Web/API/Element/scrollIntoView"" rel=""nofollow noreferrer"">here</a>.</p></li>
<li><p>Using the Actions class of selenium webdriver.</p></li>
</ol>

<p>from selenium.webdriver.common.action_chains import ActionChains</p>

<pre><code>element = driver.find_element_by_css_selector('.PeriodCell input')
actions = ActionChains(driver)
actions.move_to_element(element).perform()
</code></pre>

<p>You can read the difference between these two methods <a href=""https://stackoverflow.com/a/35090932/5074293"">here</a></p>

<p>If you still need to use the TAB action to reach the element</p>

<pre><code>from selenium.webdriver.common.keys import Keys
</code></pre>

<p>and using <code>.send_keys(Keys.TAB)</code> send the TAB key to the element</p>
","5074293","","","2","1053","GPT14","2015-07-02 14:32:13","663","108","26","34","50637373","50641224","2018-06-01 06:32:51","1","84","<p>I want to perform TAB action until I have reached a particular web-element. Until the active element is the below mentioned element, TAB action has to be performed.</p>

<pre><code>&gt;name = driver.find_element_by_name(""name"")
&gt;name.send_keys(""ABC"")
&gt;group = driver.find_element_by_name(""group"") 
&gt;group.send_keys(""DEF"")
</code></pre>

<p>I am able to find element till the above state. After that, I want to perform TAB action until the below mentioned element is found. I guess using a loop would help.</p>

<blockquote>
  <p>elem = driver.find_element_by_css_selector('.PeriodCell input')</p>
</blockquote>

<p>Please find below the HTML code</p>

<pre><code>&lt;div class=""PeriodCell"" style=""left:px; width:112px;""&gt;
&lt;div class=""Effort forecasting""&gt;
&lt;div class=""entity field-value-copy-selected""&gt;
&lt;input type=""text"" value=""0.0"" data-start=""2014-09-20""&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=""Effort unmet zero"" title=""""&gt;0.0
&lt;/div&gt;
&lt;/div&gt;
</code></pre>

<p>Please help. Thanks in Advance.</p>
","9835644","","","Perform TAB action until active element is the required element - Python","<python><selenium><selenium-webdriver>","2","7","1046"
"50638245","2018-06-01 07:29:53","0","","<p>Using a simple Iteration.</p>

<p><strong>Ex:</strong></p>

<pre><code>my_list = ['hsdpa_-_rl_fail', 'avg_reported_cqi.', 'fach-c_load_ratio', 'canc_isho_cpich/ecno_nrt_m1010c209']
my_dict = {'-' : 'dash' , '.':'dot' , '/':'slash'}

for i, v in enumerate(my_list):
    for k in my_dict:
        if k in v:
            my_list[i] = v.replace(k, my_dict[k])

print(my_list)
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>['hsdpa_dash_rl_fail', 'avg_reported_cqidot', 'fachdashc_load_ratio', 'canc_isho_cpichslashecno_nrt_m1010c209']
</code></pre>
","532312","","","0","560","Rakesh","2010-12-06 13:07:54","56694","5302","758","1508","50638159","","2018-06-01 07:24:58","1","114","<p>I have python code</p>

<pre><code>my_list = ['hsdpa_-_rl_fail', 'avg_reported_cqi.', 'fach-c_load_ratio', 'canc_isho_cpich/ecno_nrt_m1010c209']
my_dict = {'-' : 'dash' , '.':'dot' , '/':'slash'}
</code></pre>

<p>I want search and replace in my_list as per mapping given in my_dict that is my_dict key shall be replaced by respective value in all element of my_list. </p>

<p>How this can be done in pythonic way? </p>
","7566673","","","replace list of strings using dictionary","<python>","1","3","423"
"50638247","2018-06-01 07:29:55","0","","<p>I think need remove last <code>4</code> characters and <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html"" rel=""nofollow noreferrer""><code>map</code></a> by <code>Series</code> created by <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html"" rel=""nofollow noreferrer""><code>set_index</code></a>, last add <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.fillna.html"" rel=""nofollow noreferrer""><code>fillna</code></a> for replace non matched values to original values of <code>D</code>:</p>

<pre><code>d['D'] = d['D'].str[:-4].map(m.set_index('Code')['Place']).fillna(d['D'])
print (d)
   A      B         C     D     E  F
0  A   Stop  08:00:00  Home  Num:  1
1                  XX               
2  A    Res  08:10:00  Home  Num:  1
3                  XX               
4  A   Stop  08:41:42  Away  Num:  1
5                  XX               
6  A  Start  08:50:00  Home  Num:  1
7                  XX               
8  A    Res  09:00:00  Away  Num:  1
</code></pre>

<p><strong>Details</strong>:</p>

<pre><code>print (d['D'].str[:-4])
0    ABC
1       
2    ABC
3       
4    DEF
5       
6    ABC
7       
8    DEF
Name: D, dtype: object

print (m.set_index('Code')['Place'])
Code
ABC    Home
DEF    Away
Name: Place, dtype: object
</code></pre>
","2901002","","","1","1360","jezrael","2013-10-20 20:27:26","427380","89269","18260","743","50637444","50638247","2018-06-01 06:37:44","2","49","<p>I am trying to efficiently replace certain values with meaningful information in a <code>pandas df</code>. Below is an example of the <code>df's</code> I'm working with. </p>

<p>This <code>df</code> is an example of the meaningful information. Each 3 letter code is equal to an actual place. E.g. <code>ABC = Home</code></p>

<pre><code>import pandas as pd

m = pd.DataFrame({
        'Place' : ['Home','Away'],
        'Code' : ['ABC','DEF']})  
</code></pre>

<p>Output:</p>

<pre><code>  Code  Place
0  ABC   Home
1  DEF   Away
</code></pre>

<p>I want to replace values <code>Column D</code> with the place information. So <code>Home</code> would replace <code>ABC</code>. There's additional items after this code but I can remove those easily enough. </p>

<pre><code>d = pd.DataFrame({
    'C' : ['08:00:00','XX','08:10:00','XX','08:41:42','XX','08:50:00','XX', '09:00:00'],
    'D' : ['ABC-Thu','','ABC-Thu','','DEF-Thu','','ABC-Thu','','DEF-Thu'],
    'E' : ['Num:','','Num:','','Num:','','Num:','','Num:'],
    'F' : ['1','','1','','1','','1','','1'],   
    'A' : ['A','','A','','A','','A','','A'],           
    'B' : ['Stop','','Res','','Stop','','Start','','Res']
    })
</code></pre>

<p>At the moment I'm doing this manually as such,</p>

<pre><code>#remove last 4 items
d['D'] = [x[:-4] for x in d['D']]

#replace with appropriate place
d['D'] = d['D'].replace(['ABC'], 'Home')
d['D'] = d['D'].replace(['DEF'], 'Away')
</code></pre>

<p>Output:</p>

<pre><code>   A      B         C     D     E  F
0  A   Stop  08:00:00  Home  Num:  1
1                  XX               
2  A    Res  08:10:00  Home  Num:  1
3                  XX               
4  A   Stop  08:41:42  Away  Num:  1
5                  XX               
6  A  Start  08:50:00  Home  Num:  1
7                  XX               
8  A    Res  09:00:00  Away  Num:  1
</code></pre>

<p>But the amount of different places that need to be replaced can be up to 40-50. The codes can also change with each dataset. So <code>ABC</code> may equal <code>Home</code> one day and <code>Pool</code> the next. As you can imagine it's not very efficient to alter 40-50 different places everyday. </p>

<p>Is there a more efficient to loop through each code and replace with place information?     </p>
","","","2018-06-01 07:20:03","Efficiently replace values in a pandas df","<python><pandas><loops><for-loop><replace>","1","0","2274"
"50638256","2018-06-01 07:30:25","1","","<p>So, after some research I used what I found here:
<a href=""https://stackoverflow.com/a/50620593/9419748"">https://stackoverflow.com/a/50620593/9419748</a></p>

<p>to output on monitor all the layers of the graph. In this way I was able to find the exact name of a suitable layer and passing the right name everything works.</p>
","9419748","","","0","330","cristian b","2018-02-27 16:17:17","26","2","0","0","50626890","50638256","2018-05-31 14:38:46","1","846","<p>I am trying to use tensorflow to retrain a net. I think I did it correctly (I had the graph and labels). </p>

<p>Now I am using label_image.py to classify an image.
This is the file: <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/label_image.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/label_image.py</a> </p>

<p>I call:</p>

<pre><code>py label_image.py --image=test.jpg --graph=retrained_graph.pb --labels=retrained_labels.txt --input_layer=input --output_layer=final_result:0 --input_width=160 --input_height=120
</code></pre>

<p>but I have this error:</p>

<pre><code>The name 'input' refers to an Operation not in the graph.
</code></pre>

<p>This is the graph I can see with tensorboard <a href=""https://i.stack.imgur.com/5c1k2.png"" rel=""nofollow noreferrer"">graph tensorboard</a></p>

<p>I see people saying: ""use Mul/input/placeholder"" and other stuff as parameter for ""--input_layer=..."" but none of them work for me. </p>

<p>i.e. <a href=""https://stackoverflow.com/questions/46325799/tensorflow-for-poets-the-name-import-input-refers-to-an-operation-not-in-the"">tensorflow for poets: &quot;The name &#39;import/input&#39; refers to an Operation not in the graph.&quot;</a> </p>

<p>And then I would like to understand what I'm doing... </p>

<p>The same thing maybe apply also for the output layer (I don't have error know, but maybe I will get when the first problem will be solved)</p>

<p>Any suggestion? Thanks</p>
","9419748","9419748","2018-05-31 15:50:30","What is input_layer in tensorflow (label_image)?","<python><tensorflow>","1","0","1554"
"50638310","2018-06-01 07:34:08","0","","<p>first find out the path of the spark. for example for pyspark</p>

<pre><code>    which pyspark
</code></pre>

<p>it will return you the path for example like this- 
/home/ubuntu/bin/pyspark</p>

<p>then run this command by change the path as per your spark path
general-: path --packages com.databricks:spark-csv_2.10:1.0.3</p>

<pre><code>    /home/ubuntu/bin/pyspark --packages com.databricks:spark-csv_2.10:1.0.3
</code></pre>
","9879897","","","0","434","Shubham Sinha","2018-06-01 07:28:03","9","2","0","0","30757439","30765306","2015-06-10 13:13:47","26","29424","<p>I have build the <a href=""https://github.com/databricks/spark-csv"">Spark-csv</a> and able to use the same from pyspark shell using the following command</p>

<pre><code>bin/spark-shell --packages com.databricks:spark-csv_2.10:1.0.3
</code></pre>

<p>error getting </p>

<pre><code>&gt;&gt;&gt; df_cat.save(""k.csv"",""com.databricks.spark.csv"")
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/Users/abhishekchoudhary/bigdata/cdh5.2.0/spark-1.3.1/python/pyspark/sql/dataframe.py"", line 209, in save
    self._jdf.save(source, jmode, joptions)
  File ""/Users/abhishekchoudhary/bigdata/cdh5.2.0/spark-1.3.1/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__
  File ""/Users/abhishekchoudhary/bigdata/cdh5.2.0/spark-1.3.1/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value
py4j.protocol.Py4JJavaError
</code></pre>

<p>Where should I place the jar file in my spark pre-built setup so that I will be able to access <code>spark-csv</code> from python editor directly as well.</p>
","552521","552521","2015-06-10 18:31:11","How to add any new library like spark-csv in Apache Spark prebuilt version","<python><apache-spark><apache-spark-sql>","6","0","1075"
"50638319","2018-06-01 07:34:44","0","","<p>As per the <em>HTML</em> you have provided, to monitor the live clock on the website and execute your <code>@Tests</code> at a certain time e.g. <strong>6:00:00 PM</strong> you can use the following solution:</p>

<pre><code>myElem = driver.find_element_by_xpath(""//span[contains(.,'The Time is:')]"").get_attribute(""innerHTML"")
if ""6:00:00 PM"" in myElem
    driver.find_element_by_name(""submit"").click()
</code></pre>
","7429447","7429447","2018-06-01 08:01:54","0","421","DebanjanB","2017-01-17 08:59:30","63154","13103","3455","2612","50634669","50635051","2018-06-01 00:48:00","0","162","<p>I am writing a selenium script to perform form fills at a certain time of day. </p>

<p>I currently have the form fills scheduled with Python's schedule module however this uses my system time. The issue is my system time is not synced with the websites server time. Fortunately, the website has a live clock in the page. </p>

<p>I inspected the clock element which is as follows:</p>

<p><code>&lt;span&gt;The Time is: &lt;b class=""jquery_server_clock"" dfc=""pv""&gt;5:24:59 PM&lt;/b&gt;&lt;/span&gt;</code></p>

<p>What I am trying to do is monitor the live clock on the website and when it hits a certain time perform an action - I was thinking with a 'while loop'. </p>

<p>Something like:</p>

<pre><code>while true:
    t = ""jquery_server_clock""
    if t = ""6:00:00 PM""
        driver.find_element_by_name(""submit"").click()
</code></pre>

<p>I can't seem to get the clock time into a variable to monitor at this point. After I get that I'm sure I could get the while loop working. </p>

<p>Thank you for the help!</p>
","7406623","","","Monitor live clock with selenium / python","<python><selenium>","2","3","1026"
"50638327","2018-06-01 07:35:01","0","","<p>If you are looking for one specific key's total, you can use the <code>.count()</code> <a href=""https://docs.python.org/3.6/tutorial/datastructures.html"" rel=""nofollow noreferrer"">list method</a> as a direct query.</p>

<p>Given a list of dicts:</p>

<pre><code>&gt;&gt;&gt; employees = [{'name': 'John', 'empID': '102', 'dpt': 'tech', 'title': 'programmer', 'salary': '75'},
         {'name': 'Jane', 'empID': '202', 'dpt': 'tech', 'title': 'programmer', 'salary': '80'},
         {'name': 'Joe', 'empID': '303', 'dpt': 'accounting', 'title': 'accountant', 'salary': '85'}]
</code></pre>

<p>You can create a list of each with a list comprehension:</p>

<pre><code>&gt;&gt;&gt; [d['dpt'] for d in employees]
['tech', 'tech', 'accounting']
</code></pre>

<p>And that can then be tallied:</p>

<pre><code>&gt;&gt;&gt; [d['dpt'] for d in employees].count('tech')
2
&gt;&gt;&gt; [d['dpt'] for d in employees].count('accounting')
1
&gt;&gt;&gt; [d['dpt'] for d in employees].count('nothing')
0
</code></pre>

<p>Alternatively, you can construct a generator that will tally specific values:</p>

<pre><code>&gt;&gt;&gt; sum(1 for d in employees if d['dpt']=='tech')
2
</code></pre>

<p>If you want to have the count of all elements (other than using a Counter) you can do:</p>

<pre><code>&gt;&gt;&gt; lot=[d['dpt'] for d in employees]
&gt;&gt;&gt; {c:lot.count(c) for c in set(lot)}
{'accounting': 1, 'tech': 2}
</code></pre>
","298607","298607","2018-06-01 21:52:02","0","1425","dawg","2010-03-21 20:37:33","63972","4386","4958","1643","50637701","50637801","2018-06-01 06:54:48","1","572","<p>python is  new language for me so this question might sound simple, but if someone can point me in the right direction, I would appreciate it! I created a dictionary call employees and it holds some value about them:</p>

<p>I'm trying to read how many people are in each department, for example: tech-2, accounting-1. </p>

<p>I have something like this, but it prints out blank.</p>

<pre><code>  def main():
   employees= {'name': 'John', 'empID': '102', 'dpt': 'tech', 'title': 
   'programmer', 'salary': '75'}
   {'name': 'Jane', 'empID': '202', 'dpt': 'tech', 'title': 'programmer', 
   'salary': '80'}
   {'name': 'Joe', 'empID': '303', 'dpt': 'accounting', 'title': 
   'accountant', 'salary': '85'}
    for item in employees:
    dic = employees[item]
    if dic['dpt'[0]]==dic['dpt'[1]]:
        duplicate += 1
        print(""there are ""+ duplicate)
    else:
        print(""There are no duplicate"")
</code></pre>
","2774460","1222951","2018-06-01 06:55:39","Counting values in an employee dictionary database in python","<python><python-3.x><dictionary><counter><key-value>","4","0","928"
"50638330","2018-06-01 07:35:05","1","","<p>This should work, per your comments:</p>

<pre><code>files, tuples = list(), list()
for targetFile in os.listdir('Z:/data'):
    if re.match('.*\.json|.*\.jpg', targetFile):
        files.append(targetFile)
        tuples.append((...))
</code></pre>

<p>...where in the ellipsis you place code that extracts the filenames and appends the endings.</p>
","4526030","","","1","354","John Perry","2015-02-03 21:07:09","1663","237","521","1","50637877","50638505","2018-06-01 07:07:01","0","25","<p>I have a folder which among others contains pairs of json and jpeg files with the same file names. Based on this folder, I want to create a list of tuples containing the pairs as follows:</p>

<pre><code>[('first.json','first.jpg'),('second.json','second.jpg')...('last.json','last.jpg')]
</code></pre>

<p>Filtering for only json and and jpg files is easy:</p>

<pre><code>import os
import re
files = [targetFile for targetFile in os.listdir('Z:/data') if re.match('.*\.json|.*\.jpg', targetFile)]
print(files)
</code></pre>

<p>But how can I combine that part with the generation of the list of tuples without iterating through the file list for a second time?</p>
","1934212","1934212","2018-06-01 07:12:40","Get a list of tuples containing files with the same names but different endings","<python>","3","3","670"
"50638352","2018-06-01 07:36:16","1","","<p>Consider using a Python <a href=""https://en.wikipedia.org/wiki/Integrated_development_environment"" rel=""nofollow noreferrer"">integrated development environment</a> in your development workflow. IDEs often include features such as <code>Go To Declaration</code> or <code>Find Usages</code> that are accessible simply by right-clicking on the symbol you are looking for.</p>

<p>For example, here is how to <code>Find Usages</code> of a symbol in the PyCharm IDE: <a href=""https://www.jetbrains.com/help/pycharm/finding-usages-in-project.html"" rel=""nofollow noreferrer"">link to PyCharm docs</a>.</p>

<p>You can find a list of Python IDEs to install on the Python <a href=""https://wiki.python.org/moin/IntegratedDevelopmentEnvironments"" rel=""nofollow noreferrer"">community Wiki</a>.</p>
","7663649","7663649","2018-06-01 07:44:03","0","788","Pierre","2017-03-06 00:24:56","658","36","318","6","50638243","50638352","2018-06-01 07:29:50","1","31","<p>I'm new in coding. Let say i got a big python package contains many files and folder structures (contains many module). If  i open any random file and found custom functions/modules and want to know what their jobs and purposes. For simple example i  found this : </p>

<pre><code>value = grabData(var)
</code></pre>

<p>i want to track this grabdata() , what the definition is or where it's located  etc. But sometimes this kind of functions is located on other files (via import). What is the easiest way to track them. Tracking backward using 'import' statement seems not efficient. Any tips ?</p>
","1363960","","","Python : How to easily track the module/function definition in big package","<python><module>","2","0","604"
"50638382","2018-06-01 07:38:04","1","","<p>Assuming that the problem is caused by <code>accents['ac']</code> being either a list of string or a single string, a simple processing could be:</p>

<pre><code> #check if tag is a list, if not then make a list with empty slot at end
 if not isinstance(accents['ac'], list):
      accents['ac'] = [ accents['ac'] ]

 #loop through guaranteed list
 for ac in accents['ac']: #this line throws error if not list object!
     ...
</code></pre>
","3545273","","","1","444","Serge Ballesta","2014-04-17 12:25:02","90494","5432","1346","480","50637413","50638382","2018-06-01 06:35:21","0","38","<p>Currently i am getting information from an xml file. 
If an xml tag has more then one children it will return as a list inside that tag, however if that xml tag has only 1 child it will return not as a list and only as a regular string.</p>

<p>My question is: is there a better way to iterate through this tag? if it is a list, iterate through the list length amount of times, but if it is a string only iterate once?</p>

<p>This is my current approach:</p>

<pre><code> #check if tag is a list, if not then make a list with empty slot at end
 if not isinstance(accents['ac'], list):
      accents['ac'] = list((accents['ac'], {}))

 #loop through guaranteed list
 for ac in accents['ac']: #this line throws error if not list object!

      #if the empty slot added is encountered at end, break out of loop
      if bool(ac) == False:
            break
</code></pre>

<p>any ideas on how to make this cleaner or more professional is appreciated.</p>
","5859663","","","Proper way to iterate through a list which may not be a list","<python><python-3.x><list><iteration>","3","3","955"
"50638392","2018-06-01 07:38:45","0","","<p>The first time your code calls <code>file.read()</code> it is reading the whole file. The second time your code calls it, without first closing the file and reopening it, it is at EOF.</p>

<p>If you want to extract stuff from individual lines of the file then iterate through the file:</p>

<pre><code>codes = []
functions = []
with open(fname,'r') as file:
    for line in file:
        codes.extend(re.findall('LIR: (.+)',line))
        functions.extend(re.findall(';(.+);LIR', line))
</code></pre>
","2084384","","","1","505","BoarGules","2013-02-18 18:26:42","10770","849","166","27","50638264","50638392","2018-06-01 07:31:03","-1","114","<p><code>file = open(fname, 'r')
codes = re.findall('LIR: (.+)', file.read())
functions = re.findall(';(.+);LIR', file.read())</code></p>

<p>Im trying to extract 2 different strings from every 1 line in single file.</p>

<p>It gets 
<code>SyntaxError: unexpected EOF while parsing</code></p>
","9879869","","","Unexpected EOF while parsing Python: 2 regex in 1 open file","<python><regex>","1","5","293"
"50638411","2018-06-01 07:39:53","0","","<pre><code>with open('filename') as f:
    d = {k + 1: list(map(int, l.split())) for k, l in enumerate(f)}
</code></pre>
","8033585","8033585","2018-06-01 07:46:21","0","121","AGN Gazer","2017-05-18 21:39:33","6041","460","707","281","50638043","50638411","2018-06-01 07:17:33","0","25","<p>I would like to make this text file as a dictionnary. 
Like this:</p>

<pre><code>{1: [37, 79, ..., 196], ..., 200: [149,...,35]}
</code></pre>

<p><a href=""https://i.stack.imgur.com/EVA8t.jpg"" rel=""nofollow noreferrer"">Picture</a></p>

<p>I used a picture, but normaly, it's a text file. 
Thanks for your help !! </p>

<p>Dropbox link of the file: <a href=""https://www.dropbox.com/s/uvvga3pypy8drm2/kargerMinCut.txt?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/uvvga3pypy8drm2/kargerMinCut.txt?dl=0</a></p>
","5335342","8033585","2018-06-01 07:34:38","Open a text file and make a dictionnary variable","<python><dictionary><text-files>","1","5","524"
"50638427","2018-06-01 07:41:00","1","","<p>The easiest way of doing it is by using the transition matrix <strong>T</strong> and then using a plain Markovian random walk (in brief, the graph can be considered as a finite-state Markov chain).</p>

<p>Let <strong>A</strong> and <strong>D</strong> be the adjacency and degree matrices of a graph <em>G</em>, respectively. The transition matrix <strong>T</strong> is defined as <strong>T</strong> = <strong>D</strong>^(-1) <strong>A</strong>.<br>
Let <strong>p</strong>^(0) be the state vector (in brief, the <em>i</em>-th component indicates the probability of being at node <em>i</em>) at the beginning of the walk, the first step (walk) can be evaluated as <strong>p</strong>^(1) = <strong>T</strong> <strong>p</strong>^(0).<br>
Iteratively, the <em>k</em>-th random walk step can be evaluated as <strong>p</strong>^(k) = <strong>T</strong> <strong>p</strong>^(k-1).  </p>

<p>In plain Networkx terms...</p>

<pre><code>import networkx
import numpy
# let's generate a graph G
G = networkx.gnp_random_graph(5, 0.5)
# let networkx return the adjacency matrix A
A = networkx.adj_matrix(G)
A = A.todense()
A = numpy.array(A, dtype = numpy.float64)
# let's evaluate the degree matrix D
D = numpy.diag(numpy.sum(A, axis=0))
# ...and the transition matrix T
T = numpy.dot(numpy.linalg.inv(D),A)
# let's define the random walk length, say 10
walkLength = 10
# define the starting node, say the 0-th
p = numpy.array([1, 0, 0, 0, 0]).reshape(-1,1)
visited = list()
for k in range(walkLength):
    # evaluate the next state vector
    p = numpy.dot(T,p)
    # choose the node with higher probability as the visited node
    visited.append(numpy.argmax(p))
</code></pre>
","5149764","5149764","2018-06-01 07:48:16","0","1668","AlessioX","2015-07-23 19:59:44","2729","505","619","47","37311651","","2016-05-18 23:27:19","2","4654","<p>I am new to networkX. I created a graph as follows:</p>

<pre><code>G = nx.read_edgelist(filename,
                     nodetype=int,
                     delimiter=',',
                     data=(('weight', float),))
</code></pre>

<p>where the edges are positive, but do not sum up to one. </p>

<p>Is there a built-in method that makes a random walk of <code>k</code> steps from a certain node and return the node list? If not, what is the easiest way of doing it (nodes can repeat)?  </p>

<p>Pseudo-code:  </p>

<pre><code>node = random
res = [node]
for i in range(0, k)
    read edge weights from this node
    an edge from this node has probability weight / sum_weights
    node = pick an edge from this node 
    res.append(node)
</code></pre>
","464277","464277","2016-05-19 00:17:29","Get node list from random walk in networkX","<python><graph><machine-learning><statistics><networkx>","2","1","755"
"50638500","2018-06-01 07:45:28","1","","<p>Thanks to @John Perry, who reminded me of the fact that style shouldn't prevail over functionality, I came up with the following simple solution:</p>

<pre><code>import os
from collections import defaultdict
listOfRelevantFiles = defaultdict(list)
for targetFile in os.listdir('Z:/data'):
    if '.jpg' in targetFile or '.json' in targetFile:
        listOfRelevantFiles[targetFile.split('.')[0]].append(targetFile)
print(listOfRelevantFiles) 
</code></pre>
","1934212","","","1","461","user1934212","2012-12-28 10:20:00","2514","326","150","0","50637877","50638505","2018-06-01 07:07:01","0","25","<p>I have a folder which among others contains pairs of json and jpeg files with the same file names. Based on this folder, I want to create a list of tuples containing the pairs as follows:</p>

<pre><code>[('first.json','first.jpg'),('second.json','second.jpg')...('last.json','last.jpg')]
</code></pre>

<p>Filtering for only json and and jpg files is easy:</p>

<pre><code>import os
import re
files = [targetFile for targetFile in os.listdir('Z:/data') if re.match('.*\.json|.*\.jpg', targetFile)]
print(files)
</code></pre>

<p>But how can I combine that part with the generation of the list of tuples without iterating through the file list for a second time?</p>
","1934212","1934212","2018-06-01 07:12:40","Get a list of tuples containing files with the same names but different endings","<python>","3","3","670"
"50638505","2018-06-01 07:45:59","0","","<p>If you are looking for that compact way of doing this, </p>

<p>ps: note that this returns list of lists instead of list of tuples,</p>

<pre><code># bunch of files os.listdir() returns
files = ['first.jpg', 'first.json', 'second.jpg', 'second.json']

print([re.findall(fileName + r'(?:.jpg|.json)', ' '.join(files)) \
       for fileName in set(re.findall(r'(\w*?)(?:\.jpg|\.json)', ' '.join(files)))])

# [['second.jpg', 'second.json'], ['first.jpg', 'first.json']]
</code></pre>
","4237254","","","0","485","BcK","2014-11-10 21:01:26","1519","137","63","23","50637877","50638505","2018-06-01 07:07:01","0","25","<p>I have a folder which among others contains pairs of json and jpeg files with the same file names. Based on this folder, I want to create a list of tuples containing the pairs as follows:</p>

<pre><code>[('first.json','first.jpg'),('second.json','second.jpg')...('last.json','last.jpg')]
</code></pre>

<p>Filtering for only json and and jpg files is easy:</p>

<pre><code>import os
import re
files = [targetFile for targetFile in os.listdir('Z:/data') if re.match('.*\.json|.*\.jpg', targetFile)]
print(files)
</code></pre>

<p>But how can I combine that part with the generation of the list of tuples without iterating through the file list for a second time?</p>
","1934212","1934212","2018-06-01 07:12:40","Get a list of tuples containing files with the same names but different endings","<python>","3","3","670"
"50638526","2018-06-01 07:47:25","0","","<p>You will need to interface firebase from within your python script. Have a look at this interface. <a href=""https://github.com/ozgur/python-firebase"" rel=""nofollow noreferrer"">https://github.com/ozgur/python-firebase</a></p>
","8274585","","","1","228","Shubham Jaiswal","2017-07-08 10:02:31","189","69","11","1","50632275","","2018-05-31 20:25:29","1","257","<p>Today I successfully build a custom object detection system using tensorflow (deep learning) and cv2.</p>

<p>Now for the next step, I want to implement an Action(or trigger) like if this object is detected then do this.</p>

<p>example: if a baseball bat is detected then print ""let's try to make a home run""</p>

<p>Actually, after a particular object is detected then I want to send a message to my android app using google firebase.</p>

<pre><code># coding: utf-8

# # Object Detection Demo
# Welcome to the object detection inference walkthrough!  This notebook will                         
walk you step by step through the process of using a pre-trained model to 
detect objects in an image. Make sure to follow the [installation 
instructions]        
# # Imports
import numpy as np
import os
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
import zipfile

from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image

import cv2
cap = cv2.VideoCapture(0)


# ## Env setup
# This is needed since the notebook is stored in the object_detection folder.
sys.path.append("".."")
sys.path.append(""D:\work\O_detection\models"")
sys.path.append(""D:\work\O_detection\models\slim"")

# ## Object detection imports
# Here are the imports from the object detection module.
from utils import label_map_util
from utils import visualization_utils as vis_util

# # Model preparation
# ## Variables 
# Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_CKPT` to point to a new .pb file.  
# By default we use an ""SSD with Mobilenet"" model here. See the [detection model zoo]                
# What model to download. / load from memory
MODEL_NAME = 'gun_detection_graph'
#MODEL_FILE = MODEL_NAME + '.tar.gz'
#DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'

# Path to frozen detection graph. This is the actual model that is used for the object detection.
PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'

# List of the strings that is used to add correct label for each box.
PATH_TO_LABELS = os.path.join('training', 'object-detection.pbtxt')

NUM_CLASSES = 1

detection_graph = tf.Graph()
with detection_graph.as_default():
  od_graph_def = tf.GraphDef()
  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:
    serialized_graph = fid.read()
    od_graph_def.ParseFromString(serialized_graph)
    tf.import_graph_def(od_graph_def, name='')

# ## Loading label map
# Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine

label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(label_map,         
max_num_classes=NUM_CLASSES, use_display_name=True)
category_index = label_map_util.create_category_index(categories)


# ## Helper code
def load_image_into_numpy_array(image):
  (im_width, im_height) = image.size
  return np.array(image.getdata()).reshape(
      (im_height, im_width, 3)).astype(np.uint8)


# # Detection
with detection_graph.as_default():
  with tf.Session(graph=detection_graph) as sess:
    while True:

      ret,image_np = cap.read()

      # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
      image_np_expanded = np.expand_dims(image_np, axis=0)
      image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
      # Each box represents a part of the image where a particular object was detected.
      boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
      # Each score represent how level of confidence for each of the objects.
      # Score is shown on the result image, together with the class label.
      scores = detection_graph.get_tensor_by_name('detection_scores:0')
      classes = detection_graph.get_tensor_by_name('detection_classes:0')
      num_detections = detection_graph.get_tensor_by_name('num_detections:0')
      # Actual detection.
      (boxes, scores, classes, num_detections) = sess.run([boxes, scores, classes, num_detections],feed_dict={image_tensor: image_np_expanded})
      # Visualization of the results of a detection.
      vis_util.visualize_boxes_and_labels_on_image_array(image_np, np.squeeze(boxes),np.squeeze(classes).astype(np.int32),np.squeeze(scores),category_index,use_normalized_coordinates=True,line_thickness=8)

      cv2.imshow('object detection', cv2.resize(image_np,(800,600)))
      if cv2.waitKey(25) &amp; 0xFF == ord('q'):
          cv2.destroyAllWindows()
          break
</code></pre>
","8750813","8750813","2018-06-01 07:07:12","Action on detection of object","<python><opencv><tensorflow><automation><computer-vision>","1","0","4789"
"50638538","2018-06-01 07:48:04","-1","","<p>I don't know exactly how scipy's minimize works, but I think you're right that it doesn't follow the constraints.</p>

<p>If, in obj(x,b), I try printing x - b, I get negative output for the last loop when it throws the error.</p>

<p>this isn't totally surprising though. the function you're minimizing is essentially log(z) with the constraint z > 0. that's not going to converge very slowly... :p</p>

<p>EDIT: setting the keyword parameter ""tol"" (error tolerance) to around 0.1 for the minimizer gets rid of the error ( as in <code>minimize(..., tol=0.1)</code>). I still don't know why it's so significant in the first place.</p>

<p>EDIT (since I still can't comment):
@sascha, that makes sense, but why does the function even evaluate negative values of x - b? Does the minimize function ""test"" outside of its constraints just in order to better optimize the solution within it?
(If I should ask this as a new question, let me know.)</p>
","9878957","9878957","2018-06-01 18:25:34","0","948","Jay Calamari","2018-06-01 02:37:50","335","32","205","2","50638315","","2018-06-01 07:34:32","1","111","<p>scipy.minimize does not seem to adhere to constraints.
Here is a simple example where the constraint is for preventing a negative argument in the logarithm, but the minimize function does not adhere :</p>

<pre><code>import math
from scipy.optimize import minimize

def obj(x,b):
    print ""obj x"",x
    return math.log(x-b)

def constr(x,b):
    print ""constr x"",x
    return x-b

x=3.1
b=3
a=minimize(obj,x,args=(b),constraints={'type': 'ineq', 'fun':constr,'args':[b]})
</code></pre>

<p>the output is:</p>

<pre><code>constr x [ 3.1]
obj x [ 3.1]
constr x [ 3.1]
obj x [ 3.1]
obj x [ 3.10000001]
constr x [ 3.1]
constr x [ 3.10000001]
obj x [ 3.]
Traceback (most recent call last):
File ""scipy_minimize_constraints.py"", line 19, in 
a=minimize(obj,x,args=(b),constraints={'type': 'ineq', 'fun':constr,'args':[b]})
File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/optimize/_minimize.py"", line 495, in minimize
constraints, callback=callback, **options)
File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/optimize/slsqp.py"", line 378, in _minimize_slsqp
fx = func(x)
File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/optimize/optimize.py"", line 292, in function_wrapper
return function(*(wrapper_args + args))
File ""scipy_minimize_constraints.py"", line 9, in obj
return math.log(x-b)
ValueError: math domain error
</code></pre>

<p>python2.7
Scipy version 1.0.0</p>

<p>Am I doing something wrong?</p>
","9717415","","","scipy.minimize does not seem to adhere to constraints","<python><scipy><mathematical-optimization>","2","0","1557"
"50638540","2018-06-01 07:48:12","2","","<p>Upgrade <code>pip</code> and <code>setuptools</code>:</p>

<pre><code>pip install -U pip setuptools
</code></pre>
","7976758","","","0","117","phd","2017-05-07 15:40:03","32575","3331","2903","3411","50637089","","2018-06-01 06:12:43","4","2130","<p>I want to update aws-sam-cli on my ubuntu 14.04. I have sam 0.2.11 version. I want to update in 0.3.0. When I run </p>

<pre><code>pip install --user aws-sam-cli
</code></pre>

<p>or </p>

<pre><code>pip install --user --upgrade aws-sam-cli
</code></pre>

<p>I got</p>

<blockquote>
  <p>Downloading/unpacking aws-sam-cli
    Downloading aws-sam-cli-0.3.0.tar.gz (128kB): 128kB downloaded
    Running setup.py (path:/tmp/pip_build_amber/aws-sam-cli/setup.py) egg_info for package aws-sam-cli
      /usr/lib/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'python_requires'
        warnings.warn(msg)
      error in aws-sam-cli setup command: 'install_requires' must be a string or list of strings containing valid project/version requirement specifiers
      Complete output from command python setup.py egg_info:
      /usr/lib/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'python_requires'
    warnings.warn(msg)
  error in aws-sam-cli setup command: 'install_requires' must be a string or list of strings containing valid project/version requirement specifiers</p>
  
  <p>Cleaning up...
  Command python setup.py egg_info failed with error code 1 in /tmp/pip_build_amber/aws-sam-cli
  Storing debug log for failure in /home/amber/.pip/pip.log**</p>
</blockquote>
","9463661","7003620","2018-06-01 07:39:25","How to Install aws-sam-cli in ubuntu 14?","<python><amazon-web-services><pip>","3","1","1327"
"50638582","2018-06-01 07:50:27","1","","<p>You nornally do not 'track' functions, but there should not be as much confusion: package is normally structured to make imports and locations easy to understand. </p>

<p>You can use any search tool that gives you entries of the function or class, eg as @Pierre suggests for Pycharm. </p>

<p>If you are interested about what the function, module or a class is, you can use:</p>

<ul>
<li><code>dir</code>: <code>import re; dir(re)</code> - gives you a glimpse of methods available in a module or a class</li>
<li>reading a docstring: <code>re.search.__doc__</code></li>
<li>using IPyhton 'magic' to learn about object: <code>re.search?</code></li>
<li>of course reading the docs ;)</li>
</ul>

<p>Hope it helps.</p>
","1758363","","","0","721","Evgeny","2012-10-19 06:01:21","2200","433","940","145","50638243","50638352","2018-06-01 07:29:50","1","31","<p>I'm new in coding. Let say i got a big python package contains many files and folder structures (contains many module). If  i open any random file and found custom functions/modules and want to know what their jobs and purposes. For simple example i  found this : </p>

<pre><code>value = grabData(var)
</code></pre>

<p>i want to track this grabdata() , what the definition is or where it's located  etc. But sometimes this kind of functions is located on other files (via import). What is the easiest way to track them. Tracking backward using 'import' statement seems not efficient. Any tips ?</p>
","1363960","","","Python : How to easily track the module/function definition in big package","<python><module>","2","0","604"
"50638587","2018-06-01 07:50:50","0","","<p>I think <code>np.where</code>  and <code>pandas shifts</code> does this
This subtract between two consecutive Time, only if the users are same</p>

<pre><code>df1 = np.where (df['users'] == df['users'].shifts(-1), df['time'] - df['time'].shifts(-1), 'NaN')
</code></pre>
","3280146","","","0","274","user3280146","2014-02-06 15:06:09","776","131","908","314","50637942","50638192","2018-06-01 07:11:17","1","87","<p>This is part of a larger project, but I've broken my problem down into steps, so here's the first step. Take a Pandas dataframe, like this:</p>

<pre><code>index | user   time     
---------------------
 0      F       0   
 1      T       0   
 2      T       0   
 3      T       1   
 4      B       1 
 5      K       2 
 6      J       2 
 7      T       3 
 8      J       4 
 9      B       4 
</code></pre>

<p>For each unique user, can I extract the difference between the values in column ""time,"" but with some conditions?</p>

<p>So, for example, there are two instances of user J, and the ""time"" difference between these two instances is 2. Can I extract the difference, 2, between these two rows? Then if that user appears again, extract the difference between that row and the previous appearance of that user in the dataframe?</p>
","1298833","","","In a Pandas dataframe, how can I extract the difference between the values on separate rows within the same column, conditional on a second column?","<python><pandas><slice>","2","1","849"
"50638625","2018-06-01 07:53:16","4","","<p>Here's how I would do it:</p>

<ul>
<li>compute differences <code>d</code> between consecutive elements (remove <code>0</code>s from result)</li>
<li>count the number of times the sign changes in <code>d</code></li>
<li>return 2 plus that count (because there is a hill and a valley even in a monotonically increasing sequence)</li>
</ul>

<p>In code:</p>

<pre><code>def hill_and_vally(s):
    d=[x1-x0 for x0,x1 in zip(s,s[1:]) if x1!=x0]
    return 2+sum(d0*d1&lt;0 for d0,d1 in zip(d,d[1:]))
</code></pre>

<p>of course it can be implemented with <code>for</code> loops and indexes, but <code>zip</code> and list comprehensions is more pythonic.</p>

<p><code>zip(s,s[1:])</code> is a common way to get pairs of adjacent elements in a list.</p>

<p>Tests:</p>

<pre><code>&gt;&gt;&gt; hill_and_vally([1,0,0,0,1])
3
&gt;&gt;&gt; hill_and_vally([0,1,0,1,0])
5
&gt;&gt;&gt; hill_and_vally([0,2,2,1,1,0,0])
3
</code></pre>

<p>Handling corner cases such as the reported <code>[1,1,1,1]</code> is left as an exercise  :-)</p>
","1044117","1044117","2019-10-16 09:28:46","2","1028","fferri","2011-11-13 11:47:27","12765","819","702","613","50638502","50638625","2018-06-01 07:45:43","1","1958","<p>I am writing a function to find all the hills and valleys in a given list. For instance, [1,0,0,0,1] returns 3 and [0,1,0,1,0] returns 5. [0,2,2,1,1,0,0] returns 3. If a number (or consecutive numbers with same values) are bigger or smaller than both of its neighbors, it is considered as a hill or a valley. </p>

<p>Below is my code:</p>

<pre><code>def hill_and_vally(s):
    if not s or len(s) &lt; 2:
        return 0
    i = 0
    count = 0
    pre = None
    while i &lt; len(s):
        if i == 0:
            while s[i] == s[i+1]:  # loop until value is different
                i += 1
            i += 1
            if i &lt; len(s):       # check if it reaches the end 
                count += 1
                pre = s[i-1]     # track the previous value
        elif i == len(s) - 1:
            while s[i] == s[i-1]:  
                i -= 1
            i -= 1
            if i &gt;= 0:
                count += 1
            break
        else:
            while s[i] == s[i+1]:
                i += 1
            i += 1
            if s[i] &gt; s[i-1] and pre &gt; s[i-1]:  # it is a valley
                count += 1
            elif s[i] &lt; s[i-1] and pre &lt; s[i-1]:  # it is a hill
                count += 1
            pre = s[i-1]
    return count
</code></pre>

<p>Can someone help me to improve the complexity to O(N). Or show me another way to do it with better complexity? Please show me some examples. Thanks in advance. </p>
","8530384","","","find all the hills and valley in a list","<python><algorithm>","2","4","1462"
"50638626","2018-06-01 07:53:16","2","","<p>Define plot axis and specify it when you make the first hist. Then make twin axis and use to plot the second one. Parameter <code>alpha</code> allows you to distinguish hists.</p>

<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

housing = pd.read_csv(""C:\\Users\\&lt;username&gt;\\handson-ml\\datasets\\housing\\housing.csv"")
fig, ax = plt.subplots()
housing.hist(bins=50, figsize=(20,15), ax=ax, alpha=.2)

housing[""income_cat""] = np.ceil(housing[1]/1.5)
housing[""income_cat""].where(housing[""income_cat""]&lt;5, 5.0, inplace=True)

ax2 = ax.twinx()
housing['income_cat'].hist(ax=ax2, color='r', alpha=.2)
plt.show()
</code></pre>

<p>Use <code>plt.show()</code> at the end to show the figure once.</p>
","6799088","","","0","740","Ilya","2016-09-06 07:16:57","183","10","15","0","50638314","","2018-06-01 07:34:28","0","70","<p>I am currently using Atom and when i run my code the output graphs are being shown in a sequential order such that I can only see the next graph after closing the first graph. </p>

<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

housing = pd.read_csv(""C:\\Users\\&lt;username&gt;\\handson-ml\\datasets\\housing\\housing.csv"")

housing.hist(bins=50, figsize=(20,15))
plt.show()


housing[""income_cat""] = np.ceil(housing[""median_income""]/1.5)
housing[""income_cat""].where(housing[""income_cat""]&lt;5, 5.0, inplace=True)

plt.hist(housing[""income_cat""])
plt.show()
</code></pre>

<p>How to correct this so as to see all the graphs simultaneously? Being used to Jupyter I am having trouble performing data visualization on other platforms.</p>
","9865163","","2018-06-01 07:46:20","How to include multiple graphs in one window","<python><pandas><numpy><matplotlib><data-visualization>","1","2","777"
"50638691","2018-06-01 07:57:22","2","","<p>You are right about the need for coordinate transformation. Here is a working code with the resulting plot.</p>

<pre><code>import numpy as np 
import matplotlib as mpl        
import matplotlib.pyplot as plt 
import cartopy.crs as ccrs
import cartopy.io.img_tiles as cimgt

request = cimgt.OSM()
fig, ax = plt.subplots(figsize=(10,16),
                       subplot_kw=dict(projection=request.crs))
extent = [-89, -88, 41, 42]  # (xmin, xmax, ymin, ymax)
ax.set_extent(extent)
ax.add_image(request, 8)

# generate (x, y) centering at (extent[0], extent[2])
x = extent[0] + np.random.randn(1000)
y = extent[2] + np.random.randn(1000)

# do coordinate conversion of (x,y)
xynps = ax.projection.transform_points(ccrs.Geodetic(), x, y)

# make a 2D histogram
h = ax.hist2d(xynps[:,0], xynps[:,1], bins=40, zorder=10, alpha=0.5)
#h: (counts, xedges, yedges, image)

cbar = plt.colorbar(h[3], ax=ax, shrink=0.45, format='%.1f')  # h[3]: image

plt.show()
</code></pre>

<p>The resulting plot:<a href=""https://i.stack.imgur.com/lFO8D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lFO8D.png"" alt=""enter image description here""></a></p>

<p><strong>Edit 1</strong></p>

<p>When <code>ax</code> is created with <code>plt.subplots()</code>, it has a certain projection defined. In this case, the projection is defined by the keyword <code>projection=request.crs</code>.
To plot something on <code>ax</code>, you must use its coordinate system.</p>

<p>The coordinate conversion is done with the function <code>transform_points()</code> in the statement</p>

<p><code>xynps=ax.projection.transform_points(ccrs.Geodetic(), x, y)</code></p>

<p>where</p>

<ul>
<li>(x, y) is a list of (longitude, latitude) values, </li>
<li>ccrs.Geodetic() signifies the values are (long, lat) in degrees</li>
</ul>

<p>The return values <code>xynps</code> is an array of map coordinates. It has 2 columns for x and y that are in the appropriate coordinate system used by current <code>ax</code>.</p>
","2177413","2177413","2018-06-02 02:41:37","2","1997","swatchai","2013-03-16 16:11:51","5581","329","267","28","50611018","50638691","2018-05-30 18:08:42","1","1764","<p>I have a created a Open Street Map plot using cartopy:</p>

<pre><code>from __future__ import division
import numpy as np 
import matplotlib as mpl        
import matplotlib.pyplot as plt 
import cartopy.crs as ccrs
import cartopy.io.img_tiles as cimgt

request = cimgt.OSM()
extent = [-89, -88, 41, 42]

ax = plt.axes(projection=request.crs)
ax.set_extent(extent)

ax.add_image(request, 8)
plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/jF72u.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jF72u.png"" alt=""enter image description here""></a></p>

<p>Now I also have a list of longitude and latitude points. How can I overlay a heatmap of these longitude and latitude points over the streetmap?</p>

<p>I've tried using hist2d, but this doesn't work.</p>

<pre><code>lons = (-88 --89)*np.random.random(100)+-89
lats = (41 - 42)*np.random.random(100)+42
ax.hist2d(lons,lats)
plt.show()
</code></pre>

<p>But this doesn't work.</p>

<p>I'm guessing I have to throw a transform argument in the plotting command somewhere? But I'm unsure how to go about it.</p>

<p>Thanks!</p>
","3826115","","","Cartopy Heatmap over OpenStreetMap Background","<python><heatmap><cartopy>","1","0","1113"
"50638709","2018-06-01 07:58:17","1","","<p>It is not supported in <code>style</code>s yet.</p>

<p>Possible solution is convert non numeric to <code>NaN</code>s:</p>

<pre><code>df = df.apply(pd.to_numeric, errors='coerce')
</code></pre>

<p>And if necessary highlight it:</p>

<pre><code>df.style.background_gradient(cmap='RdYlGn',
                         low=0.6, 
                         high=0.8, 
                         subset=pd.IndexSlice[1:4,:]).highlight_null('red')
</code></pre>

<p><a href=""https://i.stack.imgur.com/QI5wr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QI5wr.png"" alt=""pic""></a></p>
","2901002","","","1","596","jezrael","2013-10-20 20:27:26","427380","89269","18260","743","50638073","50638709","2018-06-01 07:19:45","1","175","<p>I have the following DataFrame with mixed types and I would like to color the cells only on the first 4 rows:</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; np.random.seed(0)
&gt;&gt;&gt; df = pd.DataFrame(index=[1,2,3,4],
                      columns = ['A','B'],             
                      data=np.random.uniform(low=0.,
                      high=1.,size=8).reshape(4,2)
                      )
&gt;&gt;&gt; df = pd.concat([df,pd.DataFrame(index=[5],
                       columns=['A','B'],
                       data=[['OK', 'OK']])])
&gt;&gt;&gt; df
           A           B
1   0.548814    0.715189
2   0.602763    0.544883
3   0.423655    0.645894
4   0.437587    0.891773
5         OK          OK
</code></pre>

<p>Using the <code>subset</code> to only apply the style only on the first 4 rows, this returns me an error:</p>

<pre><code>&gt;&gt;&gt; df.style.background_gradient(cmap='RdYlGn',
                         low=0.6, 
                         high=0.8, 
                         subset=pd.IndexSlice[1:4,:])
TypeError: (""Cannot cast array data from dtype('O') to dtype('int64') according to the rule 'safe'"", 'occurred at index A')
</code></pre>

<p>Any idea of a workaround?</p>

<p>I'm using Pandas 0.22.0</p>

<p>thanks,
Greg</p>
","1613796","","","Pandas DataFrame style with mixed type yields TypeError","<python><pandas><dataframe>","1","0","1314"
"50638769","2018-06-01 08:02:40","2","","<p>Had some time at my hand and wondered how I would have solved that.</p>

<p>So here is my solution, maybe it sparks some ideas:</p>

<pre><code>labels = """"""\
+--------------------+--------------------+--------------------+
| L1 - A             |                    |                    |
|                    |  L2 - B            |                    |
|                    |                    |  L3 - C            |
|                    |                    |                    |
| L1 - D             |                    |                    |
|                    |  L2 - E            |                    |
|                    |                    |  L3 - F            |
+--------------------+--------------------+--------------------+
""""""

lines = [[(s.strip()[-1:] if s.strip() else None)
             for s in line[1:-1].split('|')]
                 for line in labels.splitlines()[1:-1]]

for index, labels in enumerate(lines):
    if not any(labels):
        continue
    for i, label in enumerate(labels):
        if label:
            break
        if not label:
            lines[index][i] = lines[index-1][i]

print([tuple(labels) for labels in lines])

# --&gt; [('A', None, None), ('A', 'B', None), ('A', 'B', 'C'), (None, None, None), ('D', None, None), ('D', 'E', None), ('D', 'E', 'F')]
</code></pre>
","4350517","","","1","1325","Sebastian Loehner","2014-12-11 15:00:53","877","43","21","6","50636877","50638769","2018-06-01 05:52:42","1","62","<p>I am parsing a file where labels are defined as below, with hierarchies represented by using new lines</p>

<pre><code>+--------------------+--------------------+--------------------+
| L1 - A             |                    |                    |
|                    |  L2 - B            |                    |
|                    |                    |  L3 - C            |
|                    |                    |                    |
| L1 - D             |                    |                    |
|                    |  L2 - E            |                    |
|                    |                    |  L3 - F            |
+--------------------+--------------------+--------------------+
</code></pre>

<p>I represent the above as:</p>

<pre><code>labels = [
   ['A', None, None, None, 'D', None, None],
   [None, 'B', None, None, None, 'E', None],
   [None, None, 'C', None, None, None, 'F']
]
</code></pre>

<p>I tried</p>

<pre><code>def joinfoo(items):
   if len(items) == 1:
      return items[0]

   result = []
   active = None
   for x, y in zip(items[0], joinfoo(items[1:])):
      active = x if x else active
      if type(y) is tuple:
         result.append((active, y[0], y[1]))
      else:
         result.append((active, y))

   return result
</code></pre>

<p>I wanted </p>

<pre><code>[
   ('A', None, None), ('A', 'B', None), ('A', 'B', 'C'),
   (None, None, None),
   ('D', None, None), ('D', 'E', None), ('D', 'E', 'F')
]
</code></pre>

<p>and got this</p>

<pre><code>[
   ('A', None, None), ('A', 'B', None), ('A', 'B', 'C'),
   ('A', 'B', None),
   ('D', 'B', None), ('D', 'E', None), ('D', 'E', 'F')
]
</code></pre>

<p>Suggestions on how to fix <code>joinfoo()</code> to achieve the desired result? Solution needs to support a variable number of columns.</p>

<p>It should be something like <code>for x, y in zip(joinfoo(items[:-1]), items[-1]):</code> instead of <code>for x, y in zip(items[0], joinfoo(items[1:])):</code> to go in the right direction...?</p>

<p>Edit:
The original list of lists may have wrongly implied a pattern to the hierarchy. There is no defined pattern. Number of columns are also variable. A better test case maybe.. </p>

<pre><code>+--------------+--------------+--------------+
|   L1 - A     |              |              |    = A
|              |    L2 - B    |              |    = A - B
|              |              |    L3 - C    |    = A - B - C
|              |              |    L3 - D    |    = A - B - D
|              |    L2 - E    |              |    = A - E
|              |              |              |    =   
|   L1 - F     |              |              |    = F
|              |    L2 - G    |              |    = F - G
|              |              |    L3 - H    |    = F - G - H
+--------------+--------------+--------------+

labels = [
   ['A', None, None, None, None, None, 'F', None, None],
   [None, 'B', None, None, 'E', None, None, 'G', None],
   [None, None, 'C', 'D', None, None, None, None, 'H']
]
</code></pre>
","2193381","2193381","2018-06-01 09:12:30","Denormalizing hierarchy in list of lists","<python><python-2.7>","3","2","3017"
"50638840","2018-06-01 08:07:07","3","","<p>The <code>voxels</code> method <a href=""https://matplotlib.org/2.1.2/users/whats_new.html#voxels-function-for-mplot3d"" rel=""nofollow noreferrer"">has been introduced in matplotlib 2.1</a>.</p>

<p>Any earlier version of matplotlib does not have this method available.</p>
","4124317","4124317","2018-06-01 16:36:27","2","274","ImportanceOfBeingErnest","2014-10-09 07:50:43","173272","28712","2299","3162","50635610","50638840","2018-06-01 03:15:14","3","1303","<p>I'm trying to use matplotlib to display some 3d perlin noise. I have read that the <code>voxels</code> method from <code>Axes3DSubplot</code> could be used to simply display values. However, when I try and call <code>ax.voxels(voxels, facecolors=colors, edgecolor='k')</code>, it throws the exception <code>AttributeError: 'Axes3DSubplot' object has no attribute 'voxels'</code>. Here is my code:</p>

<pre><code>import noise
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

x, y, z = np.indices((8,8,8))
voxels = np.zeros((8,8,8), dtype=np.bool)

for xp in range(8):
    for yp in range(8):
        for zp in range(8):
            voxels[xp,yp,zp] = True if abs(noise.pnoise3(xp/8,yp/8,zp/8)) &gt; 0.5 else False

colors = np.empty(voxels.shape, dtype=object)
colors[voxels] = 'green'

fig = plt.figure()
ax = fig.gca(projection='3d')
ax.voxels(voxels, facecolors=colors, edgecolor='k')  #EXCEPTION


plt.show()
</code></pre>

<p>My python version is 3.6.2 (Anaconda 64-bit). My matplotlib version is 2.0.2. I have used both the ipynb (<code>module://backend_interagg</code>) and <code>Qt5Agg</code> backends, which both give the same problem. I'm running Windows 10.</p>
","1486262","","","'Axes3DSubplot' object has no attribute 'voxels'","<python><matplotlib><voxels>","2","0","1219"
"50638918","2018-06-01 08:11:31","-1","","<p>Remove <code>action = ''</code> and try again:</p>

<pre><code>&lt;form method='post'&gt;{% csrf_token %}

    {{ form.username.label_tag }}
    {{ form.username }}
    {{ form.password.label_tag }}
    {{ form.password }}

   &lt;input type='submit' class='btn btn-primary btn-block' value='{{ title }}' /&gt;
&lt;/form&gt;
</code></pre>

<p>Or install django-tweaks in a virtual environment</p>

<pre><code>pip install pipenv --user
cd
. .bashrc
cd workspace/mysite
pipenv --three
pipenv shell
pipenv install django django-widget-tweaks
python manage.py runserver
</code></pre>

<p>Also suggest you to upgrade python to version 3.6</p>
","9586338","","","0","641","Waket Zheng","2018-04-02 13:50:59","1227","101","88","7","50637081","","2018-06-01 06:12:20","0","48","<p>Right now I'm stuck between two things: Either I have a login form that works but isn't exactly what you call visually appealing and one that looks flawless but doesn't do anything. (I know quite the situation)</p>

<p><a href=""https://i.stack.imgur.com/OJqCN.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OJqCN.jpg"" alt=""If you look at this image right here you&#39;ll know exactly what I&#39;m talking about:""></a></p>

<p>As shown the one up top is clearly quite the looker (sarcasm*), but I want the hideous one at the bottom to work.</p>

<p>For the for up top, I just used <code>{{ form }}</code> tag and have at one point my code looked like this: </p>

<pre><code>&lt;form method='POST' action='' enctype='multipart/form-data'&gt;{% csrf_token %}

    {{ form.username.label_tag }}
    {{ form.username }}
    {{ form.password.label_tag }}
    {{ form.password }}

   &lt;input type='submit' class='btn btn-primary btn-block' value='{{ title }}' /&gt;
&lt;/form&gt;
</code></pre>

<p>But again this doesn't give me the look I want. As of right now, to get the non-functional (but pretty) form look at the bottom I'm using the code for the <a href=""https://getbootstrap.com/docs/4.0/examples/sign-in/"" rel=""nofollow noreferrer"">bootstrap template signin</a> (signin.css included in my base template). This is what my form.html looks like right now: </p>

<pre><code>{% extends 'accounts/base.html' %}
{% block main_content %}
    &lt;div class=""container""&gt;
        &lt;div class='col-sm-6 col-sm-offset-3'&gt;
        &lt;form method='POST' action='' enctype='multipart/form-data' class=""form-signin""&gt;{% csrf_token %}
            {{ form}}
            &lt;h2 class=""h3 mb-3 font-weight-normal""&gt;Please Sign In&lt;/h2&gt;
            &lt;input type=""username"" id=""inputUsername"" class=""form-control"" placeholder=""Username"" required autofocus&gt;
            &lt;input type=""password"" id=""inputPassword"" class=""form-control"" placeholder=""Password"" required&gt;
            &lt;input type='submit' class='""btn btn-lg btn-primary btn-block' value='{{ title }}' /&gt;
        &lt;/form&gt;
        &lt;/div&gt;
    &lt;/div&gt;
{% endblock %}
</code></pre>

<p>I have seen another StackOverflow post <a href=""https://stackoverflow.com/questions/38496439/django-fields-assign-to-html-fields"">such as this one</a> and tried installing <code>django-widget-tweaks</code> but since I'm using cloud9-ide it didn't work </p>

<p><a href=""https://i.stack.imgur.com/2aO79.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2aO79.jpg"" alt=""cloud9-ide it didn&#39;t work""></a></p>

<p>So please if there's a way where I could either somehow implement the form tags in my Html or successfully download django-tweaks, please let me know and I would greatly appreciate it.</p>

<p>Thanks in advance.</p>
","9820620","5689074","2018-06-01 10:19:28","Is there a way to link input fields from HTML to the ACTUAL form fields used in DJANGO","<python><html><django><twitter-bootstrap>","2","4","2840"
"50638933","2018-06-01 08:12:22","1","","<p>Try using raw string.</p>

<p><strong>Ex:</strong></p>

<pre><code>os.makedirs(r'\\SERVERNAME\SharedFolder\NewFolder')
</code></pre>
","532312","","","2","136","Rakesh","2010-12-06 13:07:54","56694","5302","758","1508","50638884","50638933","2018-06-01 08:09:43","0","147","<p>Is possible to use windows network path with python functions?</p>

<p>ex. <code>//SERVERNAME/SharedFolder</code></p>

<p>For example something like <code>os.makedirs('////SERVERNAME//SharedFolder//NewFolder')</code></p>

<p>It seems it doesn't work for me</p>

<p>Thanks,
Federico</p>
","7046886","532312","2018-06-01 08:55:06","Using Windows network path with python","<python><windows><shared-directory>","1","0","289"
"50638961","2018-06-01 08:14:13","0","","<p>Check out <a href=""https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-tips-and-tricks-running-spark-windows.html"" rel=""nofollow noreferrer"">this</a> git book</p>

<blockquote>
  <p>Download winutils.exe binary from <a href=""https://github.com/steveloughran/winutils"" rel=""nofollow noreferrer"">https://github.com/steveloughran/winutils</a>  repository.</p>
  
  <p>Note
      You should select the version of Hadoop the Spark distribution was compiled with, e.g. use hadoop-2.7.1 for Spark 2 (here is the direct link to winutils.exe binary).</p>
  
  <p>Save winutils.exe binary to a directory of your choice, e.g. c:\hadoop\bin.</p>
</blockquote>
","4390959","","","0","663","gorros","2014-12-24 07:47:19","890","85","135","3","50637728","","2018-06-01 06:56:55","1","3446","<p>I am trying to integrate pyspark with python 2.7 (Pycharm IDE). I need to run some huge text files.</p>

<p>So this is what i am doing.</p>

<p>Download Spark (2.3.0-bin-hadoop-2.7) and extract it
Install JDK</p>

<p>And then i am trying to run this script</p>

<p>spark_home = os.environ.get('SPARK_HOME', None)
os.environ[""SPARK_HOME""] = ""C:\spark-2.3.0-bin-hadoop2.7""
import pyspark
from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession</p>

<pre><code>conf = SparkConf()
sc = SparkContext(conf=conf)
spark = SparkSession.builder.config(conf=conf).getOrCreate() 
import pandas as pd
ip = spark.read.format(""csv"").option(""inferSchema"",""true"").option(""header"",""true"").load(r""D:\some file.csv"")
</code></pre>

<p>Pycharm says that no module named Pyspark is found.</p>

<p>I am solving that by adding content roots and pointing to the folders where it is installed.</p>

<p>But the problem is every time i reopen pycharm, i have to add the content roots. How do i fix this?</p>

<p>Next is, when i do manage to run the script it throws up the following error.</p>

<pre><code>2018-06-01 12:20:49 ERROR Shell:397 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
    at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:379)
    at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:394)
    at org.apache.hadoop.util.Shell.&lt;clinit&gt;(Shell.java:387)
    at org.apache.hadoop.util.StringUtils.&lt;clinit&gt;(StringUtils.java:80)
    at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
    at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:273)
    at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:261)
    at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:791)
    at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:761)
    at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:634)
    at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2464)
    at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2464)
    at scala.Option.getOrElse(Option.scala:121)
    at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2464)
    at org.apache.spark.SecurityManager.&lt;init&gt;(SecurityManager.scala:222)
    at org.apache.spark.deploy.SparkSubmit$.secMgr$lzycompute$1(SparkSubmit.scala:393)
    at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$secMgr$1(SparkSubmit.scala:393)
    at org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply(SparkSubmit.scala:401)
    at org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply(SparkSubmit.scala:401)
    at scala.Option.map(Option.scala:146)
    at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:400)
    at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:170)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/C:/spark-2.3.0-bin-hadoop2.7/jars/hadoop-auth-2.7.3.jar) to method sun.security.krb5.Config.getInstance()
WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
2018-06-01 12:20:49 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
2018-06-01 12:20:56 ERROR Executor:91 - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ArrayIndexOutOfBoundsException: 63
    at org.apache.spark.unsafe.types.UTF8String.numBytesForFirstByte(UTF8String.java:191)
    at org.apache.spark.unsafe.types.UTF8String.numChars(UTF8String.java:206)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    at org.apache.spark.scheduler.Task.run(Task.scala:109)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:844)
2018-06-01 12:20:56 WARN  TaskSetManager:66 - Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 63
    at org.apache.spark.unsafe.types.UTF8String.numBytesForFirstByte(UTF8String.java:191)
    at org.apache.spark.unsafe.types.UTF8String.numChars(UTF8String.java:206)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    at org.apache.spark.scheduler.Task.run(Task.scala:109)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:844)

2018-06-01 12:20:56 ERROR TaskSetManager:70 - Task 0 in stage 0.0 failed 1 times; aborting job
Traceback (most recent call last):
  File ""D:/Microsoft/ThemeSpark.py"", line 13, in &lt;module&gt;
    ip = spark.read.format(""csv"").option(""inferSchema"",""true"").option(""header"",""true"").load(r""D:\Microsoft\xbox_13.5_26.5\Xbox Family.csv"")
  File ""C:\spark-2.3.0-bin-hadoop2.7\python\pyspark\sql\readwriter.py"", line 166, in load
    return self._df(self._jreader.load(path))
  File ""C:\spark-2.3.0-bin-hadoop2.7\python\lib\py4j-0.10.6-src.zip\py4j\java_gateway.py"", line 1160, in __call__
  File ""C:\spark-2.3.0-bin-hadoop2.7\python\pyspark\sql\utils.py"", line 63, in deco
    return f(*a, **kw)
  File ""C:\spark-2.3.0-bin-hadoop2.7\python\lib\py4j-0.10.6-src.zip\py4j\protocol.py"", line 320, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o25.load.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 63
    at org.apache.spark.unsafe.types.UTF8String.numBytesForFirstByte(UTF8String.java:191)
    at org.apache.spark.unsafe.types.UTF8String.numChars(UTF8String.java:206)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    at org.apache.spark.scheduler.Task.run(Task.scala:109)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:844)

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
    at scala.Option.foreach(Option.scala:257)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)
    at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)
    at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
    at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3272)
    at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)
    at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)
    at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3253)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
    at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3252)
    at org.apache.spark.sql.Dataset.head(Dataset.scala:2484)
    at org.apache.spark.sql.Dataset.take(Dataset.scala:2698)
    at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:148)
    at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)
    at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)
    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:202)
    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:202)
    at scala.Option.orElse(Option.scala:289)
    at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:201)
    at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:392)
    at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)
    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)
    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:564)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:214)
    at java.base/java.lang.Thread.run(Thread.java:844)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 63
    at org.apache.spark.unsafe.types.UTF8String.numBytesForFirstByte(UTF8String.java:191)
    at org.apache.spark.unsafe.types.UTF8String.numChars(UTF8String.java:206)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    at org.apache.spark.scheduler.Task.run(Task.scala:109)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    ... 1 mo
</code></pre>

<p>I did some research and inferred that it is caused by the absence of the <code>winutils.exe</code>from the spark folder. I downloaded and placed it in the spark bin. Still this error keeps coming. How do i fixed this?</p>
","5440236","3269809","2018-06-01 08:50:29","Pyspark - Failed to locate the winutils binary in the hadoop binary path","<python><apache-spark><pyspark>","2","2","17093"
"50638962","2018-06-01 08:14:16","1","","<p>Try to do the following:</p>

<pre><code>$ rm -rf /home/ubuntu/.local/lib/python2.7/site-packages/OpenSSL
$ rm -rf /home/ubuntu/.local/lib/python2.7/site-packages/pyOpenSSL-0.15.1.egg-info
</code></pre>

<p>In the last line, you might have another version of <code>pyOpenSSL</code>, specify yours.</p>
","7127824","","","0","305","techkuz","2016-11-07 17:50:48","818","144","1783","60","41102299","45853179","2016-12-12 13:45:05","17","24481","<p>I tried to install from pip and keep on getting similar type of errors.</p>

<pre><code>$ pip install quandl
Traceback (most recent call last):
  File ""/usr/bin/pip"", line 9, in &lt;module&gt;
    load_entry_point('pip==1.5.6', 'console_scripts', 'pip')()
  File ""/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 558, in load_entry_point
    return get_distribution(dist).load_entry_point(group, name)
  File ""/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 2682, in load_entry_point
    return ep.load()
  File ""/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 2355, in load
    return self.resolve()
  File ""/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 2361, in resolve
    module = __import__(self.module_name, fromlist=['__name__'], level=0)
  File ""/usr/lib/python2.7/dist-packages/pip/__init__.py"", line 74, in &lt;module&gt;
    from pip.vcs import git, mercurial, subversion, bazaar  # noqa
  File ""/usr/lib/python2.7/dist-packages/pip/vcs/mercurial.py"", line 9, in &lt;module&gt;
    from pip.download import path_to_url
  File ""/usr/lib/python2.7/dist-packages/pip/download.py"", line 22, in &lt;module&gt;
    import requests, six
  File ""/usr/lib/python2.7/dist-packages/requests/__init__.py"", line 53, in &lt;module&gt;
    from .packages.urllib3.contrib import pyopenssl
  File ""/usr/lib/python2.7/dist-packages/urllib3/contrib/pyopenssl.py"", line 53, in &lt;module&gt;
    import OpenSSL.SSL
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/OpenSSL/__init__.py"", line 8, in &lt;module&gt;
    from OpenSSL import rand, crypto, SSL
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/OpenSSL/SSL.py"", line 112, in &lt;module&gt;
    if _lib.Cryptography_HAS_SSL_ST:
AttributeError: 'module' object has no attribute 'Cryptography_HAS_SSL_ST'
</code></pre>

<p>Now even though i tried to install different pip modules iam getting same error.Is there any solution for this ? This was caused due to the unexpected killing of the process while a pip module is being downloaded.</p>

<p>Please help me with the necessary steps to rectify this error.</p>

<p>I tried to install this </p>

<pre><code>$ pip install -U cryptography
Traceback (most recent call last):
  File ""/usr/bin/pip"", line 9, in &lt;module&gt;
    load_entry_point('pip==1.5.6', 'console_scripts', 'pip')()
  File ""/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 558, in load_entry_point
    return get_distribution(dist).load_entry_point(group, name)
  File ""/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 2682, in load_entry_point
    return ep.load()
  File ""/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 2355, in load
    return self.resolve()
  File ""/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 2361, in resolve
    module = __import__(self.module_name, fromlist=['__name__'], level=0)
  File ""/usr/lib/python2.7/dist-packages/pip/__init__.py"", line 74, in &lt;module&gt;
    from pip.vcs import git, mercurial, subversion, bazaar  # noqa
  File ""/usr/lib/python2.7/dist-packages/pip/vcs/mercurial.py"", line 9, in &lt;module&gt;
    from pip.download import path_to_url
  File ""/usr/lib/python2.7/dist-packages/pip/download.py"", line 22, in &lt;module&gt;
    import requests, six
  File ""/usr/lib/python2.7/dist-packages/requests/__init__.py"", line 53, in &lt;module&gt;
    from .packages.urllib3.contrib import pyopenssl
  File ""/usr/lib/python2.7/dist-packages/urllib3/contrib/pyopenssl.py"", line 53, in &lt;module&gt;
    import OpenSSL.SSL
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/OpenSSL/__init__.py"", line 8, in &lt;module&gt;
    from OpenSSL import rand, crypto, SSL
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/OpenSSL/SSL.py"", line 112, in &lt;module&gt;
    if _lib.Cryptography_HAS_SSL_ST:
AttributeError: 'module' object has no attribute 'Cryptography_HAS_SSL_ST'
</code></pre>
","5908241","5908241","2018-06-16 20:20:16","Pip Error : 'module' object has no attribute 'Cryptography_HAS_SSL_ST'","<python><linux><ubuntu><openssl><pip>","9","7","3966"
"50638973","2018-06-01 08:14:50","0","","<p>It doesn't avoid <code>for</code> completely but replaces it with list comprehension:</p>

<pre><code>from operator import attrgetter

max_val_company = max((c for c in companies), key=attrgetter('value'))
</code></pre>

<p><code>max_val_company</code> will contain <code>Company</code> object having max <code>value</code> attribule. </p>
","2616105","","","0","343","Torbik","2013-07-24 19:54:47","372","28","13","1","50600750","50638973","2018-05-30 09:11:52","0","43","<p><strong>Intro</strong> </p>

<p>Comming from this question</p>

<p><a href=""https://stackoverflow.com/questions/30420621/python-creating-object-instances-in-a-loop-with-independent-handling"">Python- creating object instances in a loop with independent handling</a></p>

<p>I was asking myself how to access the attributes of all rather than just a single instance, if you have created a list of instances. </p>

<p><strong>Minimal Example</strong></p>

<p>Consider creating a list of company instances with the attributes name and value. Now I would like to know, which of these companies does have the highest value. </p>

<p>So far I am simply storing the values of the value attribute in an array and then find the index of the maximum value.</p>

<pre><code>import numpy as np

class Company(object):
    def __init__(self, name, value):
        self.name = name
        self.value = value

companies = []
for name in 'ABC':
    companies.append(Company(name, np.random.rand()))

l = len(companies)
liste = np.zeros(l)
for i in range(l):
    liste[i] = companies[i].value

ind = np.unravel_index(np.argmax(liste), liste.shape)
print(""Highest value: "", liste[ind])
print(""Company of highest value: "", companies[ind[0]].name)
</code></pre>

<p><strong>Questions</strong></p>

<p>1) Is there a different way to create this list of all attribute values without a for-loop?</p>

<p>2) Is there a direct way to find the instance in a list, for which the value of a certain attribute is maximal?</p>
","6806746","","","How to access the attributes of all instances stored in a list?","<python><class><oop><attributes><instances>","2","0","1500"
"50638996","2018-06-01 08:16:37","0","","<p>Use <a href=""https://docs.python.org/3/library/collections.html#collections.Counter"" rel=""nofollow noreferrer""><code>collections.Counter</code></a>:</p>

<pre><code>import csv

with open('2017CountryData.csv') as csvfile:
    countries = []
    readCSV = csv.reader(csvfile, delimiter=',')
    for row in readCSV:
        if row[1] not in countries:
            countries.append(row[1])

print(Counter(countries))
</code></pre>
","8472377","8472377","2018-06-01 08:19:08","2","431","Austin","2017-08-16 11:54:23","18860","1780","141","2099","50638907","","2018-06-01 08:10:38","0","114","<p>I am pulling data from a csv file and have it printing out three letter country codes from each row of data.  How do I make python identify the number of occurrences of each unique country code from the outputted data?  Here is what I have that is printing the country codes.</p>

<pre><code>import csv

with open('2017CountryData.csv') as csvfile:

  readCSV = csv.reader(csvfile, delimiter=',')
  for row in readCSV:
        countries = row[1]
        print(countries)
</code></pre>
","9880039","8472377","2018-06-01 08:11:52","Identifying and counting distinct outputs in python from csv file","<python>","1","2","488"
"50639029","2018-06-01 08:19:10","1","","<p>Not sure what you are trying to achieve for 3 vectors, but for two the code has to be much, much simplier:</p>

<pre><code>test2 = [[0.0, 0.0, 0.0, 72.0, 12.9], [0.0, 0.0, 0.0, 80.0, 11.3]]

def distance(list1, list2):
    """"""Distance between two vectors.""""""
    squares = [(p-q) ** 2 for p, q in zip(list1, list2)]
    return sum(squares) ** .5

d2 = distance(test2[0], test2[1])  
</code></pre>

<p>With <code>numpy</code> is even a <a href=""https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy"">shorter statement</a>.</p>

<p>PS. python 3 recommened</p>
","1758363","","","1","607","Evgeny","2012-10-19 06:01:21","2200","433","940","145","50637446","50639386","2018-06-01 06:37:49","0","3288","<p>I'm writing a simple program to compute the euclidean distances between multiple lists using python. This is the code I have so fat</p>

<pre><code>import math
euclidean = 0
euclidean_list = []
euclidean_list_complete = []

test1 = [[0.0, 0.0, 0.0, 152.0, 12.29], [0.0, 0.0, 0.357, 245.0, 10.4], [0.0, 0.0, 0.10, 200.0, 11.0]]

test2 = [[0.0, 0.0, 0.0, 72.0, 12.9], [0.0, 0.0, 0.0, 80.0, 11.3]]

for i in range(len(test2)):
    for j in range(len(test1)):
        for k in range(len(test1[0])):
            euclidean += pow((test2[i][k]-test1[j][k]),2)

        euclidean_list.append(math.sqrt(euclidean))
        euclidean = 0

    euclidean_list_complete.append(euclidean_list)


print euclidean_list_complete
</code></pre>

<p>my problem with this code is it doesn't print the output i want properly. The output should be 
<code>[[80.0023, 173.018, 128.014], [72.006, 165.002, 120.000]]</code> </p>

<p>but instead, it prints</p>

<p><code>[[80.00232559119766, 173.01843095173416, 128.01413984400315, 72.00680592832875, 165.0028407300917, 120.00041666594329], [80.00232559119766, 173.01843095173416, 128.01413984400315, 72.00680592832875, 165.0028407300917, 120.00041666594329]]</code></p>

<p>I'm guessing it has something to do with the loop. What should I do to fix it? By the way, I don't want to use numpy or scipy for studying purposes</p>

<p>If it's unclear, I want to calculate the distance between lists on test2 to each lists on test1</p>
","8126411","8126411","2018-06-01 07:05:52","Computing euclidean distance with multiple list in python","<python><list><euclidean-distance>","4","6","1456"
"50639038","2018-06-01 08:20:01","3","","<p>Your code works fine, just change base_url to</p>

<pre><code>base_url = ""http://www.midiworld.com/download/""
</code></pre>

<p>Right now, i.e. ""1.mid"" contains the HTML for this site: <a href=""http://www.midiworld.com/files/1"" rel=""nofollow noreferrer"">http://www.midiworld.com/files/1</a>
(You can open it with a text editor.)</p>

<p>The MIDI-files can be downloaded the url <a href=""http://www.midiworld.com/download/"" rel=""nofollow noreferrer"">http://www.midiworld.com/download/</a>{insert number}</p>

<p>I downloaded the first 100 but it seems there are currently 4992 downloadable midi files, so if you want more files, just change</p>

<pre><code>for i in range(1,4992):
</code></pre>

<p>As a side-note, the site gives you download ""_-_.mid"" which is 0 bytes, if the requested .mid doesn't exist. So, if you are going to repeat downloading the files and you want all the files they have, consider setting range to for example 100 000 and break the loop if downloaded file-size is 0 bytes.</p>

<pre><code>for i in range(1,100000):
    if (urllib.request.urlopen(base_url+str(i)).length == 0):
        break
</code></pre>
","9833349","9833349","2018-06-01 11:09:45","0","1134","Toivo Mattila","2018-05-23 08:54:50","65","1","84","0","50636524","50639038","2018-06-01 05:18:57","1","155","<p>I want to download the MIDI files from this website for a project. I have written the following code to download the files:</p>

<pre><code>from bs4 import BeautifulSoup
import requests
import re, os
import urllib.request
import string

base_url = ""http://www.midiworld.com/files/""

base_path = 'path/where/I/will/save/the/downloaded/MIDI/files'
os.chdir(base_path + '/MIDI Files')

for i in range(1,2386):
    page = requests.get(base_url + str(i))
    soup = BeautifulSoup(page.text, ""html.parser"")

    li_box = soup.select(""div ul li a"")
    urllib.request.urlretrieve(base_url+str(i), str(i)+'.mid')
</code></pre>

<p>This is downloading the files, but when I click on them to play, they don't play; I get this error:</p>

<p><a href=""https://i.stack.imgur.com/AWRvy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AWRvy.png"" alt=""enter image description here""></a></p>

<p>But if I download the files manually (I checked for a couple of them), I can play the files. In case its relevant, those files also have different names, not numbers like how I am saving them. Could it be the cause for this? The files are not empty too, as can be seen from this screenshot below:</p>

<p><a href=""https://i.stack.imgur.com/imKTj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/imKTj.png"" alt=""enter image description here""></a></p>

<p><strong>EDIT:</strong> When I tried to load a programmatically downloaded MIDI file to compare it to its corresponding manually downloaded MIDI file in <a href=""https://codebeautify.org/file-diff"" rel=""nofollow noreferrer"">this</a> website, I got this error:</p>

<p><code>Failed to load data=error</code></p>

<p>But no such error when loading the manually downloaded one.</p>

<p><strong>EDIT 2:</strong> These are the first 50 bytes of the hex dump:</p>

<p>For the programmatically downloaded file:</p>

<pre><code>file name: 1.mid
mime type: 

0000-0010:  3c 21 44 4f-43 54 59 50-45 20 68 74-6d 6c 20 50  &lt;!DOCTYP E.html.P
0000-0020:  55 42 4c 49-43 20 22 2d-2f 2f 57 33-43 2f 2f 44  UBLIC.""- //W3C//D
0000-0030:  54 44 20 58-48 54 4d 4c-20 31 2e 30-20 53 74 72  TD.XHTML .1.0.Str
0000-0032:  69 63
</code></pre>

<p>For the corresponding manually downloaded file:</p>

<pre><code>file name: Adson_John_-_Courtly_Masquing_Ayres.mid
mime type: 

0000-0010:  4d 54 68 64-00 00 00 06-00 01 00 0b-00 f0 4d 54  MThd.... ......MT
0000-0020:  72 6b 00 00-00 7b 00 ff-58 04 04 02-18 08 00 ff  rk...{.. X.......
0000-0030:  59 02 00 00-00 ff 51 03-07 a1 20 f0-40 ff 51 03  Y.....Q. ....@.Q.
0000-0032:  09 27
</code></pre>
","5305512","5305512","2019-01-17 14:22:24","Why can't I play the MIDI files I have downloaded programmatically, but I can play them when I download them manually?","<python><audio><web-crawler><midi>","1","10","2595"
"50639043","2018-06-01 08:20:10","0","","<p>use supervisord to start the gunicorn service with logging parameters.</p>

<pre><code>[program:sanic]
directory=/home/ubuntu/api
command=/home/ubuntu/api/venv/bin/gunicorn api:app --bind 0.0.0.0:8000 --worker-class sanic.worker.GunicornWorker -w 2
stderr_logfile = log/api_stderr.log
stdout_logfile = log/api_stdout.log
</code></pre>

<p>This works perfectly for me so I can just tail -f log/api_stderr.log</p>
","9340433","","","1","415","Christo Goosen","2018-02-09 20:58:32","310","39","13","0","49706746","","2018-04-07 11:09:41","2","9378","<p>currently i am running my sanic(microframework) webservice with gunicorn as a daemon and i would like to save all logs in files(access and error)</p>

<p>My config:</p>

<pre><code>reload = True
daemon = True
bind = '0.0.0.0:6666'
worker_class = 'sanic.worker.GunicornWorker'
loglevel = 'debug'
accesslog = 'access.log'
access_log_format = '%(h)s %(l)s %(u)s %(t)s ""%(r)s"" %(s)s %(b)s ""%(f)s"" ""%(a)s""'
errorlog = 'error.log'
</code></pre>

<p>Next i start the webservice:</p>

<pre><code>gunicorn --config config.py app:app
</code></pre>

<p>Sooo.. my errorlog works, but i get absolutely no accesslogs..</p>

<p>There is no hint in the documentation about this issue, could anybody help me?</p>

<p>Thanks and Greetings!</p>
","4958604","","","Gunicorn - No access logs","<python><logging><gunicorn><sanic>","4","0","729"
"50639066","2018-06-01 08:21:30","6","","<p>searching inside the combobox. Finally a good solution. Thank you guys!! It helped me a lot.</p>

<p>And here is the adjusted code to PyQt5:</p>

<pre><code>#!/usr/bin/env python
# -*- coding: utf-8 -*-
from PyQt5 import QtCore, QtGui, QtWidgets
from PyQt5.QtCore import Qt, QSortFilterProxyModel
from PyQt5.QtWidgets import QCompleter, QComboBox

class ExtendedComboBox(QComboBox):
    def __init__(self, parent=None):
        super(ExtendedComboBox, self).__init__(parent)

        self.setFocusPolicy(Qt.StrongFocus)
        self.setEditable(True)

        # add a filter model to filter matching items
        self.pFilterModel = QSortFilterProxyModel(self)
        self.pFilterModel.setFilterCaseSensitivity(Qt.CaseInsensitive)
        self.pFilterModel.setSourceModel(self.model())

        # add a completer, which uses the filter model
        self.completer = QCompleter(self.pFilterModel, self)
        # always show all (filtered) completions
        self.completer.setCompletionMode(QCompleter.UnfilteredPopupCompletion)
        self.setCompleter(self.completer)

        # connect signals
        self.lineEdit().textEdited.connect(self.pFilterModel.setFilterFixedString)
        self.completer.activated.connect(self.on_completer_activated)


    # on selection of an item from the completer, select the corresponding item from combobox 
    def on_completer_activated(self, text):
        if text:
            index = self.findText(text)
            self.setCurrentIndex(index)
            self.activated[str].emit(self.itemText(index))


    # on model change, update the models of the filter and completer as well 
    def setModel(self, model):
        super(ExtendedComboBox, self).setModel(model)
        self.pFilterModel.setSourceModel(model)
        self.completer.setModel(self.pFilterModel)


    # on model column change, update the model column of the filter and completer as well
    def setModelColumn(self, column):
        self.completer.setCompletionColumn(column)
        self.pFilterModel.setFilterKeyColumn(column)
        super(ExtendedComboBox, self).setModelColumn(column)    


if __name__ == ""__main__"":
    import sys
    from PyQt5.QtWidgets import QApplication
    from PyQt5.QtCore import QStringListModel

    app = QApplication(sys.argv)

    string_list = ['hola muchachos', 'adios amigos', 'hello world', 'good bye']

    combo = ExtendedComboBox()

    # either fill the standard model of the combobox
    combo.addItems(string_list)

    # or use another model
    #combo.setModel(QStringListModel(string_list))

    combo.resize(300, 40)
    combo.show()

    sys.exit(app.exec_())
</code></pre>
","9880057","","","0","2650","Tamas Haver","2018-06-01 08:04:57","61","0","0","0","4827207","4829759","2011-01-28 10:32:48","8","9570","<p>I need a QCombox which Items are filtered based on the text input. If I set the QCombobox editable, the user can insert text and the QCompleter is automatically created. But the items are not filtered and I don’t want the user to add new Items.</p>

<p>Is there any possibility to add this functionality to the QCombobox?</p>
","281126","","","How do I Filter the PyQt QCombobox Items based on the text input?","<python><qt><pyqt><qcombobox>","4","0","329"
"50639075","2018-06-01 08:22:08","4","","<p>You will be able to resolve your error with following way.</p>

<p>First check whether your input image has only single channel. You can check it by running <code>print img.shape</code>. If the result is like <code>(height, width, 3)</code>, then the image is not single channel. You can convert the image into a single channel one by:</p>

<pre><code>img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
</code></pre>

<p>Then check whether the image type is not float. You can check it by running <code>print img.dtype</code>. If the result is related to <code>float</code>, you will need to change it also by:</p>

<pre><code>img = img.astype('uint8')
</code></pre>

<p>And also the last thing, it is not actually an error in this case. But it can be an error in the future if you keep practicing this method of combining multiple flags. When you are using multiple flags, remember to combine then with not a <strong>plus sign</strong>, but with a <strong>| sign</strong>.</p>

<pre><code> ret3,th3 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY | cv2.THRESH_OTSU)
</code></pre>

<p>Finally, you can you <code>opencv</code> functions to show the image. No need to depend on other libraries.</p>

<p>Final code is as follows:</p>

<pre><code>img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
img = img.astype('uint8')
blur = cv2.GaussianBlur(img,(5,5),0)
ret3,th3 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY | cv2.THRESH_OTSU)
image = numpy.invert(th3)
cv2.show('image_out', image)
cv2.waitKey(0)
cv2.destroyAllWindows() 
</code></pre>
","2924323","","","0","1525","Ramesh-X","2013-10-27 03:41:51","2699","486","1439","1","50631195","","2018-05-31 19:07:00","1","8426","<p>I ran the below code and I am getting an error as</p>

<blockquote>
  <p>OpenCV(3.4.1)
  C:\projects\opencv-python\opencv\modules\imgproc\src\thresh.cpp:1406:
  error: (-215) src.type() == (((0) &amp; ((1 &lt;&lt; 3) - 1)) + (((1)-1) &lt;&lt; 3))
  in function cv::threshold</p>
</blockquote>

<p>I am not clear on what this means and how to fix it</p>

<pre><code>import numpy as numpy
from matplotlib import pyplot as matplot
import pandas as pandas
import math
from sklearn import preprocessing
from sklearn import svm
import cv2

blur = cv2.GaussianBlur(img,(5,5),0)
ret3,th3 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)
image = numpy.invert(th3)
matplot.imshow(image,'gray')
matplot.show()
</code></pre>
","9877690","5839007","2018-05-31 20:32:10","Getting an error OpenCV(3.4.1) C:\projects\opencv-python\opencv\modules\imgproc\src\thresh.cpp:1406: error: (-215)","<python><opencv>","1","1","729"
"50639088","2018-06-01 08:23:11","1","","<p><code>active = x if x else active</code> from this line you were keeping the original value of active if x is None, however, examining your desired output, you needed a way to reset active to None if you reach the count of the tuple.</p>

<p>here is how i achieved your desired output</p>

<pre><code>def joinfoo(items):
   if len(items) == 1:
      return items[0]

   result = []
   active_counter=0
   count=0
   active = None
   for x, y in zip(items[0], joinfoo(items[1:])):
      count=len(y) if type(y) is tuple else 0
      if active_counter &gt;count:
          active_counter=0
          active=None
      else:
          active_counter +=1

      active = x if x else active
      if type(y) is tuple:
         result.append((active, y[0], y[1]))
      else:
         result.append((active, y))

   return result
</code></pre>

<p>and i got the output</p>

<pre><code>    [('A', None, None), ('A', 'B', None), ('A', 'B', 'C'), 
(None, None, None), 
('D', None, None), ('D', 'E', None), ('D', 'E', 'F')]
</code></pre>

<p>hope it solves your problem</p>
","9669507","","","0","1067","James Bincom","2018-04-19 11:27:22","61","16","0","0","50636877","50638769","2018-06-01 05:52:42","1","62","<p>I am parsing a file where labels are defined as below, with hierarchies represented by using new lines</p>

<pre><code>+--------------------+--------------------+--------------------+
| L1 - A             |                    |                    |
|                    |  L2 - B            |                    |
|                    |                    |  L3 - C            |
|                    |                    |                    |
| L1 - D             |                    |                    |
|                    |  L2 - E            |                    |
|                    |                    |  L3 - F            |
+--------------------+--------------------+--------------------+
</code></pre>

<p>I represent the above as:</p>

<pre><code>labels = [
   ['A', None, None, None, 'D', None, None],
   [None, 'B', None, None, None, 'E', None],
   [None, None, 'C', None, None, None, 'F']
]
</code></pre>

<p>I tried</p>

<pre><code>def joinfoo(items):
   if len(items) == 1:
      return items[0]

   result = []
   active = None
   for x, y in zip(items[0], joinfoo(items[1:])):
      active = x if x else active
      if type(y) is tuple:
         result.append((active, y[0], y[1]))
      else:
         result.append((active, y))

   return result
</code></pre>

<p>I wanted </p>

<pre><code>[
   ('A', None, None), ('A', 'B', None), ('A', 'B', 'C'),
   (None, None, None),
   ('D', None, None), ('D', 'E', None), ('D', 'E', 'F')
]
</code></pre>

<p>and got this</p>

<pre><code>[
   ('A', None, None), ('A', 'B', None), ('A', 'B', 'C'),
   ('A', 'B', None),
   ('D', 'B', None), ('D', 'E', None), ('D', 'E', 'F')
]
</code></pre>

<p>Suggestions on how to fix <code>joinfoo()</code> to achieve the desired result? Solution needs to support a variable number of columns.</p>

<p>It should be something like <code>for x, y in zip(joinfoo(items[:-1]), items[-1]):</code> instead of <code>for x, y in zip(items[0], joinfoo(items[1:])):</code> to go in the right direction...?</p>

<p>Edit:
The original list of lists may have wrongly implied a pattern to the hierarchy. There is no defined pattern. Number of columns are also variable. A better test case maybe.. </p>

<pre><code>+--------------+--------------+--------------+
|   L1 - A     |              |              |    = A
|              |    L2 - B    |              |    = A - B
|              |              |    L3 - C    |    = A - B - C
|              |              |    L3 - D    |    = A - B - D
|              |    L2 - E    |              |    = A - E
|              |              |              |    =   
|   L1 - F     |              |              |    = F
|              |    L2 - G    |              |    = F - G
|              |              |    L3 - H    |    = F - G - H
+--------------+--------------+--------------+

labels = [
   ['A', None, None, None, None, None, 'F', None, None],
   [None, 'B', None, None, 'E', None, None, 'G', None],
   [None, None, 'C', 'D', None, None, None, None, 'H']
]
</code></pre>
","2193381","2193381","2018-06-01 09:12:30","Denormalizing hierarchy in list of lists","<python><python-2.7>","3","2","3017"
"50639096","2018-06-01 08:23:47","2","","<p>As pointed out by Jim Morris, the (currently accepted) answer by kilbee is not correct and ""feels"" quiet dirty (in both variations).</p>

<p>The following works for me and ""feels"" like a clean solution:</p>

<pre><code>TextInput:
    on_text:
        something
        something_else
</code></pre>
","2936442","","","1","301","ChristophK","2013-10-30 13:00:19","510","48","88","6","38520218","38520572","2016-07-22 07:05:27","1","494","<p>I have to do:</p>

<pre><code>TextInput:
    on_text: something ; something_else
</code></pre>

<p>How can I perform this without getting errors in kv language?</p>
","5019281","","","KIVY language: multiple commands in a single line","<python><syntax><kivy><kivy-language>","2","1","168"
"50639183","2018-06-01 08:29:37","2","","<p>When it comes to time performance test, I do rely on the <code>timeit</code> module because it automatically executes multiple runs of the code.</p>

<p>On my system, timeit gives following results (I strongly reduced sizes because of the numerous runs):</p>

<pre><code>&gt;&gt;&gt; timeit.timeit(""sum([i for i in numbers if i % 3 == 0])"", ""numbers = range(1, 1000)"")
59.54427594248068
&gt;&gt;&gt; timeit.timeit(""sum((i for i in numbers if i % 3 == 0))"", ""numbers = range(1, 1000)"")
64.36398425334801
</code></pre>

<p>So the generator is slower by about 8% (*). This is not really a surprize because the generator has to execute some code <em>on the fly</em> to get next value, while a precomputed list only increment its current pointer.</p>

<p>Memory evalutation is IMHO more complex, so I have used <a href=""https://code.activestate.com/recipes/577504/"" rel=""nofollow noreferrer"">Compute Memory footprint of an object and its contents (Python recipe)</a> from activestate</p>

<pre><code>&gt;&gt;&gt; numbers = range(1, 100)
&gt;&gt;&gt; numbers = range(1, 100000)
&gt;&gt;&gt; l = [i for i in numbers if i % 3 == 0]
&gt;&gt;&gt; g = (i for i in numbers if i % 3 == 0)
&gt;&gt;&gt; total_size(l)
1218708
&gt;&gt;&gt; total_size(g)
88
&gt;&gt;&gt; total_size(numbers)
48
</code></pre>

<p>My interpretation is that a list uses memory for all of its items (which is not a surprize), while a generator only need few values and some code so the memory footprint is much lesser for the generator.</p>

<p>I strongly think that you have used <code>tracemalloc</code> for something it is not intended for. It is aimed at searching possible memory leaks (large blocs of memory never deallocated) and not at controlling the memory used by individual objects.</p>

<hr>

<p><strong>BEWARE</strong>: I could only test for small sizes. But for very large sizes, the list could exhaust the available memory and the machine will use virtual  memory from swap. In that case, the list version will become much slower. More details <a href=""https://stackoverflow.com/a/11964478/3545273"">there</a></p>
","3545273","3545273","2018-06-01 09:58:32","7","2094","Serge Ballesta","2014-04-17 12:25:02","90494","5432","1346","480","50637863","","2018-06-01 07:05:57","5","109","<p>At first, I want to test the memory usage between generator and list-comprehension.The book  give me a little bench code snippet and I run it on my PC(python3.6, Windows),find something unexpected.</p>

<ol>
<li>On the book, it said, because list-comprehension has to create a real list and allocate memory for it, itering from a list-comprehension must be slower than itering from a generator.</li>
<li>Ofcourse, list-comprehension use more memory than generator.</li>
</ol>

<p>FOllowing is my code,which is not satisfy previous opinion(within sum function).</p>

<pre><code>import tracemalloc
from time import time


def timeIt(func):
    start = time()
    func()
    print('%s use time' % func.__name__, time() - start)
    return func


tracemalloc.start()

numbers = range(1, 1000000)


@timeIt
def lStyle():
    return sum([i for i in numbers if i % 3 == 0])


@timeIt
def gStyle():
    return sum((i for i in numbers if i % 3 == 0))


lStyle()

gStyle()

shouldSize = [i for i in numbers if i % 3 == 0]

snapshotL = tracemalloc.take_snapshot()
top_stats = snapshotL.statistics('lineno')
print(""[ Top 10 ]"")
for stat in top_stats[:10]:
    print(stat)
</code></pre>

<p>The output:</p>

<pre><code>lStyle use time 0.4460000991821289
gStyle use time 0.6190001964569092
[ Top 10 ]
F:/py3proj/play.py:31: size=11.5 MiB, count=333250, average=36 B
F:/py3proj/play.py:33: size=448 B, count=1, average=448 B
F:/py3proj/play.py:22: size=136 B, count=1, average=136 B
F:/py3proj/play.py:17: size=136 B, count=1, average=136 B
F:/py3proj/play.py:14: size=76 B, count=2, average=38 B
F:/py3proj/play.py:8: size=34 B, count=1, average=34 B
</code></pre>

<p>Two points: </p>

<ul>
<li><strong><em>Generator use more time and same memory space.</em></strong></li>
<li><strong><em>The list-comprehension in sum function seems not create the total list</em></strong></li>
</ul>

<p>I think maybe the sum function did something i don't know.Who can explain this?</p>

<p>The book is High Perfoamance Python.chapter 5.But i did sth myself different from the  book to check the validity in other context. And his code is here <a href=""https://github.com/mynameisfiber/high_performance_python/blob/master/05_iterators/iter_vs_list_comprehension.py"" rel=""nofollow noreferrer"">book_code</a>,he didn't put the list-comprehension in sum funciton.</p>
","4957623","9209546","2018-06-01 08:53:35","What does sum do?","<python><sum><generator><python-internals>","1","7","2340"
"50639199","2018-06-01 08:30:18","1","","<p>You seem to use python 2.x, here is a python 3.x solution, since I do not have a python 2.x environment at the moment :</p>

<pre><code>from bs4 import BeautifulSoup
import urllib.request as urllib


url1 = ""&lt;URL&gt;""

# Read the HTML page
content1 = urllib.urlopen(url1).read()
soup = BeautifulSoup(content1, ""lxml"")

# Find the div (there is only one, so you do not need findAll) -&gt; this is your problem
div = soup.find(""div"", class_=""iw_component"", id=""c1417094965154"")
# Now you retrieve all the span within this div
rows = div.find_all(""span"")

# You can do what you want with it !
line = """"
for row in rows:
    row_str = row.get_text()
    row_str = row_str.replace('\t', '')
    line += row_str + "", ""
print(line)
</code></pre>
","8811838","9846358","2018-08-13 09:34:39","0","745","nero","2017-10-21 17:29:35","296","21","60","3","50638894","50639394","2018-06-01 08:10:01","2","94","<p>cannot get the span text within the ""table"", thanks !</p>

<pre><code>from bs4 import BeautifulSoup
import urllib2

url1 = ""url""

content1 = urllib2.urlopen(url1).read()
soup = BeautifulSoup(content1,""lxml"")
table = soup.findAll(""div"", {""class"" : ""iw_component"",""id"":""c1417094965154""})
rows = table.find_all('span',recursive=False)
for row in rows:
    print(row.text)
</code></pre>
","9846358","9846358","2018-08-08 07:38:17","cannot get the <span></span> texts","<python><beautifulsoup>","3","0","386"
"50639220","2018-06-01 08:31:45","0","","<p>The previous answer is great. Anyway, it took to me a bit to understand what was going on. So, I refactored the code in this way that is easier to read for me. I leave here the code in case someone founds it easier too (it runs in python 3.6)</p>

<pre><code>def get_all_connected_groups(graph):
    already_seen = set()
    result = []
    for node in graph:
        if node not in already_seen:
            connected_group, already_seen = get_connected_group(node, already_seen)
            result.append(connected_group)
    return result


def get_connected_group(node, already_seen):
        result = []
        nodes = set([node])
        while nodes:
            node = nodes.pop()
            already_seen.add(node)
            nodes = nodes or graph[node] - already_seen
            result.append(node)
        return result, already_seen


graph = {
     0: {0, 1, 2, 3},
     1: set(),
     2: {1, 2},
     3: {3, 4, 5},
     4: {3, 4, 5},
     5: {3, 4, 5, 7},
     6: {6, 8},
     7: set(),
     8: {8, 9},
     9: set()}

components = get_all_connected_groups(graph)
print(components)
</code></pre>

<p>Result:</p>

<pre><code>Out[0]: [[0, 1, 2, 3, 4, 5, 7], [6, 8, 9]] 
</code></pre>

<p>Also, I simplified the input and output. I think it's a bit more clear to print all the nodes that are in a group</p>
","4825783","3961420","2018-10-22 10:28:46","2","1324","Matias Thayer","2015-04-23 19:16:31","381","69","17","5","10301000","12672454","2012-04-24 15:23:37","7","21951","<p>I'm writing a function <code>get_connected_components</code> for a class <code>Graph</code>:</p>

<pre><code>def get_connected_components(self):
    path=[]
    for i in self.graph.keys():
        q=self.graph[i]
        while q:
            print(q)
            v=q.pop(0)
            if not v in path:
                path=path+[v]
    return path
</code></pre>

<p>My graph is:</p>

<pre><code>{0: [(0, 1), (0, 2), (0, 3)], 1: [], 2: [(2, 1)], 3: [(3, 4), (3, 5)], \
4: [(4, 3), (4, 5)], 5: [(5, 3), (5, 4), (5, 7)], 6: [(6, 8)], 7: [], \
8: [(8, 9)], 9: []}
</code></pre>

<p>where the keys are the nodes and the values are the edge. My function gives me this connected component:</p>

<pre><code>[(0, 1), (0, 2), (0, 3), (2, 1), (3, 4), (3, 5), (4, 3), (4, 5), (5, 3), \
(5, 4), (5, 7), (6, 8), (8, 9)]
</code></pre>

<p>But I would have two different connected components, like:</p>

<pre><code>[[(0, 1), (0, 2), (0, 3), (2, 1), (3, 4), (3, 5), (4, 3), (4, 5), \
(5, 3), (5, 4), (5, 7)],[(6, 8), (8, 9)]]
</code></pre>

<p>I don't understand where I made the mistake.
Can anyone help me?</p>
","917563","10263","2015-09-04 20:31:03","Python connected components","<python><graph-algorithm><connected-components>","4","8","1101"
"50639264","2018-06-01 08:34:37","0","","<pre><code>test1 = [[0.0, 0.0, 0.0, 152.0, 12.29], [0.0, 0.0, 0.357, 245.0, 10.4], [0.0, 0.0, 0.10, 200.0, 11.0]]

test2 = [[0.0, 0.0, 0.0, 72.0, 12.9], [0.0, 0.0, 0.0, 80.0, 11.3]]

final_list = []

for a in test2:
    temp = [] #temporary list
    for b in test1:
        dis = sum([pow(a[i] - b[i], 2) for i in range(len(a))])
        temp.append(round(pow(dis, 0.5),4))

    final_list.append(temp)
print(final_list)
</code></pre>
","6468916","","","0","435","Mohan Babu","2016-06-15 10:09:53","301","50","79","0","50637446","50639386","2018-06-01 06:37:49","0","3288","<p>I'm writing a simple program to compute the euclidean distances between multiple lists using python. This is the code I have so fat</p>

<pre><code>import math
euclidean = 0
euclidean_list = []
euclidean_list_complete = []

test1 = [[0.0, 0.0, 0.0, 152.0, 12.29], [0.0, 0.0, 0.357, 245.0, 10.4], [0.0, 0.0, 0.10, 200.0, 11.0]]

test2 = [[0.0, 0.0, 0.0, 72.0, 12.9], [0.0, 0.0, 0.0, 80.0, 11.3]]

for i in range(len(test2)):
    for j in range(len(test1)):
        for k in range(len(test1[0])):
            euclidean += pow((test2[i][k]-test1[j][k]),2)

        euclidean_list.append(math.sqrt(euclidean))
        euclidean = 0

    euclidean_list_complete.append(euclidean_list)


print euclidean_list_complete
</code></pre>

<p>my problem with this code is it doesn't print the output i want properly. The output should be 
<code>[[80.0023, 173.018, 128.014], [72.006, 165.002, 120.000]]</code> </p>

<p>but instead, it prints</p>

<p><code>[[80.00232559119766, 173.01843095173416, 128.01413984400315, 72.00680592832875, 165.0028407300917, 120.00041666594329], [80.00232559119766, 173.01843095173416, 128.01413984400315, 72.00680592832875, 165.0028407300917, 120.00041666594329]]</code></p>

<p>I'm guessing it has something to do with the loop. What should I do to fix it? By the way, I don't want to use numpy or scipy for studying purposes</p>

<p>If it's unclear, I want to calculate the distance between lists on test2 to each lists on test1</p>
","8126411","8126411","2018-06-01 07:05:52","Computing euclidean distance with multiple list in python","<python><list><euclidean-distance>","4","6","1456"
"50639268","2018-06-01 08:34:51","0","","<p>Simply testing @meissner_ comment: you can easily replace the <code>d</code> inside your subcalsses by other ones.</p>

<pre><code>class BaseA():
    """"""Provides basic initialization for dicts used in A() and deriveds.""""""
    @classmethod
    def InitDefaults(cls):
        return { 1: ""this"", 2:""are"", 3:""some"", 4:""defaults""}

class A():
    d = BaseA.InitDefaults()

class B(A):
    d = BaseA.InitDefaults()

class C(A):
    d = BaseA.InitDefaults()


print(A.d)
print(B.d)
print(C.d)

print(""\n"")
B.d[99] = ""Not a default""
C.d[42] = ""Not THE answer""

print(A.d)
print(B.d)
print(C.d)
</code></pre>

<p>Output:</p>

<pre><code>{1: 'this', 2: 'are', 3: 'some', 4: 'defaults'}
{1: 'this', 2: 'are', 3: 'some', 4: 'defaults'}
{1: 'this', 2: 'are', 3: 'some', 4: 'defaults'}


{1: 'this', 2: 'are', 3: 'some', 4: 'defaults'}
{1: 'this', 2: 'are', 3: 'some', 4: 'defaults', 99: 'Not a default'}
{1: 'this', 2: 'are', 3: 'some', 4: 'defaults', 42: 'Not THE answer'}
</code></pre>

<p>This is kinda ugly but would work. You have a central place for code modifying the ""base-default"" and A,B,C get distinct dicts - initted the same - that can develop into different directions.</p>

<p>Probably better would be to use Instances and <strong>init</strong>() though ...</p>
","7505395","7505395","2018-06-01 09:15:43","6","1268","Patrick Artner","2017-02-02 10:46:51","30736","5120","3506","4713","50639137","50641763","2018-06-01 08:26:31","0","58","<p>I have the following situation in Python 3:</p>

<pre><code>class A:
    d = {}

class B(A):  pass
class C(A):  pass
</code></pre>

<p>I work with the classes only, no instances get created.  When I access <code>B.d</code> I will get a shared reference to <code>A.d</code>.  That's not what I want.  I would like to have each class which inherits <code>A</code> have its own <code>d</code> which is set to a dictionary.  <code>d</code> is an implementation detail to <code>A</code>.  All access to <code>d</code> is done in the code of <code>A</code>.  Just <code>B</code>'s <code>d</code> should not be identical to <code>A</code>'s <code>d</code>.</p>

<p>With instances, I would create that dictionary in the <code>__init__()</code> function.  But in my case I work with the classes only.</p>

<p>Is there a standard way to achieve this (EDIT: without changing the implementation of the subclasses <code>B</code> and <code>C</code>)?  Is there something analog to <code>__init__()</code> which gets called for (or in) each derived class at the time of deriving?</p>
","1281485","1281485","2018-06-01 09:40:09","Prevent sharing of class variables with base class in Python","<python><python-3.x><class><inheritance><class-variables>","2","4","1072"
"50639293","2018-06-01 08:36:07","2","","<p>Use below code for Python3. It is working fine.</p>

<pre><code>import shutil,os

def callback():
  src = askopenfilename()

  des = 'C://Users//priyanka.rani//.spyder-py3'
  shutil.copy(src, des)


errmsg = 'Error!'
Button(text='File Open', command=callback).pack(fill=X)# calling function to add in Tkinter GUI
mainloop()
</code></pre>
","9879710","","","1","341","Priyanka","2018-06-01 06:43:36","21","2","0","0","30704355","","2015-06-08 08:27:38","0","1231","<p>I'm new to python but I have to create a simple GUI application to copy files. This is what I tried. </p>

<pre><code>from Tkinter import *
from  tkFileDialog import askopenfilenames
import shutil,os,glob

def callback():
    src = askopenfilenames()
    des = ""C:\Users\Ravi\Desktop\des""
    sourceFiles = os.listdir(src)

    try:
        for fileName in sourceFiles:
            fullName = os.path.join(src, fileName)
            if (os.path.isfile(fullName)):
                shutil.copy(fullName, des)

    except Exception, e:
        print(""Error %s"" %e)

errmsg = 'Error!'
Button(text='File Open', command=callback).pack(fill=X)
mainloop()
</code></pre>

<p>When I run this code and trying to copy 1.txt file which is in src, I got following error . </p>

<pre><code>The directory name is invalid: u'C:/Users/Ravi/Desktop/1.txt\\*.*'
</code></pre>

<p>I tried this code also to copy files, but it gives no result.</p>

<pre><code>for files in glob.iglob(os.path.join(src, '*.*')):
        shutil.copy(files, des)
        print(""copied"")
</code></pre>

<p>I have no idea, how to configure this application. What I just need is, when I click a ""File Open"" button and select files and copy selected files in to the destination (<code>des</code>) which I have hard coded(<code>C:\Users\Ravi\Desktop\des</code>). Please help me to correct that code or find another solution for that matter.  </p>
","2679280","","","Python GUI application to copy files one location to another location","<python><python-2.7>","2","7","1403"
"50639294","2018-06-01 08:36:08","0","","<p>Not 100% sure what you are trying to achieve, but if you want to get all the combination of options, you should use <a href=""https://docs.python.org/3/library/itertools.html#itertools.product"" rel=""nofollow noreferrer""><code>itertools.product</code></a>:</p>

<pre><code>&gt;&gt;&gt; list(itertools.product(op1[""options""], op2[""options""], op3[""options""]))
[('Glass', 'Chrome', 'Red'),
 ('Glass', 'Chrome', 'Blue'),
 ('Glass', 'Chrome', 'Green'),
 ...
 ('Wood', 'Red Gold', 'Cyan'),
 ('Wood', 'Red Gold', 'SomeWonderfulNewColor')]
</code></pre>

<p>If you want to combine those with the respective <code>control</code>, you could write a helper function to get the pairs, then get the <code>product</code> of those:</p>

<pre><code>&gt;&gt;&gt; pairs = lambda op: [(op[""control""], o) for o in op[""options""]]
&gt;&gt;&gt; pairs(op1)
[('Material', 'Glass'), ('Material', 'Metal'), ('Material', 'Wood')]

&gt;&gt;&gt; list(itertools.product(*map(pairs, (op1, op2, op3))))
[(('Material', 'Glass'), ('Base', 'Chrome'), ('Color', 'Red')),
 (('Material', 'Glass'), ('Base', 'Chrome'), ('Color', 'Blue')),
 (('Material', 'Glass'), ('Base', 'Chrome'), ('Color', 'Green')),
 ...
 (('Material', 'Wood'), ('Base', 'Red Gold'), ('Color', 'Cyan')),
 (('Material', 'Wood'), ('Base', 'Red Gold'), ('Color', 'SomeWonderfulNewColor'))]
</code></pre>
","1639625","","","0","1334","tobias_k","2012-08-31 20:34:34","63214","4895","5994","655","50639090","","2018-06-01 08:23:26","0","53","<p>I have a problem with a problem, that got a bit problematic as problems go.</p>

<p>situation:</p>

<p>I need to combine items in list(s) of variable sizes, with variable sized elements, store those combinations and then iterate through them. I tried itertools, but I get too many combinations, which I have no idea how to properly ""clean"".
I get correct combinations by just creating as many for loops as the number of ""op"" elements in the input list is.</p>

<p>example:
NOTE: number of ""op"" dictionaries may vary! Ignore values as such, what matters is, I use list of ""op"" dictionaries to basically get all custom controls in Nuke GUI element called NoOp node. I need to iterate through each of the controls for each value, making all possible combinations:</p>

<pre><code>for option1 in op1[""options""]:
    for option2 in op2[""options""]:
        for option3 in op3[""options""]:
            print op1[""control""], option1, op2[""control""], option2,   op3[""control""], option3     
</code></pre>

<p>For now i am just trying to get my head around how to define the base case :/</p>

<pre><code>def getCombos(controls, n = 0):
        #combos = []
        if n == 0:
            #return [(control[""control""], option) for control in controls for option in control[""options""]]
            return [(item[""control""], option) for item in controls for option in item[""options""]]
        else:
            for control in controls:
                return(getCombos(controls, n-1))
                n -= 1


op1 = {""control"": ""Material"", ""options"": [""Glass"", ""Metal"", ""Wood""]}
op2 = {""control"": ""Base"",
       ""options"": [""Chrome"", ""Brass"", ""Bronce"", ""Gold"", ""Nickel"", ""Red Gold""]}
op3 = {""control"": ""Color"", ""options"": [""Red"", ""Blue"", ""Green"", ""Cyan"", ""SomeWonderfulNewColor""]}


controls = [op1, op2, op3]
#NOTE: number of elements (dict) in list controls may vary!

for i,combo in enumerate(getCombos(controls, n=len(controls))):
    print i, combo
</code></pre>

<p>ATM this script just recursively prints the controls</p>

<p>How do I use recursion for this case, and more importantly, should I use recursion at all, and if yes, how do I approach such a case and break it down to it's components?
Cheers,</p>
","7474696","","","Use recursion in python to combine lists of variable sizes","<python><loops><for-loop><recursion><iteration>","3","3","2205"
"50639339","2018-06-01 08:38:44","0","","<p>Absolutely you should use recursion:</p>

<pre><code>def print_all(controls, idx, combination):
    if idx == len(controls):
        print(combination)
        return

    for x in controls[idx]['options']:
        print_all(controls, idx+1, combination + "" "" + controls[idx]['control'] + "" "" + str(x))


op1 = {""control"": ""Material"", ""options"": [""Glass"", ""Metal"", ""Wood""]}
op2 = {""control"": ""Base"",
   ""options"": [""Chrome"", ""Brass"", ""Bronce"", ""Gold"", ""Nickel"", ""Red Gold""]}
op3 = {""control"": ""Color"", ""options"": [""Red"", ""Blue"", ""Green"", ""Cyan"", ""SomeWonderfulNewColor""]}
op4 = {""control"": ""year"", ""options"": [2010, 2020]}

controls = [op1, op2, op3, op4]

print_all(controls, 0, """")
</code></pre>
","4428342","","","10","701","MeHdi","2015-01-07 10:15:18","385","61","55","1","50639090","","2018-06-01 08:23:26","0","53","<p>I have a problem with a problem, that got a bit problematic as problems go.</p>

<p>situation:</p>

<p>I need to combine items in list(s) of variable sizes, with variable sized elements, store those combinations and then iterate through them. I tried itertools, but I get too many combinations, which I have no idea how to properly ""clean"".
I get correct combinations by just creating as many for loops as the number of ""op"" elements in the input list is.</p>

<p>example:
NOTE: number of ""op"" dictionaries may vary! Ignore values as such, what matters is, I use list of ""op"" dictionaries to basically get all custom controls in Nuke GUI element called NoOp node. I need to iterate through each of the controls for each value, making all possible combinations:</p>

<pre><code>for option1 in op1[""options""]:
    for option2 in op2[""options""]:
        for option3 in op3[""options""]:
            print op1[""control""], option1, op2[""control""], option2,   op3[""control""], option3     
</code></pre>

<p>For now i am just trying to get my head around how to define the base case :/</p>

<pre><code>def getCombos(controls, n = 0):
        #combos = []
        if n == 0:
            #return [(control[""control""], option) for control in controls for option in control[""options""]]
            return [(item[""control""], option) for item in controls for option in item[""options""]]
        else:
            for control in controls:
                return(getCombos(controls, n-1))
                n -= 1


op1 = {""control"": ""Material"", ""options"": [""Glass"", ""Metal"", ""Wood""]}
op2 = {""control"": ""Base"",
       ""options"": [""Chrome"", ""Brass"", ""Bronce"", ""Gold"", ""Nickel"", ""Red Gold""]}
op3 = {""control"": ""Color"", ""options"": [""Red"", ""Blue"", ""Green"", ""Cyan"", ""SomeWonderfulNewColor""]}


controls = [op1, op2, op3]
#NOTE: number of elements (dict) in list controls may vary!

for i,combo in enumerate(getCombos(controls, n=len(controls))):
    print i, combo
</code></pre>

<p>ATM this script just recursively prints the controls</p>

<p>How do I use recursion for this case, and more importantly, should I use recursion at all, and if yes, how do I approach such a case and break it down to it's components?
Cheers,</p>
","7474696","","","Use recursion in python to combine lists of variable sizes","<python><loops><for-loop><recursion><iteration>","3","3","2205"
"50639340","2018-06-01 08:38:45","66","","<p>This did it for me:</p>

<pre><code>python -m pip install --upgrade pip
</code></pre>

<p>Environment: OSX &amp;&amp; Python installed via <a href=""https://docs.brew.sh/Homebrew-and-Python"" rel=""noreferrer"">brew</a></p>
","109304","3924118","2019-03-02 20:14:06","2","223","magicrebirth","2009-05-19 11:15:48","1996","295","255","3","49940813","","2018-04-20 11:39:08","69","63576","<p>I have a problem when I try to use pip in any way. I'm using Ubuntu 16.04.4</p>

<p>I should say that I've used it already, and I never had any problem, but starting today when I use any command I always get the same error (as an example using <code>pip --upgrade</code>).</p>

<pre><code>Traceback (most recent call last):
  File ""/usr/local/bin/pip"", line 7, in &lt;module&gt;
    from pip._internal import main
ImportError: No module named _internal
</code></pre>

<p>I have tried doing <code>sudo apt-get remove python-pip</code> followed by <code>sudo apt-get install python-pip</code> but nothing changed.</p>

<p>Thank you for your time!</p>
","7196040","1848654","2018-04-20 11:43:05","pip: no module named _internal","<python><pip>","22","5","652"
"50639355","2018-06-01 08:39:38","3","","<p>As suggested in the comments you should read the file line by line and not the entire file.</p>

<p>For example :</p>

<pre><code>count = 0
with open('words.txt','r') as f:
    for line in f:
        for word in line.split():
          if(1 &lt;= len(word) &lt;=5):
              count=count+1
print(count)
</code></pre>

<p><strong>EDIT :</strong></p>

<p>If you only want to count the words in 14-th colomun and split by ""|"" instead then :</p>

<pre><code>count = 0
with open('words.txt','r') as f:
    for line in f:
        iterator = 0
        for word in line.split(""|""):
            if(1 &lt;= len(word) &lt;=5 and iterator == 13):
                count=count+1
            iterator = iterator +1
print(count)
</code></pre>

<p>note that you should avoid to write this </p>

<pre><code>arr = line.split(""|"")
word = arr[13]
</code></pre>

<p>since the line may contains less than 14 words, which can result in a segmentation error.</p>
","4855828","4855828","2018-06-01 09:06:31","4","945","Cryckx","2015-05-02 00:15:32","187","16","98","3","50639222","","2018-06-01 08:31:47","1","126","<p>I am trying to count number of words that has length between 1 and 5, file size is around 4GB end I am getting memory error.</p>

<pre><code>import os 
files = os.listdir('C:/Users/rram/Desktop/') 
for file_name in files:     
    file_path = ""C:/Users/rram/Desktop/""+file_name     
    f = open (file_path, 'r')    
    text = f.readlines()
    update_text = '' 
    wordcount = {}
    for line in text:         
        arr = line.split(""|"")
        word = arr[13]
        if 1&lt;=len(word)&lt;6:
            if word not in wordcount:
                wordcount[word] = 1
        else:
            wordcount[word] += 1
            update_text+= '|'.join(arr)
print (wordcount)     #print update_text
print 'closing', file_path, '\t', 'total files' , '\n\n'
f.close()
</code></pre>

<p>At the end i get a <code>MemoryError</code> on this line <code>text = f.readlines()</code></p>

<p>Can you pelase help to optimize it.</p>
","9840260","8708364","2018-06-01 09:44:36","Memory Error while running python script on 4GB file","<python><python-2.7>","1","5","929"
"50639386","2018-06-01 08:41:09","0","","<p>The question has partly been answered by @Evgeny. The answer the OP posted to his own question is an example how to not write Python code. Here is a shorter, faster and more readable solution, given <code>test1</code> and <code>test2</code> are lists like in the question:</p>

<pre><code>def euclidean(v1, v2):
    return sum((p-q)**2 for p, q in zip(v1, v2)) ** .5

d2 = []
for i in test2:
    foo = [euclidean(i, j) for j in test1]
    d2.append(foo)


print(d2)
#[[80.00232559119766, 173.01843095173416, 128.01413984400315],
# [72.00680592832875, 165.0028407300917, 120.00041666594329]]
</code></pre>
","1448641","","","3","608","MaxPowers","2012-06-11 09:51:06","2706","275","263","369","50637446","50639386","2018-06-01 06:37:49","0","3288","<p>I'm writing a simple program to compute the euclidean distances between multiple lists using python. This is the code I have so fat</p>

<pre><code>import math
euclidean = 0
euclidean_list = []
euclidean_list_complete = []

test1 = [[0.0, 0.0, 0.0, 152.0, 12.29], [0.0, 0.0, 0.357, 245.0, 10.4], [0.0, 0.0, 0.10, 200.0, 11.0]]

test2 = [[0.0, 0.0, 0.0, 72.0, 12.9], [0.0, 0.0, 0.0, 80.0, 11.3]]

for i in range(len(test2)):
    for j in range(len(test1)):
        for k in range(len(test1[0])):
            euclidean += pow((test2[i][k]-test1[j][k]),2)

        euclidean_list.append(math.sqrt(euclidean))
        euclidean = 0

    euclidean_list_complete.append(euclidean_list)


print euclidean_list_complete
</code></pre>

<p>my problem with this code is it doesn't print the output i want properly. The output should be 
<code>[[80.0023, 173.018, 128.014], [72.006, 165.002, 120.000]]</code> </p>

<p>but instead, it prints</p>

<p><code>[[80.00232559119766, 173.01843095173416, 128.01413984400315, 72.00680592832875, 165.0028407300917, 120.00041666594329], [80.00232559119766, 173.01843095173416, 128.01413984400315, 72.00680592832875, 165.0028407300917, 120.00041666594329]]</code></p>

<p>I'm guessing it has something to do with the loop. What should I do to fix it? By the way, I don't want to use numpy or scipy for studying purposes</p>

<p>If it's unclear, I want to calculate the distance between lists on test2 to each lists on test1</p>
","8126411","8126411","2018-06-01 07:05:52","Computing euclidean distance with multiple list in python","<python><list><euclidean-distance>","4","6","1456"
"50639388","2018-06-01 08:41:23","0","","<p>Here is a way to print all combinations in three steps:</p>

<ol>
<li>Go from <code>{control: control_val, options: options_vals}</code> to <code>{control_val: options_vals}</code>. This is done by the <code>from_record</code> function.</li>
<li>Transform <code>{control_val: options_vals}</code> to a list of <code>[(control_val, options_val)]</code>. This is done by <code>iter_dict</code>.</li>
<li>Take a product of such lists across all different op's.</li>
<li>Format and print. 3 and 4 are handled by <code>print_combinations</code>.</li>
</ol>

<hr>

<pre><code>from itertools import product


def from_record(dct):
    return {dct[""control""]: dct[""options""]}


def iter_dict(dct):
    yield from ((k, v) for k, vs in dct.items() for v in vs)


def print_combinations(dcts):
    for item in product(*(iter_dict(from_record(dct)) for dct in dcts)):
        print("", "".join([""{}: {}"".format(*t) for t in item]))


op1 = {""control"": ""Material"", ""options"": [""Glass"", ""Metal"", ""Wood""]}
op2 = {
    ""control"": ""Base"",
    ""options"": [""Chrome"", ""Brass"", ""Bronce"", ""Gold"", ""Nickel"", ""Red Gold""],
}
op3 = {
    ""control"": ""Color"",
    ""options"": [""Red"", ""Blue"", ""Green"", ""Cyan"", ""SomeWonderfulNewColor""],
}

print_combinations([op1, op2, op3])

# e.g.
# Material: Glass, Base: Chrome, Color: Red
# Material: Glass, Base: Chrome, Color: Blue
# Material: Glass, Base: Chrome, Color: Green
# Material: Glass, Base: Chrome, Color: Cyan
# Material: Glass, Base: Chrome, Color: SomeWonderfulNewColor
</code></pre>

<hr>

<p>Alternatively, all possible combinations may be listed more compactly by combining different op dicts:</p>

<pre><code>res = dict()
ops = [op1, op2, op3]
for op in ops:
    res.update(from_record(op))


# {'Base': ['Chrome', 'Brass', 'Bronce', 'Gold', 'Nickel', 'Red Gold'],
 # 'Color': ['Red', 'Blue', 'Green', 'Cyan', 'SomeWonderfulNewColor'],
 # 'Material': ['Glass', 'Metal', 'Wood']}
</code></pre>
","4585963","4585963","2018-06-01 08:56:46","1","1923","hilberts_drinking_problem","2015-02-19 22:15:40","7219","620","1491","135","50639090","","2018-06-01 08:23:26","0","53","<p>I have a problem with a problem, that got a bit problematic as problems go.</p>

<p>situation:</p>

<p>I need to combine items in list(s) of variable sizes, with variable sized elements, store those combinations and then iterate through them. I tried itertools, but I get too many combinations, which I have no idea how to properly ""clean"".
I get correct combinations by just creating as many for loops as the number of ""op"" elements in the input list is.</p>

<p>example:
NOTE: number of ""op"" dictionaries may vary! Ignore values as such, what matters is, I use list of ""op"" dictionaries to basically get all custom controls in Nuke GUI element called NoOp node. I need to iterate through each of the controls for each value, making all possible combinations:</p>

<pre><code>for option1 in op1[""options""]:
    for option2 in op2[""options""]:
        for option3 in op3[""options""]:
            print op1[""control""], option1, op2[""control""], option2,   op3[""control""], option3     
</code></pre>

<p>For now i am just trying to get my head around how to define the base case :/</p>

<pre><code>def getCombos(controls, n = 0):
        #combos = []
        if n == 0:
            #return [(control[""control""], option) for control in controls for option in control[""options""]]
            return [(item[""control""], option) for item in controls for option in item[""options""]]
        else:
            for control in controls:
                return(getCombos(controls, n-1))
                n -= 1


op1 = {""control"": ""Material"", ""options"": [""Glass"", ""Metal"", ""Wood""]}
op2 = {""control"": ""Base"",
       ""options"": [""Chrome"", ""Brass"", ""Bronce"", ""Gold"", ""Nickel"", ""Red Gold""]}
op3 = {""control"": ""Color"", ""options"": [""Red"", ""Blue"", ""Green"", ""Cyan"", ""SomeWonderfulNewColor""]}


controls = [op1, op2, op3]
#NOTE: number of elements (dict) in list controls may vary!

for i,combo in enumerate(getCombos(controls, n=len(controls))):
    print i, combo
</code></pre>

<p>ATM this script just recursively prints the controls</p>

<p>How do I use recursion for this case, and more importantly, should I use recursion at all, and if yes, how do I approach such a case and break it down to it's components?
Cheers,</p>
","7474696","","","Use recursion in python to combine lists of variable sizes","<python><loops><for-loop><recursion><iteration>","3","3","2205"
"50639394","2018-06-01 08:41:36","2","","<p><code>table = soup.findAll(""div"", {""class"" : ""iw_component"",""id"":""c1417094965154""})</code></p>

<p>In the above line, <code>findAll()</code> returns a list. 
So, in the next line you are getting the error because its expecting an HTML string. </p>

<p>If you expect only one table, try using the following code. Just replace </p>

<p><code>rows = table.find_all('span',recursive=False)</code></p>

<p>with</p>

<p><code>rows = table[0].find_all('span')</code></p>

<p>If you expect multiple tables in the page, run a for loop on the table and then run the rest of the statements inside the for loop.</p>

<p>Also, for pretty output, you can replace the <code>tabs</code> with spaces as in the following code:</p>

<pre><code>row = row.get_text()
row = row.replace('\t', '')
print(row)
</code></pre>

<p>The final working code for you is:</p>

<pre><code>from bs4 import BeautifulSoup
import urllib2

url1 = ""url""

content1 = urllib2.urlopen(url1).read()
soup = BeautifulSoup(content1,""lxml"")
table = soup.findAll(""div"", {""class"" : ""iw_component"",""id"":""c1417094965154""})
rows = table[0].find_all('span')
for row in rows:
    row_str = row.get_text()
    row_str = row_str.replace('\t', '')
    print(row_str)
</code></pre>

<p>Regarding <code>recursive=False</code> parameter, if it's set to false, it will only find in direct children which, in your case will give no result. </p>

<p><a href=""https://www.crummy.com/software/BeautifulSoup/bs4/doc/#the-recursive-argument"" rel=""nofollow noreferrer"">Recursive Argument in find()</a></p>

<blockquote>
  <p>If you only want Beautiful Soup to consider direct children, you can pass in <code>recursive=False</code></p>
</blockquote>
","2256258","9846358","2018-08-09 08:24:31","2","1682","amulya349","2013-04-08 05:41:34","793","99","110","7","50638894","50639394","2018-06-01 08:10:01","2","94","<p>cannot get the span text within the ""table"", thanks !</p>

<pre><code>from bs4 import BeautifulSoup
import urllib2

url1 = ""url""

content1 = urllib2.urlopen(url1).read()
soup = BeautifulSoup(content1,""lxml"")
table = soup.findAll(""div"", {""class"" : ""iw_component"",""id"":""c1417094965154""})
rows = table.find_all('span',recursive=False)
for row in rows:
    print(row.text)
</code></pre>
","9846358","9846358","2018-08-08 07:38:17","cannot get the <span></span> texts","<python><beautifulsoup>","3","0","386"
"50639411","2018-06-01 08:43:05","1","","<p>Try to change city value by latitude and longitude and it's not necessary to put the keyword because you are specified that on Type try to put this code it's work for me :</p>

<pre><code>query_result = google_places.nearby_search(
        lat_lng={'lat': 46.1667, 'lng': -1.15}, 
        radius=5000,
        types=[types.TYPE_RESTAURANT] or [types.TYPE_CAFE] or [type.TYPE_BAR] or [type.TYPE_CASINO])
</code></pre>
","6325966","","","0","420","Mohammed Rasfa","2016-05-12 14:01:45","65","38","2","0","39124510","","2016-08-24 13:19:39","3","7879","<p>I want to get all restaurants in London by using python 3.5 and the module <code>googleplaces</code> with the Google Places API. I read the <code>googleplaces</code> documentation and searched here, but I don't get it. Here is my code so far:</p>

<pre><code>from googleplaces import GooglePlaces, types, lang

API_KEY = 'XXXCODEXXX'

google_places = GooglePlaces(API_KEY)

query_result = google_places.nearby_search(
    location='London', keyword='Restaurants',
    radius=1000, types=[types.TYPE_RESTAURANT])

if query_result.has_attributions:
print query_result.html_attributions


for place in query_result.places:
    place.get_details()
    print place.rating 
</code></pre>

<p>The code doesn't work. What can I do to get a list with all restaurants in this area?</p>
","6614309","3552063","2017-04-29 23:30:33","Python & Google Places API | Want to get all Restaurants at a specific postion","<python><python-3.x><google-places-api><google-places>","2","5","779"
"50639429","2018-06-01 08:43:53","0","","<p>I found the bug. The problem is that spark will simply append the dataframes. it will not append by using columns names. </p>

<p>If you are using union then you should make sure the columns in the dataframe appear in same order because the appending appears to be happening in the order they appear. </p>

<p>In this example, I have reversed the order of columns and the values in the second dataframe(<code>df_2</code>) and then took a union. </p>

<pre><code>&gt;&gt;&gt; df_1 = spark.createDataFrame([['a',1]], ['col_1', 'col_2'])
&gt;&gt;&gt; df_2 = spark.createDataFrame([[2,'b']], ['col_2', 'col_1'])
&gt;&gt;&gt; df_3 = unionAll(*[df_1, df_2])
&gt;&gt;&gt; df_3
DataFrame[col_1: string, col_2: string]
&gt;&gt;&gt; df_3.show()
+-----+-----+
|col_1|col_2|
+-----+-----+
|    a|    1|
|    2|    b|
+-----+-----+
</code></pre>

<p>And now when I use the correct order, I get the expected output</p>

<pre><code>&gt;&gt;&gt; df_3 = unionAll(*[df_1.select(*['col_1', 'col_2']), df_2.select(*['col_1', 'col_2'])])
&gt;&gt;&gt; df_3.show()
+-----+-----+                                                                   
|col_1|col_2|
+-----+-----+
|    a|    1|
|    b|    2|
+-----+-----+
</code></pre>
","2324298","2324298","2018-06-01 08:49:27","0","1210","Clock Slave","2013-04-26 14:40:49","3113","643","1885","88","50639233","50639429","2018-06-01 08:32:35","1","704","<p>I have three dataframes stored in a list <code>data_multi</code>. Each of these dataframes have the same column names and also same dtypes</p>

<p><em>columns</em></p>

<pre><code>&gt;&gt;&gt; set(data_multi[0].columns) == set(data_multi[1].columns)  == set(data_multi[2].columns)
True
</code></pre>

<p><em>dtypes (showing only one column)</em></p>

<pre><code>&gt;&gt;&gt; data_multi[0].select('aml_id_key_12739').dtypes
[('aml_id_key_12739', 'bigint')]

&gt;&gt;&gt; data_multi[1].select('aml_id_key_12739').dtypes
[('aml_id_key_12739', 'bigint')]

&gt;&gt;&gt; data_multi[2].select('aml_id_key_12739').dtypes
[('aml_id_key_12739', 'bigint')]
</code></pre>

<p>I copied a function mentioned from a post on SO that unions(rbinds) all dataframes </p>

<pre><code>def unionAll(*dfs):
    return reduce(DataFrame.unionAll, dfs)
</code></pre>

<p>Using this, I union the three dataframes to get a single one
data_single = unionAll(*ddata_multi)</p>

<p>This is where I am facing a problem. The <code>aml_id_key_12739</code> column in each dataframe is a <code>'bigint'</code> but right after the union it becomes a <code>'double'</code></p>

<pre><code>&gt;&gt;&gt; pprint(data_single.select('aml_id_key_12739').dtypes)
[('aml_id_key_12739', 'double')]
</code></pre>

<p>Because of this my whole id column is messed up. What am I overlooking? </p>
","2324298","","","PySpark: column dtype changes in performing union","<python><apache-spark><pyspark>","1","2","1349"
"50639432","2018-06-01 08:43:59","1","","<p>Here is a simple solution using <code>tf.sequence_mask</code>:</p>

<pre><code>import tensorflow as tf

v = tf.constant([0.5, 0.1, 0.7])
w = tf.constant([2, 3, 0])

m = tf.sequence_mask(w)
v2 = tf.tile(v[:, None], [1, tf.shape(m)[1]])
res = tf.boolean_mask(v2, m)

sess = tf.InteractiveSession()
print(res.eval())
# array([0.5, 0.5, 0.1, 0.1, 0.1], dtype=float32)
</code></pre>
","1735003","","","2","381","P-Gn","2012-10-10 13:22:24","14425","1153","273","168","50637921","50639432","2018-06-01 07:09:38","0","75","<p>I have the following task: having two vectors
<code>[v_1, ..., v_n]</code> and <code>[w_1, ..., w_n]</code> build new vector <code>[v_1] * w_1 + ... + [v_n] * w_n</code>. </p>

<p>For exmaple for <code>v = [0.5, 0.1, 0.7]</code> and <code>w = [2, 3, 0]</code> the result will be </p>

<p><code>[0.5, 0.5, 0.1, 0.1, 0.1]</code>.</p>

<p>In case of using vanilla python, the solution would be</p>

<pre><code>v, w = [...], [...]
res = []
for i in range(len(v)):
    res += [v[i]] * w[i]
</code></pre>

<p>Is it possible to build such code within TensorFlow function? It seems to be an extension of <a href=""https://www.tensorflow.org/api_docs/python/tf/boolean_mask"" rel=""nofollow noreferrer"">tf.boolean_mask</a> with additional argument like <code>weights</code> or <code>repeats</code>.</p>
","7473896","","","Weighted masking in TensorFlow","<python><tensorflow>","1","0","794"
"50639442","2018-06-01 08:44:51","1","","<p>When running in a docker container, this fixed it for me (on the project django-postgrespool, but this should also work here).</p>

<pre><code># Set the locale
RUN sed -i -e 's/# en_US.UTF-8 UTF-8/en_US.UTF-8 UTF-8/' /etc/locale.gen &amp;&amp; \
    locale-gen
ENV LANG en_US.UTF-8  
ENV LANGUAGE en_US:en  
ENV LC_ALL en_US.UTF-8   
</code></pre>

<p>see <a href=""https://stackoverflow.com/a/28406007/1876203"">https://stackoverflow.com/a/28406007/1876203</a></p>
","1876203","","","1","467","Jan DB","2012-12-04 15:35:40","117","12","24","0","26473681","26474062","2014-10-20 19:46:18","66","62301","<p>I have a freshly installed Ubuntu on a freshly built computer. I just installed python-pip using apt-get. Now when I try to pip install Numpy and Pandas, it gives the following error.</p>

<p>I've seen this error mentioned in quite a few places on SO and Google, but I haven't been able to find a solution. Some people mention it's a bug, some threads are just dead... What's going on?</p>

<pre><code>Traceback (most recent call last):
  File ""/usr/bin/pip"", line 9, in &lt;module&gt;
    load_entry_point('pip==1.5.4', 'console_scripts', 'pip')()
  File ""/usr/lib/python2.7/dist-packages/pip/__init__.py"", line 185, in main
    return command.main(cmd_args)
  File ""/usr/lib/python2.7/dist-packages/pip/basecommand.py"", line 161, in main
    text = '\n'.join(complete_log)
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 72: ordinal not in range(128)
</code></pre>
","3884713","","","PIP Install Numpy throws an error ""ascii codec can't decode byte 0xe2""","<python><numpy><pandas><pip>","18","4","891"
"50639456","2018-06-01 08:45:49","1","","<p><code>row</code> contains a newline, so it's not empty. But <code>row.split()</code> doesn't find any non-whitespace characters, so it returns an empty list.</p>

<p>Use</p>

<pre><code>if len(row.strip()):
</code></pre>

<p>to ignore the newline (and any other leading/trailing spaces).</p>

<p>Or more simply:</p>

<pre><code>if row.strip():
</code></pre>

<p>since an empty string is falsy.</p>
","1491895","1491895","2018-06-01 08:47:35","2","401","Barmar","2012-06-29 18:12:29","477375","68451","6422","3351","50639390","50639456","2018-06-01 08:41:29","0","59","<p>I am trying to write a music program in Python that takes some music written by the user in a text file and turns it into a midi. I'm not particularly experienced with python at this stage so I'm not sure what the reason behind this issue is. I am trying to write the source file parser for the program and part of this process is to create a list containing all the lines of the text file and breaking each line down into its own list to make them easier to work with. I'm successfully able to do that, but there is a problem.</p>

<p>I want the code to <strong>ignore</strong> lines that are only whitespace (So the user can make their file at least kind of readable without having all the lines thrown together one on top of the other), but I can't seem to figure out how to do that. I tried doing this</p>

<pre><code>with open(path, ""r"") as infile:
    for row in infile:
        if len(row):
            srclines.append(row.split())
</code></pre>

<p>And this <strong>does</strong> work as far as creating the list of lines and separating each word goes, BUT it still appends the empty lines that are only whitespace... I confirmed this by doing this</p>

<pre><code>for entry in srclines:
    print entry
</code></pre>

<p>Which gives, for example</p>

<pre><code>['This', 'is']
[]
['A', 'test']
</code></pre>

<p>With the original text being</p>

<pre><code>This is

A test
</code></pre>

<p>But strangely, if during the printing stage I do another len() check then the empty lines are actually <strong>ignored</strong> like I want, and it looks like this</p>

<pre><code>['This', 'is']
['A', 'test']
</code></pre>

<p>What is the cause of this? Does this mean I can only go over the list and remove empty entries after I generate It? Or am I just doing the line import code wrong? I am using python 3.6 to test this code with by the way</p>
","6283375","","","Python: if len(list) giving different results in different circumstances?","<python><list><printing>","3","1","1853"
"50639467","2018-06-01 08:46:14","1","","<p>Try this code out. It approximates the parabola through trial and error. </p>

<pre><code>import matplotlib.pyplot as plt
import numpy as np
s = list([0,1,2,3,4,5,6,7,8])
k = list([0,1,4,9,16,25,36,49,64]) #Insert Data Here(positive side only)

def parab(x):
    for f in range(0,len(s)): 
        v = f**2/(x/2)
        yield v

def parab2():
    t = 201
    for i in range(0,200):
        g = 0
        t-=1
        foo = np.fromiter(parab(t), dtype = 'int')
        for bar in range(0,len(s)):
            if foo[bar] &lt; k[bar]+0.5 and foo[bar] &gt; k[bar]-0.5:
                g += 1
        if g &gt;= len(s)/2:
            print(foo)
            print(""x**2/"",t/2)

parab2()
</code></pre>

<p>Try it. If it approximates to roughly for you try fiddling around with what x^2 is divided by.</p>

<p>Also, as your dataset size increases, try to increase the value in the line
<code>if g&gt;=4:</code></p>

<p>You can decide on which size works best for you.</p>

<p><strong>EDIT</strong> Updated code: this works better.</p>
","9143011","9143011","2018-06-01 09:35:21","0","1032","3141","2017-12-26 21:49:58","361","133","91","30","50638823","50694476","2018-06-01 08:05:38","3","203","<p>I have the x and y coordinates of the following graph as two different lists s and k. If I want to determine the equation of the parabola that best encloses these points, how do I do it?</p>

<p><a href=""https://i.stack.imgur.com/e41Ix.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e41Ix.png"" alt=""enter image description here""></a></p>

<p>Thanks!</p>

<p>I am not sure how to link to an external data set.</p>

<p><a href=""https://meta.stackoverflow.com/questions/250975/what-is-the-best-way-to-add-your-dataset-to-a-question"">What is the best way to add your dataset to a question?</a></p>

<p>This was the best I could find. So, I am putting up a sample data set here. I will be removing this later, though. </p>

<pre><code>                K             S
0    9.500000e-01 -6.500000e-01
1    8.500000e-01 -6.000000e-01
2    9.000000e-01 -6.000000e-01
3    9.500000e-01 -6.000000e-01
4    7.000000e-01 -5.500000e-01
5    7.500000e-01 -5.500000e-01
6    8.000000e-01 -5.500000e-01
7    8.500000e-01 -5.500000e-01
8    9.000000e-01 -5.500000e-01
9    9.500000e-01 -5.500000e-01
10   6.000000e-01 -5.000000e-01
11   6.500000e-01 -5.000000e-01
12   7.000000e-01 -5.000000e-01
13   7.500000e-01 -5.000000e-01
14   8.000000e-01 -5.000000e-01
15   8.500000e-01 -5.000000e-01
16   9.000000e-01 -5.000000e-01
17   9.500000e-01 -5.000000e-01
18   5.000000e-01 -4.500000e-01
19   5.500000e-01 -4.500000e-01
20   6.000000e-01 -4.500000e-01
21   6.500000e-01 -4.500000e-01
22   7.000000e-01 -4.500000e-01
23   7.500000e-01 -4.500000e-01
24   8.000000e-01 -4.500000e-01
25   8.500000e-01 -4.500000e-01
26   9.000000e-01 -4.500000e-01
27   9.500000e-01 -4.500000e-01
28   4.000000e-01 -4.000000e-01
29   4.500000e-01 -4.000000e-01
30   5.000000e-01 -4.000000e-01
31   5.500000e-01 -4.000000e-01
32   6.000000e-01 -4.000000e-01
33   6.500000e-01 -4.000000e-01
34   7.000000e-01 -4.000000e-01
35   7.500000e-01 -4.000000e-01
36   8.000000e-01 -4.000000e-01
37   8.500000e-01 -4.000000e-01
38   9.000000e-01 -4.000000e-01
39   9.500000e-01 -4.000000e-01
40   3.000000e-01 -3.500000e-01
41   3.500000e-01 -3.500000e-01
42   4.000000e-01 -3.500000e-01
43   4.500000e-01 -3.500000e-01
44   5.000000e-01 -3.500000e-01
45   5.500000e-01 -3.500000e-01
46   6.000000e-01 -3.500000e-01
47   6.500000e-01 -3.500000e-01
48   7.000000e-01 -3.500000e-01
49   7.500000e-01 -3.500000e-01
50   8.000000e-01 -3.500000e-01
51   8.500000e-01 -3.500000e-01
52   9.000000e-01 -3.500000e-01
53   9.500000e-01 -3.500000e-01
54   2.000000e-01 -3.000000e-01
55   2.500000e-01 -3.000000e-01
56   3.000000e-01 -3.000000e-01
57   3.500000e-01 -3.000000e-01
58   4.000000e-01 -3.000000e-01
59   4.500000e-01 -3.000000e-01
60   5.000000e-01 -3.000000e-01
61   5.500000e-01 -3.000000e-01
62   6.000000e-01 -3.000000e-01
63   6.500000e-01 -3.000000e-01
64   7.000000e-01 -3.000000e-01
65   7.500000e-01 -3.000000e-01
66   8.000000e-01 -3.000000e-01
67   8.500000e-01 -3.000000e-01
68   9.000000e-01 -3.000000e-01
69   9.500000e-01 -3.000000e-01
70   1.000000e-01 -2.500000e-01
71   1.500000e-01 -2.500000e-01
72   2.000000e-01 -2.500000e-01
73   2.500000e-01 -2.500000e-01
74   3.000000e-01 -2.500000e-01
75   3.500000e-01 -2.500000e-01
76   4.000000e-01 -2.500000e-01
77   4.500000e-01 -2.500000e-01
78   5.000000e-01 -2.500000e-01
79   5.500000e-01 -2.500000e-01
80   6.000000e-01 -2.500000e-01
81   6.500000e-01 -2.500000e-01
82   7.000000e-01 -2.500000e-01
83   7.500000e-01 -2.500000e-01
84   8.000000e-01 -2.500000e-01
85   8.500000e-01 -2.500000e-01
86   9.000000e-01 -2.500000e-01
87   9.500000e-01 -2.500000e-01
88   5.000000e-02 -2.000000e-01
89   1.000000e-01 -2.000000e-01
90   1.500000e-01 -2.000000e-01
91   2.000000e-01 -2.000000e-01
92   2.500000e-01 -2.000000e-01
93   3.000000e-01 -2.000000e-01
94   3.500000e-01 -2.000000e-01
95   4.000000e-01 -2.000000e-01
96   4.500000e-01 -2.000000e-01
97   5.000000e-01 -2.000000e-01
98   5.500000e-01 -2.000000e-01
99   6.000000e-01 -2.000000e-01
100  6.500000e-01 -2.000000e-01
101  7.000000e-01 -2.000000e-01
102  7.500000e-01 -2.000000e-01
103  8.000000e-01 -2.000000e-01
104  8.500000e-01 -2.000000e-01
105  9.000000e-01 -2.000000e-01
106  9.500000e-01 -2.000000e-01
107  8.881784e-16 -1.500000e-01
108  5.000000e-02 -1.500000e-01
109  1.000000e-01 -1.500000e-01
110  1.500000e-01 -1.500000e-01
111  2.000000e-01 -1.500000e-01
112  2.500000e-01 -1.500000e-01
113  3.000000e-01 -1.500000e-01
114  3.500000e-01 -1.500000e-01
115  4.000000e-01 -1.500000e-01
116  4.500000e-01 -1.500000e-01
117  5.000000e-01 -1.500000e-01
118  5.500000e-01 -1.500000e-01
119  6.000000e-01 -1.500000e-01
120  6.500000e-01 -1.500000e-01
121  7.000000e-01 -1.500000e-01
122  7.500000e-01 -1.500000e-01
123  8.000000e-01 -1.500000e-01
124  8.500000e-01 -1.500000e-01
125  9.000000e-01 -1.500000e-01
126  9.500000e-01 -1.500000e-01
127 -5.000000e-02 -1.000000e-01
128  8.881784e-16 -1.000000e-01
129  5.000000e-02 -1.000000e-01
130  1.000000e-01 -1.000000e-01
131  1.500000e-01 -1.000000e-01
132  2.000000e-01 -1.000000e-01
133  2.500000e-01 -1.000000e-01
134  3.000000e-01 -1.000000e-01
135  3.500000e-01 -1.000000e-01
136  4.000000e-01 -1.000000e-01
137  4.500000e-01 -1.000000e-01
138  5.000000e-01 -1.000000e-01
139  5.500000e-01 -1.000000e-01
140  6.000000e-01 -1.000000e-01
141  6.500000e-01 -1.000000e-01
142  7.000000e-01 -1.000000e-01
143  7.500000e-01 -1.000000e-01
144  8.000000e-01 -1.000000e-01
145  8.500000e-01 -1.000000e-01
146  9.000000e-01 -1.000000e-01
147  9.500000e-01 -1.000000e-01
148 -1.000000e-01 -5.000000e-02
149 -5.000000e-02 -5.000000e-02
150  8.881784e-16 -5.000000e-02
151  5.000000e-02 -5.000000e-02
152  1.000000e-01 -5.000000e-02
153  1.500000e-01 -5.000000e-02
154  2.000000e-01 -5.000000e-02
155  2.500000e-01 -5.000000e-02
156  3.000000e-01 -5.000000e-02
157  3.500000e-01 -5.000000e-02
158  4.000000e-01 -5.000000e-02
159  4.500000e-01 -5.000000e-02
160  5.000000e-01 -5.000000e-02
161  5.500000e-01 -5.000000e-02
162  6.000000e-01 -5.000000e-02
163  6.500000e-01 -5.000000e-02
164  7.000000e-01 -5.000000e-02
165  7.500000e-01 -5.000000e-02
166  8.000000e-01 -5.000000e-02
167  8.500000e-01 -5.000000e-02
168  9.000000e-01 -5.000000e-02
169  9.500000e-01 -5.000000e-02
170 -1.500000e-01  8.881784e-16
171 -1.000000e-01  8.881784e-16
172 -5.000000e-02  8.881784e-16
173  8.881784e-16  8.881784e-16
174  5.000000e-02  8.881784e-16
175  1.000000e-01  8.881784e-16
176  1.500000e-01  8.881784e-16
177  2.000000e-01  8.881784e-16
178  2.500000e-01  8.881784e-16
179  3.000000e-01  8.881784e-16
180  3.500000e-01  8.881784e-16
181  4.000000e-01  8.881784e-16
182  4.500000e-01  8.881784e-16
183  5.000000e-01  8.881784e-16
184  5.500000e-01  8.881784e-16
185  6.000000e-01  8.881784e-16
186  6.500000e-01  8.881784e-16
187  7.000000e-01  8.881784e-16
188  7.500000e-01  8.881784e-16
189  8.000000e-01  8.881784e-16
190  8.500000e-01  8.881784e-16
191  9.000000e-01  8.881784e-16
192  9.500000e-01  8.881784e-16
193 -2.000000e-01  5.000000e-02
194 -1.500000e-01  5.000000e-02
195 -1.000000e-01  5.000000e-02
196 -5.000000e-02  5.000000e-02
197  8.881784e-16  5.000000e-02
198  5.000000e-02  5.000000e-02
199  1.000000e-01  5.000000e-02
200  1.500000e-01  5.000000e-02
201  2.000000e-01  5.000000e-02
202  2.500000e-01  5.000000e-02
203  3.000000e-01  5.000000e-02
204  3.500000e-01  5.000000e-02
205  4.000000e-01  5.000000e-02
206  4.500000e-01  5.000000e-02
207  5.000000e-01  5.000000e-02
208  5.500000e-01  5.000000e-02
209  6.000000e-01  5.000000e-02
210  6.500000e-01  5.000000e-02
211  7.000000e-01  5.000000e-02
212  7.500000e-01  5.000000e-02
213  8.000000e-01  5.000000e-02
214  8.500000e-01  5.000000e-02
215  9.000000e-01  5.000000e-02
216  9.500000e-01  5.000000e-02
217 -2.000000e-01  1.000000e-01
218 -1.500000e-01  1.000000e-01
219 -1.000000e-01  1.000000e-01
220 -5.000000e-02  1.000000e-01
221  8.881784e-16  1.000000e-01
222  5.000000e-02  1.000000e-01
223  1.000000e-01  1.000000e-01
224  1.500000e-01  1.000000e-01
225  2.000000e-01  1.000000e-01
226  2.500000e-01  1.000000e-01
227  3.000000e-01  1.000000e-01
228  3.500000e-01  1.000000e-01
229  4.000000e-01  1.000000e-01
230  4.500000e-01  1.000000e-01
231  5.000000e-01  1.000000e-01
232  5.500000e-01  1.000000e-01
233  6.000000e-01  1.000000e-01
234  6.500000e-01  1.000000e-01
235  7.000000e-01  1.000000e-01
236  7.500000e-01  1.000000e-01
237  8.000000e-01  1.000000e-01
238  8.500000e-01  1.000000e-01
239  9.000000e-01  1.000000e-01
240  9.500000e-01  1.000000e-01
241 -1.500000e-01  1.500000e-01
242 -1.000000e-01  1.500000e-01
243 -5.000000e-02  1.500000e-01
244  8.881784e-16  1.500000e-01
245  5.000000e-02  1.500000e-01
246  1.000000e-01  1.500000e-01
247  1.500000e-01  1.500000e-01
248  2.000000e-01  1.500000e-01
249  2.500000e-01  1.500000e-01
250  3.000000e-01  1.500000e-01
251  3.500000e-01  1.500000e-01
252  4.000000e-01  1.500000e-01
253  4.500000e-01  1.500000e-01
254  5.000000e-01  1.500000e-01
255  5.500000e-01  1.500000e-01
256  6.000000e-01  1.500000e-01
257  6.500000e-01  1.500000e-01
258  7.000000e-01  1.500000e-01
259  7.500000e-01  1.500000e-01
260  8.000000e-01  1.500000e-01
261  8.500000e-01  1.500000e-01
262  9.000000e-01  1.500000e-01
263  9.500000e-01  1.500000e-01
264 -1.000000e-01  2.000000e-01
265 -5.000000e-02  2.000000e-01
266  8.881784e-16  2.000000e-01
267  5.000000e-02  2.000000e-01
268  1.000000e-01  2.000000e-01
269  1.500000e-01  2.000000e-01
270  2.000000e-01  2.000000e-01
271  2.500000e-01  2.000000e-01
272  3.000000e-01  2.000000e-01
273  3.500000e-01  2.000000e-01
274  4.000000e-01  2.000000e-01
275  4.500000e-01  2.000000e-01
276  5.000000e-01  2.000000e-01
277  5.500000e-01  2.000000e-01
278  6.000000e-01  2.000000e-01
279  6.500000e-01  2.000000e-01
280  7.000000e-01  2.000000e-01
281  7.500000e-01  2.000000e-01
282  8.000000e-01  2.000000e-01
283  8.500000e-01  2.000000e-01
284  9.000000e-01  2.000000e-01
285  9.500000e-01  2.000000e-01
286 -5.000000e-02  2.500000e-01
287  8.881784e-16  2.500000e-01
288  5.000000e-02  2.500000e-01
289  1.000000e-01  2.500000e-01
290  1.500000e-01  2.500000e-01
291  2.000000e-01  2.500000e-01
292  2.500000e-01  2.500000e-01
293  3.000000e-01  2.500000e-01
294  3.500000e-01  2.500000e-01
295  4.000000e-01  2.500000e-01
296  4.500000e-01  2.500000e-01
297  5.000000e-01  2.500000e-01
298  5.500000e-01  2.500000e-01
299  6.000000e-01  2.500000e-01
300  6.500000e-01  2.500000e-01
301  7.000000e-01  2.500000e-01
302  7.500000e-01  2.500000e-01
303  8.000000e-01  2.500000e-01
304  8.500000e-01  2.500000e-01
305  9.000000e-01  2.500000e-01
306  9.500000e-01  2.500000e-01
307  8.881784e-16  3.000000e-01
308  5.000000e-02  3.000000e-01
309  1.000000e-01  3.000000e-01
310  1.500000e-01  3.000000e-01
311  2.000000e-01  3.000000e-01
312  2.500000e-01  3.000000e-01
313  3.000000e-01  3.000000e-01
314  3.500000e-01  3.000000e-01
315  4.000000e-01  3.000000e-01
316  4.500000e-01  3.000000e-01
317  5.000000e-01  3.000000e-01
318  5.500000e-01  3.000000e-01
319  6.000000e-01  3.000000e-01
320  6.500000e-01  3.000000e-01
321  7.000000e-01  3.000000e-01
322  7.500000e-01  3.000000e-01
323  8.000000e-01  3.000000e-01
324  8.500000e-01  3.000000e-01
325  9.000000e-01  3.000000e-01
326  9.500000e-01  3.000000e-01
327  5.000000e-02  3.500000e-01
328  1.000000e-01  3.500000e-01
329  1.500000e-01  3.500000e-01
330  2.000000e-01  3.500000e-01
331  2.500000e-01  3.500000e-01
332  3.000000e-01  3.500000e-01
333  3.500000e-01  3.500000e-01
334  4.000000e-01  3.500000e-01
335  4.500000e-01  3.500000e-01
336  5.000000e-01  3.500000e-01
337  5.500000e-01  3.500000e-01
338  6.000000e-01  3.500000e-01
339  6.500000e-01  3.500000e-01
340  7.000000e-01  3.500000e-01
341  7.500000e-01  3.500000e-01
342  8.000000e-01  3.500000e-01
343  8.500000e-01  3.500000e-01
344  9.000000e-01  3.500000e-01
345  9.500000e-01  3.500000e-01
346  1.500000e-01  4.000000e-01
347  2.000000e-01  4.000000e-01
348  2.500000e-01  4.000000e-01
349  3.000000e-01  4.000000e-01
350  3.500000e-01  4.000000e-01
351  4.000000e-01  4.000000e-01
352  4.500000e-01  4.000000e-01
353  5.000000e-01  4.000000e-01
354  5.500000e-01  4.000000e-01
355  6.000000e-01  4.000000e-01
356  6.500000e-01  4.000000e-01
357  7.000000e-01  4.000000e-01
358  7.500000e-01  4.000000e-01
359  8.000000e-01  4.000000e-01
360  8.500000e-01  4.000000e-01
361  9.000000e-01  4.000000e-01
362  9.500000e-01  4.000000e-01
363  2.000000e-01  4.500000e-01
364  2.500000e-01  4.500000e-01
365  3.000000e-01  4.500000e-01
366  3.500000e-01  4.500000e-01
367  4.000000e-01  4.500000e-01
368  4.500000e-01  4.500000e-01
369  5.000000e-01  4.500000e-01
370  5.500000e-01  4.500000e-01
371  6.000000e-01  4.500000e-01
372  6.500000e-01  4.500000e-01
373  7.000000e-01  4.500000e-01
374  7.500000e-01  4.500000e-01
375  8.000000e-01  4.500000e-01
376  8.500000e-01  4.500000e-01
377  9.000000e-01  4.500000e-01
378  9.500000e-01  4.500000e-01
379  3.000000e-01  5.000000e-01
380  3.500000e-01  5.000000e-01
381  4.000000e-01  5.000000e-01
382  4.500000e-01  5.000000e-01
383  5.000000e-01  5.000000e-01
384  5.500000e-01  5.000000e-01
385  6.000000e-01  5.000000e-01
386  6.500000e-01  5.000000e-01
387  7.000000e-01  5.000000e-01
388  7.500000e-01  5.000000e-01
389  8.000000e-01  5.000000e-01
390  8.500000e-01  5.000000e-01
391  9.000000e-01  5.000000e-01
392  9.500000e-01  5.000000e-01
393  4.000000e-01  5.500000e-01
394  4.500000e-01  5.500000e-01
395  5.000000e-01  5.500000e-01
396  5.500000e-01  5.500000e-01
397  6.000000e-01  5.500000e-01
398  6.500000e-01  5.500000e-01
399  7.000000e-01  5.500000e-01
400  7.500000e-01  5.500000e-01
401  8.000000e-01  5.500000e-01
402  8.500000e-01  5.500000e-01
403  9.000000e-01  5.500000e-01
404  9.500000e-01  5.500000e-01
405  5.000000e-01  6.000000e-01
406  5.500000e-01  6.000000e-01
407  6.000000e-01  6.000000e-01
408  6.500000e-01  6.000000e-01
409  7.000000e-01  6.000000e-01
410  7.500000e-01  6.000000e-01
411  8.000000e-01  6.000000e-01
412  8.500000e-01  6.000000e-01
413  9.000000e-01  6.000000e-01
414  9.500000e-01  6.000000e-01
415  6.000000e-01  6.500000e-01
416  6.500000e-01  6.500000e-01
417  7.000000e-01  6.500000e-01
418  7.500000e-01  6.500000e-01
419  8.000000e-01  6.500000e-01
420  8.500000e-01  6.500000e-01
421  9.000000e-01  6.500000e-01
422  9.500000e-01  6.500000e-01
423  7.500000e-01  7.000000e-01
424  8.000000e-01  7.000000e-01
425  8.500000e-01  7.000000e-01
426  9.000000e-01  7.000000e-01
427  9.500000e-01  7.000000e-01
428  8.500000e-01  7.500000e-01
429  9.000000e-01  7.500000e-01
430  9.500000e-01  7.500000e-01
</code></pre>
","8300107","8300107","2018-06-01 08:27:02","Equation of the parabola enclosing a scatter plot","<python><python-3.x><curve-fitting>","2","12","14591"
"50639478","2018-06-01 08:46:52","1","","<p>Here's another approach using lxml instead of beautifulsoup:</p>

<pre><code>import requests
from lxml import html

req = requests.get(""&lt;URL&gt;"")
raw_html = html.fromstring(req.text)
spans = raw_html.xpath('//div[@id=""c1417094965154""]//span/text()')
print("""".join([x.replace(""\t"", """").replace(""\r\n"","""").strip() for x in spans]))
</code></pre>

<p>Output: Kranji Mile Day simulcast races,   Kranji Racecourse, SINClass 3 Handicap   -    1200M TURFSaturday, 26 May 2018Race 1, 5:15 PM</p>

<p>As you see, the output need a little formatting, spans is a list of all spans text, so you can do any processing you need. </p>
","1038301","9846358","2018-08-13 08:41:53","0","627","Roomm","2011-11-09 18:34:56","587","53","336","6","50638894","50639394","2018-06-01 08:10:01","2","94","<p>cannot get the span text within the ""table"", thanks !</p>

<pre><code>from bs4 import BeautifulSoup
import urllib2

url1 = ""url""

content1 = urllib2.urlopen(url1).read()
soup = BeautifulSoup(content1,""lxml"")
table = soup.findAll(""div"", {""class"" : ""iw_component"",""id"":""c1417094965154""})
rows = table.find_all('span',recursive=False)
for row in rows:
    print(row.text)
</code></pre>
","9846358","9846358","2018-08-08 07:38:17","cannot get the <span></span> texts","<python><beautifulsoup>","3","0","386"
"50639480","2018-06-01 08:46:58","2","","<p>As jonrsharpe mentions, this:</p>

<pre><code>mapping_dict = self.__dict__
</code></pre>

<p>does not create a copy of <code>self.__dict__</code> - it only binds the local name <code>mapping_dict</code> to the object also bound to <code>self.__dict__</code> (you definitly want to <a href=""https://nedbatchelder.com/text/names.html"" rel=""nofollow noreferrer"">read this</a> for more about python names / bindings etc).</p>

<p>So this next line:</p>

<pre><code>   mapping_dict['component'] = self.component.__dict__
</code></pre>

<p>is actually the equivalent of:</p>

<pre><code>   self.component = self.component.__dict__
</code></pre>

<p>which is obviously not what you want.</p>

<p>A simple solution is to create a new dict from <code>self.__dict__</code>. Since we need this for both  <code>Component</code> and <code>Entity</code>, the better solution is to factor this out to a <a href=""https://en.wikipedia.org/wiki/Mixin"" rel=""nofollow noreferrer"">mixin class</a>:</p>

<pre><code>class AsDictMixin(object):
    def as_dict(self):
        return {
          k:(v.as_dict() if isinstance(v, AsDictMixin) else v) 
          for k, v in self.__dict__.items()
          }

class Component(AsDictMixin):
    def __init___(self, a, b):
        self.a = a
        self.b = b        

class Entity(AsDictMixin):
    def __init__(self, component, c, d):
       self.component= component
       self.c = c
       self.d = d  
</code></pre>

<p>Note that this won't take computed attributes (properties etc) in account, only plain instance attributes, but I assume that it's what you want.</p>
","41316","","","1","1598","bruno desthuilliers","2008-11-27 10:40:14","57630","7809","2022","2444","50638104","50639480","2018-06-01 07:21:22","0","53","<p>I need a method in a composite class to return the mapping of the instance.</p>

<pre><code>class Component:
    def __init___(self, a, b):
        self.a = a
        self.b = b

class Entity:
   def __init__(self, component, c, d):
       self.component= component
       self.c = c
       self.d = d

   def as_dict(self):
       mapping_dict = self.__dict__
       mapping_dict['component'] = self.component.__dict__
       return mapping_dict
</code></pre>

<p>This solution will only work once when <code>as_dict()</code> method is called for the first time. When it's called for the second time, this would not work since <code>self.entity</code> will now refer to a dictionary and calling <code>__dict__</code> on it will raise <code>AttributeError</code>. </p>

<p>So I came up with a not so efficient solution;</p>

<pre><code>def as_dict(self):
    temp_1 = temp_2 = deepcopy(self)
    mapping_dict = temp_1.__dict__
    mapping_dict['component'] = temp_2.component.__dict__
    return mapping_dict
</code></pre>

<p>This works but not so efficient because I am making a deepcopy of the instance every time I call the function. </p>

<p>My question is, why when I call <code>self.entity.__dict__</code>, <code>entity</code> becomes type <code>dict</code> instead of type <code>Entity</code>? What is the mechanism behind this? And also whats the most efficient implementation to obtain a mapping of a composite object?</p>
","7549615","7549615","2018-06-01 07:25:09","Generating mapping dictionary of composite object in python","<python><dictionary><magic-methods>","1","2","1436"
"50639482","2018-06-01 08:47:04","1","","<p>After researching on the web about it, it appears to be a Windows issue which has been resolved in the Windows 1803 release. See <a href=""https://github.com/Microsoft/vscode/issues/36630#issuecomment-357084696"" rel=""nofollow noreferrer"">https://github.com/Microsoft/vscode/issues/36630#issuecomment-357084696</a></p>

<p><strong>Edit</strong>:</p>

<p>A workaround is to use external instead of redirected terminal.</p>

<p>Add the line:</p>

<pre><code>""console"": ""externalTerminal"",
</code></pre>

<p>to your <code>launch.json</code>.</p>
","1089693","1089693","2018-06-07 14:55:06","0","544","Alexper","2011-12-09 12:17:05","18","7","2","0","49701932","","2018-04-06 22:46:51","3","1208","<p>I'm very new to Visual Studio Code even beginner in python coding. I have tried the following very simple code:</p>

<pre><code>for i in range(1000):
    print i
</code></pre>

<p>Each time I run the code I got the following error after printing some of i's:</p>

<pre><code>print i
</code></pre>

<blockquote>
  <p>IOError: [Errno 0] Error</p>
</blockquote>

<p>I'm using python <code>2.7</code></p>

<p>Your help on this is highly appreciated. Thanks.</p>
","9609760","6925187","2018-04-07 11:28:03","IOError: [Errno 0] Error when running python code using visual studio code","<python><python-2.7><visual-studio-code>","2","2","461"
"50639490","2018-06-01 08:47:36","0","","<p>You can use something like this for appending panda dataframe df to test.csv without including headers:</p>

<pre><code>with open('test.csv', 'a') as csv_file:
    df.to_csv(csv_file, header=False)
</code></pre>

<p>The usage of 'with open' helps us to not bother about explicitly closing file</p>
","9752415","","","0","301","SDK4551","2018-05-07 10:56:26","51","1","5","0","42662744","","2017-03-08 03:58:27","2","1593","<p>I want to save data that I received from mosquitto broker as csv file. Below are the script for <code>mqtt_subscribe.py</code></p>

<pre><code>import paho.mqtt.client as mqtt
import pandas as pd

def on_connect(client, userdata, rc):
    print(""Connected with result code ""+str(rc))
    client.subscribe(""test"")

def on_message(client, userdata, msg):
    print str(msg.payload)
    datas = map(int, msg.payload)
    df = pd.DataFrame(data=datas, columns=['numbers'])

    f = open(""test.csv"", 'a')
    df.to_csv(f)
    f.close()

client = mqtt.Client()
client.on_connect = on_connect
client.on_message = on_message

client.connect(""localhost"", 1883, 60)

client.loop_forever()
</code></pre>

<p>This script will print random number/data received from the broker (until I manually stop the script) like this(Example)</p>

<pre><code>Connected with result code 0
4
7
7
</code></pre>

<p>I am hoping to write this result in CSV file like this</p>

<pre><code>,numbers
0,4
1,7
2,7
</code></pre>

<p>but instead I got this</p>

<pre><code>,numbers
0,4
,numbers
0,7
,numbers
0,7
</code></pre>

<p>Am I missing something here? I believe it is because method <code>on_message</code> keep overwriting the dataframe with the column but I do not know where should I initialize the dataframe other than inside <code>on_message</code> method.</p>

<p>Thank you in advance.</p>
","5687439","","","How to append new row to csv file with pandas?","<python><csv><pandas>","1","1","1368"
"50639517","2018-06-01 08:48:58","0","","<p>Try creating a <a href=""https://www.python-course.eu/python3_list_comprehension.php"" rel=""nofollow noreferrer"">list comprehension</a>:</p>

<pre><code>with open('d.txt', ""r"") as infile:
    print([i.strip().split() for i in infile if i.strip()])
</code></pre>

<p>Output:</p>

<pre><code>[['This', 'is'], ['A', 'test']]
</code></pre>
","8708364","8708364","2018-06-01 09:30:08","5","337","U10-Forward","2017-10-02 12:38:41","29180","4315","2153","1036","50639390","50639456","2018-06-01 08:41:29","0","59","<p>I am trying to write a music program in Python that takes some music written by the user in a text file and turns it into a midi. I'm not particularly experienced with python at this stage so I'm not sure what the reason behind this issue is. I am trying to write the source file parser for the program and part of this process is to create a list containing all the lines of the text file and breaking each line down into its own list to make them easier to work with. I'm successfully able to do that, but there is a problem.</p>

<p>I want the code to <strong>ignore</strong> lines that are only whitespace (So the user can make their file at least kind of readable without having all the lines thrown together one on top of the other), but I can't seem to figure out how to do that. I tried doing this</p>

<pre><code>with open(path, ""r"") as infile:
    for row in infile:
        if len(row):
            srclines.append(row.split())
</code></pre>

<p>And this <strong>does</strong> work as far as creating the list of lines and separating each word goes, BUT it still appends the empty lines that are only whitespace... I confirmed this by doing this</p>

<pre><code>for entry in srclines:
    print entry
</code></pre>

<p>Which gives, for example</p>

<pre><code>['This', 'is']
[]
['A', 'test']
</code></pre>

<p>With the original text being</p>

<pre><code>This is

A test
</code></pre>

<p>But strangely, if during the printing stage I do another len() check then the empty lines are actually <strong>ignored</strong> like I want, and it looks like this</p>

<pre><code>['This', 'is']
['A', 'test']
</code></pre>

<p>What is the cause of this? Does this mean I can only go over the list and remove empty entries after I generate It? Or am I just doing the line import code wrong? I am using python 3.6 to test this code with by the way</p>
","6283375","","","Python: if len(list) giving different results in different circumstances?","<python><list><printing>","3","1","1853"
"50639541","2018-06-01 08:50:08","7","","<p>From my previous experiences, you are probably consuming the map object before printing it out. <code>map</code> returns a consumable iterator so if you want to print the values of it, make sure you don't consume it.</p>

<p>For example,</p>

<pre><code>&gt;&gt;&gt; n = map(int, input().strip().split())
1 2 3 4 5

&gt;&gt;&gt; for i in n:
...     print(i)
...
1
2
3
4
5

&gt;&gt;&gt; for i in n:
...     print(i)

# prints nothing
</code></pre>
","4237254","3620003","2018-06-01 08:51:42","1","450","BcK","2014-11-10 21:01:26","1519","137","63","23","50639430","50639545","2018-06-01 08:43:54","-3","282","<p>I have a map of several integer values and I would like to iterate over it and print its values. I tried this:</p>

<pre><code>n = map(int, input().split())

1 2 3 4 5

for i in n:
    print(i)
</code></pre>

<p>This gives me an error:</p>

<pre><code>ValueError: invalid literal for int() with base 10: ' '
</code></pre>

<p>Doing the same operation above by using <code>.strip()</code> does the job of printing the integers.</p>

<pre><code> del(n)
 n = map(int, input().strip().split())
 1 2 3 4 5

 for i in n:
     print(i)
 1
 2
 3
 4
 5
</code></pre>

<p>What does an 'invalid literal for base 10' mean and why using <code>.strip()</code> fixes the error? Also, is a map object a single entity in Python since using <code>range(map)</code> gives error 'map' object cannot be interpreted as an integer?</p>

<pre><code>for i in range(n):
    print(i)

TypeError: 'map' object cannot be interpreted as an integer
</code></pre>
","6061080","6061080","2018-06-01 13:19:35","Invalid literal for base 10 error while iterating on map","<python>","3","3","935"
"50639545","2018-06-01 08:50:17","6","","<p><code>map</code> returns a generator that you can consume once, not twice.</p>

<pre><code>n = map(int, input().strip().split())   
print(*n) 
</code></pre>

<p>will print them with a default seperator of <code>' '</code>. If you want to do 2+ things with the result of your map, store it in a list:</p>

<pre><code>n = list( map(int, input().strip().split()) )   
</code></pre>

<p>so you are not operating on a generator - the list will keep the values for you to use a second/multiple times. </p>

<p>This</p>

<pre><code>for i in range(map(int, input().strip().split())):
    # do smth
</code></pre>

<p>does not work as map returns <code>&lt;map object at 0x7f9ff77c12b0&gt;</code> not an integer as needed for <code>range</code>.</p>
","7505395","7505395","2018-06-01 08:55:34","1","743","Patrick Artner","2017-02-02 10:46:51","30736","5120","3506","4713","50639430","50639545","2018-06-01 08:43:54","-3","282","<p>I have a map of several integer values and I would like to iterate over it and print its values. I tried this:</p>

<pre><code>n = map(int, input().split())

1 2 3 4 5

for i in n:
    print(i)
</code></pre>

<p>This gives me an error:</p>

<pre><code>ValueError: invalid literal for int() with base 10: ' '
</code></pre>

<p>Doing the same operation above by using <code>.strip()</code> does the job of printing the integers.</p>

<pre><code> del(n)
 n = map(int, input().strip().split())
 1 2 3 4 5

 for i in n:
     print(i)
 1
 2
 3
 4
 5
</code></pre>

<p>What does an 'invalid literal for base 10' mean and why using <code>.strip()</code> fixes the error? Also, is a map object a single entity in Python since using <code>range(map)</code> gives error 'map' object cannot be interpreted as an integer?</p>

<pre><code>for i in range(n):
    print(i)

TypeError: 'map' object cannot be interpreted as an integer
</code></pre>
","6061080","6061080","2018-06-01 13:19:35","Invalid literal for base 10 error while iterating on map","<python>","3","3","935"
"50639553","2018-06-01 08:50:55","0","","<p>testdoc.txt has lot of empty lines; but in output they are out; </p>

<pre><code>src = 'testdoc.txt'
with open(src, 'r') as f:
    for r in f:
        if len(r) &gt; 1:
            print(r.strip())
</code></pre>

<p>and now you can obviously put this all in list or in list line by line instead of print whatever fits your further logic</p>
","4763487","","","0","344","Drako","2015-04-08 10:58:07","521","191","288","258","50639390","50639456","2018-06-01 08:41:29","0","59","<p>I am trying to write a music program in Python that takes some music written by the user in a text file and turns it into a midi. I'm not particularly experienced with python at this stage so I'm not sure what the reason behind this issue is. I am trying to write the source file parser for the program and part of this process is to create a list containing all the lines of the text file and breaking each line down into its own list to make them easier to work with. I'm successfully able to do that, but there is a problem.</p>

<p>I want the code to <strong>ignore</strong> lines that are only whitespace (So the user can make their file at least kind of readable without having all the lines thrown together one on top of the other), but I can't seem to figure out how to do that. I tried doing this</p>

<pre><code>with open(path, ""r"") as infile:
    for row in infile:
        if len(row):
            srclines.append(row.split())
</code></pre>

<p>And this <strong>does</strong> work as far as creating the list of lines and separating each word goes, BUT it still appends the empty lines that are only whitespace... I confirmed this by doing this</p>

<pre><code>for entry in srclines:
    print entry
</code></pre>

<p>Which gives, for example</p>

<pre><code>['This', 'is']
[]
['A', 'test']
</code></pre>

<p>With the original text being</p>

<pre><code>This is

A test
</code></pre>

<p>But strangely, if during the printing stage I do another len() check then the empty lines are actually <strong>ignored</strong> like I want, and it looks like this</p>

<pre><code>['This', 'is']
['A', 'test']
</code></pre>

<p>What is the cause of this? Does this mean I can only go over the list and remove empty entries after I generate It? Or am I just doing the line import code wrong? I am using python 3.6 to test this code with by the way</p>
","6283375","","","Python: if len(list) giving different results in different circumstances?","<python><list><printing>","3","1","1853"
"50639570","2018-06-01 08:51:47","0","","<p>The second code does not work because range takes an <strong>int object</strong> not a <strong>map object</strong> and <strong>n is a map object.</strong></p>

<p>The first part works for me.</p>
","9871094","","","0","199","Agile_Eagle","2018-05-30 15:01:00","1179","604","74","44","50639430","50639545","2018-06-01 08:43:54","-3","282","<p>I have a map of several integer values and I would like to iterate over it and print its values. I tried this:</p>

<pre><code>n = map(int, input().split())

1 2 3 4 5

for i in n:
    print(i)
</code></pre>

<p>This gives me an error:</p>

<pre><code>ValueError: invalid literal for int() with base 10: ' '
</code></pre>

<p>Doing the same operation above by using <code>.strip()</code> does the job of printing the integers.</p>

<pre><code> del(n)
 n = map(int, input().strip().split())
 1 2 3 4 5

 for i in n:
     print(i)
 1
 2
 3
 4
 5
</code></pre>

<p>What does an 'invalid literal for base 10' mean and why using <code>.strip()</code> fixes the error? Also, is a map object a single entity in Python since using <code>range(map)</code> gives error 'map' object cannot be interpreted as an integer?</p>

<pre><code>for i in range(n):
    print(i)

TypeError: 'map' object cannot be interpreted as an integer
</code></pre>
","6061080","6061080","2018-06-01 13:19:35","Invalid literal for base 10 error while iterating on map","<python>","3","3","935"
"50639601","2018-06-01 08:53:55","9","","<p>By using <code>**a</code> you're <em>unpacking</em> the ordered dictionary into an argument dictionary.</p>

<p>So when you enter in <code>foo</code>, <code>kwargs</code> is just a plain dictionary, with order not guaranteed (unless you're using Python 3.6+, but that's still an implementation detail in 3.6, but the ordering becomes official in 3.7: <a href=""https://stackoverflow.com/questions/39980323/are-dictionaries-ordered-in-python-3-6"">Are dictionaries ordered in Python 3.6+?</a>)</p>

<p>You could just lose the packing/unpacking in that case so it's portable for older versions of python.</p>

<pre><code>from collections import OrderedDict

def foo(kwargs):
    for k, v in kwargs.items():
        print(k, v)

a = OrderedDict([
    ('a', 1),
    ('b', 2),
    ('c', 3),
    ('d', 4),
    ])

foo(a)
</code></pre>
","6451573","6451573","2018-06-01 08:59:47","3","830","Jean-François Fabre","2016-06-10 19:19:53","113106","37329","9248","14670","50639495","50639601","2018-06-01 08:47:58","2","1213","<p>This is the following problem:</p>

<p><strong>main_module.py</strong></p>

<pre><code>from collections import OrderedDict
from my_other_module import foo

a = OrderedDict([
    ('a', 1),
    ('b', 2),
    ('c', 3),
    ('d', 4),
    ])

foo(**a)
</code></pre>

<p><strong>my_other_module.py</strong></p>

<pre><code>def foo(**kwargs):
    for k, v in kwargs.items():
        print k, v
</code></pre>

<p>When i run <code>main_module.py</code> I'm expecting to get printout with the order I specified:</p>

<pre><code>a 1
b 2
c 3
d 4
</code></pre>

<p>But instead I'm getting:</p>

<pre><code>a 1
c 3
b 2
d 4
</code></pre>

<p>I do understand that this has something to do with the way <code>**</code> operator is implemented and somehow it looses order how dictionary pairs are passed in. Also I do understand that dictionaries in python are not ordered as lists are, because they're implemented as hash tables. Is there any kind of 'hack' that I could apply so I get the behaviour that is needed in this context?</p>

<p>P.S. - In my situation I can't sort the dictionary inside foo function since there are no rules which could be followed except strict order that values are passed in.</p>
","4676162","","","How to unpack dictionary in order that it was passed?","<python><dictionary><ordereddictionary>","1","3","1197"
"50639662","2018-06-01 08:57:19","0","","<pre><code># -*- coding: utf-8 -*-
from passlib.apps import custom_app_context as pwd_contex

password='£$'

encrypted=pwd_contex.encrypt(password)

print(pwd_contex.verify('£$', encrypted))
print(pwd_contex.verify('john', encrypted))

True
False
</code></pre>

<p>works perfectly well on my system. Perhaps you need to set your default encoding <code># -*- coding: utf-8 -*-</code> at the top of your script</p>
","9669507","","","0","413","James Bincom","2018-04-19 11:27:22","61","16","0","0","50636805","","2018-06-01 05:46:54","0","72","<p>I'm using passlib==1.7.1 with the following import:</p>

<pre><code>from passlib.apps import custom_app_context as pwd_context
</code></pre>

<p>Then hashing the password with the following:</p>

<p><code>pwd_context.encrypt(password)</code></p>

<p>I then verify with:</p>

<pre><code>pwd_context.verify(password, self.password_hash)
</code></pre>

<p>This is fine, but verification fails with certain characters.  e.g. ""£"" or ""$"".</p>

<p>Does anyone know why this would be the case please?</p>

<p>Thank you!</p>

<hr>

<p>Update:</p>

<p>Thank you all very much.  Armed with this info I investigated a bit more and it seems that the problem is not passlib but sits somewhere between angular4 where I send a base64 authorisation header to the flask app.  </p>

<p>I'm currently using the following to do this:</p>

<pre><code>let headers: Headers = new Headers({
        'Content-Type': 'application/json',
        'Authorization': 'Basic ' + btoa(userLoginRequest.username + ':' + userLoginRequest.password)
    });
</code></pre>

<p>I have read a lot today about unescape (and it's depreciation in favour of decodeURI()).  I have also read a lot about support for unicode in base64 encoding.  I tried a number of combinations of these things and it made no difference.  I am now really rather confused!</p>

<p>To test what's going on I do the following.  In angular4 I execute the following:</p>

<pre><code>let encodedString = btoa('doug:Tree£9')
console.log(encodedString)
console.log(atob(encodedString))
</code></pre>

<p>As expected, this prints the following to the console.  </p>

<pre><code>ZG91ZzpUcmVlozk=
doug:Tree£9
</code></pre>

<p>So it's clearly ok encoding and decoding.</p>

<p>Doing the same process in Python...</p>

<pre><code>import base64
encoded = base64.b64encode('doug:Tree£9')
print encoded
print base64.b64decode(encoded)
</code></pre>

<p>I get the the following in terminal.</p>

<pre><code>ZG91ZzpUcmVlwqM5
doug:Tree£9
</code></pre>

<p>I note that ""ZG91ZzpUcmVlozk="" and ""ZG91ZzpUcmVlwqM5"" are not the same.  However, both methods are working within their own languages.</p>

<p>If I put the ""ZG91ZzpUcmVlozk="" encoded string from javascript into python and decode it as follows...</p>

<pre><code>import base64
print base64.b64decode(""ZG91ZzpUcmVlozk="")
</code></pre>

<p>I get:</p>

<pre><code>doug:Tree�9
</code></pre>

<p>Note that the £ character has now been mashed.</p>

<p>Other Unicode characters fail too.</p>

<p>So I think the question is how to I encode the Authorisation header so that python correctly recognises the £ character, and any other character users choose for their passwords?</p>

<p>Thanks so much!</p>

<hr>

<p>Edit: Resolved!</p>

<p>I found this <a href=""https://stackoverflow.com/questions/30106476/using-javascripts-atob-to-decode-base64-doesnt-properly-decode-utf-8-strings"">Using Javascript&#39;s atob to decode base64 doesn&#39;t properly decode utf-8 strings</a> which goes into some detail.  I resolved it by using the following approach recommended by @brandonscript.</p>

<pre><code>b64EncodeUnicode(str) : string{
    return btoa(encodeURIComponent(str).replace(/%([0-9A-F]{2})/g, function(match, p1) {
        return String.fromCharCode(parseInt(p1, 16))
    }))
}
</code></pre>

<p>Works perfectly!  Phew!</p>
","7412939","7412939","2018-06-05 03:39:42","Verifying a password with £ or $ characters failing using passlib","<python><passlib>","2","1","3295"
"50639664","2018-06-01 08:57:21","1","","<p>By default there is a <code>one2many</code> field on the <code>product.template</code> that is called <code>seller_ids</code>. This is the relation between <code>product_supplierinfo</code> and <code>product_template</code>. So you can do something like this to get all the supplier codes:</p>

<pre><code>&lt;span&gt;&lt;t t-esc=""', '.join([x.product_code for x in order_line.product_id.product_tmpl_id.seller_ids])"" /&gt;&lt;/span&gt;
</code></pre>

<p>You can show all the product codes in a table as well</p>

<pre><code>&lt;table class=""table table-condensed""&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;Supplier&lt;/th&gt;
            &lt;th&gt;Product Code&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr t-foreach=""order_line.product_id.product_tmpl_id.seller_ids"" t-as=""s""&gt;
            &lt;td&gt;
                &lt;span t-esc=""s.name.name""/&gt;
            &lt;/td&gt;
            &lt;td&gt;
                &lt;span t-esc=""s.product_code""/&gt;
            &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
</code></pre>
","4891717","4891717","2018-06-01 11:22:40","7","1108","ChesuCR","2015-05-12 13:42:16","6525","839","1697","92","50622153","50639664","2018-05-31 10:28:40","0","52","<p>i'm trying to customized <strong>report_purchasequotation.xml</strong> and <strong>report_purchaseorder.xml</strong>. I have added a new <strong>th</strong> to add Product reference at the supplier in my reports. My problem is when i use <strong>span t-field=""order_line.product_id.product_code""</strong> (field product_code in the model  product.supplierinfo ) it shows error <strong>QWebException: 'product_code'</strong>. Any help please ?</p>

<pre><code>&lt;table class=""table table-condensed""&gt;
            &lt;thead&gt;
                &lt;tr&gt;
                    &lt;th&gt;&lt;strong&gt;Article&lt;/strong&gt;&lt;/th&gt;
                    &lt;th&gt;&lt;strong&gt;Référence fournisseur&lt;/strong&gt;&lt;/th&gt;
                    &lt;th&gt;&lt;strong&gt;Désignation&lt;/strong&gt;&lt;/th&gt;
                    &lt;th class=""text-center""&gt;&lt;strong&gt;Expected Date&lt;/strong&gt;&lt;/th&gt;
                    &lt;th class=""text-right""&gt;&lt;strong&gt;Qty&lt;/strong&gt;&lt;/th&gt;
                &lt;/tr&gt;
            &lt;/thead&gt;
            &lt;tbody&gt;
                &lt;tr t-foreach=""o.order_line"" t-as=""order_line""&gt;
                    &lt;td&gt;
                        &lt;span t-field=""order_line.name""/&gt;
                    &lt;/td&gt;

                    &lt;td&gt;
                        &lt;span t-field=""order_line.product_id.product_code""/&gt;
                    &lt;/td&gt;
                    &lt;td&gt;

                    &lt;/td&gt;

                    &lt;td class=""text-center""&gt;
                        &lt;span t-field=""order_line.date_planned""/&gt;
                    &lt;/td&gt;
                    &lt;td class=""text-right""&gt;
                        &lt;span t-field=""order_line.product_qty""/&gt;
                        &lt;span t-field=""order_line.product_uom"" groups=""product.group_uom""/&gt;
                    &lt;/td&gt;
                &lt;/tr&gt;
            &lt;/tbody&gt;
        &lt;/table&gt;
</code></pre>
","9657279","4891717","2018-06-01 09:04:17","How to print the field product_code in purchase reports?","<python><xml><python-2.7><odoo><odoo-9>","1","0","1996"
"50639669","2018-06-01 08:57:48","0","","<p>I had a similar issue trying to import rasterio with GDAL 2.x installed. You should try to install GDAL 1.11 instead.</p>
","9497468","","","0","125","L. Chaumartin","2018-03-15 12:57:32","54","7","8","0","50382930","","2018-05-17 03:30:53","2","255","<p>I have an AWS EC2 p2.xlarge instance running on Ubuntu 16.04.4 LTS that was created using the <a href=""https://aws.amazon.com/marketplace/pp/B077GCH38C"" rel=""nofollow noreferrer"">AWS Deep Learning AMI (DLAMI)</a>. I am using the keras/Tensor Flow conda environment:</p>

<pre><code>$ source activate tensorflow_p36
</code></pre>

<p>I am attempting to install Rasterio and GDAL on top of the Keras - Tensor Flow AMI installations using these commands (<a href=""http://rasterio.readthedocs.io/en/latest/installation.html#linux"" rel=""nofollow noreferrer"">source</a>):</p>

<pre><code>$ sudo add-apt-repository ppa:ubuntugis/ppa
$ sudo apt-get update
$ sudo apt-get install python-numpy gdal-bin libgdal-dev
$ pip install rasterio
</code></pre>

<p>The GDAL install seems to work:</p>

<pre><code>$ gdalinfo --version
GDAL 2.1.3, released 2017/20/01
</code></pre>

<p>However, when I try to import rasterio in Python, it yields the following error:</p>

<pre><code>(tensorflow_p36) ubuntu@ip-171-11-7-03:~$ python
Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19) 
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import rasterio
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/home/ubuntu/anaconda3/envs/tensorflow_p36_test/lib/python3.6/site-packages/rasterio/__init__.py"", line 15, in &lt;module&gt;
    from rasterio._base import (
ImportError: /usr/lib/libgdal.so.20: undefined symbol: sqlite3_column_table_name
&gt;&gt;&gt; 
</code></pre>

<p>How can I clear this error so that I can import and use rasterio in Python?</p>
","1446289","1446289","2018-05-17 03:55:42","Installing Rasterio on Ubuntu fails with ImportError","<python><amazon-web-services><tensorflow><gdal><rasterio>","1","3","1644"
"50639825","2018-06-01 09:05:36","0","","<p>Your url : <code>127.0.0.1:5000/profile/Sarthak</code>
seems to be correct. 
I think you might have put the <code>HTML</code> file in the same directory as the <code>Sample.py</code>.</p>

<p>By default flask application searches for templates in <code>templates</code> directory. To make it work, create a templates directory and put the html file inside it. No need to change the python code. It will work. </p>

<p>You can refer to this directory structure:</p>

<pre><code>/Sample.py
/templates
    /profile.html
</code></pre>

<p>Refer to this link:
<a href=""http://flask.pocoo.org/docs/0.12/quickstart/#rendering-templates"" rel=""nofollow noreferrer"">Flask Templates Guide</a></p>
","2256258","","","0","689","amulya349","2013-04-08 05:41:34","793","99","110","7","50638138","","2018-06-01 07:23:23","0","501","<p>I am new to Python web programming, I created a small program. But everytime I run, I am getting this error, <code>The debugger caught an exception in your WSGI application. You can now look at the traceback which led to the error.</code></p>

<p>This is my code below</p>

<p>Sample.py:</p>

<pre><code>from flask import Flask, render_template
app = Flask(__name__)

@app.route(""/profile/&lt;name&gt;"")
def profile(name):
    return render_template(""profile.html"", name=name)

if __name__ == ""__main__"":
    app.run(debug=True)
</code></pre>

<p>profile.html:</p>

<pre><code>&lt;!DOCTYPE html&gt;
&lt;title&gt;Welcome to thefunnybone&lt;/title&gt;
&lt;h1&gt;Hey there, {{name}}&lt;/h1&gt;
</code></pre>

<p>Description:
I am writting a name in the url, and it returns some text on the browser.</p>

<p>As said earlier, I am new to this language, therfore, please help me that I could provide any info regarding the program.</p>

<p>This is my project structure:</p>

<p><a href=""https://i.stack.imgur.com/1NbgK.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1NbgK.jpg"" alt=""enter image description here""></a></p>
","7913882","7913882","2018-06-01 09:03:36","python - Not Found The requested URL was not found on the server","<python><python-3.x><python-requests><pycharm>","2","7","1138"
"50639827","2018-06-01 09:05:37","0","","<p>OK I tested this code: it works provided the file tree is right. You need a directory (folder) in the same directory as you program file containing the template file, this directory must be called 'templates' </p>
","2041983","","","3","217","Paula Thomas","2013-02-05 05:55:24","912","161","36","11","50638138","","2018-06-01 07:23:23","0","501","<p>I am new to Python web programming, I created a small program. But everytime I run, I am getting this error, <code>The debugger caught an exception in your WSGI application. You can now look at the traceback which led to the error.</code></p>

<p>This is my code below</p>

<p>Sample.py:</p>

<pre><code>from flask import Flask, render_template
app = Flask(__name__)

@app.route(""/profile/&lt;name&gt;"")
def profile(name):
    return render_template(""profile.html"", name=name)

if __name__ == ""__main__"":
    app.run(debug=True)
</code></pre>

<p>profile.html:</p>

<pre><code>&lt;!DOCTYPE html&gt;
&lt;title&gt;Welcome to thefunnybone&lt;/title&gt;
&lt;h1&gt;Hey there, {{name}}&lt;/h1&gt;
</code></pre>

<p>Description:
I am writting a name in the url, and it returns some text on the browser.</p>

<p>As said earlier, I am new to this language, therfore, please help me that I could provide any info regarding the program.</p>

<p>This is my project structure:</p>

<p><a href=""https://i.stack.imgur.com/1NbgK.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1NbgK.jpg"" alt=""enter image description here""></a></p>
","7913882","7913882","2018-06-01 09:03:36","python - Not Found The requested URL was not found on the server","<python><python-3.x><python-requests><pycharm>","2","7","1138"
"50639840","2018-06-01 09:06:30","0","","<p>Spark normally requires a full Hadoop installation. However winutils.exe is a tool created to help if you don't plan to use Hadoop in order to perform distrubite computing, for example because you are only testing Spark locally, on Windows.</p>

<p>Press WIN+PAUSE, go to Advanced Settings and Environment variables.</p>

<p>Set the new environmental variable <code>HADOOP_HOME</code> to a directory of your choice. I recommend <code>C:\winutils</code> and not hadoop since this is not a full hadoop installation.</p>

<p>Create the directory <code>bin</code> inside it, place the file <code>winutils.exe</code> inside bin.</p>

<p>Edit PATH , append <code>%HADOOP_HOME%\</code> to it.</p>

<p>Now pyspark should work fine, as long as you work locally without distrubuted features.</p>
","9770840","","","0","789","Attersson","2018-05-10 12:44:01","3639","379","203","5","50637728","","2018-06-01 06:56:55","1","3446","<p>I am trying to integrate pyspark with python 2.7 (Pycharm IDE). I need to run some huge text files.</p>

<p>So this is what i am doing.</p>

<p>Download Spark (2.3.0-bin-hadoop-2.7) and extract it
Install JDK</p>

<p>And then i am trying to run this script</p>

<p>spark_home = os.environ.get('SPARK_HOME', None)
os.environ[""SPARK_HOME""] = ""C:\spark-2.3.0-bin-hadoop2.7""
import pyspark
from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession</p>

<pre><code>conf = SparkConf()
sc = SparkContext(conf=conf)
spark = SparkSession.builder.config(conf=conf).getOrCreate() 
import pandas as pd
ip = spark.read.format(""csv"").option(""inferSchema"",""true"").option(""header"",""true"").load(r""D:\some file.csv"")
</code></pre>

<p>Pycharm says that no module named Pyspark is found.</p>

<p>I am solving that by adding content roots and pointing to the folders where it is installed.</p>

<p>But the problem is every time i reopen pycharm, i have to add the content roots. How do i fix this?</p>

<p>Next is, when i do manage to run the script it throws up the following error.</p>

<pre><code>2018-06-01 12:20:49 ERROR Shell:397 - Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
    at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:379)
    at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:394)
    at org.apache.hadoop.util.Shell.&lt;clinit&gt;(Shell.java:387)
    at org.apache.hadoop.util.StringUtils.&lt;clinit&gt;(StringUtils.java:80)
    at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
    at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:273)
    at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:261)
    at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:791)
    at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:761)
    at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:634)
    at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2464)
    at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2464)
    at scala.Option.getOrElse(Option.scala:121)
    at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2464)
    at org.apache.spark.SecurityManager.&lt;init&gt;(SecurityManager.scala:222)
    at org.apache.spark.deploy.SparkSubmit$.secMgr$lzycompute$1(SparkSubmit.scala:393)
    at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$secMgr$1(SparkSubmit.scala:393)
    at org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply(SparkSubmit.scala:401)
    at org.apache.spark.deploy.SparkSubmit$$anonfun$prepareSubmitEnvironment$7.apply(SparkSubmit.scala:401)
    at scala.Option.map(Option.scala:146)
    at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:400)
    at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:170)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/C:/spark-2.3.0-bin-hadoop2.7/jars/hadoop-auth-2.7.3.jar) to method sun.security.krb5.Config.getInstance()
WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
2018-06-01 12:20:49 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
2018-06-01 12:20:56 ERROR Executor:91 - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ArrayIndexOutOfBoundsException: 63
    at org.apache.spark.unsafe.types.UTF8String.numBytesForFirstByte(UTF8String.java:191)
    at org.apache.spark.unsafe.types.UTF8String.numChars(UTF8String.java:206)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    at org.apache.spark.scheduler.Task.run(Task.scala:109)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:844)
2018-06-01 12:20:56 WARN  TaskSetManager:66 - Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 63
    at org.apache.spark.unsafe.types.UTF8String.numBytesForFirstByte(UTF8String.java:191)
    at org.apache.spark.unsafe.types.UTF8String.numChars(UTF8String.java:206)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    at org.apache.spark.scheduler.Task.run(Task.scala:109)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:844)

2018-06-01 12:20:56 ERROR TaskSetManager:70 - Task 0 in stage 0.0 failed 1 times; aborting job
Traceback (most recent call last):
  File ""D:/Microsoft/ThemeSpark.py"", line 13, in &lt;module&gt;
    ip = spark.read.format(""csv"").option(""inferSchema"",""true"").option(""header"",""true"").load(r""D:\Microsoft\xbox_13.5_26.5\Xbox Family.csv"")
  File ""C:\spark-2.3.0-bin-hadoop2.7\python\pyspark\sql\readwriter.py"", line 166, in load
    return self._df(self._jreader.load(path))
  File ""C:\spark-2.3.0-bin-hadoop2.7\python\lib\py4j-0.10.6-src.zip\py4j\java_gateway.py"", line 1160, in __call__
  File ""C:\spark-2.3.0-bin-hadoop2.7\python\pyspark\sql\utils.py"", line 63, in deco
    return f(*a, **kw)
  File ""C:\spark-2.3.0-bin-hadoop2.7\python\lib\py4j-0.10.6-src.zip\py4j\protocol.py"", line 320, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o25.load.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 63
    at org.apache.spark.unsafe.types.UTF8String.numBytesForFirstByte(UTF8String.java:191)
    at org.apache.spark.unsafe.types.UTF8String.numChars(UTF8String.java:206)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    at org.apache.spark.scheduler.Task.run(Task.scala:109)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:844)

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
    at scala.Option.foreach(Option.scala:257)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)
    at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)
    at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
    at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3272)
    at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)
    at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)
    at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3253)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
    at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3252)
    at org.apache.spark.sql.Dataset.head(Dataset.scala:2484)
    at org.apache.spark.sql.Dataset.take(Dataset.scala:2698)
    at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:148)
    at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)
    at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)
    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:202)
    at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:202)
    at scala.Option.orElse(Option.scala:289)
    at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:201)
    at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:392)
    at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)
    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)
    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:564)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:282)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:214)
    at java.base/java.lang.Thread.run(Thread.java:844)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 63
    at org.apache.spark.unsafe.types.UTF8String.numBytesForFirstByte(UTF8String.java:191)
    at org.apache.spark.unsafe.types.UTF8String.numChars(UTF8String.java:206)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    at org.apache.spark.scheduler.Task.run(Task.scala:109)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    ... 1 mo
</code></pre>

<p>I did some research and inferred that it is caused by the absence of the <code>winutils.exe</code>from the spark folder. I downloaded and placed it in the spark bin. Still this error keeps coming. How do i fixed this?</p>
","5440236","3269809","2018-06-01 08:50:29","Pyspark - Failed to locate the winutils binary in the hadoop binary path","<python><apache-spark><pyspark>","2","2","17093"
"50639849","2018-06-01 09:07:02","0","","<p>From the <a href=""https://seaborn.pydata.org/generated/seaborn.factorplot.html"" rel=""nofollow noreferrer"">documentation</a> it appears that the seaborn API has updated again, the argument <code>x_order</code> should be replaced by <code>order</code>: </p>

<pre><code>sns.factorplot('model', 'rate', data=m, kind=""bar"", order=['1','2','13'])
</code></pre>

<p>Also, <code>factorplot</code> has been renamed and will be removed in future releases; it is replaced by <code>catplot</code>:</p>

<pre><code>sns.catplot('model', 'rate', data=m, kind=""bar"", order=['1','2','13'])
</code></pre>
","5609221","5609221","2019-05-14 08:57:31","1","591","Archie","2015-11-26 16:08:58","814","121","732","9","24618862","24620595","2014-07-07 20:03:36","14","12867","<p>My data looks like this: </p>

<pre><code>m=pd.DataFrame({'model':['1','1','2','2','13','13'],'rate':randn(6)},index=['0', '0','1','1','2','2'])
</code></pre>

<p>I want to have the x-axis of factor plot ordered in [1,2,13] but the default is [1,13,2]. </p>

<p>Does anyone know how to change it?</p>

<p><strong>Update</strong>:
    I think I have figured it out in the following way, but maybe there is a better way by using an index to do that?</p>

<pre><code>sns.factorplot('model','rate',data=m,kind=""bar"",x_order=['1','2','13'])
</code></pre>
","3394937","5609221","2018-06-01 09:46:20","how to change the order of factor plot in seaborn","<python><pandas><bar-chart><seaborn>","2","5","553"
"50639857","2018-06-01 09:07:24","1","","<p>One should augment the data after Train and Test split. To work correctly one needs to make sure to <strong>augment data only from the train split</strong>.</p>

<p>If one augments data and before splitting the dataset, it will likely inject small variations of the train dataset into the test dataset. Thus the network will be overestimating its accuracy (and it might be over-fitting as well, among other issues).</p>

<p>A good way to avoid this pitfall it is to augment the data after the original dataset was split.</p>

<p>A lot of libraries implement python generators that randomly apply one or more combination of image modifications to augment the data.  These might include</p>

<ul>
<li>Image rotation</li>
<li>Image Shearing</li>
<li>Image zoom ( Cropping and re-scaling)</li>
<li>Adding noise</li>
<li>Small shift in hue</li>
<li>Image shifting</li>
<li>Image padding</li>
<li>Image Blurring</li>
<li>Image embossing</li>
</ul>

<p>This github library has a good overview of classical image augmentation techniques: <a href=""https://github.com/aleju/imgaug"" rel=""nofollow noreferrer"">https://github.com/aleju/imgaug</a> ( I have not used this library. Thus cannot endorse it speed or implementation quality, but their overview in <code>README.md</code> seems to be quite comprehensive.)</p>

<p>Some neural network libraries already have some utilities to do that. For example:  Keras has methods for Image Preprocessing  <a href=""https://keras.io/preprocessing/image/"" rel=""nofollow noreferrer"">https://keras.io/preprocessing/image/</a></p>
","2904237","","","0","1559","OddNorg","2013-10-21 17:38:44","450","26","93","31","50639003","50639857","2018-06-01 08:17:40","0","771","<p>for my exam based around data crunching, we've received a small simpsons dataset of 4 characters (Bart, Homer, Lisa, Marge) to build a convolutional neural network around. However, the dataset contains only a rather small amount of images: around 2200 to split into test &amp; train.</p>

<p>Since I'm very new to neural networks and deep learning, is it acceptable to augment my data (i'm turning the images X degrees 9 times) and splitting my data afterwards using sklearn's testtrainsplit function.</p>

<p>Since I've made this change, I'm getting a training and test accuracy of around 95% after 50 epochs with my current model. Since that's more than I've expected to get, I started questioning if augmenting test-data mainly is accepted without having a biased or wrong result in the end.</p>

<p>so:</p>

<p>a) Can you augment your data before splitting it with sklearn's TrainTestSplit without influencing your results in a wrong way? </p>

<p>b) if my method is wrong, what's another method I could try out?</p>

<p>Thanks in advance!</p>
","9880075","1735003","2018-06-01 08:23:31","Data augmentation before splitting","<python><scikit-learn><deep-learning><conv-neural-network>","1","4","1051"
"50639934","2018-06-01 09:11:54","1","","<p>You can use standart <a href=""https://docs.python.org/3/library/xml.etree.elementtree.html"" rel=""nofollow noreferrer"">xml</a> library to parse it to dict and then dump dict to json if it needed:</p>

<pre><code>xml_raw = """"""&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;root&gt;
  &lt;row&gt;
    &lt;Member_ID&gt;926494&lt;/Member_ID&gt;
    &lt;First_Name&gt;Corissa&lt;/First_Name&gt;
    &lt;Last_Name&gt;Aguiler&lt;/Last_Name&gt;
    &lt;Gender&gt;F&lt;/Gender&gt;
    &lt;Age&gt;39&lt;/Age&gt;
    &lt;Height&gt;5,3&lt;/Height&gt;
    &lt;Weight&gt;130&lt;/Weight&gt;
    &lt;Hours_Sleep&gt;8&lt;/Hours_Sleep&gt;
    &lt;Calories_Consumed&gt;2501&lt;/Calories_Consumed&gt;
    &lt;Exercise_Calories_Burned&gt;990&lt;/Exercise_Calories_Burned&gt;
    &lt;Date&gt;9/11/2017&lt;/Date&gt;
  &lt;/row&gt;
  &lt;row&gt;
    &lt;Member_ID&gt;926494&lt;/Member_ID&gt;
    &lt;First_Name&gt;Corissa&lt;/First_Name&gt;
    &lt;Last_Name&gt;Aguiler&lt;/Last_Name&gt;
    &lt;Gender&gt;F&lt;/Gender&gt;
    &lt;Age&gt;39&lt;/Age&gt;
    &lt;Height&gt;5,3&lt;/Height&gt;
    &lt;Weight&gt;130&lt;/Weight&gt;
    &lt;Hours_Sleep&gt;8&lt;/Hours_Sleep&gt;
    &lt;Calories_Consumed&gt;2501&lt;/Calories_Consumed&gt;
    &lt;Exercise_Calories_Burned&gt;990&lt;/Exercise_Calories_Burned&gt;
    &lt;Date&gt;9/11/2017&lt;/Date&gt;
  &lt;/row&gt;
&lt;/root&gt;""""""

import xml.etree.ElementTree as ET

root = ET.fromstring(xml_raw)

xml_dict_list = list()
for row in root.findall('.//row'):
    xml_dict = dict()
    for item in row.findall('./*'):
        xml_dict[item.tag] = item.text
    xml_dict_list.append(xml_dict)

print('dict -&gt;', xml_dict_list)
import json

json_str = json.dumps(xml_dict_list)
print('str -&gt;', json_str)
</code></pre>

<p>OUTPUT:</p>

<pre><code>dict -&gt; [{'Member_ID': '926494', 'First_Name': 'Corissa', 'Last_Name': 'Aguiler', 'Gender': 'F', 'Age': '39', 'Height': '5,3', 'Weight': '130', 'Hours_Sleep': '8', 'Calories_Consumed': '2501', 'Exercise_Calories_Burned': '990', 'Date': '9/11/2017'}, {'Member_ID': '926494', 'First_Name': 'Corissa', 'Last_Name': 'Aguiler', 'Gender': 'F', 'Age': '39', 'Height': '5,3', 'Weight': '130', 'Hours_Sleep': '8', 'Calories_Consumed': '2501', 'Exercise_Calories_Burned': '990', 'Date': '9/11/2017'}]
str -&gt; [{""Member_ID"": ""926494"", ""First_Name"": ""Corissa"", ""Last_Name"": ""Aguiler"", ""Gender"": ""F"", ""Age"": ""39"", ""Height"": ""5,3"", ""Weight"": ""130"", ""Hours_Sleep"": ""8"", ""Calories_Consumed"": ""2501"", ""Exercise_Calories_Burned"": ""990"", ""Date"": ""9/11/2017""}, {""Member_ID"": ""926494"", ""First_Name"": ""Corissa"", ""Last_Name"": ""Aguiler"", ""Gender"": ""F"", ""Age"": ""39"", ""Height"": ""5,3"", ""Weight"": ""130"", ""Hours_Sleep"": ""8"", ""Calories_Consumed"": ""2501"", ""Exercise_Calories_Burned"": ""990"", ""Date"": ""9/11/2017""}]
</code></pre>
","9369728","9369728","2018-06-01 16:28:19","2","2772","Konstantin","2018-02-16 13:03:46","507","35","13","19","50639418","","2018-06-01 08:43:23","0","371","<p>I need to convert XML to json without root in python. Here is an example of XML</p>

<pre><code>  &lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;root&gt;
  &lt;row&gt;
    &lt;Member_ID&gt;926494&lt;/Member_ID&gt;
    &lt;First_Name&gt;Corissa&lt;/First_Name&gt;
    &lt;Last_Name&gt;Aguiler&lt;/Last_Name&gt;
    &lt;Gender&gt;F&lt;/Gender&gt;
    &lt;Age&gt;39&lt;/Age&gt;
    &lt;Height&gt;5,3&lt;/Height&gt;
    &lt;Weight&gt;130&lt;/Weight&gt;
    &lt;Hours_Sleep&gt;8&lt;/Hours_Sleep&gt;
    &lt;Calories_Consumed&gt;2501&lt;/Calories_Consumed&gt;
    &lt;Exercise_Calories_Burned&gt;990&lt;/Exercise_Calories_Burned&gt;
    &lt;Date&gt;9/11/2017&lt;/Date&gt;
  &lt;/row&gt;
&lt;/root&gt;
</code></pre>

<p>I need to convert into JSON in the following format</p>

<pre><code>   {
    ""Member_ID"": 926494,
    ""First_Name"": ""Corissa"",
    ""Last_Name"": ""Aguiler"",
    ""Gender"": ""F"",
    ""Age"": 39,
    ""Height"": ""5,3"",
    ""Weight"": 130,
    ""Hours_Sleep"": 8,
    ""Calories_Consumed"": 2501,
    ""Exercise_Calories_Burned"": 990,
    ""Date"": ""9/11/2017""
  },
</code></pre>

<p>I am trying to use the parker convention from xmljson library but all the examples I'm finding are using string as input. I can't seem to figure out how to pass the actual .xml file instead of a string</p>

<p>For example:</p>

<pre><code>from xmljson import parker, Parker
from xml.etree.ElementTree import fromstring
from json import dumps
dumps(parker.data(fromstring('&lt;x&gt;&lt;a&gt;1&lt;/a&gt;&lt;b&gt;2&lt;/b&gt;&lt;/x&gt;')))
'{""a"": 1, ""b"": 2}'
</code></pre>
","9335915","","","How to convert an xml to json without root in Python?","<python><json><xml><python-3.x>","1","0","1553"
"50639963","2018-06-01 09:13:30","0","","<p>Why not write your scenario as</p>

<pre><code>Given I have a valid client auth token
And I request a user with an unknown ""valid"" uuid
Then I should get user not found response
</code></pre>

<p>Then you can put all the details of what a user not found response is in a helper method. This gives you:</p>

<ul>
<li>a simple scenario</li>
<li>no table processing for your scenario</li>
<li>a method you can reuse in other scenarios where a user is not found</li>
<li>much lower cost of change if for example you want to ad another field to the response.</li>
</ul>

<p>In general keeping details of HOW you do things out of your scenarios makes them much simpler to implement and much cheaper to maintain.</p>

<p>Hope ^^ is of some use :)</p>
","4341733","","","1","747","diabolist","2014-12-09 14:00:37","2603","219","57","18","50627578","","2018-05-31 15:13:38","0","895","<p>Here I've written a test to determine if an API is responding to a bad request with the expected content...</p>

<pre><code>Scenario: Unkown user response body properties contain expected content
    Given I have a valid client auth token
    And I request a user with an unknown ""valid"" uuid
    And I get the response json
    Then the expected fields should contain expected content
    | field      | content               |
    | statusCode | 404                   |
    | error      | Not Found             |
    | message    | User record not found |
</code></pre>

<p>This is the corresponding step:</p>

<pre><code>@then(u'the expected fields should contain expected content')
def step_impl(context):
    for row in context.table:
        received_content = str(context.request_json.get(row['field']))
        expected_content = row['content']
        assert_equal(received_content, expected_content)
</code></pre>

<p>It seems that Behave converts table row content to strings.</p>

<p>My question is: Is it possible to specify the data type of a cell in a Behave table?</p>

<p>In the actual response <code>statusCode</code> is an integer, but as you can see in my step function I'm forced to convert the request content to a string in order to validate it. I wouldn't need to do this if I could specify that the <code>404</code> I'm passing in the content column is an integer.</p>
","8506575","8506575","2018-06-01 08:02:51","Specify Behave table row data type","<python><cucumber><bdd><python-behave>","2","0","1397"
"50640163","2018-06-01 09:23:38","1","","<p>I believe need:</p>

<pre><code>df4 = pd.DataFrame({'CreatedDate':['09-08-16 0:00','22-08-16 0:00','23-08-16 0:00','28-08-16 0:00','29-08-16 0:00','30-08-16 0:00','31-08-16 0:00']})
df4[""CreatedDate""] = pd.to_datetime(df4.CreatedDate)
</code></pre>

<hr>

<pre><code>df4 = df4.sort_values(""CreatedDate"")
count = df4.groupby((df4[""CreatedDate""].diff().dt.days &gt; 1).cumsum()).size()
print (count)
CreatedDate
0    2
1    4
2    1
dtype: int64

a = (pd.cut(count, bins=[0,3,7,15,31], labels=['1-3', '4-7','8-15', '&gt;=16'])
       .value_counts()
       .sort_index()
       .rename_axis('Streak')
       .reset_index(name='Count'))
print (a)
  Streak  Count
0    1-3      2
1    4-7      1
2   8-15      0
3   &gt;=16      0
</code></pre>
","2901002","2901002","2018-06-01 10:51:00","1","744","jezrael","2013-10-20 20:27:26","427380","89269","18260","743","50637603","50640550","2018-06-01 06:47:51","0","149","<p>So I have a set of 50 dates  I have specified 7 here for example</p>

<pre><code>df[""CreatedDate""] = pd.DataFrame('09-08-16 0:00','22-08-16 0:00','23-08-16 0:00',28-08-16 0:00,'29-08-16 0:00','30-08-16 0:00','31-08-16 0:00')
df[""CreatedDate""] = pd.to_datetime(df4.CreatedDate)
df4[""DAY""] = df4.CreatedDate.dt.day
</code></pre>

<p>How to find the continuous days  which form a streak range [1-3],[4-7],[8-15],[>=16]</p>

<pre><code> Streak Count 
 1-3     3    #(9),(22,23) are in range [1-3]
 4-7     1    #(28,29,30,31) are in range [4-7]
 8-15    0
 &gt;=16    0
</code></pre>

<p>let's just say the product (pen) has been launched 2 yrs back we are taking the dataset for last 10 months from today and from what I want to find is that if people are buying that pen continuously for 1 or 2 or 3 days and if yes place the count [1-3] and if they are buying it continuously for 4 or 5 or 6 or 7 days we place the count in [4- 7] and so on for other ranges</p>

<p>I dont know which condition to specify to match the criteria </p>
","9846590","9846590","2018-06-01 08:46:18","Finding the streak of days in python","<python><python-3.x><pandas><group-by><pivot-table>","2","7","1034"
"50640176","2018-06-01 09:24:17","1","","<p>Arguments are assigned inside the function as it's local variables. So all principles apply here.</p>

<ul>
<li>Immutable objects cannot be changed.</li>
<li>Mutable objects can be modified in place.</li>
</ul>

<p>you're indenting to modify an immutable object, which is not possible. So your only options are :-</p>

<pre><code>def init_param(param):
    param = 10
    return param

n = 1
n = init_param(n)
print n
</code></pre>

<p>which is pretty much useless OR</p>

<pre><code>def init_param(param):
    param[0] = 10

n = [1]
init_param(n)
print n   
</code></pre>
","4047092","4047092","2018-06-01 09:31:23","0","576","ravi","2014-09-16 15:47:55","9810","1016","157","223","50640122","50640176","2018-06-01 09:22:12","0","74","<p>I'd like to initialise a variable inside a function (so that the final print statement in my example outputs 10): </p>

<pre><code>def init_param(param):
    param = 10

n = 1
init_param(n)
print n                   # prints 1
</code></pre>

<p>Is this doable in python?</p>
","282307","","","How to assign value to function parameter in python","<python>","4","8","278"
"50640336","2018-06-01 09:32:00","1","","<p>I would like to provide a more intuitive patterning solution with regex. 
The below function takes as input a stringified list containing arbitrary strings. </p>

<p><strong>Stepwise explanation:</strong>
You remove all whitespacing,bracketing and value_separators (provided they are not part of the values you want to extract, else make the regex more complex). Then you split the cleaned string on single or double quotes and take the non-empty values (or odd indexed values, whatever the preference). </p>

<pre><code>def parse_strlist(sl):
import re
clean = re.sub(""[\[\],\s]"","""",sl)
splitted = re.split(""[\'\""]"",clean)
values_only = [s for s in splitted if s != '']
return values_only
</code></pre>

<p><strong>testsample</strong>: ""['21',""foo"" '6', '0', "" A""]""</p>
","7141994","","","0","774","Jordy Van Landeghem","2016-11-10 15:02:00","11","6","0","0","1894269","1894296","2009-12-12 18:19:03","453","277474","<p>I was wondering what the simplest way is to convert a <code>string</code> list like the following to a <code>list</code>:</p>

<pre><code>x = u'[ ""A"",""B"",""C"" , "" D""]'
</code></pre>

<p>Even in case user puts spaces in between the commas, and spaces inside of the quotes. I need to handle that as well to:</p>

<pre><code>x = [""A"", ""B"", ""C"", ""D""] 
</code></pre>

<p>in Python.</p>

<p>I know I can strip spaces with <code>strip()</code> and <code>split()</code> using the split operator and check for non alphabets. But the code was getting very kludgy. Is there a quick function that I'm not aware of?</p>
","65424","355230","2018-03-21 16:36:20","Convert string representation of list to list","<python><string>","16","3","609"
"50640396","2018-06-01 09:35:12","1","","<p>First of all python function passes the value by object and the reference name here <code>param</code> is just a reference to a value hold by <code>n</code>. </p>

<p>Now coming to the solution, yes it could be possible provided you pass the variable name</p>

<pre><code>def init_param(var_name):
    globals()[var_name] = 10

n = 1
init_param('n')
print n
</code></pre>

<p>Hope it will answer!</p>
","1704235","","","0","404","MaNKuR","2012-09-27 18:07:56","1450","161","64","9","50640122","50640176","2018-06-01 09:22:12","0","74","<p>I'd like to initialise a variable inside a function (so that the final print statement in my example outputs 10): </p>

<pre><code>def init_param(param):
    param = 10

n = 1
init_param(n)
print n                   # prints 1
</code></pre>

<p>Is this doable in python?</p>
","282307","","","How to assign value to function parameter in python","<python>","4","8","278"
"50640406","2018-06-01 09:35:31","3","","<p>Do not use chained indexing for assigning values.</p>

<p>Instead, use <a href=""https://pandas.pydata.org/pandas-docs/version/0.21/generated/pandas.DataFrame.loc.html"" rel=""nofollow noreferrer""><code>pd.DataFrame.loc</code></a> to specify rows and columns:</p>

<pre><code>df.loc[df['b'] &gt; 3, 'b'] = 10
</code></pre>

<p>The <code>.loc</code> indexer accepts lists, scalars, or Boolean arrays.</p>

<p>The <code>pandas</code> docs <a href=""https://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy"" rel=""nofollow noreferrer"">explain in detail</a> why chained indexing should be avoided.</p>
","9209546","","","0","624","jpp","2018-01-12 14:47:22","109049","18235","7890","3496","50640334","50640406","2018-06-01 09:31:55","1","50","<p>I've created a pandas DataFrame</p>

<p><code>df = DataFrame(np.arange(15).reshape(3,5), columns=['a','b','c', 'd', 'e'])</code></p>

<pre><code>df  a   b   c   d   e
0   0   1   2   3   4
1   5   6   7   8   9
2  10  11  12  13  14
</code></pre>

<p>And I want to set values for particular cells:</p>

<pre><code>flag = df['b'] &gt; 3 

df[flag]['b']=10
</code></pre>

<p>But it <strong>doesn't</strong> work.</p>

<pre><code>df  a   b   c   d   e
0   0   1   2   3   4
1   5   6   7   8   9
2  10  11  12  13  14
</code></pre>

<p>I use the following codes. <strong>It works, but i don't know why?</strong></p>

<p><code>df['b'][flag] = 10</code></p>

<pre><code>df  a   b   c   d   e
0   0   1   2   3   4
1   5  10   7   8   9
2  10  10  12  13  14
</code></pre>
","5501046","9209546","2018-06-01 14:52:04","How to set values for particular cells in pandas.Dataframe correctly?","<python><pandas><dataframe>","1","0","770"
"50640411","2018-06-01 09:35:40","1","","<p>Short answer: no, you can't.</p>

<p>Longer answer: in </p>

<pre><code>def init_param(param):
    param = 10
</code></pre>

<p>the name <code>param</code> is local to the <code>init_param</code> function. Rebinding this name will change the value bound to the name <code>param</code> in the function's local scope, but will have absolutely no effect on the name <code>n</code> in the caller's scope - those names live in totally distinct namespaces. You can <a href=""https://nedbatchelder.com/text/names.html"" rel=""nofollow noreferrer"">read Ned Batcheler's reference article on Python's names and binding</a> for more in-depth explanations.</p>

<p>What would work would be to use a mutable container - a <code>dict</code> for example - and mutate this container, ie:</p>

<pre><code>def init_param(params, name, value):
    params[name] = value

params = {
   ""n"": 1,
   ""answer"": 42,
   ""parrot"": ""dead""
   }

init_params(params, ""n"", 10)
print(params)
</code></pre>

<p>(if you don't understand why this one works, re-read Ned Batcheler's article linked above)</p>
","41316","","","0","1072","bruno desthuilliers","2008-11-27 10:40:14","57630","7809","2022","2444","50640122","50640176","2018-06-01 09:22:12","0","74","<p>I'd like to initialise a variable inside a function (so that the final print statement in my example outputs 10): </p>

<pre><code>def init_param(param):
    param = 10

n = 1
init_param(n)
print n                   # prints 1
</code></pre>

<p>Is this doable in python?</p>
","282307","","","How to assign value to function parameter in python","<python>","4","8","278"
"50640531","2018-06-01 09:41:26","-1","","<p>Try this</p>

<pre><code>brackets = set('(()())')
</code></pre>
","8078949","","","1","67","Stanislav Goncharick","2017-05-28 22:48:26","49","4","0","0","50640012","","2018-06-01 09:16:02","0","21","<pre><code>from functools import reduce
import array
import numpy as np

Stack = []
StackSize = 20

arr=[]

exp=0|1|0|1
brackets = set['(()())']
arr=np.asarray(brackets)
print(type(brackets))
i=0

def push(brackets):
    Stack.push()

def pop():
 while len(Stack) &gt; 0:
     Stack.pop()
     evaluate()

def evaluate():
    print(eval([exp[0:x] for x in range(3, len(exp) + 1, 2)]))


def main():

    #while len(exp)&gt;0:
        for symbol in brackets:
            if symbol == '(':
                push(brackets)
            elif symbol == ')':
                 pop()

if __name__ == '__main__':
    main()
</code></pre>

<p>I am facing errors stating 'type' object has no attribute '<strong>getitem</strong>' and secondly I want to covert the bracket into an array instead of class list.
How can we achieve it.</p>
","8839696","1222951","2018-06-01 09:19:57","The code has errors stating 'type' object has no attribute '__getitem__' and covert the bracket into an array instead of class list?","<python><python-3.x><stack><brackets>","1","0","822"
"50640550","2018-06-01 09:42:08","0","","<p>Here's an attempt, binning is the same as @jezrael (except the last bin which I'm not sure should be limited to <code>31</code>... is there a way to have open intervals with <code>pd.cut</code>?)</p>

<pre><code>import pandas as pd

df = pd.DataFrame({ ""CreatedDate"": ['09-08-16 0:00','22-08-16 0:00','23-08-16 0:00','28-08-16 0:00','29-08-16 0:00','30-08-16 0:00','31-08-16 0:00']})
df[""CreatedDate""] = pd.to_datetime(df.CreatedDate)

# sort by date
df = df.sort_values(""CreatedDate"")

# group consecutive dates
oneday = pd.Timedelta(""1 day"")
df[""groups""] = (df.diff() &gt; oneday).cumsum()
counts = df.groupby(""groups"").count()[""CreatedDate""]

# bin
streaks = (pd.cut(counts, bins=[0,3,7,15,1000000], labels=['1-3', '4-7','8-15', '&gt;=16'])
           .value_counts()
           .rename_axis(""streak"")
           .reset_index(name=""count""))

print(streaks)

  streak  count
0    1-3      2
1    4-7      1
2   &gt;=16      0
3   8-15      0
</code></pre>
","5629339","5629339","2018-06-01 10:27:19","4","961","filippo","2015-12-02 08:32:28","3326","239","339","9","50637603","50640550","2018-06-01 06:47:51","0","149","<p>So I have a set of 50 dates  I have specified 7 here for example</p>

<pre><code>df[""CreatedDate""] = pd.DataFrame('09-08-16 0:00','22-08-16 0:00','23-08-16 0:00',28-08-16 0:00,'29-08-16 0:00','30-08-16 0:00','31-08-16 0:00')
df[""CreatedDate""] = pd.to_datetime(df4.CreatedDate)
df4[""DAY""] = df4.CreatedDate.dt.day
</code></pre>

<p>How to find the continuous days  which form a streak range [1-3],[4-7],[8-15],[>=16]</p>

<pre><code> Streak Count 
 1-3     3    #(9),(22,23) are in range [1-3]
 4-7     1    #(28,29,30,31) are in range [4-7]
 8-15    0
 &gt;=16    0
</code></pre>

<p>let's just say the product (pen) has been launched 2 yrs back we are taking the dataset for last 10 months from today and from what I want to find is that if people are buying that pen continuously for 1 or 2 or 3 days and if yes place the count [1-3] and if they are buying it continuously for 4 or 5 or 6 or 7 days we place the count in [4- 7] and so on for other ranges</p>

<p>I dont know which condition to specify to match the criteria </p>
","9846590","9846590","2018-06-01 08:46:18","Finding the streak of days in python","<python><python-3.x><pandas><group-by><pivot-table>","2","7","1034"
"50640559","2018-06-01 09:42:41","0","","<p>every time guys...
EVERY. TIME.
i look for the answer for hours, and immediately after posting for help, i find the answer.</p>

<p>Commented out method #1 is correct, however you can't do a specific action in the connect() method like window.destroy or something.</p>

<p>correct way:`</p>

<pre><code>    self.anim.finished.connect(self.someMethod)
def someMethod(self):
    window.destroy
</code></pre>

<p>what was throwing me off was; the IDE was not offering a code completion suggestion for finished.connect() (same with button.clicked.connect() actually)</p>

<p>this is what i get for relying too much on an IDE i suppose. hope this helps someone in the future.</p>
","8926886","6622587","2018-06-01 14:14:23","0","678","Dsabadash","2017-11-12 06:36:43","78","13","2","0","50640196","","2018-06-01 09:25:13","-1","415","<p>In essence, i'm trying to close a window after the animation completes.
In all the documentation and examples i've looked at, they are either in:</p>

<ul>
<li>C++</li>
<li>vague ""method definitions""</li>
<li>Old style slots and connectors</li>
</ul>

<p>how do i access the finished() that gets 'supposedly' called when the animation finishes?</p>

<pre><code>self.anim = QtCore.QPropertyAnimation(window, b""windowOpacity""
self.anim.setStartValue(1)
self.anim.setEndValue(0)
self.anim.setDuration(3000)
#self.anim.finished.connect() does not exist
#QtCore.QObject.connect(stuff) is deprecated
#self.anim.finished(window.destroy) destroys window immediately
</code></pre>

<p>in all the examples i am reading, they use the first commented out method, but the compiler complains about 'finished' not having a 'connect()' method</p>
","8926886","","","PyQt5 QPropertyAnimation finished() how to connect","<python><qt><pyqt><pyqt5>","1","0","834"
"50640629","2018-06-01 09:45:42","0","","<p>Move and align your code with IF statement. Bease you are defining clf under IF statement which is local to them.</p>
","9880501","","","0","121","Irfan Khan","2018-06-01 09:45:10","1","1","0","0","38793032","","2016-08-05 15:37:24","0","434","<p>I have been struggling with Gridsearchcv for a long time now.</p>

<p>After vectorizing my train data I used grid search for efficient parameter setting but i am getting continuous errors . </p>

<p>My code is something like this :</p>

<pre><code>x = HashVectorizer().fit_transform( train_data.data ) 
parameters = { ""c"" : [0.001 , 0.01 , 0.1 , 1 , 10 , 100 , 1000]}
if __name__ == ""__main__"":# i recetly fixed this error 
    clf = GridSearchCV(LogisticRegression() , parameters , n_jobs = -1 , verbose = 1)
   train = clf.fit( x , x.label)


y = HashVectorizer().transform( test_data.data )
   test = clf.predict(y)# here is my PROBLEM line , the error is coming as ""clf is not defined ""
</code></pre>

<p>But I have defined <code>clf</code> , also after checking <a href=""http://scikit-learn.org/stable/modules/grid_search.html#grid-search-searching-for-estimator-parameters"" rel=""nofollow"">gridsearchcv documentation</a> I found nothing of help . </p>

<p>Please help.</p>
","5264973","3329664","2016-08-07 17:09:54","gridsearchcv taking too much time and throwing random errors?","<python><scikit-learn><grid-search>","1","7","981"
"50640734","2018-06-01 09:51:07","0","","<pre><code>def init_param(param):
    param = 10

n = 1
init_param(n)
print n 
</code></pre>

<p>here <code>n</code> is a integer (<code>immutable data type</code>) so it will be passed by value and so value of n will be unchanged.</p>

<p>lets take a <code>mutable data type</code> (ex. list) then it will be passed by referenced and so values in list will be changed.</p>

<pre><code>def init_param(a):
    a[0] = 10

arr = [1] 
init_param(arr)
print(arr[0]) # print 10
</code></pre>

<p>so you have to check first whether the data is mutable or immutable.</p>

<p>otherwise you can use <code>global keyword</code> to access <code>global variables</code>.</p>

<pre><code>def f():
    global n
    n = 10
n = 1
f()
print(n) # print 10
</code></pre>
","6350219","6350219","2018-06-01 10:08:05","0","751","praveen chaudhary","2016-05-18 09:52:42","2","40","0","0","50640122","50640176","2018-06-01 09:22:12","0","74","<p>I'd like to initialise a variable inside a function (so that the final print statement in my example outputs 10): </p>

<pre><code>def init_param(param):
    param = 10

n = 1
init_param(n)
print n                   # prints 1
</code></pre>

<p>Is this doable in python?</p>
","282307","","","How to assign value to function parameter in python","<python>","4","8","278"
"50640745","2018-06-01 09:52:07","0","","<p>You can try with below CSS Selector</p>

<pre><code>action.action-primary.action-update.js-login.login-button
</code></pre>

<p>Update</p>

<p>Just noticed that you have missing dot (.) in your implementation</p>

<pre><code>browser.find_element_by_xpath('.//*[@id='login']/button')
</code></pre>
","9684373","9684373","2018-06-01 10:14:27","2","300","Prany","2018-04-23 06:34:30","1277","185","9","7","50640686","","2018-06-01 09:48:30","0","290","<p>I am not sure why selenium is not sending submit request. </p>

<p>edx.py           or Coursera                                                           </p>

<pre><code>from selenium import webdriver
browser = webdriver.Chrome()
browser.get('https://courses.edx.org/login')
email = browser.find_element_by_id('login-email')
email.send_keys('xxxxx@gmail.com')
pwd = browser.find_element_by_id('login-password')
pwd.send_keys('password')
login_attempt = browser.find_element_by_xpath('//*[@id=""login""]/button')
login_attempt.submit()
</code></pre>
","9880297","7429447","2018-06-01 10:59:23","How to login/submit request through Selenium and Python","<python><selenium><selenium-webdriver><xpath><css-selectors>","3","2","551"
"50640781","2018-06-01 09:53:53","1","","<p>You could loop through a copy of your dict to avoid your specific problem:</p>

<pre><code>for x in list1:
   for y in dict1.copy():
      if x in y:
         dict1[x] = dict1.pop(y)
</code></pre>
","3301980","","","1","200","Pax Vobiscum","2014-02-12 14:06:58","1825","217","447","19","50640730","50640781","2018-06-01 09:50:52","0","34","<p>On one hand I have <strong>1 dictionary</strong> with 100 keys and behind each key there is a list with many more entries:</p>

<pre><code>dict1 = {""/*** Hello1   ***/"": [""1"", ""2"", ""3"", ""4""....],
         ""/*** Hello2   ***/"": [""1"", ""2"", ""3"", ""4""....]
         ""/*** Hello2   ***/"": [""1"", ""2"", ""3"", ""4""....]}
</code></pre>

<p>The keys of the dict are strings which have a weird looking syntax (out of .c file headers, thats why they have /*** in it).</p>

<p>On the other hand I have a list with all the key values of the dictionary in it but with only the core values:</p>

<pre><code>list1 = [""Hello3"", ""Hello2"", ""Hello1""]
</code></pre>

<p>Both the dictionary and the list are unordered and it could be that the dict1 has keys in it, which the list1 does not contain!</p>

<p>Is there a fast way to compare the keys of the dict1 and the values of list1 and change the keys of the dict1 to the matched entries of list1?</p>

<p>Something like this?:</p>

<pre><code>for x in list1:
   for y in dict1:
      if x in y:
         dict1[x] = dict1.pop(y)
</code></pre>

<p>Error: dictionary changed size during iteration</p>

<p>Don't get how I can fix it...</p>

<p>Thanks a lot!</p>

<p>EDIT: Is it only possible by creating a new dictionary and simply adding the given values? Sorry for not mentioning!</p>
","7114307","7114307","2018-06-01 10:01:15","Changing key of dictionary while comparing its values (list) with another list","<python><python-3.x><list><dictionary>","1","0","1312"
"50640828","2018-06-01 09:56:49","0","","<p>try <code>login_attempt.click()</code></p>

<p><a href=""https://i.stack.imgur.com/R3n10.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/R3n10.png"" alt=""enter image description here""></a></p>

<p>You form not has <code>action</code> attribute, so the <code>form.submit()</code> won't know the destination to submit.</p>

<p>So for safe purpose, recommend to find the button and click on it. Rather than use the convenient API: <code>element.submit()</code>.</p>
","8582353","8582353","2018-06-01 10:12:00","0","483","yong","2017-09-08 23:49:16","8862","704","116","5","50640686","","2018-06-01 09:48:30","0","290","<p>I am not sure why selenium is not sending submit request. </p>

<p>edx.py           or Coursera                                                           </p>

<pre><code>from selenium import webdriver
browser = webdriver.Chrome()
browser.get('https://courses.edx.org/login')
email = browser.find_element_by_id('login-email')
email.send_keys('xxxxx@gmail.com')
pwd = browser.find_element_by_id('login-password')
pwd.send_keys('password')
login_attempt = browser.find_element_by_xpath('//*[@id=""login""]/button')
login_attempt.submit()
</code></pre>
","9880297","7429447","2018-06-01 10:59:23","How to login/submit request through Selenium and Python","<python><selenium><selenium-webdriver><xpath><css-selectors>","3","2","551"
"50640851","2018-06-01 09:57:47","1","","<p>This:</p>

<pre><code>coin_amount = [Portfolio.objects.filter(user=request.user, coin=key['coin']).values('amount') for key in coin_sell_options]
</code></pre>

<p>does NOT ""returns a ValuesQuerySet"", it returns a <code>list</code> of <code>ValuesQuerySet</code>. What you want is the <code>__in</code> lookup operator:</p>

<pre><code>coins = [key['coin'] for key in coin_sell_options]
coin_amount = list(Portfolio.objects.filter(user=request.user, coin__in=coins).values_list('amount', flat=True))
coin_amount = [str(x) for x in coin_amount]

print(coin_amount)
</code></pre>
","41316","3219210","2018-06-01 11:12:10","0","581","bruno desthuilliers","2008-11-27 10:40:14","57630","7809","2022","2444","50638902","50640851","2018-06-01 08:10:17","0","2180","<p>I have the following data which I want to pass to JsonResponse.</p>

<pre><code>coin_amount = [Portfolio.objects.filter(user = request.user, coin = key['coin']).values('amount') for key in coin_sell_options]

print(list(coin_amount))
</code></pre>

<p>However this returns a <a href=""https://docs.djangoproject.com/en/dev/ref/models/querysets/#django.db.models.query.QuerySet.values"" rel=""nofollow noreferrer"">ValuesQuerySet</a>, which is not Json serializable:</p>

<pre><code>[&lt;QuerySet [{'amount': Decimal('3.0000000')}]&gt;, &lt;QuerySet [{'amount': 
Decimal('0.1000000')}]&gt;, &lt;QuerySet [{'amount': Decimal('9.0000000')}]&gt;]
</code></pre>

<p>That's problematic because I need a list that is JSON serializable.</p>

<p>So I need to get a list like this from my ValuesQuerySet somehow:</p>

<pre><code>['3.0000000', '0.1000000', '9.0000000']
</code></pre>
","3219210","","","Django: ""Object of type 'QuerySet' is not JSON serializable""","<python><django><django-models>","1","6","872"
"50640864","2018-06-01 09:58:23","1","","<pre><code>                            OpenCV - BGR and Matplotlib - RGB
</code></pre>

<p><strong>OpenCV:</strong> </p>

<p><a href=""https://docs.opencv.org/2.4/doc/tutorials/introduction/display_image/display_image.html"" rel=""nofollow noreferrer"">https://docs.opencv.org/2.4/doc/tutorials/introduction/display_image/display_image.html</a></p>

<p><a href=""https://i.stack.imgur.com/AJz6F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AJz6F.png"" alt=""enter image description here""></a></p>

<p><strong>Matplotlib:</strong> </p>

<p><a href=""https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html"" rel=""nofollow noreferrer"">https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html</a></p>

<p><a href=""https://i.stack.imgur.com/umb96.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/umb96.png"" alt=""enter image description here""></a></p>
","5594712","","","0","895","MedImage","2015-11-23 10:14:29","451","132","51","6","38598118","38907583","2016-07-26 19:14:17","12","8397","<p>Why is there a difference in the output image when calling the same image using <code>plt.show</code> &amp; <code>cv2.imshow()</code>?</p>

<p>Here is my code:</p>

<pre><code>import cv2
import numpy as np
from matplotlib import pyplot as plt

src=cv2.imread('fruits1.jpg') # Source image

plt.subplot(211),plt.imshow(src),plt.title('image')
plt.xticks([]),plt.yticks([])
plt.show()

cv2.imshow('image',src)
cv2.waitKey(0)
cv2.destroyWindow()
</code></pre>

<p>Here is the image from <code>plt.show</code>:</p>

<p><a href=""https://i.stack.imgur.com/gKjWJ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/gKjWJ.png"" alt=""image output for plt.show""></a></p>

<p>and the second one is the original image:</p>

<p><a href=""https://i.stack.imgur.com/0XGun.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/0XGun.jpg"" alt=""image output from cv2.show""></a></p>

<p>Is there some modification required with the <code>plt.show()</code>?</p>
","5642723","7851470","2018-06-01 11:03:08","Difference between plt.show and cv2.imshow?","<python><opencv><matplotlib><image-processing>","2","0","953"
"50640923","2018-06-01 10:01:02","1","","<p>try this,</p>

<pre><code>ids=df['ID'].unique()
t= ids[:int(round(len(ids)*0.60))]

train=df[df['ID'].isin(t)]
test=df[~df['ID'].isin(t)]
</code></pre>

<p>Input:</p>

<pre><code>    Row  ID  AGE GENDER  TIME  CODE
0     0   1   66      M     1     0
1     1   1   66      M     2     0
2     2   1   66      M     3     1
3     3   2   20      F     1     0
4     4   2   20      F     2     0
5     5   2   20      F     3     0
6     6   2   20      F     4     0
7     7   3   18      F     1     0
8     8   3   18      F     2     0
9     9   3   18      F     3     0
10   10   3   18      F     4     1
</code></pre>

<p>Output:</p>

<p>Train:</p>

<pre><code>   Row  ID  AGE GENDER  TIME  CODE  flag
0    0   1   66      M     1     0     0
1    1   1   66      M     2     0     0
2    2   1   66      M     3     1     0
3    3   2   20      F     1     0     1
4    4   2   20      F     2     0     1
5    5   2   20      F     3     0     1
6    6   2   20      F     4     0     1
</code></pre>

<p>Test:</p>

<pre><code>   Row  ID  AGE GENDER  TIME  CODE  flag
7     7   3   18      F     1     0     2
8     8   3   18      F     2     0     2
9     9   3   18      F     3     0     2
10   10   3   18      F     4     1     2
</code></pre>
","4684861","","","0","1262","Mohamed Thasin ah","2015-03-18 10:49:38","4803","833","1351","258","50640267","50640923","2018-06-01 09:28:57","0","55","<p>I have a data frame as following, and I need to split it into training and test set in a way that if I have one specific ID in train it should not be repeated in test set.</p>

<pre><code>   Row  ID  AGE GENDER  TIME  CODE
    0    1   66      M     1     0
    1    1   66      M     2     0
    2    1   66      M     3     1
    3    2   20      F     1     0
    4    2   20      F     2     0
    5    2   20      F     3     0
    6    2   20      F     4     0
    7    3   18      F     1     0
    8    3   18      F     2     0
    9    3   18      F     3     0
    10   3   18      F     4     1
</code></pre>

<p>the desired output in training set should be like this</p>

<pre><code>  Row   ID  AGE GENDER  TIME  CODE
    0    1   66      M     1     0
    1    1   66      M     2     0
    2    1   66      M     3     1
    3    2   20      F     1     0
    4    2   20      F     2     0
    5    2   20      F     3     0
    6    2   20      F     4     0
</code></pre>

<p>and test set should be like</p>

<pre><code>   Row   ID  AGE GENDER  TIME  CODE
    0    3   18      F     1     0
    1    3   18      F     2     0
    2    3   18      F     3     0
    3    3   18      F     4     1
</code></pre>

<p>how is it possible doing this in pandas python?</p>

<p>Thanks in advance</p>
","8133917","8133917","2018-06-01 09:52:29","how to split/partition data set into training and test set when data is in the shape of rows for each group and not one row for each group","<python><pandas><dataframe>","1","4","1314"
"50640929","2018-06-01 10:01:19","1","","<p>It's example how to write Scrapy pileline in Django project.</p>

<pre><code>from &lt;YOU_APP_NAME&gt;.models import detikNewsItem

class DetikAppPipeline(object):
    def process_item(self, item, spider):
        d, created = detikNewsItem.objects.get_or_create(breadcrumbs=item['breadcrumbs'], url=item['url'])
        if created:        
            d.tanggal = item['tanggal']
            d.penulis = item['penulis']
            d.judul = item['judul']
            d.berita = item['berita']
            d.tag = item['tag']
            d.save()

        return item
</code></pre>

<p>By the way you need to run Scrapy in Django environment. There are several ways to do that:</p>

<ol>
<li><p>Using <code>django-extensions</code> module.
Need to create new file:</p>

<p><code>&lt;DJANG_PROJECT&gt;/scripts/__init__.py</code>
<code>&lt;DJANG_PROJECT&gt;/scripts/run_scrapy.py</code></p></li>
</ol>

<p>With code inside:</p>

<pre><code>from scrapy.cmdline import execute

execute(['run_scrapy.py', 'crawl', 'detik'])
</code></pre>

<ol start=""2"">
<li><p>Another way is to use Django Managment. Need to create folders in project with file:</p>

<p><code>&lt;folder_of_app&gt;/management/commands/__init__.py</code>
<code>&lt;folder_of_app&gt;/management/commands/scrapy.py</code>
<code>scrapy.py</code> file should contain code:</p>

<p>from scrapy.cmdline import execute</p>

<p>from django.core.management.base import BaseCommand</p>

<p>class Command(BaseCommand):
    help = 'Run scrapy.'</p>

<pre><code>def add_arguments(self, parser):
    parser.add_argument('arguments', nargs='+', type=str)

def handle(self, *args, **options):
    args = []
    args.append('scrapy.py')
    args.extend(options['arguments'])
    execute(args)
</code></pre></li>
</ol>

<p>It allow to run Scrapy in Django environment like this:</p>

<pre><code>python manage.py scrapy crawl detik
python manage.py scrapy shell 'https://news.detik.com/indeks/all/?date=02/28/2018'
</code></pre>
","5909920","9214835","2019-06-12 21:48:07","3","1975","Oleg T.","2016-02-10 18:13:06","150","28","27","0","50637920","","2018-06-01 07:09:34","3","3304","<p>I'm still very new in Django
I am following <a href=""https://medium.com/@ali_oguzhan/how-to-use-scrapy-with-django-application-c16fabd0e62e"" rel=""nofollow noreferrer"">this</a> tutorial on how to integrate scrapy and django.</p>

<p>the problem is when i am trying to use my own spider it's just wont work.
I have tried the spider outside django and it's work just fine, some help will be very helpful.</p>

<p>This is my spider.py file</p>

<pre><code>import scrapy
from scrapy_splash import SplashRequest

class NewsSpider(scrapy.Spider):
   name = 'detik'
   allowed_domains = ['news.detik.com']
   start_urls = ['https://news.detik.com/indeks/all/?date=02/28/2018']

def parse(self, response):  
    urls = response.xpath(""//div/article/a/@href"").extract()        
    for url in urls:
        url = response.urljoin(url)
        yield scrapy.Request(url=url, callback=self.parse_detail)

    # follow pagination link
    page_next =   response.xpath(""//a[@class = 'last']/@href"").extract_first()
    if page_next:
        page_next = response.urljoin(page_next)
        yield scrapy.Request(url=page_next, callback=self.parse)

def parse_detail(self,response):
    x = {}
    x['breadcrumbs'] = response.xpath(""//div[@class='breadcrumb']/a/text()"").extract(),
    x['tanggal'] = response.xpath(""//div[@class='date']/text()"").extract_first(),
    x['penulis'] = response.xpath(""//div[@class='author']/text()"").extract_first(),
    x['judul'] = response.xpath(""//h1/text()"").extract_first(),
    x['berita'] = response.xpath(""normalize-space(//div[@class='detail_text'])"").extract_first(),
    x['tag'] = response.xpath(""//div[@class='detail_tag']/a/text()"").extract(),
    x['url'] = response.request.url,
    return x
</code></pre>

<p>this is my pipeline file </p>

<pre><code>class DetikAppPipeline(object):

def process_item(self, item, spider):
    item = detikNewsItem()
    self.items.append(item['breadcrumbs'])
    self.items.append(item['tanggal'])
    self.items.append(item['penulis'])
    self.items.append(item['judul'])
    self.items.append(item['berita'])
    self.items.append(item['tag'])
    self.items.append(item['url'])
    item.save()
</code></pre>

<p>And this is the models file in django </p>

<pre><code>class detikNewsItem(models.Model):
    breadcrumbs = models.TextField()
    tanggal = models.TextField()
    penulis = models.TextField()
    judul = models.TextField()
    berita = models.TextField()
    tag = models.TextField()
    url = models.TextField()

    @property
    def to_dict(self):
    data = {
        'url': json.loads(self.url),
        'tanggal': self.tanggal
    }
    return data

    def __str__(self):
        return self.url
</code></pre>
","9749845","","","Integrate Scrapy with Django : How to","<python><django><scrapy>","1","1","2701"
"50640931","2018-06-01 10:01:28","5","","<p>Using <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/top_k"" rel=""nofollow noreferrer""><code>tf.nn.top_k()</code></a>:</p>

<pre class=""lang-python prettyprint-override""><code>top_k_values, top_k_indices = tf.nn.top_k(predictions, k=k)
</code></pre>

<p>If <code>predictions</code> is a vector of probabilities per class (i.e. <code>predictions[i] = prediction probability for class i</code>), then <code>top_k_values</code> will contain the <code>k</code> highest probabilities in <code>predictions</code>, and <code>top_k_indices</code> will contain the indices of these probabilities, i.e. the corresponding classes.</p>

<hr>

<p>Supposing that in your code, <code>y_</code> is the vector of predicted probabilities per class:</p>

<pre><code>k = 3  # replace with your value
# Instead of `y_pred`:
y_k_probs, y_k_pred = sess.run(
    tf.nn.top_k(y_, k=k), feed_dict={X: ts_features})
</code></pre>
","624547","624547","2018-06-01 14:56:28","3","916","benjaminplanche","2011-02-19 17:06:24","8823","606","381","76","50640687","50640931","2018-06-01 09:48:32","1","708","<p>I am relatively new in machine learning especially when it comes to implementing algorithms. I am using python and tensorflow library to implement a neural network to train on a dataset which has about 20 classes. I am able to train and get predictions successfully but I have a question, </p>

<blockquote>
  <p>Is it possible to get top k classes along with their probabilities using tensorflow instead of just a single prediction?</p>
</blockquote>

<p>If it is possible how can this be done? Thanks for your guidance.</p>

<p><strong>Update 01:</strong>
I am adding code of what I am doing. So I build a neural network with 3 layers having tanh, sigmoid, &amp; sigmoid respectively as activation functions for the hidden layers and softmax for output layer. The code for training and prediction is as follows:</p>

<pre><code>y_pred = None
    with tf.Session() as sess:
        sess.run(init)
        for epoch in range(training_epochs):            
            # running the training_epoch numbered epoch
            _,cost = sess.run([optimizer,cost_function],feed_dict={X:tr_features,Y:tr_labels})
            cost_history = np.append(cost_history,cost)
        # predict results based on the trained model
        y_pred = sess.run(tf.argmax(y_,1),feed_dict={X: ts_features})
</code></pre>

<p>Right now y_pred is a list of class labels for each test example of ts_features. But instead of getting 1 single class label for each test example I am hoping to get top-k predictions for each example each of the k-predictions accompanied by some kind of probability.</p>
","2555668","2555668","2018-06-01 14:44:39","Get top-k predictions from tensorflow","<python><tensorflow>","1","0","1578"
"50640942","2018-06-01 10:02:06","0","","<p>crop them with a small padding to prevent border effects and merge them back. you can see how this is done over here, line 552.</p>

<p><a href=""https://github.com/lopuhin/kaggle-dstl/blob/master/train.py"" rel=""nofollow noreferrer"">https://github.com/lopuhin/kaggle-dstl/blob/master/train.py</a></p>
","1662574","","","0","303","Zaw Lin","2012-09-11 10:27:28","4683","666","121","21","50640623","","2018-06-01 09:45:13","0","256","<p>I am trying to implement object detection on satellite images. I have a annotated dataset, but the images are large and the model accepts only 416 x 416 size inputs. How can I pass small parts of the image in the network, ensuring that the annotations are retained. Also, how to merge these results at test time?</p>
","6455600","","","How to pass small cutout parts of a large image to train a CNN for object detection and localization(satellite images)","<python><deep-learning><computer-vision><satellite-image>","1","3","320"
"50640969","2018-06-01 10:03:03","1","","<p>Try to use loc instead:</p>

<pre><code>import pandas as pd
df=pd.DataFrame(np.arange(15).reshape(5,3), columns=['a0','a1','a2'])
dg=pd.DataFrame(np.arange(9).reshape(3,3), columns=['b0','b1','b2'])
print('df=', df)
print('\ndg=', dg)

#replacement of [5,8,11] by [1,4,7]
df.loc[1:3, 'a2']=dg.b1.values
print(""\ndf (after replacement) \n "",df)
df=    a0  a1  a2
0   0   1   2
1   3   4   5
2   6   7   8
3   9  10  11
4  12  13  14

dg=    b0  b1  b2
0   0   1   2
1   3   4   5
2   6   7   8

df (after replacement) 
     a0  a1  a2
0   0   1   2
1   3   4   1
2   6   7   4
3   9  10   7
4  12  13  14
</code></pre>
","9868976","","","1","621","DTT","2018-05-30 08:31:33","19","9","0","0","50640038","","2018-06-01 09:16:55","0","66","<p>I'm trying to replace the values in the rows 500 to 750 in column <code>col1</code> in dataframe <code>df_A</code> with the values column <code>col1</code> of dataframe <code>df_B</code> (with altogether 250 rows) in Python Pandas.</p>

<p>I tried doing it like this</p>

<pre><code>df_A.col1.iloc[500:750] = df_B.col1
</code></pre>

<p>But this yields the notorious</p>

<pre><code>A value is trying to be set on a copy of a slice from a DataFrame
</code></pre>

<p>and the values in <code>df_A.col1.iloc[500:750]</code> get replaced by <code>NaN</code>s . So how can I do this kind of replacement of several rows with rows from another dataframe in Pandas without using a for-loop?</p>
","179014","","","Replace a subset of rows of dataframe A with rows of dataframe B in python pandas","<python><pandas><dataframe>","1","2","691"
"50640990","2018-06-01 10:04:07","0","","<p>Don't have a raspberry pi to hand... could try this, works with keyboard input in ipython. </p>

<pre><code>try:
    while True:
    # i=int(input('input number: '))
    i=int(i=GPIO.input(18))
        if i!=1:
            print(""No intruder"")
        else:
            print(""Intruder"")
        time.sleep(60)
</code></pre>
","5888317","","","0","328","matman9","2016-02-05 13:22:37","96","8","54","0","50639809","50640991","2018-06-01 09:04:45","0","259","<p>Basically I am working on PIR sensor, when intruder is detected it goes to 1 min of sleep time. I want to reset this sleep time when  the intruder is detected during sleep time.
Below is the code:  </p>

<pre><code>`import RPi.GPIO as GPIO
import time

GPIO.setmode(GPIO.BCM)
GPIO.setup(18,GPIO.IN)

try:
    while True:
        i=GPIO.input(18)
        if i==1:
            print(""Intruder"")
            time.sleep(60)
        elif i==0:
            print(""No intruder"")
            time.sleep(60)
except KeyboardInterrupt:
    GPIO.cleanup()
    exit(0)`
</code></pre>
","8449495","8449495","2018-06-01 09:50:29","Reset sleep time after every interrupt in python","<python><raspberry-pi><iot>","2","2","574"
"50640991","2018-06-01 10:04:08","1","","<p>Here's a solution using thread:</p>

<pre><code>from threading import Thread, Event
import time

import RPi.GPIO as GPIO


class MyThread(Thread):
    def __init__(self, timeout=60):
        super(MyThread, self).__init__()
        self.intruder_spotted = Event()
        self.timeout = timeout

        self.daemon = True

    def run(self):
        while True:
            if self.intruder_spotted.wait(self.timeout):
                self.intruder_spotted.clear()
                print(""Intruder"")
            else:
                print(""No intruder"")



t = MyThread(60)

GPIO.setmode(GPIO.BCM)
GPIO.setup(18,GPIO.IN)

try:
    t.start()
    while True:
        i=GPIO.input(18)
        if i==1:
            t.intruder_spotted.set()

        time.sleep(1)

except KeyboardInterrupt:
    GPIO.cleanup()
    exit(0)
</code></pre>
","7529716","7529716","2018-06-12 09:14:37","3","835","Yassine Faris","2017-02-07 15:47:46","735","56","111","96","50639809","50640991","2018-06-01 09:04:45","0","259","<p>Basically I am working on PIR sensor, when intruder is detected it goes to 1 min of sleep time. I want to reset this sleep time when  the intruder is detected during sleep time.
Below is the code:  </p>

<pre><code>`import RPi.GPIO as GPIO
import time

GPIO.setmode(GPIO.BCM)
GPIO.setup(18,GPIO.IN)

try:
    while True:
        i=GPIO.input(18)
        if i==1:
            print(""Intruder"")
            time.sleep(60)
        elif i==0:
            print(""No intruder"")
            time.sleep(60)
except KeyboardInterrupt:
    GPIO.cleanup()
    exit(0)`
</code></pre>
","8449495","8449495","2018-06-01 09:50:29","Reset sleep time after every interrupt in python","<python><raspberry-pi><iot>","2","2","574"
"50641002","2018-06-01 10:04:41","0","","<p>Use the <code>newline</code> argument to <code>open()</code>.</p>

<p>Nearly a duplicate of <a href=""https://stackoverflow.com/questions/14202438/dont-convert-newline-when-reading-a-file"">Don&#39;t convert newline when reading a file</a>, from where I got this solution:</p>

<pre><code>with open(sys.argv[1], 'r', newline='\n') as fh:
    for i, line in enumerate(fh):
        print(i, line)
</code></pre>

<p>(Be aware that, when printing as in this example, the <code>^M</code> (<code>'\r'</code>) character will put the current point at the start of a line, overwriting existing characters.)</p>
","9769953","","","0","603","0 0","2018-05-10 09:37:25","2583","545","58","441","50640627","","2018-06-01 09:45:32","-1","25","<p>I have a file that contains text to generate LaTeX mathematical expressions, one per line. This file should contain exactly 103,559 lines. But some lines contain the character sequence '^M' (CTRL-v CTRL-m) either at the end or interspersed within the lines, possibly multiple times. As a result, when I try to read the lines from the file using Python, the number of lines returned is greater than expected (actually returns 104,654 lines).</p>

<p>How do I tell Python to not generate a newline on each occurrence of the sequence '^M'? Thank you.</p>
","6636972","","","Treat '^M' as regular characters in Python","<python><python-3.x>","1","8","555"
"50641027","2018-06-01 10:05:53","2","","<p>Install the missing packages:</p>

<pre><code>FROM python:3

ADD main.py /
RUN apt-get update
RUN apt-get install -y libhunspell-1.3-0
RUN pip install cyhunspell

CMD [ ""python"", ""main.py"" ]
</code></pre>
","6603816","","","1","208","yamenk","2016-07-18 13:25:55","18981","552","60","39","50640875","50641027","2018-06-01 09:58:48","1","337","<p>When I run the program following script:</p>

<pre><code>from hunspell import Hunspell

if __name__ == '__main__':
    h = Hunspell()
    print(h.spell('test'))
</code></pre>

<p>On local machine everything is OK but when as soon as I build and run the code on docker, it throws following exception:</p>

<pre><code>    from hunspell import Hunspell
  File ""/usr/local/lib/python3.6/site-packages/hunspell/__init__.py"", line 3, in &lt;module&gt;
    from .hunspell import HunspellWrap as Hunspell
ImportError: libhunspell-1.3.so.0: cannot open shared object file: No such file or directory
</code></pre>

<p>My Dockerfile is something like:</p>

<pre><code>FROM python:3

ADD main.py /

RUN pip install cyhunspell

CMD [ ""python"", ""main.py"" ]
</code></pre>

<p><a href=""https://github.com/OpenGov/cython_hunspell"" rel=""nofollow noreferrer"">Hunspell</a> use c++ binary files that I think causing such exception.</p>

<p>Does anybody know how to fix this issue? Is it necessary to use Linux for base image in Dockerfile?</p>
","1462770","","","Cannot open shared object file when using docker","<python><python-3.x><docker><hunspell>","1","0","1026"
"50641035","2018-06-01 10:06:04","3","","<blockquote>
  <p>'CaptureManager' object has no attribute 'suspendcapture'</p>
</blockquote>

<p>That's true. The methods are now called <a href=""https://github.com/pytest-dev/pytest/blob/eaa882f3d5340956beb176aa1753e07e3f3f2190/src/_pytest/capture.py#L99"" rel=""nofollow noreferrer"">""global""</a>: <code>start_global_capturing</code>, <code>stop_global_capturing</code>, <code>resume_global_capture</code>, <code>suspend_global_capture</code>, ``.</p>
","7976758","","","0","452","phd","2017-05-07 15:40:03","32575","3331","2903","3411","50638571","50641035","2018-06-01 07:49:52","1","335","<p>I have the following function called from a pytest.</p>

<pre><code>def ask_user_input(msg=''):
    """""" Asks user to check something manually and answer a question """"""
    notification = ""\n\n???\tANSWER NEEDED\t???\n\n{}"".format(msg)

    # suspend input capture by py.test so user input can be recorded here
    capture_manager = pytest.config.pluginmanager.getplugin('capturemanager')
    capture_manager.suspendcapture(in_=True)

    answer = input(notification)

    # resume capture after question have been asked
    capture_manager.resumecapture()

    logging.debug(""Answer: {}"".format(answer))
    return answer
</code></pre>

<p>However, I get the following error:
<strong>Error: AttributeError: 'CaptureManager' object has no attribute 'suspendcapture'</strong></p>

<p>I'm using Python 3.6+. How can I use CaptureManager as pytest.config in the above example seems to no longer exists. </p>
","5246980","5246980","2018-06-01 08:19:53","In PyTest how to config CaptureManager plugin error 'CaptureManager' object has no attribute 'suspendcapture'","<python><python-3.x><pytest>","1","0","907"
"50641082","2018-06-01 10:08:32","0","","<p>You are creating too many connections to the same file and there is an operating system limit on this type of operations.</p>

<p>try unnesting your script as much as possible</p>

<pre><code>    new_list = []
    with open(""file2"",'rU') as gg:
        for g in gg:
            g = g.rstrip().split('\t')
            with open(file1) as cc:
                c = cc.rstrip().split('\t')
                if int(c[0]) == int(g[0]) and int(c[1]) &gt;= int(g[2]) and int(g[3]) &gt;= int(c[1]):
                    new_list.append(c[1]+'\t'+'\t'.join(g)+'\n')

    with open('output.txt', 'a') as ii:
        for e in new_list:
                    ii.write(e)
</code></pre>
","9875315","9875315","2018-06-01 10:12:32","1","670","Dataichou","2018-05-31 10:09:04","307","12","6","3","50640883","","2018-06-01 09:58:55","0","28","<p>Scenario:</p>

<p>I have two files, <em>file1 size = 19.7MB</em> and <em>file2 size = 446KB</em>. I am running the following code to process the data from both files and get an output data file. But after a certain output file size (332KB) the program stops writing data to the output file. I tried using <code>flush()</code> function but again the output file contains the exact same size as output file without using <code>flush()</code> function (and took exact same time to write these data in both conditions (file created and last modifed)) while loop is still running. </p>

<p>Please someone suggest the potential reason(s)? Should I use <code>sleep()</code> function to wake up the program after a certain time? Thanks </p>

<pre><code>with open(""file2"",'rU') as gg:
    for g in gg:
        g = g.rstrip().split('\t')
        with open(file1) as cc:
            c = c.rstrip().split('\t')
                if int(c[0]) == int(g[0]) and int(c[1]) &gt;= int(g[2]) and int(g[3]) &gt;= int(c[1]):
                    with open('output.txt', 'a') as ii:
                        ii.write(c[1]+'\t'+'\t'.join(g)+'\n')
                        ii.flush()
</code></pre>
","3698773","3032364","2018-06-01 10:19:40","function is not able to properly write output data file - Python","<python><flush>","1","3","1172"
"50641094","2018-06-01 10:09:16","2","","<p>To extract data from a tensor into a python-variable use </p>

<p><code>label = sess.run(y_pred_cls)</code></p>

<p>This will give you an array for a one-hot-vector label or an int variable for a scalar label.</p>

<p>To save arrays to images you can use the PIL-library</p>

<pre><code>from PIL import Image
img = Image.fromarray(data, 'RGB')
img.save('name.png')
</code></pre>

<p>The rest should be straight forward, </p>

<ol>
<li>extract data from your batch_tx, batch_ty and y_pred_cls tensors</li>
<li>iterate over each triplet</li>
<li>create an RGB image out of current <code>x</code></li>
<li>create a string of the form <code>name = str(y)+'_'+str(y_hat)</code></li>
<li>save your image</li>
</ol>

<p>If you have trouble applying these steps I can help you out further</p>
","9280994","9280994","2018-06-01 10:16:30","8","788","Jonathan R","2018-01-28 21:47:13","1615","128","51","6","50640254","50641094","2018-06-01 09:28:11","1","521","<p>I am trying to save the predicted images on my CNN network which I wrote with Tensorflow. In my code <code>y_pred_cls</code> contain my predicted labels and the <code>y_pred_cls</code> is a tensor of dimensions 1 x batch size. Now, I want to iterate over y_pred_cls as an array and make a file name including pred class, true class, and some index number, then find out images relate to predicted labels and use <code>imsave</code> to save as image. </p>

<pre><code>with tf.Session() as sess:
sess.run(tf.global_variables_initializer())
train_writer.add_graph(sess.graph)



print(""{} Start training..."".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
print(""{} Open Tensorboard at --logdir {}"".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), tensorboard_dir))

for epoch in range(FLAGS.num_epochs):
    print(""{} Epoch number: {}"".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), epoch + 1))
    step = 1

    # Start training
    while step &lt; train_batches_per_epoch:
        batch_xs, batch_ys = train_preprocessor.next_batch(FLAGS.batch_size)
        opt, train_acc = sess.run([optimizer, accuracy], feed_dict={x: batch_xs, y_true: batch_ys})

        # Logging
        if step % FLAGS.log_step == 0:
            s = sess.run(sum, feed_dict={x: batch_xs, y_true: batch_ys})
            train_writer.add_summary(s, epoch * train_batches_per_epoch + step)

        step += 1

    # Epoch completed, start validation
    print(""{} Start validation"".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
    val_acc = 0.
    val_count = 0
    cm_running_total = None

    for _ in range(val_batches_per_epoch):
        batch_tx, batch_ty = val_preprocessor.next_batch(FLAGS.batch_size)
        acc, loss , conf_m= sess.run([accuracy, cost, tf.confusion_matrix(y_true_cls, y_pred_cls, FLAGS.num_classes)],
                                      feed_dict={x: batch_tx, y_true: batch_ty})



        if cm_running_total is None:
            cm_running_total = conf_m
        else:
            cm_running_total += conf_m


        val_acc += acc
        val_count += 1

    val_acc /= val_count

    s = tf.Summary(value=[
        tf.Summary.Value(tag=""validation_accuracy"", simple_value=val_acc),
        tf.Summary.Value(tag=""validation_loss"", simple_value=loss)
    ])

    val_writer.add_summary(s, epoch + 1)
    print(""{} -- Training Accuracy = {:.4%} -- Validation Accuracy = {:.4%} -- Validation Loss = {:.4f}"".format(
        datetime.now().strftime('%Y-%m-%d %H:%M:%S'), train_acc, val_acc, loss))

    # Reset the dataset pointers
    val_preprocessor.reset_pointer()
    train_preprocessor.reset_pointer()

    print(""{} Saving checkpoint of model..."".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))

    # save checkpoint of the model
    checkpoint_path = os.path.join(checkpoint_dir, 'model_epoch.ckpt' + str(epoch+1))
    save_path = saver.save(sess, checkpoint_path)
    print(""{} Model checkpoint saved at {}"".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), checkpoint_path))
</code></pre>

<p>batch_tx, batch_ty is my RGB data and labels respectively.</p>

<p>Thanks in advance.</p>
","5573737","5573737","2018-06-05 15:40:19","Iterate over tensor as an array Tensorflow","<python><python-3.x><numpy><matplotlib><tensorflow>","1","6","3131"
"50641100","2018-06-01 10:09:43","0","","<p>for this you don't have to use <code>df.loc</code></p>

<pre><code>dprev = ""eiffel tower""

df[df['place'] == dprev] 
</code></pre>

<p><a href=""https://stackoverflow.com/a/17071908/7053679"">See this answer</a></p>
","7053679","7053679","2018-06-01 10:16:55","1","217","Nihal","2016-10-21 14:31:00","3964","761","1306","121","50641053","50641227","2018-06-01 10:07:35","-1","489","<p>I am trying to get the subset dataframe using the below code:</p>

<pre><code>dprev = ""eiffel tower""

df.loc[df['place'] == dprev] -&gt; returns empty

drandom = random.choice(df['place'].unique())

df.loc[df['place'] == drandom] -&gt; returns the subset
</code></pre>

<p>why am i not seeing the same thing when <code>dprev</code> is string variable?</p>
","3779226","2613005","2018-06-01 10:34:23","python pandas loc return empty dataframe","<python><pandas>","3","6","359"
"50641103","2018-06-01 10:09:53","0","","<p>The time module is your friend here. You can set up an infinite loop with <code>while True:</code> and use <code>time.sleep(secs)</code> at the end of the loop (after output).</p>
","2041983","","","0","183","Paula Thomas","2013-02-05 05:55:24","912","161","36","11","50640980","50641341","2018-06-01 10:03:40","1","244","<p>I got temperature, pressure, and altitude readings on my <code>PI</code> using a sensor:</p>

<ol>
<li>The problem is, to see the results, I have to execute the <code>code.py</code> every time by myself. I am trying to automate it somehow so it will keep running itself for the time I want. </li>
<li>Once that is automated, would like to save the results and analyze the output after some time.</li>
</ol>

<p>Is there a way I can write code for both the tasks?</p>

<p>Thank you.</p>
","9880587","5279133","2018-06-01 11:24:02","How to write an autoscript in Python?","<python><python-2.7>","3","3","489"
"50641105","2018-06-01 10:10:04","0","","<p>You are facing this problem because the image might not have been read properly while scanning. So do make sure tat image is loaded.  <br></p>

<pre><code>if input_img is not None:
            img = cv2.resize(input_img, (IMG_SIZE,IMG_SIZE))
            training_data.append([np.array(img), np.array(label)])
        else:
            print(""image not loaded"")
</code></pre>

<p><br> This skips the current image and then continue the scan. This breaks into two segments results as follows: <a href=""https://i.stack.imgur.com/FTpi1.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FTpi1.jpg"" alt=""enter image description here""></a>
<br> Hope this helps :) </p>
","8076093","","","0","682","Ashwin Dhakal","2017-05-28 01:27:41","31","23","6","0","47172770","47173472","2017-11-08 06:14:24","5","35579","<p>I am building image processing classifier the whole code in right except this line  -</p>

<p><code>input_img_resize=cv2.resize(input_img,(128,128))</code></p>

<p>This line givin me an error </p>

<pre><code>('error: /io/opencv/modules/imgproc/src/imgwarp.cpp:3483: error: (-215) ssize.width &gt; 0 &amp;&amp; ssize.height &gt; 0 in function resize')
</code></pre>

<p>My code - </p>

<pre><code>PATH = os.getcwd()
# Define data path
data_path = PATH + '/data'
data_dir_list = os.listdir(data_path)

img_rows=128
img_cols=128
num_channel=3
num_epoch=30

num_classes = 67

img_data_list=[]

for dataset in data_dir_list:
    img_list=os.listdir(data_path+'/'+ dataset)
    print ('Loaded the images of dataset-'+'{}\n'.format(dataset))
    for img in img_list:
        input_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )

        input_img_resize=cv2.resize(input_img,(128,128))
        img_data_list.append(input_img_resize)
</code></pre>
","7993917","8927035","2019-09-02 18:13:39","error: (-215) ssize.width > 0 && ssize.height > 0 in function resize","<python><opencv><machine-learning><image-processing><image-resizing>","5","4","950"
"50641125","2018-06-01 10:11:19","0","","<p>A solution with <code>.format()</code> is the following:</p>

<pre><code>items_ids = tuple([3, 2])
items_placeholders = ', '.join(['{}'] * len(items_ids))

sql = ""SELECT * FROM items WHERE item_id IN {} ORDER BY FIELD(item_id, {});"".format(items_ids, items_placeholders).format(*items_ids)

# with `.format(items_ids, items_placeholders)` you get this: SELECT * FROM items WHERE item_id IN (3, 2) ORDER BY FIELD(item_id, {}, {});
# and then with `.format(*items_ids)` you get this: SELECT * FROM items WHERE item_id IN (3, 2) ORDER BY FIELD(item_id, 3, 2);
</code></pre>

<p>A rather tricky solution with <code>f-strings</code> is the following:</p>

<pre><code>sql1 = f""SELECT * FROM items WHERE item_id IN {item_ids} ORDER BY FIELD(item_id, ""
sql2 = f""{items_ids};""
sql = sql1 + sql2[1:]

# SELECT * FROM items WHERE item_id IN (3, 2) ORDER BY FIELD(item_id, 3, 2);
</code></pre>

<p>But as <code>@IIija</code> mentions,  I may get a <code>SQL injection</code> with it because <code>IN {item_ids}</code> cannot  accommodate one-element tuples as such.</p>

<p>Additionally, using <code>f-strings</code> to unpack tuples in strings is perhaps more difficult than using <code>.format()</code> as others have mentioned before (<a href=""https://stackoverflow.com/questions/38763895/formatted-string-literals-in-python-3-6-with-tuples?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa"">Formatted string literals in Python 3.6 with tuples</a>) since you cannot use <code>*</code> to unpack a tuple within a <code>f-string</code>. However, perhaps you may come up with a solution for this (which is using a iterator?) to produce this </p>

<pre><code>sql = f""SELECT * FROM items WHERE item_id IN ({t[0]}, {t[1]}) ORDER BY FIELD(item_id, {t[0]}, {t[1]});""
</code></pre>

<p>even though I do not have the solution for this in my mind right now. You are welcome to post a solution of this kind if you have it in your mind.</p>
","9024698","9024698","2018-06-01 11:00:24","5","1953","Poete Maudit","2017-11-29 09:11:29","1748","540","462","48","50625183","50641125","2018-05-31 13:11:53","0","922","<p>I am using <code>Python</code> and <code>PyMySQL</code>. I want to fetch a number of items from a  MySQL database according to their ids:</p>

<pre><code>items_ids = tuple([3, 2])
sql = f""SELECT * FROM items WHERE item_id IN {items_ids};""
</code></pre>

<p>I am using the formatted string literals (<code>f"" ""</code>, <a href=""https://docs.python.org/3/whatsnew/3.6.html#whatsnew36-pep498"" rel=""nofollow noreferrer"">https://docs.python.org/3/whatsnew/3.6.html#whatsnew36-pep498</a>) to evaluate the tuple inside the SQL statement.</p>

<p>However,I want to get back the items in the order specified by the tuple so firstly the item with <code>item_id = 3</code> and then the item with <code>item_id = 2</code>. To accomplish this I have to use the <code>ORDER BY FIELD</code> clause (see also here: <a href=""https://stackoverflow.com/questions/396748/ordering-by-the-order-of-values-in-a-sql-in-clause?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa"">Ordering by the order of values in a SQL IN() clause</a>). </p>

<p>But if I write something like this:</p>

<pre><code>items_ids = tuple([3, 2])
sql = f""SELECT * FROM items WHERE item_id IN {items_ids} ORDER BY FIELD{(item_id,) + items_ids};""
</code></pre>

<p>then <code>item_id</code> in the <code>ORDER BY FIELD</code> clause is considered as an undeclared python variable</p>

<p>and if I write something like this:</p>

<pre><code>items_ids = tuple([3, 2])
sql = f""SELECT * FROM items WHERE item_id IN {items_ids} ORDER BY FIELD{('item_id',) + items_ids};""
</code></pre>

<p>then <code>item_id</code> in the <code>ORDER BY FIELD</code> clause is considered as a string and not as a SQL variable and in this case <code>ORDER BY FIELD</code> does not do anything. </p>

<p><strong>How can I evaluate the tuple <code>(item_id,) + items_ids</code> in the SQL statement by maintaining <code>item_id</code> as a SQL variable in the <code>ORDER BY FIELD</code> clause?</strong></p>

<p>Obviously I can sort the items after they have returned from the database according to <code>items_ids</code> and without bothering so much with MySQL but I was just wondering how to do this.</p>
","9024698","9024698","2018-06-01 10:11:51","Unpack tuple within SQL statement","<python><mysql>","2","2","2168"
"50641161","2018-06-01 10:13:07","0","","<p>I managed to create a plot with an Italian map.
I downloaded the borders of Italy from github (<a href=""https://raw.githubusercontent.com/pasqualemauriello/italy/master/json/regioni.json"" rel=""nofollow noreferrer"">region borders</a> or <a href=""https://raw.githubusercontent.com/pasqualemauriello/italy/master/json/province.json"" rel=""nofollow noreferrer"">province borders</a>).
Then I created a dataframe containing 3 columns: [Province (or region), x_bord, y_bord].
In the end I used it as a ColumnDataSource and create a Figure with patches.</p>

<p>Suggestion: if you want to improve your render, you can add a hover tool creating a new column with desired values (e.g. number of inhabitants). Moreover you can create new column containing rgb values (RGB(r,g,b)) if you want different colors for each province or region.</p>

<pre><code>source= ColumnDataSource(dict(x=italy.x_bord,y=italy.y_bord,prov=italy.prov,val=italy.val,colors=italy.colors))
f = figure(...)
f.patches(xs='x',ys='y',source=source, fill_color='colors')
</code></pre>
","9591828","9591828","2018-06-01 15:40:38","0","1047","yami","2018-04-03 14:55:09","11","5","0","0","41932493","","2017-01-30 09:28:01","1","404","<p>I need to use Bokeh to plot datas on Italian map.</p>

<p>To explain, something similar with:</p>

<p><a href=""http://docs.bokeh.org/en/latest/docs/gallery/texas.html"" rel=""nofollow noreferrer"">http://docs.bokeh.org/en/latest/docs/gallery/texas.html</a></p>

<p>... but using italian provinces instead of Texas counties.</p>

<p>Can you help me pointing in the right direction?
Other tools suggested?</p>

<p>Thanks in advance, Gianluca</p>
","2849150","3406693","2019-10-14 04:37:44","Bokeh for presenting data on Italy map","<python><geolocation><geospatial><bokeh>","2","3","444"
"50641173","2018-06-01 10:13:58","1","","<p>Expanding on the comments to an answer; the TimeDistributed layer applies the given layer to every <em>time step</em> of the input. Hence, your TimeDistributed would apply to every frame giving an input <code>shape=(F_NUM, W, H, C)</code>. After applying the convolution to every image, you get back <code>(F_NUM, 64)</code>  which are features for every frame.</p>
","9758922","","","0","369","nuric","2018-05-08 13:57:46","6956","503","353","45","50626630","50641173","2018-05-31 14:25:01","0","180","<p>I'm working on a classifier for video sequences. It should take several video frames on input and output a label, either 0 or 1. So, it is a many-to-one network.</p>

<p>I already have a classifier for single frames. This classifier makes several convolutions with <code>Conv2D</code>, then applies <code>GlobalAveragePooling2D</code>. This results in 1D vector of length 64. Then original per-frame classifier has a <code>Dence</code> layer with softmax activation.</p>

<p>Now I would like to extend this classifier to work with sequences. Ideally, sequences should be of varying length, but for now I fix the length to 4.</p>

<p>To extend my classifier, I'm going to replace <code>Dense</code> with an LSTM layer with 1 unit. So, my goal is to have the LSTM layer to take several 1D vectors of length 64, one by one, and output a label. </p>

<p>Schematically, what I have now:</p>

<pre><code>input(99, 99, 3) - [convolutions] - features(1, 64) - [Dense] - [softmax] - label(1, 2)
</code></pre>

<p>Desired architecture: </p>

<pre><code>4x { input(99, 99, 3) - [convolutions] - features(1, 64) } - [LSTM] - label(1, 2)
</code></pre>

<p>I cannot figure out, how to do it with Keras.</p>

<p>Here is my code for convolutions </p>

<pre><code>from keras.layers import Conv2D, BatchNormalization, GlobalAveragePooling2D, \
LSTM, TimeDistributed

IMAGE_WIDTH=99
IMAGE_HEIGHT=99
IMAGE_CHANNELS=3

convolutional_layers = Sequential([
    Conv2D(input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS),
           filters=6, kernel_size=(3, 3), strides=(2, 2), activation='relu',
           name='conv1'),
    BatchNormalization(),
    Conv2D(filters=64, kernel_size=(1, 1), strides=(1, 1), activation='relu',
           name='conv5_pixel'),
    BatchNormalization(),
    GlobalAveragePooling2D(name='avg_pool6'),
])
</code></pre>

<p>Here is the summary: </p>

<pre><code>In [24]: convolutional_layers.summary()
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv1 (Conv2D)               (None, 49, 49, 6)         168
_________________________________________________________________
batch_normalization_3 (Batch (None, 49, 49, 6)         24
_________________________________________________________________
conv5_pixel (Conv2D)         (None, 49, 49, 64)        448
_________________________________________________________________
batch_normalization_4 (Batch (None, 49, 49, 64)        256
_________________________________________________________________
avg_pool6 (GlobalAveragePool (None, 64)                0
=================================================================
Total params: 896
Trainable params: 756
Non-trainable params: 140
</code></pre>

<p>Now I want a recurrent layer to process sequences of these 64-dimensional vectors and output a label for each sequence.</p>

<p>I've read in manuals that <code>TimeDistributed</code> layer applies its input layer to every time slice of the input data.</p>

<p>I continue my code: </p>

<pre><code>FRAME_NUMBER = 4

td = TimeDistributed(convolutional_layers, input_shape=(FRAME_NUMBER, 64))
model = Sequential([
    td,
    LSTM(units=1)
])
</code></pre>

<p>Result is the exception <code>IndexError: list index out of range</code></p>

<p>Same exception for </p>

<pre><code>td = TimeDistributed(convolutional_layers, input_shape=(None, FRAME_NUMBER, 64))
</code></pre>

<p>What am I doing wrong?</p>
","704329","704329","2018-05-31 15:38:45","Stacking convolutional network and recurrent layer","<python><keras><lstm><recurrent-neural-network>","1","3","3517"
"50641183","2018-06-01 10:14:40","2","","<p>You can write something like this:</p>

<p>In your <strong>models.py</strong> file :</p>

<pre><code>class MyModel(models.Model):

    foo = models.CharField(max_length=...)
    bar = models.CharField(max_length=...)
    ...

    def __str__(self):
        return self.foo, self.bar
</code></pre>

<p>Then, in your <strong>views.py</strong> file:</p>

<pre><code>def MyFunction(request):

    my_var = MyModel.objects.all()

    return render(request, 'Template.html', {""my_var"": my_var})
</code></pre>

<p>And finally in your <strong>template.html</strong> file :</p>

<pre><code>{% load staticfiles %}
{% load static %}

{% block content %}

{% for object in my_var_list %}

&lt;table style=""width:90%""&gt;
    &lt;tbody&gt;
        &lt;p&gt;&lt;/p&gt;
        &lt;tr&gt;
            &lt;td&gt;foo&lt;/td&gt;
            &lt;td&gt;{{ object.foo }}&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;bar&lt;/td&gt;
            &lt;td&gt;{{ object.bar }}&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;

{% endfor %}

{% endblock content %}
</code></pre>

<p>Next time, read StackOverflow documentations, and what you have done before to post your question.</p>
","7473847","7473847","2018-06-01 12:01:50","2","1203","Essex","2016-03-15 15:56:23","3114","811","603","219","50641033","50641183","2018-06-01 10:06:00","-1","121","<p>I want to realize HTML page with table which consist from date from database. And if I add element to database I want to HTML table updated too. How to realize it with django?</p>
","8698126","","","Dynamic html table with django","<python><django>","1","2","183"
"50641209","2018-06-01 10:16:00","0","","<p>Services target pods (through their labels), not other services. To hit one service from another service, this is the flow:</p>

<p>User -> auth (service) -> auth (pods) -> fe (service) -> fe (pods).</p>

<p>So your pods, in auth need to forward the traffic to the other service.</p>

<p>Note that there is no need to have 2 Load Balancer type service. With the first service the data gets from outside world to your cluster. At this point it would be recommendable to use ClusterIP type service.</p>
","5564578","","","2","504","suren","2015-11-15 15:10:36","2081","310","17","35","50640962","","2018-06-01 10:02:49","0","97","<p>I have two services auth and frontend. How do I make a connection between the two?</p>

<p><strong>auth.yml</strong></p>

<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
   name: auth
spec:
  replicas: 1
  selector:
    matchLabels:
      app: auth
  template:
    metadata:
      labels:
        app: auth
        tier: backend
        track: dev
    spec:
      containers:
        - name: auth
          image:  auth:1
          ports:
            - name: auth
              containerPort: 8000
----------------------------------------------
apiVersion: v1
kind: Service
metadata:
  name: auth
spec:
  selector:
    app: auth
    tier: backend
  ports:
    - protocol: TCP
      port: 3000
      targetPort: auth
  type: LoadBalancer
</code></pre>

<p>The above service works fine with the external IP and the port.</p>

<p><strong>fe.yml</strong></p>

<pre><code>    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: fe
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: fe
      template:
        metadata:
          labels:
            app: fe
            tier: frontend
            track: dev
        spec:
          containers:
            - image: fe:1
              name: nginx
              ports:
                - name: fe
                  containerPort: 80
    ------------------------------------------------
    apiVersion: v1
    kind: Service
    metadata:
      name: fe
    spec:
      selector:
        app: fe
        tier: frontend
      ports:
        - protocol: TCP
          port: 80
          targetPort: 80
      type: LoadBalancer
</code></pre>

<p>I am able to access both the services individually with the external IP.
I want both the services to interact with each other. I have tried using name-based calling service in the frontend like - <code>http://auth:3000</code>.
It does not seem to work.</p>
","8818013","","","Kubernetes Connection Between two services?","<python><docker><kubernetes><angular5>","1","4","1896"
"50641224","2018-06-01 10:17:06","0","","<p>To perform <code>TAB Action</code> until you find a particular <em>WebElement</em> would not be as per the <em>best practices</em>. As per your comment <em>the element is hidden</em> so you need to bring the element within the <a href=""https://www.w3schools.com/css/css_rwd_viewport.asp"" rel=""nofollow noreferrer""><strong>Viewport</strong></a> first and then invoke <code>click()</code>/<code>send_keys()</code> as follows:</p>

<pre><code>myElement = driver.find_element_by_xpath(""//div[@class='PeriodCell']//input[@type='text'][@value=\""0.0\""]"")
driver.execute_script(""return arguments[0].scrollIntoView(true);"", myElement)
# perfrom any action on the element
</code></pre>

<p>However the alternative using the <code>TAB Action</code> is as follows:</p>

<pre><code>from selenium import webdriver
from selenium.webdriver.common.keys import Keys

global element
element = driver.find_element_by_name(""name"")
element.send_keys(""ABC"")
element = driver.find_element_by_name(""group"") 
element.send_keys(""DEF"")
while True:
    element.send_keys(Keys.TAB)
    element = driver.switch_to_active_element()
    if (element.get_attribute(""type"")=='text' and element.get_attribute(""value"")=='0.0' and element.get_attribute(""data-start"")=='2014-09-20'):
    print(""Element found"")
    break
# do anythin with the element
</code></pre>
","7429447","","","1","1328","DebanjanB","2017-01-17 08:59:30","63154","13103","3455","2612","50637373","50641224","2018-06-01 06:32:51","1","84","<p>I want to perform TAB action until I have reached a particular web-element. Until the active element is the below mentioned element, TAB action has to be performed.</p>

<pre><code>&gt;name = driver.find_element_by_name(""name"")
&gt;name.send_keys(""ABC"")
&gt;group = driver.find_element_by_name(""group"") 
&gt;group.send_keys(""DEF"")
</code></pre>

<p>I am able to find element till the above state. After that, I want to perform TAB action until the below mentioned element is found. I guess using a loop would help.</p>

<blockquote>
  <p>elem = driver.find_element_by_css_selector('.PeriodCell input')</p>
</blockquote>

<p>Please find below the HTML code</p>

<pre><code>&lt;div class=""PeriodCell"" style=""left:px; width:112px;""&gt;
&lt;div class=""Effort forecasting""&gt;
&lt;div class=""entity field-value-copy-selected""&gt;
&lt;input type=""text"" value=""0.0"" data-start=""2014-09-20""&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=""Effort unmet zero"" title=""""&gt;0.0
&lt;/div&gt;
&lt;/div&gt;
</code></pre>

<p>Please help. Thanks in Advance.</p>
","9835644","","","Perform TAB action until active element is the required element - Python","<python><selenium><selenium-webdriver>","2","7","1046"
"50641227","2018-06-01 10:17:18","0","","<p>Can you try using <code>str.contains</code> with <code>case=False</code></p>

<p><strong>Ex:</strong></p>

<pre><code>import pandas as pd

dprev = ""eiffel tower""
df = pd.DataFrame({""place"": [""eiffel tower"", ""Eiffel tower"", ""Hello""], ""data"":[1,2,3]})
print(df.loc[df['place'].str.contains(dprev, case=False)])
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>   data         place
0     1  eiffel tower
1     2  Eiffel tower
</code></pre>
","532312","","","3","451","Rakesh","2010-12-06 13:07:54","56694","5302","758","1508","50641053","50641227","2018-06-01 10:07:35","-1","489","<p>I am trying to get the subset dataframe using the below code:</p>

<pre><code>dprev = ""eiffel tower""

df.loc[df['place'] == dprev] -&gt; returns empty

drandom = random.choice(df['place'].unique())

df.loc[df['place'] == drandom] -&gt; returns the subset
</code></pre>

<p>why am i not seeing the same thing when <code>dprev</code> is string variable?</p>
","3779226","2613005","2018-06-01 10:34:23","python pandas loc return empty dataframe","<python><pandas>","3","6","359"
"50641311","2018-06-01 10:21:36","1","","<p>Ok, after the discussion above, you added the line</p>

<blockquote>
  <p>sess.run(y_pred_cls, {x:batch_tx})</p>
</blockquote>

<p>after your confusion matrix summation and now you have your predicted labels. Print them out in a format that you can turn into a np array for the code below. Provided your test code runs in a single thread, and it does not shuffle the test batches, you now have your predicted labels in the <strong>same order</strong> as the images appear in the input file. Assuming your input file is a .bin file, you should be able to extract images (using PIL) from it like this:</p>

<pre><code>from PIL import Image

# your image dimensions here
width = 80
height = 80
channels = 3

# most labels are 1 byte
labelSize = 1
pixelSize = width * height * channels
recordSize = labelSize + pixelSize

label_names = ['cat', 'horse', 'dog'....]
predictions = [...] # put your predictions here

with open(inputFilename, ""rb"") as f:
    allTheData = np.fromfile(f, 'u1')

numRecords = allTheData.shape[0] / recordSize
allTheData = allTheData.reshape(numRecords, recordSize)

for idx, d in enumerate(allTheData):
   label = label_names[d[0]]
   rgbData = d[1:] #records are label first, then all pixel data and rgb


   predlabel = label_names[data_labels[idx]]


   filename = ""{}_pred{}_actual{}.png"".format(idx, predlabel, label)
   pictureA = rgbData.reshape(3, width, height)
   pictureA = np.swapaxes(pictureA,0,1)
   pictureA = np.swapaxes(pictureA,1,2)
   pictureA = np.ndarray.flatten(pictureA)

   imageA = Image.frombytes('RGB', (height, width), pictureA)
   #display(imageA)
   imageA.save(filename, ""PNG"")
</code></pre>

<p>Please note that the code above won't run until you add in proper label names and your predictions. Also, if the input file is a .csv, you will have to change the reading of it slightly.</p>
","9161094","","","0","1843","Pam","2018-01-01 17:08:41","751","137","289","5","50449114","50641311","2018-05-21 12:38:31","2","125","<p>I wrote a CNN network with Tensorflow which works properly and I want to check classified images during the testing stage.</p>

<p>In my dataset, I have 5 different categories, during the test stage I am looking for a way to save classified images in a new folder for each category to check the results from my net make or not sense. </p>

<p>This is the whole of my code in the test stage:</p>

<pre><code>tf.app.flags.DEFINE_float('learning_rate', 0.0001, 'Learning rate for adam optimizer')
tf.app.flags.DEFINE_integer('num_classes', 3, 'Number of classes')
tf.app.flags.DEFINE_integer('batch_size', 128, 'Batch size')
tf.app.flags.DEFINE_float('keep_prob', 0.8, 'Dropout keep probability')
tf.app.flags.DEFINE_integer('num_channel',3 , 'Image channel, RGB=3, Grayscale=1')
tf.app.flags.DEFINE_integer('img_size', 80, 'Size of images')
tf.app.flags.DEFINE_string('test_file', 'data/test.txt', 'Test dataset file')

FLAGS = tf.app.flags.FLAGS

checkpoint_dir = '/home/xyrio/Desktop/classier/training/checkpoints/model_epoch.ckpt89'


def main(_):

x = tf.placeholder(tf.float32, shape=[FLAGS.batch_size, FLAGS.img_size, FLAGS.img_size, FLAGS.num_channel], name='x')
y_true = tf.placeholder(tf.float32, shape=[None, FLAGS.num_classes], name='y_true')
y_true_cls = tf.argmax(y_true, axis=1)

filter_size_conv1 = 3
num_filters_conv1 = 32

filter_size_conv2 = 3
num_filters_conv2 = 32

filter_size_conv3 = 3
num_filters_conv3 = 64

filter_size_conv4 = 3
num_filters_conv4 = 128

filter_size_conv5 = 3
num_filters_conv5 = 256

fc_layer_size = 512
fc_layer_size2 = 128

def create_weights(shape):
    return tf.Variable(tf.truncated_normal(shape, mean=0, stddev=0.01))

def create_biases(size):
    return tf.Variable(tf.constant(0.01, shape=[size]))

def create_convolutional_layer(input, num_input_channels, conv_filter_size, num_filters, useBatchNorm=False,
                               usePooling=True):

    weights = create_weights(shape=[conv_filter_size, conv_filter_size, num_input_channels, num_filters])
    biases = create_biases(num_filters)
    layer = tf.nn.conv2d(input=input, filter=weights, strides=[1, 1, 1, 1], padding='SAME')
    layer += biases
    layer = tf.nn.relu(layer)

    if useBatchNorm == True:
        layer = tf.layers.batch_normalization(layer)

    if usePooling:
        layer = tf.nn.max_pool(value=layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
    return layer

def create_flatten_layer(layer):

    layer_shape = layer.get_shape()
    num_features = layer_shape[1:4].num_elements()
    layer = tf.reshape(layer, [-1, num_features])

    return layer

def create_fc_layer(input, num_inputs, num_outputs, useRelu=True, useDropout=False):
    weights = create_weights(shape=[num_inputs, num_outputs])
    biases = create_biases(num_outputs)

    layer = tf.matmul(input, weights) + biases
    if useRelu:
        layer = tf.nn.relu(layer)

    if useDropout == True:
        layer = tf.nn.dropout(layer, keep_prob=FLAGS.keep_prob)
    return layer

layer_conv1 = create_convolutional_layer(x, FLAGS.num_channel, filter_size_conv1, num_filters_conv1,
                                         useBatchNorm=True, usePooling=True)
layer_conv2 = create_convolutional_layer(layer_conv1, num_filters_conv1, filter_size_conv2, num_filters_conv2,
                                         useBatchNorm=True, usePooling=True)
layer_conv3 = create_convolutional_layer(layer_conv2, num_filters_conv2, filter_size_conv3, num_filters_conv3,
                                         useBatchNorm=True, usePooling=True)
layer_conv4 = create_convolutional_layer(layer_conv3, num_filters_conv3, filter_size_conv4, num_filters_conv4,
                                         useBatchNorm=True, usePooling=True)
layer_conv5 = create_convolutional_layer(layer_conv4, num_filters_conv4, filter_size_conv5, num_filters_conv5,
                                         useBatchNorm=True, usePooling=True)

layer_flat = create_flatten_layer(layer_conv5)

layer_fc1 = create_fc_layer(layer_flat, layer_flat.get_shape()[1:4].num_elements(), fc_layer_size, useRelu=True,
                            useDropout=False)
layer_fc2 = create_fc_layer(layer_fc1, fc_layer_size, fc_layer_size2, useRelu=True, useDropout=True)
layer_fc3 = create_fc_layer(layer_fc2, fc_layer_size2, FLAGS.num_classes, useRelu=False)

y_pred = tf.nn.softmax(layer_fc3, name='y_pred', axis=1)
y_pred_cls = tf.argmax(y_pred, axis=1)


correct_prediction = tf.equal(y_pred_cls, y_true_cls)
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

saver = tf.train.Saver()

test_preprocessor = BatchPreprocessor(dataset_file_path=FLAGS.test_file, num_classes=FLAGS.num_classes,
                                     output_size=[FLAGS.img_size, FLAGS.img_size])

test_batches_per_epoch = np.floor(len(test_preprocessor.labels) / FLAGS.batch_size).astype(np.int16)

conf_mat = tf.confusion_matrix(y_true_cls,y_pred_cls,FLAGS.num_classes)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())


    saver.restore(sess, checkpoint_dir)

    # Start Testing
    test_acc = 0.
    test_count = 0
    cm_total = None
    for _ in range(test_batches_per_epoch):
        batch_tx, batch_ty = test_preprocessor.next_batch(FLAGS.batch_size)
        acc, conf_m = sess.run([accuracy, conf_mat],
                               feed_dict={x: batch_tx, y_true: batch_ty})


        if cm_total is None:
            cm_total = conf_m
        else:
            cm_total += conf_m

        test_acc += acc
        test_count += 1

    test_acc /= test_count
    print(""{} Testing Accuracy = {:.2%}"".format(datetime.now(), test_acc))


    test_preprocessor.reset_pointer()
print(cm_total)
</code></pre>

<p>This code is for test data, as you can see I restored checkpoint which I saved during training and validating, and after that, I used the best checkpoint for predict on my test data.</p>

<p><code>batch_tx</code> is my test data and <code>batch_ty</code> is my test label.</p>

<p>is anyone have an idea how can I do this?</p>

<p>thanks in advance</p>
","5573737","5573737","2018-06-05 10:56:42","Save Predicted images in CNN network","<python><python-3.x><tensorflow>","1","10","6071"
"50641341","2018-06-01 10:23:39","2","","<p>There are two things required here. First a script i.e <code>code.py</code> to log the functional behavior like <code>temperature, pressure, and altitude</code> readings along with error/response during the process. Another is the script executions logs i.e a success or failure during the scheduled time and other system logs.</p>

<p>For first job, you have to do by your self but ensure to have a <code>logger</code> module in place to log the process flow.</p>

<p>For Second job, you can use OS provided scheduler <code>crontab</code> for <code>Linux</code> based os. For example to execute script every minutes</p>

<pre><code>* * * * * python /home/script/code.py &gt; /home/script/code.log 2&gt;&amp;1
</code></pre>

<p>For more about scheduler jobs, you can refer <a href=""https://code.tutsplus.com/tutorials/scheduling-tasks-with-cron-jobs--net-8800"" rel=""nofollow noreferrer"">here</a></p>
","1704235","1704235","2018-06-02 05:18:33","0","903","MaNKuR","2012-09-27 18:07:56","1450","161","64","9","50640980","50641341","2018-06-01 10:03:40","1","244","<p>I got temperature, pressure, and altitude readings on my <code>PI</code> using a sensor:</p>

<ol>
<li>The problem is, to see the results, I have to execute the <code>code.py</code> every time by myself. I am trying to automate it somehow so it will keep running itself for the time I want. </li>
<li>Once that is automated, would like to save the results and analyze the output after some time.</li>
</ol>

<p>Is there a way I can write code for both the tasks?</p>

<p>Thank you.</p>
","9880587","5279133","2018-06-01 11:24:02","How to write an autoscript in Python?","<python><python-2.7>","3","3","489"
"50641351","2018-06-01 10:23:55","2","","<p>There are multiple things you could do; for example running the while-loop indefinitely and checking for the instruction with the ""if"" comparison-operator.  </p>

<pre><code>while True:
    if instruction == ""turn left"":
        //do something
    if instruction == ""turn right"":
        //do something
</code></pre>

<p>Or you could use multithreading, which allows you to have mulitple while-loops running in parallel.</p>
","8620128","4834","2018-06-01 10:34:15","0","428","itzFlubby","2017-09-16 19:32:42","190","47","17","1","50641069","","2018-06-01 10:08:07","0","53","<p>Can I use the while loop multiple times? And how could I do that? For example, in this code I want it to detect the string ""turn left"". If it does the button will send a signal, if it is pressed. My question now is how I can make the loop check for other strings like ""turn right"", ""take the first exit"" at the same time?</p>

<pre><code>while instruction == ""turn left"":

    if (GPIO.input(12) == False):
        print(""button press"")
        assistant.start_conversation()
        break

    else:
        GPIO.output(3, GPIO.HIGH)
        sleep(0.3)
        GPIO.output(3, GPIO.LOW)
        sleep(0.3)
</code></pre>
","9536469","9536469","2018-08-30 23:57:09","How to make multiple time while loop Raspberry Pi? Python","<python><raspberry-pi>","1","0","623"
"50641459","2018-06-01 10:31:27","1","","<p>You can achieve this with pandas. The idea is to assign each X value to an interval using <code>np.digitize</code>. Since you are using a log scale, it makes sense to use <code>np.logspace</code> to choose intervals of exponentially changing lengths. Finally, you can group X values in each interval and compute mean Y values.</p>

<hr>

<pre><code>import pandas as pd
import numpy as np

x_max = 10

xs = np.exp(x_max * np.random.rand(1000))
ys = np.exp(np.random.rand(1000))

df = pd.DataFrame({
    'X': xs,
    'Y': ys,
})

df['Xbins'] = np.digitize(df.X, np.logspace(0, x_max, 30, base=np.exp(1)))
df['Ymean'] = df.groupby('Xbins').Y.transform('mean')
df.plot(kind='scatter', x='X', y='Ymean')
</code></pre>
","4585963","","","3","716","hilberts_drinking_problem","2015-02-19 22:15:40","7219","620","1491","135","50640339","","2018-06-01 09:32:16","-1","594","<p><a href=""https://i.stack.imgur.com/tmh2b.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tmh2b.png"" alt=""""></a></p>

<p>I have two arrays of corresponding data (x and y) that I plot as above on a log-log plot. The data is currently too granular and I would like to bin them to get a smoother relationship. Could I get some guidance on how I can bin along the x-axis, in <strong>exponential</strong> bin sizes, so that it appears linear on the log-log scale?</p>

<p>For example, if the first bin is of range x = 10^0 to 10^1, I want to collect all y-values with corresponding x in that range and average them into one value for that bin. I don't think np.hist or plt.hist quite does the trick, since they do binning by counting occurrences.</p>

<p>Edit: For context, if it helps, the above plot is an assortativity plot that plots the in vs out degree of a certain network. </p>
","9246732","","","How to bin a 2D data along the x-axis with Python","<python><numpy><matplotlib><histogram><binning>","2","0","902"
"50641469","2018-06-01 10:31:45","1","","<p>This looks like a disastrous mix of NumPy and Keras. Let's look at the 2 main confusion points:</p>

<ol>
<li><p>Once you are inside a Keras model, example Lambda layer, you are dealing with <strong>tensors</strong> and <em>not</em> NumPy arrays. Although convinient it would be, you can't use any NumPy operations, external libraries inside models. Having said that, tensor operators are very similar to arrays for good reason. Because it's your first layer, you can <em>pre-process</em> it in NumPy and then pass that into your model, this would work.</p></li>
<li><p>Why you get prints working? There are 2 main steps in Keras, Tensorflow: 1-> build the computation graph, 2-> actually run it. So you are building the graph and your operations get called yes, but they create <em>symbolic</em> tensors that have no value. So you can print the shape which can be determined when building the graph but not for example the values it holds.</p></li>
</ol>

<p>Take away message, don't mix NumPy with Tensorflow inside computation graphs (models) and by all means print the shapes while building the graph to get an idea of what the graph looks like but you won't get anything more out of symbolic tensors at build time.</p>
","9758922","","","1","1227","nuric","2018-05-08 13:57:46","6956","503","353","45","50634986","50641469","2018-06-01 01:42:27","0","418","<p>I was trying to define a Lambda layer Keras, as follows:</p>

<p>First, a function which computes the wavelet transform of an image and then gloms it together:</p>

<pre><code>from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.layers import BatchNormalization
from keras.layers import Lambda
from keras import regularizers
from keras import backend as K
import pywt
import numpy as np
from keras.engine.topology import Layer

def mkwtarray(image):
    channels = K.image_data_format()
    if channels is 'channels_first':
        axbase = 1
    else:
        axbase = 0
    print(axbase)
    print(image.shape)
    (a,( b, c, d ))= pywt.dwt2(image, 'db1', axes=(axbase, axbase+1))
    ab = np.concatenate((a, b), axis=axbase)
    cd = np.concatenate((c, d), axis=axbase)
    abcd = np.concatenate((ab, cd), axis=axbase+1)
    return abcd

def wtoutshape(input_shape):
    return input_shape

train_data_dir = 'train'
validation_data_dir = 'validation'
nb_train_samples = 21558
nb_validation_samples = 3446
epochs = 30
batch_size = 32

if K.image_data_format() == 'channels_first':
    input_shape = (3, img_width, img_height)
else:
    input_shape = (img_width, img_height, 3)

model = Sequential()
model.add(Lambda(mkwtarray, input_shape=input_shape, output_shape = wtoutshape))
&lt;more random  layers&gt;
</code></pre>

<p>Much to my amazement, as I was defining the model (meaning, evaluated the lines above), it errored out, claiming:
    ValueError: Input array has fewer dimensions than the specified axes</p>

<p>Also, the 'print' statements, which printed the expected values <code>0</code> and <code>(?, 150, 150, 3)</code> fired, which means that the function was actually evaluated at definition time, not when the model was actually running. I am obviously missing something about Keras' Lambda functionality - any enlightenment would be appreciated.</p>

<p><strong>UPDATE</strong> The exact same problem presents itself if you define a layer in the ""general"" way (via a class, where the lambda is now in the call function of the layer, so this is not lambda-specific.</p>
","2977256","2977256","2018-06-01 02:04:44","complete Keras Lambda confusion","<python><keras>","1","3","2250"
"50641478","2018-06-01 10:32:27","0","","<p>Try in this way:</p>

<pre><code>dprev = ""eiffel tower""
df = df.loc[df['place'].str.lower() == dprev]
</code></pre>

<p>or:</p>

<pre><code>df = df.loc[df['place'].str.lower().str.contains(dprev)]
</code></pre>
","5178905","5178905","2018-06-01 10:38:55","0","214","Joe","2015-07-31 17:53:47","7327","442","644","3","50641053","50641227","2018-06-01 10:07:35","-1","489","<p>I am trying to get the subset dataframe using the below code:</p>

<pre><code>dprev = ""eiffel tower""

df.loc[df['place'] == dprev] -&gt; returns empty

drandom = random.choice(df['place'].unique())

df.loc[df['place'] == drandom] -&gt; returns the subset
</code></pre>

<p>why am i not seeing the same thing when <code>dprev</code> is string variable?</p>
","3779226","2613005","2018-06-01 10:34:23","python pandas loc return empty dataframe","<python><pandas>","3","6","359"
"50641487","2018-06-01 10:33:20","2","","<p>I solved it by reinstalling the older version of selenium package because the newest version doesn't support Python 2.6.6 which in my case was installed and I didn't have root access to install the new one.</p>

<p>While the newest version of selenium package doesn't have support for Python 2.6.6, I had to downgrade by reinstalling the selenium package with lower version</p>

<pre><code>pip uninstall selenium
pip install --user selenium==3.5.0
</code></pre>
","4303914","4303914","2019-05-19 12:30:09","1","465","ljmocic","2014-11-28 13:37:45","554","90","120","2","29092970","","2015-03-17 06:52:03","26","35123","<p>I am a newbie for selenium python. I have installed python, pip etc.. 
I am trying to run the below code but it is showing error:</p>

<blockquote>
  <p>ImportError: cannot import name 'webdriver'</p>
</blockquote>

<pre><code>from selenium import webdriver
from selenium.webdriver.common.keys import Keys

driver = webdriver.Firefox()
driver.get(""http://www.python.org"")
</code></pre>

<p>could anyone please solve this issue?</p>
","3872524","446792","2017-08-26 13:54:40","ImportError: cannot import name 'webdriver'","<python>","5","0","435"
"50641525","2018-06-01 10:35:11","1","","<p>You can optimize a bit. Strings are immutable - every time you append one char to a string a new string is created and replaces the old one. Use lists of chars instead - also do not use <code>"""".join()</code> all the time for printing purposes if you can print the list of chars by decomposing and a seperator of """":</p>

<pre><code>import random

def generateOne(strlen):
    """"""Create one in one random-call, return as list, do not iterativly add to string""""""
    alphabet = ""abcdefghijklmnopqrstuvwxyz ""
    return random.choices(alphabet,k=strlen)

def score(goal, teststring):
    """"""Use zip and generator expr. for summing/scoring""""""
    return sum(1 if a==b else 0 for a,b in zip(goal,teststring))/len(goal)

def main():
    goalstring = list(""methinks it is like a weasel"") # use a list
    newstring = generateOne(28) # also returns a list
    workingstring = newstring [:] # copy
    countvar = 0
    while score(goalstring, workingstring) &lt; 1:
        if score(goalstring, newstring) &gt; 0:
            for pos,c in enumerate(goalstring): # enumerate for getting the index
                # test if equal, only change if not yet ok
                if c == newstring[pos] and workingstring[pos] != c: 
                    workingstring[pos] = newstring[pos] # could use c instead
                    countvar += 1
                    print(*workingstring, sep="""") # print decomposed with sep of """"  
                                                  # instead of """".join()
        newstring = generateOne(28)

    finalstring = """".join(workingstring) # create result once ... 
                                         # although its same as goalstring
                                         # so we could just assing that one
    print(finalstring) 
    print(countvar)

print(""hi"")
if __name__ == '__main__':
    s = datetime.datetime.now()
    main()
    print(datetime.datetime.now()-s)
print(""ho"")
</code></pre>

<p>Timings with printouts are very unrelieable. If I comment the <code>print</code> printing the intermediate steps to the final solution and use a `random.seed(42)' - I get for mine:</p>

<pre><code>0:00:00.012536
0:00:00.012664
0:00:00.008590
0:00:00.012575
0:00:00.012576
</code></pre>

<p>and for yours:</p>

<pre><code>0:00:00.017490
0:00:00.017427
0:00:00.013481
0:00:00.017657
0:00:00.013210
</code></pre>

<p>I am quite sure this wont solve your laptops issues, but still - it is a bit faster. </p>
","7505395","","","0","2443","Patrick Artner","2017-02-02 10:46:51","30736","5120","3506","4713","50640450","","2018-06-01 09:37:46","-1","47","<p>I wrote the piece of code below a while back, and had this issue then as well. I ignored it at the time and when I came back to it after asking an 'expert' to look at it, it was working fine.</p>

<p>The issue is, sometimes the program seems unable to run the main() on my laptop, possibly due to how heavy the algorithm is. Is there a way around this? I would hate to keep having this problem in the future. The same code is working perfectly on another computer which i have limited access to. </p>

<p>(P.S. laptop having the issue is a MacBook Air 2015 and it should have no problem running the program. Also, it stops after printing ""hi"")</p>

<p>It does not give and error message, it just doesn't print anything from main(). It's supposed to print a series of strings which progressively converge to ""methinks it is like a weasel"". In eclipse, it shows that the code is still being processed but it does not output anything that it is supposed to</p>

<pre><code>import random

def generateOne(strlen):
    alphabet = ""abcdefghijklmnopqrstuvwxyz ""
    res = """"
    for i in range(strlen):
        res = res + alphabet[random.randrange(27)]
    return res

def score(goal, teststring):
    numSame = 0
    for i in range(len(goal)):
        if goal[i] == teststring[i]:
            numSame = numSame + 1
    return numSame / len(goal)

def main():
    goalstring = ""methinks it is like a weasel""
    chgoal = [0]*len(goalstring)
    newstring = generateOne(28)
    workingstring = list(newstring)
    countvar = 0
    finalstring = """"
    while score(list(goalstring), workingstring) &lt; 1:
        if score(goalstring, newstring) &gt; 0:
            for j in range(len(goalstring)):
                if goalstring[j] == newstring[j] and chgoal[j] == 0:
                    workingstring[j] = newstring[j]
                    chgoal[j] = 1
                    finalstring = """".join(workingstring)
                    countvar = countvar + 1
                    print(finalstring)
        newstring = generateOne(28)
    finalstring = """".join(workingstring)
    print(finalstring) 
    print(countvar)

print(""hi"")
if __name__ == '__main__':
    main()
print(""ho"")
</code></pre>
","9880398","9880398","2018-06-01 10:09:18","python functions work sometimes and fail to work other times","<python><eclipse>","1","3","2187"
"50641533","2018-06-01 10:35:42","3","","<p>You could do in one sort provided you write a custom function for comparision.
The idea is to sort the words in ascending order and integer in descending order in the same list . Incase of word and integer is compared then treat the word as smaller compared to word.</p>

<p>And then for printing the final result increment the index for word if a word is found , decrement the index for integer if digit is found.</p>

<p>The below code works in python2:</p>

<pre><code>a = ""12 I have car 8 200 a""

def custom_compare(x,y):
    if x.isdigit() and y.isdigit():
        return int(y) - int(x) #do a descending order
    if x.isdigit() and y.isdigit() == False:
        return 1
    if x.isdigit() == False and y.isdigit():
        return -1
    if x.isdigit() == False and y.isdigit() == False:
        # do ascending order
        if x.lower() == y.lower():
            return 0
        elif x.lower() &lt; y.lower():
            return -1
        else:
            return 1

original_list = a.split("" "")
sorted_list = sorted(original_list, cmp=custom_compare)

result = []
integer_index = -1
string_index = 0
for word in original_list:
    if word.isdigit():
        result.append(sorted_list[integer_index])
        integer_index = integer_index - 1
    else:
        result.append(sorted_list[string_index])
        string_index = string_index + 1

result
['8', 'a', 'car', 'have', '12', '200', 'I']
</code></pre>

<p>Python 3:
    import functools</p>

<pre><code>a = ""12 I have car 8 200 a""

def custom_compare(x,y):
    if x.isdigit() and y.isdigit():
        return int(y) - int(x) #do a descending order
    if x.isdigit() and y.isdigit() == False:
        return 1
    if x.isdigit() == False and y.isdigit():
        return -1
    if x.isdigit() == False and y.isdigit() == False:
        # do ascending order
        if x.lower() == y.lower():
            return 0
        elif x.lower() &lt; y.lower():
            return -1
        else:
            return 1

original_list = a.split("" "")
sorted_list = sorted(original_list, key=functools.cmp_to_key(custom_compare))

result = []
integer_index = -1
string_index = 0
for word in original_list:
    if word.isdigit():
        result.append(sorted_list[integer_index])
        integer_index = integer_index - 1
    else:
        result.append(sorted_list[string_index])
        string_index = string_index + 1

result
</code></pre>

<p>PS:The word comparison could be efficiently written. 
 I am from C background and I am not sure of the pythonic way of comparison.</p>
","855809","855809","2018-06-07 07:14:04","4","2535","Knight71","2011-07-21 11:30:15","1770","238","140","14","47311720","50622681","2017-11-15 15:48:55","8","701","<p>Say I have a string a.</p>

<pre><code>a = ""12 I have car 8 200 a""
</code></pre>

<p>I need to sort this string in such a way that the output should be</p>

<pre><code>8 a car have 12 200 I
</code></pre>

<p>ie, Sort the string in such a way that all words are in alphabetical order and all integers are in numerical order. Furthermore, if the nth element in the string is an integer it must remain an integer, and if it is a word it must remain a word.</p>

<p>This is what I tried.</p>

<pre><code>a = ""12 I have car 8 200 a""


def is_digit(element_):
    """"""
    Function to check the item is a number. We can make using of default isdigit function
    but it will not work with negative numbers.
    :param element_:
    :return: is_digit_
    """"""
    try:
        int(element_)
        is_digit_ = True
    except ValueError:
        is_digit_ = False

    return is_digit_



space_separated = a.split()

integers = [int(i) for i in space_separated if is_digit(i)]
strings = [i for i in space_separated if i.isalpha()]

# sort list in place
integers.sort()
strings.sort(key=str.lower)

# This conversion to iter is to make use of next method.
int_iter = iter(integers)
st_iter = iter(strings)

final = [next(int_iter) if is_digit(element) else next(st_iter) if element.isalpha() else element for element in
         space_separated]

print "" "".join(map(str, final))
# 8 a car have 12 200 I
</code></pre>

<p>I am getting the right output. But I am using two separate sorting function for sorting integers and the words(which I think is expensive). </p>

<p>Is it possible to do the entire sorting using a single sort function?.</p>
","6699447","6699447","2018-05-31 08:23:24","Sort string with integers and words without any change in their positions","<python><python-2.7><performance><sorting><iterator>","6","0","1641"
"50641654","2018-06-01 10:41:53","1","","<p>PyYAML is not part of the standard python library, and imports from the standard library, whether generic (<code>import os</code>) or specific (<code>from collections import OrderedDict</code>) should come first.</p>

<p>You should, IMO, lexicographically sort on the module names in the sections and separate the sections with an empty line:</p>

<pre><code>from collections import OrderedDict
import os
import time

from xtesting.core import testcase
import yaml
</code></pre>

<p>There are some, that want the generic ones to all come first, in each section:</p>

<pre><code>import os
import time
from collections import OrderedDict

import yaml
from xtesting.core import testcase
</code></pre>

<p>This looks nicer, but makes it more easy to overlook specific imports after a long generic lists. And it also separates a generic and specific import from one and the same module, which IMO is bad:</p>

<pre><code>import yaml
from xtesting.core import testcase
from yaml import safe_load
</code></pre>
","1307905","","","0","1007","Anthon","2012-04-02 11:30:58","37849","5547","1679","2001","50630822","50641654","2018-05-31 18:42:33","1","290","<p>I thought the correct order of imports in Python was the one described by the first answer of the question: <a href=""https://stackoverflow.com/questions/20762662/whats-the-correct-way-to-sort-python-import-x-and-from-x-import-y-statement"">What&#39;s the correct way to sort Python `import x` and `from x import y` statements?</a></p>

<p>Therefore, this code should be correct:</p>

<pre><code>import os
import time
import yaml

from collections import OrderedDict
from xtesting.core import testcase
</code></pre>

<p>However, When I run pylint I get:</p>

<pre><code>C:  5, 0: standard import ""from collections import OrderedDict"" should be placed before ""import yaml"" (wrong-import-order)
</code></pre>

<p>So I guess ""yaml"" is not a standard library. Should then the correct way to do it be this one (even if it is uglier and less readable)?</p>

<pre><code>import os
import time
from collections import OrderedDict
import yaml

from xtesting.core import testcase
</code></pre>
","6104632","","","Pylint complains when importing in the right order","<python><pylint>","1","1","984"
"50641745","2018-06-01 10:47:15","1","","<p>Another possibility, if you have no classes pre defined, is to put all the images in a sub folder from your image folder e.g:</p>

<pre><code>flow_from_directory(directory = ""/path/images/"",…)
</code></pre>

<p>Your actual data inside images/data</p>
","1507527","","","0","254","Pvic","2012-07-06 18:13:02","61","45","92","0","43318101","","2017-04-10 08:21:49","3","4729","<p>I unable to run simple data generator code from keras </p>

<pre><code>import os
import keras as K
from keras.preprocessing.image import ImageDataGenerator

def save_images_from_generator(maximal_nb_of_images, generator):
    nb_of_images_processed = 0
    for x, _ in generator:
        nb_of_images += x.shape[0]
        if nb_of_images &lt;= maximal_nb_of_images:
            for image_nb in range(x.shape[0]):
                your_custom_save(x[image_nb]) # your custom function for saving images
        else:
            break

Gen=ImageDataGenerator(featurewise_center=True,
    samplewise_center=False,
    featurewise_std_normalization=False,
    samplewise_std_normalization=False,
    zca_whitening=True,
    rotation_range=90,
    width_shift_range=0.2,
    height_shift_range=0.1,
    shear_range=0.5,
    zoom_range=0.2,
    channel_shift_range=0.1,
    fill_mode='nearest',
    cval=0.,
    horizontal_flip=True,
    vertical_flip=True,
    rescale=None,
    preprocessing_function=None)


if __name__ == '__main__':
    save_images_from_generator(40,Gen.flow_from_directory('C:\\Users\\aanilil\\PycharmProjects\\untitled\\images_input', target_size=(150, 150),class_mode=None,save_prefix='augm',save_to_dir='C:\\Users\\aanilil\\PycharmProjects\\untitled\\im_output\\'))
</code></pre>

<h1>Output</h1>

<pre><code>Using TensorFlow backend.
Found 0 images belonging to 0 classes.
Found 0 images belonging to 0 classes.
Found 0 images belonging to 0 classes.
Found 0 images belonging to 0 classes.
Found 0 images belonging to 0 classes.
Traceback (most recent call last):
  File ""C:\Program Files (x86)\JetBrains\PyCharm Community Edition 2016.3.2\helpers\pydev\pydevd.py"", line 1578, in &lt;module&gt;
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""C:\Program Files (x86)\JetBrains\PyCharm Community Edition 2016.3.2\helpers\pydev\pydevd.py"", line 1015, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""C:\Program Files (x86)\JetBrains\PyCharm Community Edition 2016.3.2\helpers\pydev\_pydev_imps\_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""C:/Users/aanilil/PycharmProjects/untitled/generate_data_from_folder.py"", line 35, in &lt;module&gt;
    save_images_from_generator(40,Gen.flow_from_directory('C:\\Users\\aanilil\\PycharmProjects\\untitled\\images_input', target_size=(150, 150),class_mode=None,save_prefix='augm',save_to_dir='C:\\Users\\aanilil\\PycharmProjects\\untitled\\im_output\\'))
  File ""C:/Users/aanilil/PycharmProjects/untitled/generate_data_from_folder.py"", line 7, in save_images_from_generator
    for x, _ in generator:
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\keras\preprocessing\image.py"", line 727, in __next__
    return self.next(*args, **kwargs)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\keras\preprocessing\image.py"", line 950, in next
    index_array, current_index, current_batch_size = next(self.index_generator)
  File ""C:\ProgramData\Anaconda3\envs\tensorflow\lib\site-packages\keras\preprocessing\image.py"", line 710, in _flow_index
    current_index = (self.batch_index * batch_size) % n
ZeroDivisionError: integer division or modulo by zero
</code></pre>

<p>When I do a os. listdir I get an output like so </p>

<pre><code>os.listdir('C:\\Users\\aanilil\\PycharmProjects\\untitled\\images_input') 
['download (1).png', 'download.jpg', 'download.png', 'images.jpg']
</code></pre>

<p>So there are images in the input folder and It still throws an error assoiciated to no files found</p>
","2527680","2527680","2017-04-10 09:29:33","Keras Image data generator throwing no files found error?","<python><machine-learning><tensorflow><computer-vision><keras>","3","0","3619"
"50641761","2018-06-01 10:47:56","3","","<p><a href=""https://www.tensorflow.org/api_docs/python/tf/map_fn"" rel=""nofollow noreferrer""><code>tf.map_fn()</code></a> can have many inputs / outputs. You could thus use <a href=""https://www.tensorflow.org/api_docs/python/tf/range"" rel=""nofollow noreferrer""><code>tf.range()</code></a> to build a tensor of row indices and use it along:</p>

<pre class=""lang-python prettyprint-override""><code>import tensorflow as tf

def some_function(x, i):
    return x + i

a = tf.constant([[2, 1], [4, 2], [-1, 2]])
a_rows = tf.expand_dims(tf.range(tf.shape(a)[0], dtype=tf.int32), 1)

res, _ = tf.map_fn(lambda x: (some_function(x[0], x[1]), x[1]), 
                   (a, a_rows), dtype=(tf.int32, tf.int32))

with tf.Session() as sess:
    print(res.eval())
    # [[2 1]
    #  [5 3]
    #  [1 4]]
</code></pre>

<hr>

<p>Note: In many cases, ""processing a matrix row by row"" can be done at once e.g. through broadcasting, instead of using loops:</p>

<pre class=""lang-python prettyprint-override""><code>import tensorflow as tf

a = tf.constant([[2, 1], [4, 2], [-1, 2]])
a_rows = tf.expand_dims(tf.range(tf.shape(a)[0], dtype=tf.int32), 1)
res = a + a_rows

with tf.Session() as sess:
    print(res.eval())
    # [[2 1]
    #  [5 3]
    #  [1 4]]
</code></pre>
","624547","","","1","1256","benjaminplanche","2011-02-19 17:06:24","8823","606","381","76","50641219","50641761","2018-06-01 10:16:53","2","1585","<p>I have a tensor that I process row by row using <code>tf.map_fn</code>. Now I want to include the index as an argument in the function that I am passing to <code>tf.map_fn</code>. In numpy I could use <code>enumerate</code> to get that information and pass it in my lambda function. Here's an example in numpy where I add 0 to the first row, 1 to the second row and so on:</p>

<pre><code>a = np.array([[2, 1], [4, 2], [-1, 2]])

def some_function(x, i):
    return x + i

res = map(lambda (i, row): some_function(row, i), enumerate(a))
print(res)

&gt; [array([2, 1]), array([5, 3]), array([1, 4])]
</code></pre>

<p>I haven't been able to find an equivalent to <code>enumerate</code> in tensorflow though and I don't know how I can achieve the same result in tensorflow. Does someone know what to use to make it work in tensorflow? Here's a sample code where I add 1 to each row of <code>a</code>:</p>

<pre><code>import tensorflow as tf

a = tf.constant([[2, 1], [4, 2], [-1, 2]])

with tf.Session() as sess:
    res = tf.map_fn(lambda row: some_function(row, 1), a)
    print(res.eval())

&gt; [[3 2]
   [5 3]
   [0 3]]
</code></pre>

<p>Thanks to anyone who can help me with this problem.</p>
","8253769","8253769","2018-06-01 10:35:58","equivalent of enumerate in tensorflow to use index in tf.map_fn","<python><tensorflow>","1","0","1201"
"50641763","2018-06-01 10:47:58","1","","<p>I found a solution myself using a metaclass:</p>

<pre><code>class M(type):
  def __init__(self, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.d = {}

class A(metaclass=M): pass
</code></pre>

<p>Then one can create subclasses of <code>A</code> which all have their own <code>d</code> attribute:</p>

<pre><code>class B(A): pass

B.d is A.d
False
</code></pre>
","1281485","","","0","383","Alfe","2012-03-20 16:44:12","36146","2787","1277","208","50639137","50641763","2018-06-01 08:26:31","0","58","<p>I have the following situation in Python 3:</p>

<pre><code>class A:
    d = {}

class B(A):  pass
class C(A):  pass
</code></pre>

<p>I work with the classes only, no instances get created.  When I access <code>B.d</code> I will get a shared reference to <code>A.d</code>.  That's not what I want.  I would like to have each class which inherits <code>A</code> have its own <code>d</code> which is set to a dictionary.  <code>d</code> is an implementation detail to <code>A</code>.  All access to <code>d</code> is done in the code of <code>A</code>.  Just <code>B</code>'s <code>d</code> should not be identical to <code>A</code>'s <code>d</code>.</p>

<p>With instances, I would create that dictionary in the <code>__init__()</code> function.  But in my case I work with the classes only.</p>

<p>Is there a standard way to achieve this (EDIT: without changing the implementation of the subclasses <code>B</code> and <code>C</code>)?  Is there something analog to <code>__init__()</code> which gets called for (or in) each derived class at the time of deriving?</p>
","1281485","1281485","2018-06-01 09:40:09","Prevent sharing of class variables with base class in Python","<python><python-3.x><class><inheritance><class-variables>","2","4","1072"
"50641813","2018-06-01 10:50:07","2","","<p>Use <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.transform.html"" rel=""nofollow noreferrer""><code>transform</code></a> for <code>Series</code> with same size as original <code>DataFrame</code>, binning with <code>cut</code> and aggregate <code>sum</code>:</p>

<pre><code>df['DATE_LOCATION'] = pd.to_datetime(df['DATE_LOCATION'], format='%d-%m-%y %H:%M')

df = df.sort_values(""DATE_LOCATION"")
s = (df[""DATE_LOCATION""].diff().dt.days &gt; 1).cumsum()

count = s.groupby(s).transform('size')
print (count)
0     2
1     2
2     4
3     4
4     4
5     4
6     2
7     2
8     1
9     3
10    3
11    3
Name: DATE_LOCATION, dtype: int32

bins = pd.cut(count, bins=[0,3,7,15,31], labels=['1-3', '4-7','8-15', '&gt;=16'])
df = df.groupby(['PRODUCT_ID', bins])['Sold'].sum().reset_index()
print (df)
  PRODUCT_ID DATE_LOCATION  Sold
0     0E4234           1-3     9
1     0E4234           4-7     7
2     0G2342           1-3    11
</code></pre>
","2901002","2901002","2018-06-01 11:31:40","9","988","jezrael","2013-10-20 20:27:26","427380","89269","18260","743","50621605","50641813","2018-05-31 09:59:58","3","123","<p>I have a dataset (Product_ID,date_time, Sold) which has products sold on various dates. The dates are being given for 9 months with random 13 days or more from a month. I have to segregate the data in a such a way that the for each product how many products were sold daily 1-3  days, sold daily 4-7 given days, sold daily 8-15 given days and sold daily for >16  days. So how can I code this in python using pandas and other packages</p>

<pre><code>PRODUCT_ID      DATE_LOCATION  Sold
0E4234          01-08-16 0:00    2
0E4234          02-08-16 0:00    7
0E4234          07-08-16 0:00    3
0E4234          08-08-16 0:00    1
0E4234          09-08-16 0:00    2
0E4234          10-08-16 0.00    1
.
. 
.
0G2342          22-08-16 0:00    1
0G2342          23-08-16 0:00    2
0G2342          26-08-16 0:00    1
0G2342          28-08-16 0:00    1
0G2342          29-08-16 0:00    3
0G2342          30-08-16 0:00    3
.
.
.(goes for 64 products each with 9 months of data)
.
</code></pre>

<p>I don't know even how to code for this in python
The output needed is</p>

<pre><code>PRODUCT_ID      Days   Sold
0E4234          1-3      9 #(1,2) dates because range is 1 to 3
                4-7      7 #(7,8,9,10) dates because range is 4 to 7
                8-15     0
                 &gt;16     0
0G2342          1-3      11 #(22,23),(26),(28,29,30) dates because range is 1 to 3
                4-7      0
                8-15     0
                 &gt;16     0
.
.(for 64 products)
.
</code></pre>

<p>Would be happy if at least someone posted a link to where to start.
I tried </p>

<pre><code>df[""DATE_LOCATION""] = pd.to_datetime(df.DATE_LOCATION)
df[""DAY""] = df.DATE_LOCATION.dt.day
def flag(x):
    if 1&lt;=x&lt;=3:
        return '1-3'
    elif 4&lt;=x&lt;=7:
        return '4-7'
    elif 8&lt;=x&lt;=15:
        return '8-15'
    else:
        return '&gt;=16'
df[""Days""] = df.DAY.apply(flag)
df.groupby([""PRODUCT_ID"",""Days""]).Sold.sum()
</code></pre>

<p>This gave me the number of products sold between these days in each month.But I need the sum of the products for the specified range were the products are sold in a streak specified.</p>
","9846590","5375464","2018-05-31 10:35:57","How to add the values for Specific days in Python Table for a given range?","<python><pandas><numpy><dataframe><pivot-table>","1","1","2152"
"50641827","2018-06-01 10:50:55","2","","<p>Constraint</p>

<pre><code>def constr(x,b):
    return x-b
</code></pre>

<p>results in <code>x-b &gt;= 0</code> (<strong>nonnegative</strong>), but allows <code>x-b = 0</code>. Then <code>log(x-b)</code> is undefined if <code>x-b = 0</code>. You would need to introduce some epsilon like:</p>

<pre><code>eps = 1e-12
def constr(x,b):
    return x-b-eps
</code></pre>

<p>which won't throw your error.</p>

<p>But there might be a more important problem: <strong>i really don't think</strong> the solver (SLSQP here) guarantees it's iterates to be feasible! There might be cases where this will be problematic. In your simple example, the way to go is to transform your constraint to bounds. That's of course less expressive (not always possible; but for your tiny example it is), but those bounds are respected in the iterates!</p>

<pre><code>inferred_lb = b + eps
a = minimize(obj, x, args=(b,), bounds=[(inferred_lb, None)])
</code></pre>

<p>(And of course: don't use minimize for single-dimension optimization. There is minimize_scalar.)</p>
","2320035","","","0","1051","sascha","2010-08-31 00:18:13","20685","3953","764","3216","50638315","","2018-06-01 07:34:32","1","111","<p>scipy.minimize does not seem to adhere to constraints.
Here is a simple example where the constraint is for preventing a negative argument in the logarithm, but the minimize function does not adhere :</p>

<pre><code>import math
from scipy.optimize import minimize

def obj(x,b):
    print ""obj x"",x
    return math.log(x-b)

def constr(x,b):
    print ""constr x"",x
    return x-b

x=3.1
b=3
a=minimize(obj,x,args=(b),constraints={'type': 'ineq', 'fun':constr,'args':[b]})
</code></pre>

<p>the output is:</p>

<pre><code>constr x [ 3.1]
obj x [ 3.1]
constr x [ 3.1]
obj x [ 3.1]
obj x [ 3.10000001]
constr x [ 3.1]
constr x [ 3.10000001]
obj x [ 3.]
Traceback (most recent call last):
File ""scipy_minimize_constraints.py"", line 19, in 
a=minimize(obj,x,args=(b),constraints={'type': 'ineq', 'fun':constr,'args':[b]})
File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/optimize/_minimize.py"", line 495, in minimize
constraints, callback=callback, **options)
File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/optimize/slsqp.py"", line 378, in _minimize_slsqp
fx = func(x)
File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/optimize/optimize.py"", line 292, in function_wrapper
return function(*(wrapper_args + args))
File ""scipy_minimize_constraints.py"", line 9, in obj
return math.log(x-b)
ValueError: math domain error
</code></pre>

<p>python2.7
Scipy version 1.0.0</p>

<p>Am I doing something wrong?</p>
","9717415","","","scipy.minimize does not seem to adhere to constraints","<python><scipy><mathematical-optimization>","2","0","1557"
"50641862","2018-06-01 10:53:39","1","","<p>Did not read all the code but my guess is that passing an index vector into the <code>train_test_split</code> function would help you keep track of the samples.</p>

<pre><code>X = clusterDF[clusterDF.columns[clusterDF.columns.str.contains('\'AB\'')]]
y = clusterDF['Class']
indices = clusterDF.index
X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X, y, indices)
</code></pre>
","6632199","","","0","414","Jan K","2016-07-24 17:42:29","2571","210","113","4","50641576","","2018-06-01 10:38:20","0","88","<p>I have a dataset that I have run a K-means algorithm on (scikit-learn), and I want to build a decision tree on each cluster. I can recuperate the values from the cluster, but not the ""class"" values (I'm doing supervised learning, each element can belong to one of two classes and I need the value associated with the data to build my trees)</p>

<p>Ex: unfiltered data set:</p>

<pre><code>[val1 val2 class]
X_train=[val1 val2]
y_train=[class]
</code></pre>

<p>The clustering code is this:</p>

<pre><code>X = clusterDF[clusterDF.columns[clusterDF.columns.str.contains('\'AB\'')]]
y = clusterDF['Class']
(X_train, X_test, y_train, y_test) = train_test_split(X, y,
        test_size=0.30)

kmeans = KMeans(n_clusters=3, n_init=5, max_iter=3000, random_state=1)
kmeans.fit(X_train, y_train)
y_pred = kmeans.predict(X_test)
</code></pre>

<p>And this is my (unbelievably clunky!) code for extracting the values to build the tree. The issue is the Y values; they aren't consistent with the X values</p>

<pre><code>cl={i: np.where(kmeans.labels_ == i)[0] for i in range(kmeans.n_clusters)}
for j in range(0,len(k_means_labels_unique)):
    Xc=None
    Y=None
    #for i in range(0,len(k_means_labels_unique)):
    indexes = cl.get(j,0)
    for i, row in X.iterrows():
        if i in indexes:
            if Xc is not None:
                Xc = np.vstack([Xc, [row['first occurrence of \'AB\''],row['similarity to \'AB\'']]])
            else:
                Xc = np.array([row['first occurrence of \'AB\''],row['similarity to \'AB\'']])
            if Y is not None:
                Y = np.vstack([Y, y[i]])
            else:
                Y = np.array(y[i])
    Xc = pd.DataFrame(data=Xc, index=range(0, len(X)),
                     columns=['first occurrence of \'AB\'',
        'similarity to \'AB\''])  # 1st row as the column names


    Y = pd.DataFrame(data=Y, index=range(0, len(Y)),columns=['Class'])


        print(""\n\t-----Classifier "", j + 1,""----"")

        (X_train, X_test, y_train, y_test) = train_test_split(X, Y,
            test_size=0.30)

        classifier = DecisionTreeClassifier(criterion='entropy',max_depth = 2)
        classifier = getResults(
            X_train,
        y_train,
        X_test,
        y_test,
        classifier,
        filename='classif'+str(3 + i),
        )
</code></pre>

<p>Any ideas (or downright more efficient ways) of taking the clustered data to make a decision tree from?</p>
","6187682","","","Get values from k-means cluster after clustering","<python><scikit-learn><k-means><decision-tree>","1","0","2444"
"50641897","2018-06-01 10:55:33","0","","<p>The issue isn't so much about how to store a <code>SQLAlchemy</code> object but rather how to store any Object instance.</p>

<p>This is from <a href=""https://pymemcache.readthedocs.io/en/latest/apidoc/pymemcache.client.base.html#pymemcache.client.base.Client"" rel=""nofollow noreferrer"">docstring</a> of the pymemcache <code>Client</code> class that you've imported:</p>

<blockquote>
  <p>Values must have a <strong>str</strong>() method to convert themselves to a byte
  string.</p>
</blockquote>

<p>You haven't included a definition of the <code>Users</code> class that you are querying your database with so I can only assume you haven't overridden <code>__str__</code>. Therefore, when <code>pymemcache</code> tries to convert your object into a byte string, it is calling python's default <code>__str__</code> implementation and storing that.</p>
","6560549","","","0","857","SuperShoot","2016-07-07 11:43:55","3932","451","371","181","50625163","","2018-05-31 13:10:33","0","167","<p>code : </p>

<pre><code>from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
DBSession = sessionmaker()
DBSession.bind = engine
session = DBSession()
engine = create_engine('mysql://root:password@localhost/db', echo=True)
Base = declarative_base(engine)

class Accounts(Base):
    __tablename__ = 'accounts'
    __table_args__ = {'autoload': True}
</code></pre>

<p>I am trying to store sqlalchemy record object into memcache</p>

<pre><code>from pymemcache.client.base import Client
client = Client(('localhost', 11211))
client.set('testkey', session.query(Users).get(1))
</code></pre>

<p>It is storing string object instead of User object<br>
output : <code>'&lt;__main__.Users object at 0x105b69b10&gt;'</code></p>

<p>Any help ? </p>

<p>Thanks advance</p>
","6508904","6508904","2018-06-01 05:01:43","How to store sqlalchemy record object into memcache","<python><serialization><sqlalchemy><memcached>","2","3","845"
"50641965","2018-06-01 11:00:02","1","","<p>Still has a list comprehension loop, but works.</p>

<pre><code>import pandas as pd
import numpy as np

# Create dataframe which contains all days
df = pd.DataFrame({'x': np.arange(10), 'y': np.arange(10)**2},
                  index=pd.date_range(start=""2018-01-01"", periods=10))

# create second dataframe which only contains week-days or whatever dates you need.
ref_dates = [x for x in df.index if x.weekday() &lt; 5]

# Set the index of df to a forward filled version of the ref days
df.index = pd.Series([x if x in ref_dates else float('nan') for x in df.index]).fillna(method='ffill')

# Group by unique dates and sum
df = df.groupby(level=0).sum()

print(df)
</code></pre>
","5173575","5173575","2018-06-01 11:38:28","3","684","PdevG","2015-07-30 12:10:14","1832","141","215","17","50641757","50641965","2018-06-01 10:47:42","3","312","<p>I have a dataframe with a consecutive index (date for every calendar day) and a reference vector that does not contain every date (only working days).</p>

<p>I want to reindex the dataframe to only the dates in the reference vector with the missing data being aggregated to the latest entry <em>before</em> a missing-date-section (i.e. weekend data shall be aggregated together to the last Friday).</p>

<p>Currently I have implemented this by looping over the reversed index and collecting the weekend data, then adding it later in the loop. <strong>I'm asking if there is a more efficient ""array-way"" to do it.</strong></p>

<pre><code>import pandas as pd
import numpy as np
df = pd.DataFrame({'x': np.arange(10), 'y': np.arange(10)**2},
                  index=pd.date_range(start=""2018-01-01"", periods=10))
print(df)
ref_dates = pd.date_range(start=""2018-01-01"", periods=10)
ref_dates = ref_dates[:5].append(ref_dates[7:])  # omit 2018-01-06 and -07

# inefficient approach by reverse-traversing the dates, collecting the data
# and aggregating it together with the first date that's in ref_dates
df.sort_index(ascending=False, inplace=True)
collector = []
for dt in df.index:
    if collector and dt in ref_dates:
        # data from previous iteration was collected -&gt; aggregate it and reset collector
        # first append also the current data
        collector.append(df.loc[dt, :].values)
        collector = np.array(collector)

        # applying aggregation function, here sum as example
        aggregates = np.sum(collector, axis=0)

        # setting the new data
        df.loc[dt,:] = aggregates

        # reset collector
        collector = []

    if dt not in ref_dates:
        collector.append(df.loc[dt, :].values)

df = df.reindex(ref_dates)
print(df)
</code></pre>

<p>Gives the output (first: source dataframe, second: target dataframe)</p>

<pre><code>            x   y
2018-01-01  0   0
2018-01-02  1   1
2018-01-03  2   4
2018-01-04  3   9
2018-01-05  4  16
2018-01-06  5  25
2018-01-07  6  36
2018-01-08  7  49
2018-01-09  8  64
2018-01-10  9  81
             x   y
2018-01-01   0   0
2018-01-02   1   1
2018-01-03   2   4
2018-01-04   3   9
2018-01-05  15  77   # contains the sum of Jan 5th, 6th and 7th
2018-01-08   7  49 
2018-01-09   8  64
2018-01-10   9  81
</code></pre>
","3104974","","","pandas: Conditionally Aggregate Consecutive Rows","<python><pandas><dataframe><aggregate>","1","0","2318"
"50641968","2018-06-01 11:00:16","0","","<p>As per your code trials to populate the <strong>Email</strong> and <strong>Password</strong> field and <code>click()</code> on <strong>Sign in</strong> button you need to induce <em>WebDriverWait</em> for the <em>elements to be clickable</em> and you can use the following code block:</p>

<ul>
<li><p>Code Block:</p>

<pre><code>from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

options = webdriver.ChromeOptions() 
options.add_argument(""start-maximized"")
options.add_argument('disable-infobars')
driver=webdriver.Chrome(chrome_options=options, executable_path=r'C:\Utility\BrowserDrivers\chromedriver.exe')
driver.get(""https://courses.edx.org/login"")
WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.CSS_SELECTOR, ""input.input-block#login-email""))).send_keys(""Sakim@gmail.com"")
driver.find_element_by_css_selector(""input.input-block#login-password"").send_keys(""Sakim"")
driver.find_element_by_css_selector(""button.action.action-primary.action-update.js-login.login-button"").click()
</code></pre></li>
<li><p>Browser Snapshot:</p></li>
</ul>

<p><a href=""https://i.stack.imgur.com/RmMTK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RmMTK.png"" alt=""courses_edx_org""></a></p>
","7429447","","","0","1360","DebanjanB","2017-01-17 08:59:30","63154","13103","3455","2612","50640686","","2018-06-01 09:48:30","0","290","<p>I am not sure why selenium is not sending submit request. </p>

<p>edx.py           or Coursera                                                           </p>

<pre><code>from selenium import webdriver
browser = webdriver.Chrome()
browser.get('https://courses.edx.org/login')
email = browser.find_element_by_id('login-email')
email.send_keys('xxxxx@gmail.com')
pwd = browser.find_element_by_id('login-password')
pwd.send_keys('password')
login_attempt = browser.find_element_by_xpath('//*[@id=""login""]/button')
login_attempt.submit()
</code></pre>
","9880297","7429447","2018-06-01 10:59:23","How to login/submit request through Selenium and Python","<python><selenium><selenium-webdriver><xpath><css-selectors>","3","2","551"
"50642029","2018-06-01 11:04:49","1","","<p>I honestly fount the best way to get around this was to just create another model with all the fields that you require and named slightly different. Run migrations. Delete unused model and run migrations again. Voila.</p>
","5403449","","","0","225","Josh","2015-10-03 03:14:46","327","55","200","1","26185687","26185765","2014-10-03 19:42:08","84","115299","<p>I know that from Django 1.7 I don't need to use South or any other migration system, so I am just using simple command <code>python manage.py makemigrations</code></p>

<p>However, all I get is this error: </p>

<pre><code>You are trying to add a non-nullable field 'new_field' to userprofile without a default;
we can't do that (the database needs something to populate existing rows).
</code></pre>

<p>Here is models.py:</p>

<pre><code>class UserProfile(models.Model):
    user = models.OneToOneField(User)
    website = models.URLField(blank=True)
    new_field = models.CharField(max_length=140)
</code></pre>

<p>What are options?</p>
","2788600","","","You are trying to add a non-nullable field 'new_field' to userprofile without a default","<python><django>","13","1","645"
"50642067","2018-06-01 11:07:26","1","","<p><code>ASG = asObj.describe_auto_scaling_groups(AutoScalingGroupNames=[event['targetASG']])</code></p>

<p>I think <code>targetASG</code> is not part of the SNS event.. SNS Events look like this:</p>

<p><code>
{
  ""Records"": [
    {
      ""EventVersion"": ""1.0"",
      ""EventSubscriptionArn"": ""arn:aws:sns:EXAMPLE"",
      ""EventSource"": ""aws:sns"",
      ""Sns"": {
        ""SignatureVersion"": ""1"",
        ""Timestamp"": ""1970-01-01T00:00:00.000Z"",
        ""Signature"": ""EXAMPLE"",
        ""SigningCertUrl"": ""EXAMPLE"",
        ""MessageId"": ""95df01b4-ee98-5cb9-9903-4c221d41eb5e"",
        ""Message"": ""Hello from SNS!"",
        ""MessageAttributes"": {
          ""Test"": {
            ""Type"": ""String"",
            ""Value"": ""TestString""
          },
          ""TestBinary"": {
            ""Type"": ""Binary"",
            ""Value"": ""TestBinary""
          }
        },
        ""Type"": ""Notification"",
        ""UnsubscribeUrl"": ""EXAMPLE"",
        ""TopicArn"": ""arn:aws:sns:EXAMPLE"",
        ""Subject"": ""TestInvoke""
      }
    }
  ]
}
</code></p>

<p>You should grab the message and parse the <code>targetASG</code> value from there like e.g.</p>

<pre><code>import json

[...]
targetASG = event[‘Records’][0][‘Sns’][‘Message’]
myMessage = json.loads(targetASG)
name = myMessage['targetASG']
ASG = asObj.describe_auto_scaling_groups(AutoScalingGroupNames=[name])
[...]
</code></pre>
","419863","419863","2018-06-01 11:14:40","1","1368","H6.","2010-08-13 18:14:13","21062","617","4748","38","50639576","50642067","2018-06-01 08:52:29","0","1154","<p>I have lambda function to create new AMI and attach to current auto scale group. This function complete works when I create a test custom test case and pass the payload. Issue occurs when I trigger this from SNS:</p>

<pre><code>import json
import boto3
import time
import sys

asObj = boto3.client('autoscaling')
ec2Obj = boto3.client('ec2')

def lambda_handler(event, context):
    targetASG = event[‘Records’][0][‘Sns’][‘Message’]
    ASG = asObj.describe_auto_scaling_groups(AutoScalingGroupNames=[event['targetASG']])
    sourceInstanceId = ASG.get('AutoScalingGroups')[0]['Instances'][0]['InstanceId']
    Date=time.strftime(""%d%m%y"")
    Time=time.strftime(""%H%M%S"")
    amiName = ""Automated_%s_%s"" % (Date, Time)
    configName = ""Automated_%s_%s"" % (Date, Time)

    CreateNewImage = ec2Obj.create_image(
        InstanceId = sourceInstanceId,
        Name = amiName,
        Description = 'Automatically Created Image from Lambda Service',
        NoReboot = True)
    Image = []
    Image.append(CreateNewImage)
    def getCreatedID(Image):
        for i in Image:
            ID = i['ImageId']
            return ID
    AMINewID = getCreatedID(Image)
    CreateNewConfig = asObj.create_launch_configuration(
        LaunchConfigurationName = configName,
        ImageId = AMINewID,
        InstanceId = sourceInstanceId)

    updateASG = asObj.update_auto_scaling_group(
        AutoScalingGroupName = event['targetASG'],
        LaunchConfigurationName = configName)
    return 'A new AMI has been Created `%s` Updated ASG `%s` with new launch configuration `%s` which includes AMI `%s`.' % (amiName,event['targetASG'], configName, AMINewID)  
</code></pre>

<p>To trigger my lambda function I use AWS SNS topic and I publish a msg
following message, message type: raw</p>

<blockquote>
<pre><code>{   ""targetASG"": ""pre-production-xxx"" }
</code></pre>
</blockquote>

<p>But I get this error:</p>

<blockquote>
  <p>'targetASG': KeyError Traceback (most recent call last): File
  ""/var/task/lambda_function.py"", line 13, in lambda_handler ASG =
  asObj.describe_auto_scaling_groups(AutoScalingGroupNames=[event['targetASG']])
  KeyError: 'targetASG'</p>
</blockquote>

<p>How can I resolve this? </p>
","9776078","419863","2018-06-01 11:12:45","AWS Lambda key error","<python><amazon-web-services><aws-lambda><boto3>","1","0","2216"
"50642109","2018-06-01 11:10:07","0","","<p>I recommend starting with the <a href=""https://developers.google.com/drive/api/v3/quickstart/python"" rel=""nofollow noreferrer"">Python Quickstart</a> as your current seems to be mixing files.list and files.get. Then  you have the official <a href=""https://developers.google.com/api-client-library/python/apis/drive/v3"" rel=""nofollow noreferrer"">Python library reference for Drive API</a>.</p>
","6143482","","","0","395","noogui","2016-04-01 06:14:24","12810","2823","368","1130","50625364","","2018-05-31 13:20:54","0","163","<p>Trying to use the Google Drive API with Python for the first time. </p>

<p>This script lists the files in my Google Drive folder.</p>

<pre><code>from __future__ import print_function

from apiclient import discovery
from httplib2 import Http
from oauth2client import file, client, tools

SCOPES = 'https://www.googleapis.com/auth/drive.readonly.metadata'
store = file.Storage('storage.json')
creds = store.get()
if not creds or creds.invalid:
    flow = client.flow_from_clientsecrets('client_id.json', SCOPES)
    creds = tools.run_flow(flow, store)
DRIVE = discovery.build('drive', 'v3', http=creds.authorize(Http()))

# List files Google Drive
files = DRIVE.files().list().execute().get('files', [])
for f in files:
    print(f['name'], f['mimeType'], f['sharedWithMeTime'])
</code></pre>

<p>Going through the <a href=""https://developers.google.com/drive/api/v3/reference/files/list"" rel=""nofollow noreferrer"">docs</a>, I'm confused regarding the syntax here:</p>

<pre><code>files = DRIVE.files().list().execute().get('files', [])
</code></pre>

<p>What I (think I) understand:</p>

<ol>
<li><code>DRIVE</code> is the drive instance;</li>
<li>Files is as explained <a href=""https://developers.google.com/drive/api/v3/about-files"" rel=""nofollow noreferrer"">here</a>;</li>
<li>List can be called on files, as explained <a href=""https://developers.google.com/drive/api/v3/reference/files/list"" rel=""nofollow noreferrer"">here</a>;</li>
</ol>

<p>What I don't understand:</p>

<ol>
<li>From reading the docs, how could I tell <code>files</code> is a method?</li>
<li>Same for <code>list</code>;</li>
<li>What is <code>execute()</code> and how would I know to use it from reading the docs?</li>
<li>Same question for <code>get()</code></li>
<li>Where can I find the explanation for the parameters to pass to <code>get()</code>?</li>
</ol>

<p>Btw, I figure <code>execute()</code> executes the request, and <code>get()</code> makes an HTTP get request. However, I would like to know it from the docs so I can use these correctly. </p>
","3861965","","","Syntax of Google Drive API requests in Python","<python><google-drive-api>","1","0","2038"
"50642130","2018-06-01 11:11:10","1","","<p>Your <code>\n</code> is taken as <code>\</code> followed by <code>n</code> and not interpreted as it should. Use a command like <code>echo</code> or <code>printf</code> to correctly interpret it. This should work on almost any shell (<code>sh</code>, <code>bash</code>, <code>zsh</code>, etc).</p>

 <pre class=""lang-none prettyprint-override""><code>$ ./newline2argparse.py ""$(echo -en 'line1\nline2')""
$ ./newline2argparse.py ""$(printf 'line1\nline2')""
$ ./newline2argparse.py `printf ""line1\nline2""`
</code></pre>

<p>There are plenty of alternatives.</p>
","3889449","3889449","2018-06-01 11:33:35","1","561","Marco Bonelli","2014-07-29 21:34:49","28777","4747","1553","2127","50642064","50642130","2018-06-01 11:07:18","1","263","<p>In Python, using <a href=""https://docs.python.org/3/library/argparse.html#module-argparse"" rel=""nofollow noreferrer"">argparse</a>, is there any way to parse text containing a newline character given as a parameter?</p>

<p>I have this script:</p>

<pre><code>#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse

parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)
parser.add_argument('text', help='some text with newline')

args = parser.parse_args([""line1\nline2""])

print(args.text)
</code></pre>

<p>which prints as expected:</p>

<pre><code>line1
line2
</code></pre>

<p>but if I give the argument at the command-line (after changing to <code>args = parser.parse_args()</code> in the script above) it's not doing the same. For example:</p>

 <pre class=""lang-none prettyprint-override""><code>$ ./newline2argparse.py ""line1\nline2""
line1\nline2
</code></pre>

<p>Any ideas on this?</p>
","1738879","3889449","2018-06-01 11:33:58","Parse text with newline using argparse","<python><python-3.x><argparse>","2","3","932"
"50642168","2018-06-01 11:13:22","1","","<p>If you want escape sequences to be processed in a shell string, surround it with <code>$''</code></p>

<pre><code>./newline2argparse.py $'line1\nline2'
</code></pre>

<p>Note that this is a <code>bash</code> extension, it may not be supported by all other shells.</p>
","1491895","","","0","271","Barmar","2012-06-29 18:12:29","477375","68451","6422","3351","50642064","50642130","2018-06-01 11:07:18","1","263","<p>In Python, using <a href=""https://docs.python.org/3/library/argparse.html#module-argparse"" rel=""nofollow noreferrer"">argparse</a>, is there any way to parse text containing a newline character given as a parameter?</p>

<p>I have this script:</p>

<pre><code>#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse

parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)
parser.add_argument('text', help='some text with newline')

args = parser.parse_args([""line1\nline2""])

print(args.text)
</code></pre>

<p>which prints as expected:</p>

<pre><code>line1
line2
</code></pre>

<p>but if I give the argument at the command-line (after changing to <code>args = parser.parse_args()</code> in the script above) it's not doing the same. For example:</p>

 <pre class=""lang-none prettyprint-override""><code>$ ./newline2argparse.py ""line1\nline2""
line1\nline2
</code></pre>

<p>Any ideas on this?</p>
","1738879","3889449","2018-06-01 11:33:58","Parse text with newline using argparse","<python><python-3.x><argparse>","2","3","932"
"50642171","2018-06-01 11:13:33","3","","<p>You can create first <code>MultiIndex</code> in columns by parameter <code>header</code> and then loop by first level with <code>concat</code>:</p>

<pre><code>df = pd.read_csv(file, header=[0,1])

L = []
cols = df.columns.get_level_values(0)
for x in cols:
    c = df[x].columns.str.split(',')[0]
    a = pd.concat([df[x].squeeze()] * len(c), axis=1, keys=c)
    L.append(a)
df = pd.concat(L, axis=1, keys=cols)
</code></pre>

<p>With sample data:</p>

<pre><code>df = pd.DataFrame(data=np.random.random(size=(5,6)), 
                      columns={'a', 'b', 'c, d', 'c', 'f, g', 'h'})

#print (df)
L = []
for x in df.columns:
    c = x.split(', ')
    a = pd.concat([df[x].squeeze()] * len(c), axis=1, keys=c)
    L.append(a)

df = pd.concat(L, axis=1)
s = df.columns.to_series()
df.columns = s + s.groupby(s).cumcount().astype(str).radd('.').str.replace('.0', '')

print (df)
          c         h         a       c.1         d         b         f  \
0  0.846482  0.285415  0.695800  0.497593  0.497593  0.159911  0.286545   
1  0.195390  0.369074  0.371147  0.102207  0.102207  0.924279  0.349958   
2  0.967811  0.059451  0.942390  0.826203  0.826203  0.722080  0.196833   
3  0.546076  0.789354  0.876819  0.243305  0.243305  0.391054  0.213517   
4  0.311528  0.544023  0.380844  0.308427  0.308427  0.511651  0.795380   

          g  
0  0.286545  
1  0.349958  
2  0.196833  
3  0.213517  
</code></pre>
","2901002","2901002","2018-06-01 13:23:17","3","1417","jezrael","2013-10-20 20:27:26","427380","89269","18260","743","50641835","50642171","2018-06-01 10:51:32","3","58","<p>I need to clean up a data set, where some columns (read from .csv file) may have several names, listed with commas. </p>

<p>I need to do the following in pandas:</p>

<p><a href=""https://i.stack.imgur.com/dftNi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dftNi.png"" alt=""enter image description here""></a></p>

<p>Any nice pandasian tricks for that?</p>

<p>Here is a simple code:</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame(data=np.random.random(size=(5,6)), 
                  columns={'a', 'b', 'c, d', 'e', 'f, g', 'h'})


df=
          a         b      c, d         e      f, g         h
0  0.771418  0.371685  0.072876  0.153071  0.169513  0.399769
1  0.667551  0.886779  0.949341  0.869588  0.226275  0.273370
2  0.768456  0.945822  0.167757  0.584886  0.328152  0.246415
3  0.354713  0.690585  0.027916  0.237110  0.875449  0.430142
4  0.590518  0.819043  0.803876  0.909385  0.382452  0.867369
</code></pre>

<p>I need:</p>

<pre><code>df_new = 

          a         b         c         d         e         f         g         h
0  0.771418  0.371685  0.072876  0.072876  0.153071  0.169513  0.169513  0.399769
1  0.667551  0.886779  0.949341  0.949341  0.869588  0.226275  0.226275  0.273370
2  0.768456  0.945822  0.167757  0.167757  0.584886  0.328152  0.328152  0.246415
3  0.354713  0.690585  0.027916  0.027916  0.237110  0.875449  0.875449  0.430142
4  0.590518  0.819043  0.803876  0.803876  0.909385  0.382452  0.382452  0.867369
</code></pre>

<p><strong>UPDATE</strong></p>

<p>And what happens if I have repeated column names:</p>

<pre><code>df = pd.DataFrame(data=np.random.random(size=(5,6)), 
                      columns={'a', 'b', 'c, d', 'c', 'f, g', 'h'})
</code></pre>

<p>and the desired results should be</p>

<p>df_new_v2 = </p>

<pre><code>          a         b         c         d       c.1         f         g         h
0  0.771418  0.371685  0.072876  0.072876  0.153071  0.169513  0.169513  0.399769
1  0.667551  0.886779  0.949341  0.949341  0.869588  0.226275  0.226275  0.273370
2  0.768456  0.945822  0.167757  0.167757  0.584886  0.328152  0.328152  0.246415
3  0.354713  0.690585  0.027916  0.027916  0.237110  0.875449  0.875449  0.430142
4  0.590518  0.819043  0.803876  0.803876  0.909385  0.382452  0.382452  0.867369
</code></pre>
","5550203","5550203","2018-06-01 12:31:10","duplicate a single column with several names in Pandas","<python><pandas><dataframe>","2","1","2338"
"50642176","2018-06-01 11:13:48","1","","<p>You should compare the list with the actual values with a <em>bitwise exclusive or</em> (<code>^</code>):</p>

<pre><code>differences = set(one[""1iG5NDGVre""][""118""]) ^ set(two[""1iG5NDGVre""][""118""])
print differences
</code></pre>
","3032364","3032364","2018-06-01 11:32:59","10","233","Gsk","2013-11-25 12:45:52","2452","426","284","146","50641788","50642176","2018-06-01 10:48:55","0","45","<p>Trying to compare two dictionaries with this code:</p>

<pre><code>def dict_compare(d1, d2):
    d1_keys = set(d1.keys())
    d2_keys = set(d2.keys())
    intersect_keys = d1_keys.intersection(d2_keys)
    added = d1_keys - d2_keys
    removed = d2_keys - d1_keys
    modified = {o: (d1[o], d2[o]) for o in intersect_keys if d1[o] != d2[o]}
    same = set(o for o in intersect_keys if d1[o] == d2[o])
    return added, removed, modified, same

one = {""1iG5NDGVre"": {""118"": [""test1"", ""test2"", ""test3"", ""tcp"", ""22"", ""Red"", ""0.0.0.0/0""]}}
two = {""1iG5NDGVre"": {""118"": [""test1"", ""test2"", ""test3"", ""tcp"", ""22"", ""Red"", ""Blue"", ""0.0.0.0/0""]}}

added, removed, modified, same = dict_compare(one,two)

print added
print removed
print modified
print same
</code></pre>

<p>However, it prints the modified key/values wrongly.</p>

<p>Output:</p>

<pre><code>set([])
set([])
{'1iG5NDGVre': ({'118': ['test1', 'test2', 'test3', 'tcp', '22', 'Red', '0.0.0.0/0']}, {'118': ['test1', 'test2', 'test3', 'tcp', '22', 'Red', 'Blue', '0.0.0.0/0']})}
set([])
</code></pre>

<p>Any ideas how to correct it?</p>

<p>I just want that it prints ""Blue"" in modified.</p>

<p><strong>Update 1:</strong></p>

<p>Works but not when the dicts have different keys number i.e</p>

<pre><code>one = {""1iG5NDGVre"": {""118"": [""test1"", ""test2"", ""test3"", ""tcp"", ""22"", ""Red"", ""0.0.0.0/0""]}}
two = {""1iG5NDGVre"": {""118"": [""test1"", ""test2"", ""test3"", ""tcp"", ""22"", ""Red"", ""Blue"", ""0.0.0.0/0""]},""119"": [""test10"",""test11""]}
</code></pre>

<p>Will not show up test10 and test11 as added.</p>

<p>Dict could also have less keys, key can be also removed. Want to also cover that case.</p>

<p>Big thanks!</p>
","5376493","5376493","2018-06-01 12:05:20","Python 2.7 - Show only modified dictionary key/value while comparing dictionaries","<python><dictionary>","1","8","1663"
"50642185","2018-06-01 11:14:38","1","","<p>AFAIK there is no easy way to achieve this in Weblate. What you can however easily do is to add <code>--no-wrap</code> parameter to <code>msgmerge</code> (assuming you use this for updating the po files).</p>
","225718","","","0","212","Michal Čihař","2009-12-06 10:01:18","8111","849","710","44","50627667","50642185","2018-05-31 15:18:15","0","59","<p>In an attempt to reduce the frequency of merge conflicts and unhelpfully large diffs in weblate generated PO files I have opted to use the ""no wrap"" formatting option of the ""Customize gettext output"" addon on an existing project.</p>

<p>This seems to work fine however the nowrap formatting is only applied to strings that are modified. Is there a way I force weblate to regenerate all strings in the PO files even if the string is unchanged (thereby applying nowrap to every translation string in the project/component)?</p>

<p><a href=""https://docs.weblate.org/en/latest/admin/addons.html#customize-gettext-output"" rel=""nofollow noreferrer"">https://docs.weblate.org/en/latest/admin/addons.html#customize-gettext-output</a></p>
","5320791","","","Is there a way to regenerate PO files from weblate with new formatting?","<python><django><translation><weblate>","1","0","735"
"50642188","2018-06-01 11:14:44","-1","","<pre><code>&gt;&gt;&gt;import datetime
&gt;&gt;&gt;year = int(input())
&gt;&gt;&gt;month = int(input())
&gt;&gt;&gt;day = int(input())
data = datetime.datetime(year,month,day)
daynew = data.toordinal()
yearstart = datetime.datetime(year,1,1)
day_yearstart = yearstart.toordinal()
print ((daynew-day_yearstart)+1)
</code></pre>
","9880846","9880846","2018-06-04 06:43:39","1","327","Egor014","2018-06-01 11:02:58","1","6","0","0","2427555","2427581","2010-03-11 18:21:43","46","28066","<p>I have a year value and a day of year and would like to convert to a date (day/month/year). </p>

<p>Thanks in advance. :) </p>
","233411","","","Python Question: Year and Day of Year to date?","<python><date><julian-date>","4","0","131"
"50642265","2018-06-01 11:19:00","0","","<p>What I was trying to do was unnecessary. </p>

<p>The correct way to achieve this was to convert all of my querysets into lists and then pass them to jquery in JsonResponse. Then I can clear and load these lists as select options in my jquery as and when I need. </p>

<p>updated code:</p>

<p>views</p>

<pre><code>if request.method == ""GET"":
        if request.is_ajax():
            print(""ajax test"")

            #print(request.GET.get)
            print(request.GET.get('coin'))
            print(request.GET.get('buysell'))

            view_coin = None 
            if request.GET.get('coin'):
                view_coin = GetCoin(request.GET.get('coin')).price

            coin_sell_options = Portfolio.objects.filter(user = request.user).values('coin').distinct()
            coin_buy_options = Coin.objects.all()

            coin_buy_options = [x.coin for x in coin_buy_options]

            coins = [key['coin'] for key in coin_sell_options]
            coin_amount = list(Portfolio.objects.filter(user = request.user, coin__in = coins).values_list('amount', flat = True))
            coin_amount = [str(x) for x in coin_amount]
            print(coin_amount)

            data = {
                'view_buysell': request.GET.get('buysell'),
                'view_coin': request.GET.get('coin'),
                'view_amount': ""test"",
                'view_price': view_coin,
                'coin_sell_options': list(coin_sell_options),
                'coin_buy_options': list(coin_buy_options),
                'coin_amounts': coin_amount
            }

            form = TransactionForm(user = request.user, coin_price = GetCoin(""Bitcoin"").price)

            return JsonResponse(data)
</code></pre>

<p>jquery</p>

<pre><code>$('#id_buysell').on('change', function(){

            console.log(""buysell"");

            var $id_buysell = $('#id_buysell').val();
            console.log($id_buysell);

            $.ajax({
                method: ""GET"",
                url: ""/myportfolio/add_transaction"",
                dataType: 'json',
                data: { buysell: $id_buysell },
                success: function(data, status) {
                    console.log(""SUCCESS:"");
                    console.log(data);
                    console.log(data['view_buysell']);

                    $(""#id_coin"").empty();                  

                    var items = data['coin_options'];

                    console.log(items);

                    $.each(items, function(key, value) {
                        console.log(value.coin);
                        $(""#id_coin"").append(new Option(value.coin));
                    });


                },
                error: function(response) {

                }
            });

        });
</code></pre>
","3219210","","","0","2783","SkillSet12345","2014-01-21 12:32:25","326","53","463","0","50581423","50642265","2018-05-29 09:55:44","3","1179","<p>I want the queryset of my coin field to change when a user selects ""Sell"" in the ""BuySell"" dropdown option with jquery. Once the dropdown is changed I send a Get Request with AJAX, pick that request up in my view and then reload the form, which is where I override the default coin field queryset in my TransactionForm's init method.</p>

<p>This isn't working as expected, nothing happens to change the coin dropdown options and I get no errors (including in the Network tab when I inspect element).</p>

<p>I wonder if this is something to do with the way I'm calling my form here:</p>

<pre><code>form = TransactionForm(user = request.user, coin_price = GetCoin(""Bitcoin"").price)
</code></pre>

<p>and the form init method:</p>

<pre><code>def __init__(self, coin_price = None, user = None, *args, **kwargs):    
        super(TransactionForm, self).__init__(*args, **kwargs)

        if user:

            self.user = user
            qs_coin = Portfolio.objects.filter(user = self.user).values('coin').distinct()
            print(""qs_coin test: {}"".format(qs_coin))
            self.fields['coin'].queryset = qs_coin
</code></pre>

<p>FULL CODE: </p>

<p>Forms</p>

<pre><code>class TransactionForm(forms.ModelForm):     
    CHOICES = (('Buy', 'Buy'), ('Sell', 'Sell'),)

    coin = forms.ModelChoiceField(queryset = Coin.objects.all()) 
    buysell = forms.ChoiceField(choices = CHOICES)

    field_order = ['buysell', 'coin', 'amount', 'trade_price']

    class Meta:
        model = Transaction
        fields = {'buysell', 'coin', 'amount', 'trade_price'}

    def __init__(self, coin_price = None, user = None, *args, **kwargs):
        super(TransactionForm, self).__init__(*args, **kwargs)
        print(""Transaction form init: "", user, coin_price)

        if user:
            self.user = user
            qs_coin = Portfolio.objects.filter(user = self.user).values('coin').distinct()
            print(""qs_coin test: {}"".format(qs_coin))
            self.fields['coin'].queryset = qs_coin
</code></pre>

<p>Views snippet</p>

<pre><code>def add_transaction(request):

    if request.method == ""GET"":
        if request.is_ajax():
            print(""ajax test"")

            #print(request.GET.get)
            print(request.GET.get('coin'))
            print(request.GET.get('buysell'))

            view_coin = None 
            if request.GET.get('coin'):
                view_coin = GetCoin(request.GET.get('coin')).price

            data = {
                'view_buysell': request.GET.get('buysell'),
                'view_coin': request.GET.get('coin'),
                'view_amount': ""test"",
                'view_price': view_coin
            }

            form = TransactionForm(user = request.user, coin_price = GetCoin(""Bitcoin"").price)

            return JsonResponse(data)
</code></pre>

<p>jquery</p>

<pre><code>$('#id_buysell').on('change', function(){

        console.log(""buysell"");

        var $id_buysell = $('#id_buysell').val();
        console.log($id_buysell);

        $.ajax({
            method: ""GET"",
            url: ""/myportfolio/add_transaction"",
            dataType: 'json',
            data: { buysell: $id_buysell },
            success: function(data, status) {
                console.log(""SUCCESS:"");
                console.log(data);
                console.log(data['view_buysell']);

            },
            error: function(response) {

            }
        });

    });

$('#id_coin').on('change', function(){

    console.log(""test"")
    console.log(""coin change"")

    var $id_coin = $('#id_coin').find(""option:selected"").text();
    console.log($id_coin);

    $.ajax({
        method: ""GET"",
        url: ""/myportfolio/add_transaction"",
        dataType: 'json',
        data: {coin: $id_coin},
        success: function(data, status) {
            console.log(""SUCCESS:"");
            console.log(data);
            console.log(data['view_buysell']);

            $(""#id_trade_price"").val(data['view_price']);
        },
        error: function(response) {

        }
    });
</code></pre>
","3219210","3219210","2018-06-01 11:21:02","Django: How to change form field select options with different Querysets, based on other form field options selected?","<python><django>","1","0","4068"
"50642268","2018-06-01 11:19:07","0","","<p>You can always convert to a list, slice off the end and convert back to a set:</p>

<pre><code>s = {1, '2', 'three', 4, (5,), 6}
print(s)

required_length = len(s) - 2
s = set(list(s)[:required_length])

print(s)
</code></pre>

<p>Output:</p>

<pre><code>{1, 4, 'three', 6, (5,), '2'}
{1, 4, 'three', 6}
</code></pre>
","4834","","","0","321","quamrana","2008-09-05 20:27:56","15246","2075","2510","88","50642150","","2018-06-01 11:12:10","0","50","<p>I would like to decrease the number of elements in a set. I'm not interested in removing specific elements. Any elements would do, but I would like to remove multiple elements at once. If set.pop() would accept count as parameter it would be great but unfortunatelly it's not the case.</p>

<p>Any ideas? I would like to avoid using pop() in a loop as I find it cumbersome.</p>
","2032932","","","How to trim the size of a set in Python","<python><set>","1","1","381"
"50642269","2018-06-01 11:19:07","0","","<p>you have to enable external execution.
goto script editor -> settings -> check ""can execute external programs""</p>

<p><a href=""https://i.stack.imgur.com/Z2uxN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Z2uxN.png"" alt=""enter image description here""></a></p>
","2342292","","","2","285","EldadT","2013-05-02 08:22:26","819","106","51","6","50623125","","2018-05-31 11:20:31","0","289","<p>I am trying to execute an external python script in QlikView Desktop 11. I've tried everything but the EXECUTE command does not seem to work. The script fetches a file and writes into it. </p>
","8897762","","","Execute Python script in QlikView 11","<python><qlikview><qliksense>","1","6","196"
"50642270","2018-06-01 11:19:14","4","","<p><strong>Solution 1 : packages.date.py :</strong></p>

<pre><code>import os
import time
from pip._internal.utils.misc import get_installed_distributions

for package in get_installed_distributions():
     print (package, time.ctime(os.path.getctime(package.location)))
</code></pre>

<p><strong>Solution 2 : packages.alt.date.py :</strong></p>

<pre><code>#!/usr/bin/env python
# Prints when python packages were installed
from __future__ import print_function
from datetime import datetime
from pip._internal.utils.misc import get_installed_distributions
import os

if __name__ == ""__main__"":
    packages = []
    for package in get_installed_distributions():
        package_name_version = str(package)
        try:
            module_dir = next(package._get_metadata('top_level.txt'))
            package_location = os.path.join(package.location, module_dir)
            os.stat(package_location)
        except (StopIteration, OSError):
            try:
                package_location = os.path.join(package.location, package.key)
                os.stat(package_location)
            except:
                package_location = package.location
        modification_time = os.path.getctime(package_location)
        modification_time = datetime.fromtimestamp(modification_time)
        packages.append([
            modification_time,
            package_name_version
        ])
    for modification_time, package_name_version in sorted(packages):
        print(""{0} - {1}"".format(modification_time,
                                 package_name_version))
</code></pre>

<p><strong>Solution 1 &amp; 2 compatibility :</strong></p>

<ul>
<li>updated solution for pip v10.x</li>
<li>python v2, v2.7, v3, v3.5</li>
</ul>
","4877948","4877948","2018-06-01 11:29:56","0","1726","intika","2015-05-08 07:17:59","2624","245","417","21","24736316","","2014-07-14 12:13:22","28","15342","<p>I know how to see installed Python packages using pip, just use <code>pip freeze</code>. But is there any way to see the date and time when package is installed or updated with pip?</p>
","3771104","592323","2014-07-14 15:17:52","See when packages were installed / updated using pip","<python><pip>","7","0","189"
"50642282","2018-06-01 11:20:10","0","","<p>Alembic doesn't have to connect to the database using the database URI to run upgrades. In your <code>alembic\env.py</code> file there will be a function like this:</p>

<pre><code>def run_migrations_online():
    """"""Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """"""
    connectable = engine_from_config(
        config.get_section(config.config_ini_section),
        prefix='sqlalchemy.',
        poolclass=pool.NullPool)

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()
</code></pre>

<p>All that matters is that that the variable <code>connectable</code> is an <code>engine</code> instance when alembic calls <code>connect()</code> on it.</p>

<p>Therefore you could do something like this (this isn't tested but I do something similar myself):</p>

<pre><code>def run_migrations_online():
    """"""Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """"""

    sshtunnel.SSH_TIMEOUT = 5.0
    sshtunnel.TUNNEL_TIMEOUT = 5.0

    server =  sshtunnel.SSHTunnelForwarder(
        ('ssh.pythonanywhere.com', 22),
        ssh_password=""mypassword"",
        ssh_username=""myusername"",
        remote_bind_address=\
            (myname.mysql.pythonanywhere-services.com', 3306))
    server.start()

    connectable = create_engine(
        'mysql+mysqldb://mynameb:dbpassword@127.0.0.1:%s/dbname' % 
        server.local_bind_port
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()
</code></pre>

<p>Ideally you'd put all of your connection logic somewhere it can be accessed both from the <code>alembic/env.py</code> and in your project so you only have it defined once and then you could just import the <code>engine</code> directly into <code>env.py</code>, but you get the idea.</p>
","6560549","","","0","2251","SuperShoot","2016-07-07 11:43:55","3932","451","371","181","50620531","50642282","2018-05-31 09:05:07","0","110","<p>I have a Flask application that I need to connect to a distant MysqlDB using a SSHTunnel like this in my config.py file I init in my <strong>init</strong>.py file:</p>

<pre><code>sshtunnel.SSH_TIMEOUT = 5.0
sshtunnel.TUNNEL_TIMEOUT = 5.0

server =  sshtunnel.SSHTunnelForwarder(
    ('ssh.pythonanywhere.com', 22),
    ssh_password=""mypassword"",
    ssh_username=""myusername"",
    remote_bind_address=(myname.mysql.pythonanywhere-services.com', 3306))
server.start()

engine = create_engine('mysql+mysqldb://mynameb:dbpassword@127.0.0.1:%s/dbname' % server.local_bind_port)
</code></pre>

<p>The connection seems to be working but I can't upgrade my DB from my migrations (flask db upgrade) since I'm not using SQLALCHEMY_DATABASE_URI to connect to my DB. Is there still a way to make the db upgrade working with a ssh connection to the DB ?</p>
","8108811","","","SQLAlchemy DB upgrade and MySQL SSH","<python><mysql><flask><sqlalchemy><flask-sqlalchemy>","1","0","850"
"50642283","2018-06-01 11:20:20","1","","<p>Try using <strong><a href=""https://pandas.pydata.org/pandas-docs/version/0.21/generated/pandas.DataFrame.isin.html"" rel=""nofollow noreferrer"">isin</a></strong>:</p>

<p><strong>Ex:</strong></p>

<pre><code>df=pd.read_csv('some.xlsx',index_col=False)
df1 = df.loc[df['Name'].isin(['A','B']), ['Name','address','post','city']]
df1.to_csv('ABC.csv')
</code></pre>

<ul>
<li>Pass your list as input to <code>isin</code></li>
</ul>

<p><em>Edit as per comment - Using a loop</em></p>

<pre><code>df1 = None
for i in ['A','B']:
    if df1 is not None:
        df1 = df1.append( df.loc[df['Name'] == i, ['Name','address','post','city']] )
    else:
        df1 =  df.loc[df['Name'] == i, ['Name','address','post','city']]

df1.to_csv(filename)
</code></pre>
","532312","532312","2018-06-01 11:39:43","5","754","Rakesh","2010-12-06 13:07:54","56694","5302","758","1508","50642204","50642283","2018-06-01 11:15:54","0","83","<p>I have a csv sheet and it looks like below , each column has  some data.
Want to filter data by Name . I know the names, from which I have to filter. </p>

<pre><code>Name  gender  address  age  post  city 
A      M       abc     20   dd     ASD
C      F       xyz     21   ll     KLM
B      M       lmn     22   mm     NOP
</code></pre>

<p>Want output like.</p>

<pre><code>Name    address     post   city 
A         abc        dd     ASD
B         lmn        mm     NOP
</code></pre>

<p>The code below obviously overwrite the first data in csv. How to avoid that and fix this. Also, want to get rid of index column from excel. Any better way to code this ? New to Panda here .   </p>

<pre><code>for i in ['A','B']: #The names list is huge , taking 2 as example
        df=pd.read_csv('some.xlsx',index_col=False)
        df1= df.loc[df['Name'] == i, ['Name','address','post','city']]
df1.to_csv('ABC.csv')
</code></pre>
","9838554","9838554","2018-06-01 18:02:59","write panda data to csv","<python><python-3.x><python-2.7><pandas><csv>","2","0","928"
"50642334","2018-06-01 11:22:46","0","","<p>the square root function, by definition, gives the positive (also called principle) root of the number it's square rooting.</p>

<p>the ""negative square root"" is part of a more general set of solutions to the equation a^n = b, where n is the degree of the root, b is the number you're taking the square root of, and a is the solution (a and b might be complex).</p>

<p>the point is that the ""general square root"" is ambiguous because it has a bunch of solutions for one input. it's not a (mathematical) function. numpy uses the standard that everyone uses, which is the positive/principle solution.</p>

<p>if you want a sphere, you can make two half spheres and flip one of them upside down. (I don't know how to fill in the gap between them :( )</p>

<p>as far as the runtime warning, you're taking the square root OF a negative number at the corners of the grid, where there's no (real) solution to the circle. there should be no solution there, since the circle doesn't exist there. (if you print zs, you should see NaNs.)</p>
","9878957","9878957","2018-06-01 11:35:06","2","1035","Jay Calamari","2018-06-01 02:37:50","335","32","205","2","50642170","","2018-06-01 11:13:31","0","1397","<p>I am trying to draw a sphere in cartesian coordinates.
However, I get a half sphere as if np.sqrt gives me only positive values.
How can I complete the sphere? Any alternative to np.sqrt?</p>

<p>I know how to draw a sphere in polar coordinates or with sin and cosine functions so I am only interested in drawing it by using x, y, z values such as;</p>

<pre><code>x**2+y**2+z**2=1
</code></pre>

<p>Here is the code gives half sphere and the warning;</p>

<pre><code>import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(20, 10))
ax = fig.add_subplot(1,2,1, projection = '3d')

xs=np.linspace(-1,1, 100)
ys=np.linspace(-1,1, 100)
xs,ys=np.meshgrid(xs,ys)
zs= np.sqrt(1-xs*xs-ys*ys)

ax.plot_surface(xs, ys, zs, lw = 0, antialiased = True)
</code></pre>
","9423188","9423188","2018-06-04 04:50:51","Numpy, RuntimeWarning: invalid value encountered in sqrt","<python><numpy><plotly>","1","0","802"
"50642337","2018-06-01 11:22:51","3","","<p>You passed a <code>set</code> of column names which don't necessarily preserve the order, if you passed a <code>list</code> then it would do what you want:</p>

<pre><code>In[32]:
df = pd.DataFrame(data=np.random.random(size=(5,3)), 
                  columns=['a', 'b', 'c'])
df

Out[32]: 
          a         b         c
0  0.227711  0.410568  0.795012
1  0.624751  0.708471  0.152641
2  0.901483  0.967297  0.884749
3  0.353622  0.220706  0.031015
4  0.628634  0.128421  0.679261
</code></pre>
","704848","","","3","500","EdChum","2011-04-12 20:25:50","211433","20724","13608","9177","50642290","50642337","2018-06-01 11:20:33","1","36","<p>I'm a bit puzzled about the behavior of DataFrame</p>

<p>For example:</p>

<pre><code>df = pd.DataFrame(data=np.random.random(size=(5,3)), 
                  columns={'a', 'b', 'c'})
</code></pre>

<p>why the default output is:</p>

<pre><code>df =      c         a         b
0  0.325172  0.831253  0.151912
1  0.558476  0.177249  0.906136
2  0.516089  0.069013  0.370251
3  0.440246  0.154116  0.494690
4  0.793981  0.409526  0.885879
</code></pre>

<p>and not the ordered list of columns ('a', 'b', 'c')</p>

<p>(Python 3.6, Pandas 0.23)</p>
","5550203","5550203","2018-06-01 11:22:26","ordering of column names in Pandas","<python><pandas>","1","1","548"
"50642343","2018-06-01 11:23:09","-1","","<p>I was able to solve this by taking contours and drawing a min bounding rectangle around the contours. </p>

<p>There were still couple of exception cases to be handled such as - what if there are other contours whose min bounding rectangle overlaps the contours of the L shaped images , but for now i am happy with this result. Thanks for all the comments</p>
","4614444","","","0","363","Thalish Sajeed","2015-02-27 11:59:40","884","118","39","18","50529731","50642343","2018-05-25 12:54:24","-1","135","<p>How would i go about extracting the region of interest - which is the rectangle enclosed by the L shaped lines. Are there any functions in opencv or skimage that would help me with this? I am working with Python. </p>

<p>Note - The green boxes were drawn by me to highlight the ROI
<a href=""https://i.stack.imgur.com/jVYHC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jVYHC.png"" alt=""image""></a></p>
","4614444","6885902","2018-07-04 12:37:57","Identifying L shapes in an image using Python","<python><opencv><image-processing><computer-vision><scikit-image>","1","10","426"
"50642373","2018-06-01 11:24:28","0","","<p><a href=""https://docs.python.org/3.6/library/xml.etree.elementtree.html#xml.etree.ElementTree.parse"" rel=""nofollow noreferrer"">et.parse</a> expects a file name but you are giving it an opened file. Try to pass your <code>file</code> variable.</p>

<pre><code>import glob
import lxml.etree as et

for f in glob.glob('*.ditamap'):
    tree = et.parse(f)
    print (et.tostring(tree, pretty_print=True))
</code></pre>

<p>You may want to consider using <a href=""https://docs.python.org/3.6/library/xml.etree.elementtree.html#xml.etree.ElementTree.parse"" rel=""nofollow noreferrer"">glob.iglob</a> because you are only using it as an iterator.</p>

<p>Edit: Overread that <code>et.parse</code> can accpect file objets. Give it a try nevertheless.</p>
","7057528","7057528","2018-06-01 18:10:06","3","748","jsmolka","2016-10-22 15:01:21","608","39","127","1","50642250","50642373","2018-06-01 11:18:13","0","87","<p>I'm having some difficulty trying to parse a folder of valid xml files <code>(*.ditamap)</code> using python 3 and <code>lxml</code>.</p>

<p>The error returned is</p>

<pre><code>""lxml.etree.XMLSyntaxError: Document is empty, line 1, column 1""
</code></pre>

<p>my code</p>

<pre><code>import glob
import lxml.etree as et

for file in glob.glob('*.ditamap'):
    with open(file) as xml_file:
        #tree = et.parse(""0579182.ditamap"")
        tree = et.parse(xml_file)
        print (et.tostring(tree, pretty_print=True))
</code></pre>

<p><code>et.parse</code> works when i pass a filename directly, but not when I pass the file variable. </p>

<p>What am I doing wrong? Seems like there is a some kind of IO error or tpye mismatch but I cannot see what I am doing wrongly...</p>
","7010427","9020340","2018-06-01 11:31:51","Parsing a folder of xml using glob and lxml","<python><lxml><glob>","1","0","786"
"50642406","2018-06-01 11:26:57","3","","<p>I know this is quite late but here is an answer:</p>

<pre><code>import boto3
bucket='sagemaker-dileepa' # Or whatever you called your bucket
data_key = 'data/stores.csv' # Where the file is within your bucket
data_location = 's3://{}/{}'.format(bucket, data_key)
df = pd.read_csv(data_location)
</code></pre>
","9880956","","","3","313","mish1818","2018-06-01 11:26:35","39","6","1","0","48111034","","2018-01-05 09:52:06","4","5118","<p>I'm trying to load a large CSV (~5GB) into pandas from S3 bucket.</p>

<p>Following is the code I tried for a small CSV of 1.4 kb :</p>

<pre><code>client = boto3.client('s3') 
obj = client.get_object(Bucket='grocery', Key='stores.csv')
body = obj['Body']
csv_string = body.read().decode('utf-8')
df = pd.read_csv(StringIO(csv_string))
</code></pre>

<p>This works well for a small CSV, but my requirement of loading a 5GB csv to pandas dataframe cannot be achieved through this (probably due to memory constraints when loading the csv by StringIO). </p>

<p>I also tried below code</p>

<pre><code>s3 = boto3.client('s3')
obj = s3.get_object(Bucket='bucket', Key='key')
df = pd.read_csv(obj['Body'])
</code></pre>

<p>but this gives below error.</p>

<pre><code>ValueError: Invalid file path or buffer object type: &lt;class 'botocore.response.StreamingBody'&gt;
</code></pre>

<p>Any help to resolve this error is much appreciated.</p>
","1633008","","","Reading a large csv from a S3 bucket using python pandas in AWS Sagemaker","<python><csv><amazon-s3><amazon-sagemaker>","1","2","941"
"50642413","2018-06-01 11:27:29","1","","<p>If you have just one column with this format <code>a 43</code>, this should work:</p>

<pre><code>df.columns = ['col']
df = pd.DataFrame(df.col.str.split(' ',1).tolist(), columns = ['col1','col2']).T.reset_index(drop=True)
df = df.rename(columns=df.iloc[0]).drop(df.index[0])
</code></pre>

<p>Input:</p>

<pre><code>df = pd.DataFrame(data=[
     ['a 43',],
     ['b 630'],
     ['r 587']],
columns=['col'])

     col
0   a 43
1  b 630
2  r 587
</code></pre>

<p>Output:</p>

<pre><code>    a    b    r
1  43  630  587
</code></pre>
","5178905","5178905","2018-06-01 11:40:39","3","536","Joe","2015-07-31 17:53:47","7327","442","644","3","50642233","50642413","2018-06-01 11:17:18","1","77","<p>I have a dataframe like this-</p>

<pre><code>                 0
0            a  43
1            b  630
2            r  587
3            i  462
4            g  153
5            t  266
</code></pre>

<p>I want to create a new dataframe which looks like this-</p>

<pre><code>     a         b     r       i     g        t
0    43       630   587    462    153      266
</code></pre>
","8660280","9209546","2018-06-01 14:52:58","Creating single row pandas dataframe","<python><pandas><dataframe>","3","0","384"
"50642443","2018-06-01 11:29:55","2","","<p>You can transpose your dataframe via <code>df.T</code> or <code>df.transpose()</code>:</p>

<pre><code>df = pd.DataFrame([['A', 1], ['B', 2], ['C', 3], ['D', 4], ['E', 5]],
                  columns=['col1', 'col2'])

print(df)

  col1  col2
0    A     1
1    B     2
2    C     3
3    D     4
4    E     5

res = df.set_index('col1').T
</code></pre>

<p>Result:</p>

<pre><code>print(res)

col1  A  B  C  D  E
col2  1  2  3  4  5
</code></pre>
","9209546","","","2","448","jpp","2018-01-12 14:47:22","109049","18235","7890","3496","50642233","50642413","2018-06-01 11:17:18","1","77","<p>I have a dataframe like this-</p>

<pre><code>                 0
0            a  43
1            b  630
2            r  587
3            i  462
4            g  153
5            t  266
</code></pre>

<p>I want to create a new dataframe which looks like this-</p>

<pre><code>     a         b     r       i     g        t
0    43       630   587    462    153      266
</code></pre>
","8660280","9209546","2018-06-01 14:52:58","Creating single row pandas dataframe","<python><pandas><dataframe>","3","0","384"
"50642473","2018-06-01 11:31:39","1","","<p>Do you really need a class for this ? a simple function would work:</p>

<pre><code>def printkey(data, key):
    for line in data[key]:
        print(line)

printkey(data, ""key1"")
printkey(data, ""key2"")
</code></pre>

<p>Actually, mixing domain logic with presentation is a well known antipattern. If you <em>really</em> want a class (to make some narrow-minded OO purist happy or for better reasons that are not explained in your question), then you can of course write some ""presenter"" class:</p>

<pre><code>class DataPrinter(object):
    def __init__(self, data):
        self.data = data

    def _printkey(self, key):
        for line in self.data[key]:
            print(line)

    def printkey1(self):
        self._printkey(""key1"")

    def printkey2(self):
        self._printkey(""key2"")
</code></pre>

<p>but that's really mostly useless IMHO, at least for the given example (it might make sense for some much more complex data model and presentation stuff).</p>

<p>Oh and yes, FWIW, Python functions ARE objects (instances of class <code>function</code>) so you can just tell some-narrow-minded-OO-purist that the function-based solution IS actually proper OO design too xD (poor Java dudes had to invent a design pattern  - the ""functor"" - to hide the fact that sometimes all you need is a plain old function...).</p>
","41316","41316","2018-06-04 07:03:22","2","1335","bruno desthuilliers","2008-11-27 10:40:14","57630","7809","2022","2444","50642012","","2018-06-01 11:03:24","0","42","<p>I have a class with one method which outputs a <code>dict</code>. The class needs a file_path to be instantiated. The method needs a string parameter to output the dict. Here is some pseudo-code.</p>

<pre><code>class A():
    def __init__(self, file_path):
        self.file_path = file_path
    def create_dict(self, param):
        #uses self.filepath
        #creates a dict called data. The dict contains 2 keys and each key contains a list. The first list is a list of strings and the second a list of integers
        return data
</code></pre>

<p>The procedure to generate this <code>dict</code> is irrelevant in my opinion. The final output is as follows:</p>

<p><code>data = {'Key1': ['String1', 'String2', 'String3',....], 'Key2': [1,2,4,....]}</code></p>

<p>When I started to implement this class I noticed that I wanted to have <code>data</code> be an object instead. This object would have methods <code>data.printKey1()</code> and <code>data.printKey2()</code> which would output the following:</p>

<pre><code>&gt;&gt;&gt; data.printKey1()
String1
String2
String3 
...
&gt;&gt;&gt;data.printKey2()
1
2
4
</code></pre>

<p>Now I'm stuck because I really don't want to reconstruct the whole procedure by which <code>data</code> is generated, but I need it to be an object.</p>

<p>I am not sure whether I should use inheritance, <a href=""https://stackoverflow.com/questions/3222251/python-2-6-class-inside-a-class"">class containment</a> or <a href=""https://stackoverflow.com/questions/49528586/redefining-a-subclass-sort-of-structure-using-inheritance"">composition</a> (or some other construct I am unaware of). In theory, what is the best practice to use here? How can I generate this new <code>data</code> class/object without having to rework the entire procedure contained in the <code>create_dict</code> method?</p>

<p>Thanks</p>
","3783002","","","Best object oriented python construct to use for particular application","<python><oop><inheritance><composition>","1","6","1855"
"50642489","2018-06-01 11:32:10","1","","<p>Some other tests. Of course to do...</p>

<pre><code>set([x for x in l if l.count(x) &gt; 1])
</code></pre>

<p>...is too costly. It's about 500 times faster (the more long array gives better results) to use the next final method:</p>

<pre><code>def dups_count_dict(l):
    d = {}

    for item in l:
        if item not in d:
            d[item] = 0

        d[item] += 1

    result_d = {key: val for key, val in d.iteritems() if val &gt; 1}

    return result_d.keys()
</code></pre>

<p>Only 2 loops, no very costly <code>l.count()</code> operations.</p>

<p>Here is a code to compare the methods for example. The code is below, here is the output:</p>

<pre><code>dups_count: 13.368s # this is a function which uses l.count()
dups_count_dict: 0.014s # this is a final best function (of the 3 functions)
dups_count_counter: 0.024s # collections.Counter
</code></pre>

<p>The testing code:</p>

<pre><code>import numpy as np
from time import time
from collections import Counter

class TimerCounter(object):
    def __init__(self):
        self._time_sum = 0

    def start(self):
        self.time = time()

    def stop(self):
        self._time_sum += time() - self.time

    def get_time_sum(self):
        return self._time_sum


def dups_count(l):
    return set([x for x in l if l.count(x) &gt; 1])


def dups_count_dict(l):
    d = {}

    for item in l:
        if item not in d:
            d[item] = 0

        d[item] += 1

    result_d = {key: val for key, val in d.iteritems() if val &gt; 1}

    return result_d.keys()


def dups_counter(l):
    counter = Counter(l)    

    result_d = {key: val for key, val in counter.iteritems() if val &gt; 1}

    return result_d.keys()



def gen_array():
    np.random.seed(17)
    return list(np.random.randint(0, 5000, 10000))


def assert_equal_results(*results):
    primary_result = results[0]
    other_results = results[1:]

    for other_result in other_results:
        assert set(primary_result) == set(other_result) and len(primary_result) == len(other_result)


if __name__ == '__main__':
    dups_count_time = TimerCounter()
    dups_count_dict_time = TimerCounter()
    dups_count_counter = TimerCounter()

    l = gen_array()

    for i in range(3):
        dups_count_time.start()
        result1 = dups_count(l)
        dups_count_time.stop()

        dups_count_dict_time.start()
        result2 = dups_count_dict(l)
        dups_count_dict_time.stop()

        dups_count_counter.start()
        result3 = dups_counter(l)
        dups_count_counter.stop()

        assert_equal_results(result1, result2, result3)

    print 'dups_count: %.3f' % dups_count_time.get_time_sum()
    print 'dups_count_dict: %.3f' % dups_count_dict_time.get_time_sum()
    print 'dups_count_counter: %.3f' % dups_count_counter.get_time_sum()
</code></pre>
","749288","749288","2018-06-20 16:43:43","0","2816","sergzach","2011-05-11 18:39:03","3991","951","1310","26","9835762","9835819","2012-03-23 07:59:59","389","573165","<p>How can I find the duplicates in a Python list and create another list of the duplicates? The list only contains integers.</p>
","682308","3924118","2018-04-02 11:33:53","How do I find the duplicates in a list and create another list with them?","<python><list><duplicates>","30","3","130"
"50642514","2018-06-01 11:33:59","0","","<p>Sphinx uses <code>requests</code> which uses <code>certifi</code> -- thanks to <a href=""https://stackoverflow.com/users/5588279/sraw"">sraw</a> who kindly pointed this out in a comment. You can modify the <code>certifi.where()</code> to include your own certificate authority.</p>

<p>Because you might run tox or re-build your virtual environement, doing so manually is tedious and error prone. A fixture makes this much easier to deal with.</p>

<p>The Python script changes to the following.</p>

<pre><code># -*- coding: utf-8 -*-
import subprocess
import certifi
import requests
import pytest

CA = '/etc/pki/ca-trust/source/anchors/company_ca.pem'


@pytest.fixture
def certificate_authority(scope=""module""):
    try:
        # Checking connection to Mantis…
        requests.get('https://mantisbt.example.com')
        # Connection to Mantis OK, thus CA should work fine.
    except requests.exceptions.SSLError:
        # SSL Error. Adding custom certs to Certifi store…
        cafile = certifi.where()
        with open(CA, 'rb') as infile:
            customca = infile.read()
        with open(cafile, 'ab') as outfile:
            outfile.write(customca)
        # That might have worked.


def test_linkcheck(certificate_authority, tmpdir):
    doctrees = tmpdir.join(""doctrees"")
    htmldir = tmpdir.join(""html"")
    subprocess.check_call([
        ""sphinx-build"", ""-W"", ""-blinkcheck"", ""-d"",
        str(doctrees), ""."",
        str(htmldir)
    ])


def test_build_docs(certificate_authority, tmpdir):
    doctrees = tmpdir.join(""doctrees"")
    htmldir = tmpdir.join(""html"")
    subprocess.check_call([
        ""sphinx-build"", ""-W"", ""-bhtml"", ""-d"",
        str(doctrees), ""."",
        str(htmldir)
    ])
</code></pre>
","232794","","","0","1736","Sardathrion","2009-12-16 09:23:59","6930","618","4962","140","50641147","","2018-06-01 10:12:29","1","79","<p>We have our own company wide certificate authority which we use to signa SSL certificates. Mostly, this is working fine as long as you have your OS (CentOS 7 in our case) register that authority. It is stored here:</p>

<pre><code>/etc/pki/ca-trust/source/anchors/company_ca.pem
</code></pre>

<p>This allows Firefox/chrome to trust the SSL certificates that were signed via it.</p>

<p>I am using <code>sphinx-build -W -blinkcheck […]</code> to check that the links in my Python project are still valid as link rot sucks in documentation. This is fine for all external links.</p>

<p>However, when linking to our own SSL version of mantis (a bug tracker), I get a </p>

<pre><code>SSLError(SSLError(1, u'[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:579)'),)))
</code></pre>

<p>error. Mantis, in our set up, only runs on https.</p>

<p><em>How do I tell sphinx to add the company-wide authority?</em></p>

<p>I run this generally via tox like thus:</p>

<p>The tox fragement which runs this:</p>

<pre><code>[testenv:docs]
basepython=python2.7
deps=-r{toxinidir}/requirements/requirements.txt
commands=./check_docs.bash
</code></pre>

<p>The bash script:</p>

<pre><code>#!/bin/bash
set -eux
sphinx-apidoc --force --separate --private --module-first -o docs src/ '*/*test*'
cd docs
pytest --maxfail=1 \
    --tb=line \
    -v \
    --junitxml=junit_sphinx.xml \
    --exitfirst \
    --failed-first \
    --full-trace \
    -ra \
    --capture=no \
    check_sphinx.py
</code></pre>

<p>And the pythons script:</p>

<pre><code>import subprocess


def test_linkcheck(tmpdir):
    doctrees = tmpdir.join(""doctrees"")
    htmldir = tmpdir.join(""html"")
    subprocess.check_call([
        ""sphinx-build"", ""-W"", ""-blinkcheck"", ""-d"",
        str(doctrees), ""."",
        str(htmldir)
    ])


def test_build_docs(tmpdir):
    doctrees = tmpdir.join(""doctrees"")
    htmldir = tmpdir.join(""html"")
    subprocess.check_call([
        ""sphinx-build"", ""-W"", ""-bhtml"", ""-d"",
        str(doctrees), ""."",
        str(htmldir)
    ])
</code></pre>
","232794","232794","2018-06-01 10:48:50","sphinx-build with -blinkcheck and custom CA","<python><python-sphinx><certificate-authority><local-security-authority>","1","2","2057"
"50642529","2018-06-01 11:34:25","1","","<p>This is quite a simple approach</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame(data=np.random.random(size=(5,6)), 
              columns={'a', 'b', 'c, d', 'e', 'f, g', 'h'})

new_df = pd.DataFrame()
for x in df.columns:
    split = x.split(',')
    for s in split:
        new_df[s] = df[x]

df = 
    f, g        h           c, d        e           a           b
0   0.104359    0.746843    0.672964    0.085768    0.088580    0.152405
1   0.530228    0.841193    0.023619    0.619892    0.254405    0.776631
2   0.193094    0.530332    0.660067    0.308105    0.936816    0.067757
3   0.314124    0.143150    0.351160    0.681030    0.307738    0.786784
4   0.214116    0.445849    0.139659    0.062285    0.835806    0.781299

new_df =
    f           g           h           c           d           e           a           b
0   0.104359    0.104359    0.746843    0.672964    0.672964    0.085768    0.088580    0.152405
1   0.530228    0.530228    0.841193    0.023619    0.023619    0.619892    0.254405    0.776631
2   0.193094    0.193094    0.530332    0.660067    0.660067    0.308105    0.936816    0.067757
3   0.314124    0.314124    0.143150    0.351160    0.351160    0.681030    0.307738    0.786784
4   0.214116    0.214116    0.445849    0.139659    0.139659    0.062285    0.835806    0.781299
</code></pre>
","9137276","","","0","1359","mtshaikh","2017-12-24 20:39:35","78","14","76","0","50641835","50642171","2018-06-01 10:51:32","3","58","<p>I need to clean up a data set, where some columns (read from .csv file) may have several names, listed with commas. </p>

<p>I need to do the following in pandas:</p>

<p><a href=""https://i.stack.imgur.com/dftNi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dftNi.png"" alt=""enter image description here""></a></p>

<p>Any nice pandasian tricks for that?</p>

<p>Here is a simple code:</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame(data=np.random.random(size=(5,6)), 
                  columns={'a', 'b', 'c, d', 'e', 'f, g', 'h'})


df=
          a         b      c, d         e      f, g         h
0  0.771418  0.371685  0.072876  0.153071  0.169513  0.399769
1  0.667551  0.886779  0.949341  0.869588  0.226275  0.273370
2  0.768456  0.945822  0.167757  0.584886  0.328152  0.246415
3  0.354713  0.690585  0.027916  0.237110  0.875449  0.430142
4  0.590518  0.819043  0.803876  0.909385  0.382452  0.867369
</code></pre>

<p>I need:</p>

<pre><code>df_new = 

          a         b         c         d         e         f         g         h
0  0.771418  0.371685  0.072876  0.072876  0.153071  0.169513  0.169513  0.399769
1  0.667551  0.886779  0.949341  0.949341  0.869588  0.226275  0.226275  0.273370
2  0.768456  0.945822  0.167757  0.167757  0.584886  0.328152  0.328152  0.246415
3  0.354713  0.690585  0.027916  0.027916  0.237110  0.875449  0.875449  0.430142
4  0.590518  0.819043  0.803876  0.803876  0.909385  0.382452  0.382452  0.867369
</code></pre>

<p><strong>UPDATE</strong></p>

<p>And what happens if I have repeated column names:</p>

<pre><code>df = pd.DataFrame(data=np.random.random(size=(5,6)), 
                      columns={'a', 'b', 'c, d', 'c', 'f, g', 'h'})
</code></pre>

<p>and the desired results should be</p>

<p>df_new_v2 = </p>

<pre><code>          a         b         c         d       c.1         f         g         h
0  0.771418  0.371685  0.072876  0.072876  0.153071  0.169513  0.169513  0.399769
1  0.667551  0.886779  0.949341  0.949341  0.869588  0.226275  0.226275  0.273370
2  0.768456  0.945822  0.167757  0.167757  0.584886  0.328152  0.328152  0.246415
3  0.354713  0.690585  0.027916  0.027916  0.237110  0.875449  0.875449  0.430142
4  0.590518  0.819043  0.803876  0.803876  0.909385  0.382452  0.382452  0.867369
</code></pre>
","5550203","5550203","2018-06-01 12:31:10","duplicate a single column with several names in Pandas","<python><pandas><dataframe>","2","1","2338"
"50642536","2018-06-01 11:34:45","2","","<p>If you update to Bokeh 0.12.* you can do this:</p>

<pre><code>from bokeh.io import show, output_file
from bokeh.plotting import figure

output_file('bar_colors.html')

data = {
    'L': ['A','B', 'C'],
    'NAME': [100, 2, 200],
}

p = figure(x_range=data['L'], y_range=(0,300), plot_height=400, plot_width=400,
           title='someTitle', toolbar_location=None, tools='')

p.vbar(x='L', top='NAME', width=0.9, legend=None, source=data)

p.xgrid.grid_line_color = None
p.yaxis.axis_label = 'NAME'

show(p)
</code></pre>

<p>Result:</p>

<p><a href=""https://i.stack.imgur.com/0PZKR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0PZKR.png"" alt=""Plot""></a></p>
","1782792","3406693","2019-10-14 04:40:20","6","685","jdehesa","2012-10-29 11:43:40","37080","2442","2562","26","50641157","50642536","2018-06-01 10:12:55","1","74","<p>Consider bar plots in bokeh (python)
<a href=""http://docs.bokeh.org/en/0.11.0/docs/user_guide/charts.html"" rel=""nofollow noreferrer"">http://docs.bokeh.org/en/0.11.0/docs/user_guide/charts.html</a></p>

<p>On the Y-axis, we always see label like ""SUM(NAME)"", here ""SUM"" is name of aggregation function (can be mean ...). </p>

<p><strong>Question</strong> Is there any way to suppress it ? Just to see ""NAME"" ?</p>

<p>Example: </p>

<pre><code>data = {
    'L': ['A','B', 'C'],
    'NAME': [100, 2, 200]
}

bar = Bar(data, values='NAME',  plot_height=400,  label=['L'], legend = None, title=""someTitle"", plot_width=400)
</code></pre>
","625396","3406693","2019-10-14 04:39:45","Bokeh bar plot: how to supress aggregation name at y-axis label?","<python><bokeh>","1","0","637"
"50642556","2018-06-01 11:35:54","6","","<p>As I can see from the docs <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt"" rel=""noreferrer"">here</a>, you have to check only for the person class. Right now <code>vis_util</code> checks for all classes. You have to add an <code>if</code> condition for only the person class. Given below is the appropriate identifier (taken from docs).
<code>
item {
  name: ""/m/01g317""
  id: 1
  display_name: ""person""
}</code></p>
","8518433","","","0","484","unholy_me","2017-08-25 20:10:02","330","61","25","6","50642057","50673976","2018-06-01 11:06:59","0","1861","<p>I am working on a robotic project that involves the detection of a human body for which I am using tensor flow and predefined data sets to create a training model. As I am new to machine learning, I am unable to properly get the output from my classifier. I require only the Person detection and want to avoid the detection of balls, laptops or other objects. 
Right now my webcam detects all the objects like ball, bat, laptops, tv etc. The output I require is only persons with a score of 80% and above.</p>

<p>The code I used for using the created model is </p>

<pre><code>import numpy as np
import os
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
import zipfile

from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image


from utils import label_map_util

from utils import visualization_utils as vis_util

MODEL_NAME = 'ssd_mobilenet_v1_coco_11_06_2017'
MODEL_FILE = MODEL_NAME + '.tar.gz'
DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'


PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'
PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')

NUM_CLASSES = 90

if not os.path.exists(MODEL_NAME + '/frozen_inference_graph.pb'):
    print ('Downloading the model')
    opener = urllib.request.URLopener()
    opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)
    tar_file = tarfile.open(MODEL_FILE)
    for file in tar_file.getmembers():
      file_name = os.path.basename(file.name)
      if 'frozen_inference_graph.pb' in file_name:
        tar_file.extract(file, os.getcwd())
    print ('Download complete')
else:
    print ('Model already exists')

detection_graph = tf.Graph()
with detection_graph.as_default():
  od_graph_def = tf.GraphDef()
  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:
    serialized_graph = fid.read()
    od_graph_def.ParseFromString(serialized_graph)
    tf.import_graph_def(od_graph_def, name='')

label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)
category_index = label_map_util.create_category_index(categories)

import cv2
cap = cv2.VideoCapture(1)


with detection_graph.as_default():
  with tf.Session(graph=detection_graph) as sess:
   ret = True
   while (ret):
      ret,image_np = cap.read()
      image_np_expanded = np.expand_dims(image_np, axis=0)
      image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
      boxes = detection_graph.get_tensor_by_name('detection_boxes:0')      
      scores = detection_graph.get_tensor_by_name('detection_scores:0')
      classes = detection_graph.get_tensor_by_name('detection_classes:0')
      num_detections = detection_graph.get_tensor_by_name('num_detections:0')

      (boxes, scores, classes, num_detections) = sess.run(
          [boxes, scores, classes, num_detections],
          feed_dict={image_tensor: image_np_expanded})
      vis_util.visualize_boxes_and_labels_on_image_array(image_np,np.squeeze(boxes),np.squeeze(classes).astype(np.int32),np.squeeze(scores),category_index,use_normalized_coordinates=True,line_thickness=8)
      cv2.imshow('image',cv2.resize(image_np,(1280,960)))
      if cv2.waitKey(27) &amp; 0xFF == ord('q'):
          cv2.destroyAllWindows()
          cap.release()
          break
</code></pre>

<p>Could anyone please explain how can I detect only persons with an accuracy score of greater than 80%.</p>
","9880800","8518433","2018-06-01 15:07:30","Human body detection using opencv, tensorflow and python","<python><opencv><image-processing><tensorflow>","2","0","3533"
"50642573","2018-06-01 11:37:12","1","","<p>This is called a namespace and is supposed to be in front. You can simply remove the namespace by splitting your string at <code>{}</code></p>
","9280994","","","2","146","Jonathan R","2018-01-28 21:47:13","1615","128","51","6","50641625","","2018-06-01 10:40:39","0","28","<p>I have an xml like shown below</p>

<pre><code>    &lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
      &lt;!DOCTYPE dtbook PUBLIC ""-//INFO//INFO info 2005-3//EN"" ""http://url""&gt;
        &lt;dtbook xmlns=""http://www.daisy.org/z3986/2005/dtbook/"" version=""2005-3"" xml:lang=""ml""&gt;
          &lt;head&gt;....
        &lt;/dtbook&gt;
</code></pre>

<p>I open the file like so,</p>

<pre><code>with open(""filename.xml"") as f:
    tree = ET.parse(f)
root = tree.getroot()
</code></pre>

<p>When I try to get the root tag, I get,</p>

<pre><code>print(root.tag)
{http://www.daisy.org/z3986/2005/dtbook/}dtbook
</code></pre>

<p>whereas if I remove all the attributes from the root tag i.e. dtbook, I get the correct output i.e. dtbook</p>

<pre><code>print(root.tag)
dtbook
</code></pre>

<p>I cannot remove the attributes. Is there a way to get this working without removing the attributes??</p>
","4303426","","","ElementTree appends extra information when getting root element","<python><xml><xml-parsing><elementtree>","1","2","894"
"50642595","2018-06-01 11:38:19","0","","<p>I would suggest a simple recursive generator that traverses the tree and yields tokens. </p>

<p>These can be put into a list very easily through a list comprehension.</p>

<pre><code>from io import StringIO

xml = """"""&lt;Component&gt;
    &lt;Custom/&gt;
    &lt;ID&gt;1&lt;/ID&gt;
    &lt;LongDescription&gt;
        &lt;html&gt;
            &lt;html&gt;
                &lt;head&gt;
                    &lt;style type=""text/css""&gt;
                        &lt;!-- .style9 { color: #ffff33; } ... --&gt; 
                    &lt;/style&gt;
                &lt;/head&gt;
                &lt;body&gt;
                &lt;/body&gt;
            &lt;/html&gt;
        &lt;/html&gt;
    &lt;/LongDescription&gt;
    &lt;Name&gt;ip_bridge&lt;/Name&gt;
&lt;/Component&gt;""""""
xml_string_file = StringIO(xml)

# -----------------------------------------------------------------------
import xml.etree.ElementTree as ET

def tokenize_tree(element):
    yield '&lt;%s&gt;' % element.tag 
    yield element.text if element.text else ''
    for child in element:
        yield from tokenize_tree(child)
    yield '&lt;/%s&gt;' % element.tag 

tree = ET.parse(xml_string_file)    

token_list = [token for token in tokenize_tree(tree.getroot())]
print(token_list)
</code></pre>

<p>The output for me is:</p>

<pre><code>['&lt;Component&gt;', '\n    ', '&lt;Custom&gt;', '', '&lt;/Custom&gt;', '&lt;ID&gt;', '1', '&lt;/ID&gt;', 
 '&lt;LongDescription&gt;', '\n        ', '&lt;html&gt;', '\n            ', '&lt;html&gt;', 
 '\n                ',  '&lt;head&gt;', '\n                    ',  '&lt;style&gt;', 
 '\n                         \n                    ', '&lt;/style&gt;', '&lt;/head&gt;', 
 '&lt;body&gt;', '\n                ', '&lt;/body&gt;', '&lt;/html&gt;', '&lt;/html&gt;', 
 '&lt;/LongDescription&gt;', '&lt;Name&gt;', 'ip_bridge', '&lt;/Name&gt;', '&lt;/Component&gt;']
</code></pre>

<p>You can handle whitespace-only text nodes and  comments (such as the one in the <code>&lt;style&gt;</code> element) as you see fit. For example by doing:</p>

<pre><code>if element.text and element.text.strip():
    yield element.text.strip()
</code></pre>

<p>For text node processing with ElementTree, look at <a href=""https://stackoverflow.com/q/19369901/18771"">Python element tree - extract text from element, stripping tags</a> - you might instead want to add something like:</p>

<pre><code>for text in element.itertext():
    yield text
</code></pre>

<p>to the function above. </p>

<p>For HTML in general, which will have text nodes and element nodes intermixed, see <a href=""https://stackoverflow.com/q/42174152/18771"">Python ElementTree - iterate through child nodes and text in order</a></p>
","18771","18771","2018-06-01 11:57:01","0","2697","Tomalak","2008-09-19 11:44:16","271209","22506","7280","284","50641312","50642595","2018-06-01 10:21:41","0","295","<p>I have this XML file :</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8"" standalone=""true""?&gt;

&lt;Component&gt;

&lt;Custom/&gt;
&lt;ID&gt;1&lt;/ID&gt;
&lt;LongDescription&gt;
&lt;html&gt;&lt;html&gt; &lt;head&gt; &lt;style type=""text/css""&gt; &lt;!-- .style9 { color: #ffff33; background-color: #ff00ff } .style8 { color: #990099; background-color: #66ffcc } .style7 { color: #0066cc; background-color: #ccffcc } .style6 { color: #009900; background-color: #ffffcc } .style11 { color: #000066; background-color: #ccffcc } .style5 { color: #cc0033; background-color: #99ff99 } .style10 { color: #99ff99; background-color: #00cccc } .style4 { color: #cc0033; background-color: #ccffff } .style3 { color: #0000dd; background-color: teal } .style2 { color: #0000cc; background-color: aqua } .style1 { color: blue; background-color: silver } .style0 { color: #000099; background-color: #ffffcc } --&gt; &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;/body&gt; &lt;/html&gt; &lt;/html&gt;
&lt;/LongDescription&gt;
&lt;Name&gt;ip_bridge&lt;/Name&gt;
&lt;/component&gt;
</code></pre>

<p>I am reading this file using the library xml.etree.ElementTree as follows :</p>

<pre><code>def getTokens(xml_string_file):
tokensList = []
tree = ET.parse(xml_string_file)
root = tree.getroot()
tokensList.append('&lt;component&gt;')
for child in root: 
    firstTag = '&lt;' + child.tag + '&gt;'
    lastTag = '&lt;/' + child.tag + '&gt;'
    tokensList.append(firstTag)
    if child.text == None:
        tokensList.append('')
    elif re.findall(r""\n"", child.text, re.DOTALL):
        tokensList = tokensList + extractTags(root=child)
    else:
        tokensList.append(child.text)
    tokensList.append(lastTag)
tokensList.append('&lt;/component&gt;')
return tokensList
</code></pre>

<p>with the  function extractTags</p>

<pre><code>def extractTags(root):
tokensList = []
for child in root:
    firstTag = '&lt;' + child.tag + '&gt;'
    lastTag = '&lt;/' + child.tag + '&gt;'
    tokensList.append(firstTag)
    if child.text == None:
        tokensList.append('')
    elif re.findall(r""\n"", child.text, re.DOTALL): #To extract the children of the children
            tokensList = tokensList + extractTags(root=child)
    else:
        tokensList.append(child.text)
    tokensList.append(lastTag)
return tokensList
</code></pre>

<p>I get as a result the tokens list <code>['&lt;omponent&gt;', '&lt;custom&gt;', '', '&lt;/custom&gt;', '&lt;ID&gt;', '1', '&lt;/ID&gt;', '&lt;LongDescription&gt;', '&lt;html&gt;', '&lt;/html&gt;', '&lt;/LongDescription&gt;', '&lt;Name&gt;', 'ip_bridge', '&lt;/Name&gt;', '&lt;/component&gt;']</code>
I want to extract also what is between the html tags as one token (one text).</p>
","5159740","5159740","2018-06-01 10:48:13","How to parse XML file with xml.etree.ElementTree that have HTML content in its child","<python><html><xml><xml-parsing>","1","9","2715"
"50642601","2018-06-01 11:38:55","4","","<p>You could use <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.stack.html#numpy.stack"" rel=""nofollow noreferrer""><code>np.stack((a,b))</code></a> to stack along a new 0-axis, then call <code>nansum</code> to sum along that 0-axis:</p>

<pre><code>C = np.nansum(np.stack((a,b)), axis=0)
</code></pre>

<hr>

<p>For example,</p>

<pre><code>In [34]: a = np.random.choice([1,2,3,np.nan], size=(6,7,180,360))

In [35]: b = np.random.choice([1,2,3,np.nan], size=(6,7,180,360))

In [36]: np.stack((a,b)).shape
Out[36]: (2, 6, 7, 180, 360)

In [37]: np.nansum(np.stack((a,b)), axis=0).shape
Out[37]: (6, 7, 180, 360)
</code></pre>

<hr>

<p>You had the right idea, but <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.dstack.html"" rel=""nofollow noreferrer""><code>np.dstack</code></a> stacks along the third axis, which is not desireable here since you already have 4 axes:</p>

<pre><code>In [31]: np.dstack((a,b)).shape
Out[31]: (6, 7, 360, 360)
</code></pre>

<hr>

<p>Regarding your point (3):
Note that the behavior of <code>np.nansum</code> <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.nansum.html"" rel=""nofollow noreferrer"">depends on the NumPy version</a>:</p>

<blockquote>
  <p>In NumPy versions &lt;= 1.8.0 Nan is returned for slices that are all-NaN or
  empty. In later versions zero is returned.</p>
</blockquote>

<p>If you are using NumPy version > 1.8.0, then you may have to use a <a href=""https://stackoverflow.com/a/50642947/190597"">solution such as 
Maarten Fabré's</a> to address this issue.</p>
","190597","190597","2018-06-01 12:03:40","1","1574","unutbu","2009-10-15 12:48:20","601449","35556","17233","661","50642414","","2018-06-01 11:27:29","1","416","<p>I have two 4D matrices, which I would like to add. The matrices have the exact same dimension and number of elements, but they both contain randomly distributed NaN values. </p>

<p>I would prefer to add them as below using numpy.nansum.<br>
(1) if two values are added I want the sum to be a value,<br>
(2) if a value and a NaN are added I want the sum to be the value and<br>
(3) if two NaN are added I want the sum to be NaN.  </p>

<p>Herewith what I tried</p>

<pre><code>a[6x7x180x360]
b[6x7x180x360]

C=np.nansum[(a,b)]
C=np.nansum(np.dstack((a,b)),2)
</code></pre>

<p>But I am unable to get the resultant matrix with same dimension as input. It means resultant matrix C should be in [6x7x180x360].
Anyone can help in this regard. Thank you in advance.</p>
","8129826","","","Avoid NaN values and add two matrices element wise in python","<python><numpy>","3","0","768"
"50642656","2018-06-01 11:42:04","1","","<p>yes, you can have user/pass per topic. see <a href=""https://kafka.apache.org/documentation/#security"" rel=""nofollow noreferrer"">official documentation</a> <strong>Authorization and ACLs</strong>.</p>

<p>You can enable security with either SSL or SASL, Kafka's SASL support:</p>

<ul>
<li>SASL/GSSAPI (Kerberos) - starting at version 0.9.0.0</li>
<li>SASL/PLAIN - starting at version 0.10.0.0</li>
<li>SASL/SCRAM-SHA-256 and SASL/SCRAM-SHA-512 - starting at version 0.10.2.0</li>
</ul>

<p>From the docs, example of adding Acls:</p>

<p>Suppose you want to add an acl ""Principals User:Bob and User:Alice are allowed to perform Operation Read and Write on Topic Test-Topic from IP 198.51.100.0 and IP 198.51.100.1"". You can do that by executing the CLI with following options:
1</p>

<pre><code>bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --add --allow-principal User:Bob --allow-principal User:Alice --allow-host 198.51.100.0 --allow-host 198.51.100.1 --operation Read --operation Write --topic Test-topic
</code></pre>

<p>Also in this <a href=""https://medium.com/@stephane.maarek/introduction-to-apache-kafka-security-c8951d410adf"" rel=""nofollow noreferrer"">Blog post</a> you can find some information</p>

<p>I'm not sure what library you are using but it should just be a matter of passing the proper properties to the producer/client; <strong>kafka-python</strong> has support:</p>

<ul>
<li><a href=""https://github.com/dpkp/kafka-python/issues/533"" rel=""nofollow noreferrer"">Support SASL/Kerberos</a></li>
<li><a href=""https://github.com/dpkp/kafka-python/issues/660"" rel=""nofollow noreferrer"">Support for ACL based kafka</a></li>
</ul>
","3224238","","","0","1677","Paizo","2014-01-22 15:52:10","2503","283","7","12","50641701","","2018-06-01 10:44:47","1","1434","<p>I read Kafka's documents</p>

<p>But I did not understand. Can I use username and password for Python Producers?</p>

<p>Can specify that any Producer can only produce a Topic, like <code>MySQL</code> .(producer has written with Python)</p>
","4460737","2308683","2018-06-02 00:12:12","kafka authentication and authorization","<python><apache-kafka>","2","0","244"
"50642664","2018-06-01 11:42:27","0","","<p>i hope this  useful for you : </p>

<pre><code>k = 3
k2 = 20

def algebra(*numbers):
    for number in numbers:
        print(5*number-10)

algebra(k,k2,k2)
</code></pre>

<p>output :
    5
    90
    90</p>
","4460737","","","1","211","Alihossein shahabi","2015-01-16 07:54:11","2155","330","173","3","50642566","","2018-06-01 11:36:34","0","65","<p>I am trying to make a program that can do my algebra formulas. This is my code 
{</p>

<pre><code>k = 3
k2 = 20

def algebra(number):
  print(5*number-10)

algebra(k)
</code></pre>

<p>}</p>

<p>I tried to do k2 and k at the same time like this</p>

<pre><code>algebra(k,k2)
</code></pre>

<p>How can I make this work?</p>
","9358799","","","Python function multiple arguments without calling function twice","<python><python-3.x><function><algebra>","3","5","326"
"50642666","2018-06-01 11:42:35","0","","<p>You could use <code>args</code> in your function, allowing you to call a function with multiple parameters. Your example would then become:</p>

<pre><code>def algebra(*args):
  for arg in args:
    print(5*arg-10)

algebra(5, 10)
&gt;&gt; 15
&gt;&gt; 40
</code></pre>
","9038396","","","0","272","Lucasje19","2017-12-01 09:53:40","8","2","0","0","50642566","","2018-06-01 11:36:34","0","65","<p>I am trying to make a program that can do my algebra formulas. This is my code 
{</p>

<pre><code>k = 3
k2 = 20

def algebra(number):
  print(5*number-10)

algebra(k)
</code></pre>

<p>}</p>

<p>I tried to do k2 and k at the same time like this</p>

<pre><code>algebra(k,k2)
</code></pre>

<p>How can I make this work?</p>
","9358799","","","Python function multiple arguments without calling function twice","<python><python-3.x><function><algebra>","3","5","326"
"50642667","2018-06-01 11:42:41","1","","<p>Create 2d array by list comprehension with split and then new <code>Dataframe</code> by constructor:</p>

<pre><code>a = np.array([x.split() for x in df['0']])

df = pd.DataFrame([a[:, 1]], columns=a[:, 0])
print (df)
    a    b    r    i    g    t
0  43  630  587  462  153  266
</code></pre>
","2901002","","","0","297","jezrael","2013-10-20 20:27:26","427380","89269","18260","743","50642233","50642413","2018-06-01 11:17:18","1","77","<p>I have a dataframe like this-</p>

<pre><code>                 0
0            a  43
1            b  630
2            r  587
3            i  462
4            g  153
5            t  266
</code></pre>

<p>I want to create a new dataframe which looks like this-</p>

<pre><code>     a         b     r       i     g        t
0    43       630   587    462    153      266
</code></pre>
","8660280","9209546","2018-06-01 14:52:58","Creating single row pandas dataframe","<python><pandas><dataframe>","3","0","384"
"50642716","2018-06-01 11:46:07","1","","<p>If you want to use username+password for authentication, you need to enable SASL authentication using the Plain mechanism on your cluster. See the <a href=""http://kafka.apache.org/documentation/#security_sasl"" rel=""nofollow noreferrer"">Authentication using SASL</a> section on the Kafka website for the full instructions.</p>

<p>Note that you also probably want to enable SSL (SASL_SSL), as otherwise, SASL Plain would transmit credentials in plaintext.</p>

<p>Several Python clients support SASL Plain, for example:</p>

<ul>
<li>kafka-python: <a href=""https://github.com/dpkp/kafka-python"" rel=""nofollow noreferrer"">https://github.com/dpkp/kafka-python</a></li>
<li>confluent-kafka-python: <a href=""https://github.com/confluentinc/confluent-kafka-python"" rel=""nofollow noreferrer"">https://github.com/confluentinc/confluent-kafka-python</a></li>
</ul>

<hr>

<p>Regarding authorizations, using the default authorizer, <code>kafka.security.auth.SimpleAclAuthorizer</code>, you can restrict a producer to only be able to produce to a topic. Again this is all fully documented on Kafka's website in the <a href=""http://kafka.apache.org/documentation/#security_authz"" rel=""nofollow noreferrer"">Authorization and ACLs</a> section.</p>

<p>For example with SASL Plain, by default, the Principal name is the username that was used to connect. Using the following command you can restrict user <code>Alice</code> to only be able to produce to the topic named <code>testtopic</code>:</p>

<pre><code>bin/kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 --add --allow-principal User:Alice --producer --topic testtopic
</code></pre>
","1765189","","","0","1650","Mickael Maison","2012-10-22 11:19:07","11586","4250","494","20","50641701","","2018-06-01 10:44:47","1","1434","<p>I read Kafka's documents</p>

<p>But I did not understand. Can I use username and password for Python Producers?</p>

<p>Can specify that any Producer can only produce a Topic, like <code>MySQL</code> .(producer has written with Python)</p>
","4460737","2308683","2018-06-02 00:12:12","kafka authentication and authorization","<python><apache-kafka>","2","0","244"
"50642726","2018-06-01 11:46:42","0","","<p>It seems like what you want is to call algebra on some variable number of k augments. There are a few ways to do this (map would likely be the most appropriate, but since you're just learning, I'll keep it simpler). One simple way is to have your function take in and return a list.</p>

<p>So you'd have somerh like</p>

<p>a = [k,k2]</p>

<p>And then in your algebra function, take a as an argument and iterate over its elements using a for loop like so:</p>

<p>For elem in a:</p>

<p>a[elem] = 5 * elem - 10</p>

<p>return a</p>

<p>And then print out the list that is returned in your main function</p>
","979616","","","0","611","user979616","2011-10-05 03:12:34","128","77","10","0","50642566","","2018-06-01 11:36:34","0","65","<p>I am trying to make a program that can do my algebra formulas. This is my code 
{</p>

<pre><code>k = 3
k2 = 20

def algebra(number):
  print(5*number-10)

algebra(k)
</code></pre>

<p>}</p>

<p>I tried to do k2 and k at the same time like this</p>

<pre><code>algebra(k,k2)
</code></pre>

<p>How can I make this work?</p>
","9358799","","","Python function multiple arguments without calling function twice","<python><python-3.x><function><algebra>","3","5","326"
"50642745","2018-06-01 11:47:35","2","","<p>Thanks ekhumoro - i didnt know that. not sure if this is the 'right' way to do it but i ended up completely uninstalling anaconda and rebuilding it. when i then made a new virtual environment the problem resolved. if others have the same issue this might work too. BTW the problem first occurred with an update to pyqt5.</p>
","6468053","","","0","328","A Rob4","2016-06-15 07:06:58","308","22","93","1","50620954","","2018-05-31 09:26:42","5","5410","<p>I'm completly stuck on this. I keep getting error message </p>

<blockquote>
  <p>Process finished with exit code -1073741819 (0xC0000005)</p>
</blockquote>

<p>I'm using pycharm with pyqt5.6 and qt5.6.2 and the problem started when I upgraded to these versions.</p>

<p>I've tried searching as much as I can, but have not been able to find an answer. Can anyone help please?</p>
","6468053","3764197","2018-12-10 13:55:41","Process finished with exit code -1073741819 (0xC0000005) Pycharm","<python><pycharm><pyqt5>","2","3","383"
"50642746","2018-06-01 11:47:39","0","","<p>Create a input-placeholder and use this as the input to both networks:</p>

<p>Eg DNN1:
<code>hidden1 = (input * W1) + b1</code></p>

<p>DNN2:
<code>hidden1 = (input * W1) + b1</code></p>

<p>Can you do this?</p>
","9280994","9280994","2018-06-01 12:28:36","5","216","Jonathan R","2018-01-28 21:47:13","1615","128","51","6","50641374","","2018-06-01 10:25:29","0","360","<p>The following situation:
There is a production server (C++) running tensorflow. The code there expects to have exactly one input <code>Placeholder</code>, that gets fed similar to <code>feed_dict:{'input':...}</code>. I can't change the server code. I can only provide a new graph, that must have a <code>Placeholder</code> called <code>input</code>.</p>

<p>Using tensorflow, I have some code that creates two identical neural networks with different scope:</p>

<pre><code>dnn1 = DNN(scope='dnn1')
dnn2 = DNN(scope='dnn2')
</code></pre>

<p>Both DNNs have an input <code>Placeholder</code>: 
<code>&lt;tf.Tensor 'dnn1/input:0' shape=(50) dtype=float32&gt;</code>
and
<code>&lt;tf.Tensor 'dnn2/input:0' shape=(50) dtype=float32&gt;</code></p>

<p>Both networks should get the same input.
How can I make the tensor <code>input</code> flow to both, <code>dnn1/input</code> and <code>dnn2/input</code> within the graph? I also can't change the <code>DNN</code> class, so in the end my graph will have <code>dnn1/input</code>,<code>dnn2/input</code> and <code>input</code> as placeholder, but only <code>input</code> will be supplied by the production code.</p>
","3110740","3110740","2018-06-01 13:11:02","Feed Placeholder with another Placeholder using tensorflow","<python><tensorflow>","2","6","1162"
"50642770","2018-06-01 11:49:18","0","","<blockquote>
  <p>I can't use pip inside the virtual environment, so it's pointless, because I
   can't install any additional packages. How to solve this problem?</p>
</blockquote>

<p>A bit to early to give up on <code>pip</code>! You can add any package you want in the new environment, eg: </p>

<pre><code>pip install pandas
</code></pre>

<p>Looks like you are unsure about a list of your dependencies. They are normally stored in a file called <code>requirements.txt</code>, which is specific to a project. For example, a github repository ususally has <code>requirements.txt</code> - so that users can replicate the dependcies. </p>

<p>You can create <code>requirements.txt</code> manually, or use <code>pip freeze</code> after you have installed the packages you want one by one. A bit more <a href=""https://stackoverflow.com/questions/31684375/automatically-create-requirements-txt"">here</a>.</p>

<p>Also recommended to look at <a href=""https://docs.pipenv.org/"" rel=""nofollow noreferrer"">pipenv</a>, which combines <code>virtualenv</code> and <code>pip</code> functionality under one roof. </p>
","1758363","","","0","1108","Evgeny","2012-10-19 06:01:21","2200","433","940","145","50642086","","2018-06-01 11:08:23","0","54","<p>When I run python venv:</p>

<pre><code>python -m venv test-env
. test-env/bin/activate
</code></pre>

<p>I have virtual environment with <code>pip</code> avaliable (although it's not avaliable for my outside the  venv. However, in this environment all packages need to be installed, which are already avaliable globally. I learned I have to set <code>venv</code> with <code>--system-site-packages</code> flag. But when I do this:</p>

<pre><code>python -m venv --system-site-packages test-env
. test-env/bin/activate
</code></pre>

<p>I can't use <code>pip</code> inside the virtual environment, so it's pointless, because I can't install any additional packages.
How to solve this problem?</p>
","7432927","","","How to use python venv with both site packages and pip?","<python><python-3.x><pip><virtualenv>","1","3","699"
"50642773","2018-06-01 11:49:24","1","","<p>You can use a regular expression to handle the replacement.</p>

<pre><code>df['TransDetails'] = df['TransDetails'].str.replace('[A-Za-z]', 'N')
df['TransDetails'] = df['TransDetails'].str.replace('\d', 'D')

df
# returns:
                      TransDetails
0   NNNN-NNNNDDDDDDD-NNNNNNNN NNNN
1   NNNN-NNNNDNNNDDD-NNNNNNNN NNNN
2     NNNN-NNNNDNNNDDD-NNNNNN NNNN
3    NNNN-NNNNDDDDDDD-NNNNNN NNNNN
4      NNNN-NNNNDDDDDDD-NNNNNNNNNN
5         NNNN-NNNNDDDDDDD-NNNNNNN
6          NNNN-NNNNDDDDDDD-NNNNNN
7    NNNN-NNNNDNNNDDD-NNNNN NNNNNN
8     NNNN-NNNNDNNNDDD-NNNN NNNNNN
9    NNNN-NNNNDNNNDDD-NNNNN NNNNNN
10   NNNN-NNNNDNNNDDD-NNNNN NNNNNN
</code></pre>
","5003756","5003756","2018-06-01 11:55:50","2","660","James","2015-06-12 15:12:01","17092","1119","652","157","50642636","50642800","2018-06-01 11:40:50","0","490","<p>I want to encode the column values in pandas dataframe, such as the all the letters should be converted to a single letter(eg., <code>'vault'</code> to <code>'NNNNN'</code> , <code>'Nan123'</code> to <code>'NNNDDD'</code>).</p>

<p>I'm thinking of something like this:</p>

<pre><code>df['TransDetails'] = df['TransDetails'].str.replace('A', 'N')
</code></pre>

<p>My data:</p>

<pre><code>    TransDetails
0   NEFT-PUNB0315500-JITENDER SING
1   NEFT-UTIB0CCH274-VIRENDER KUMA
2   NEFT-UTIB0CCH274-SUNITA DEVI
3   NEFT-PUNB0315500-AMLASH KUMAR
4   NEFT-PUNB0109800-FARIDUDDEN
5   NEFT-PUNB0109800-IDREESH
6   NEFT-PUNB0315500-BUDDHU
7   NEFT-UTIB0CCH274-SAKIL AHAMAD
8   NEFT-UTIB0CCH274-NAIM AHAMAD
9   NEFT-UTIB0CCH274-SALIM AHAMAD
10  NEFT-UTIB0CCH274-NADIM AHAMAD
</code></pre>

<p>How can I convert all the column values in such codes? Thanks in advance</p>
","9594648","5003756","2018-06-01 11:47:59","I want to encode the column values in pandas dataframe","<python><pandas><dataframe>","2","3","866"
"50642800","2018-06-01 11:51:01","1","","<p>One way would be to use <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html"" rel=""nofollow noreferrer""><code>df.replace()</code></a>. You would avoid changing numeric columns this way.</p>

<pre><code>df.replace('[A-Za-z]','N', regex=True).replace('\d','D', regex=True)
</code></pre>

<p>Full example with a numeric column called <code>D</code>, A non-numeric called <code>N</code> and <code>TransDetails</code>.</p>

<pre><code>import pandas as pd

data = '''\
D,N,TransDetails
1,ABC,NEFT-PUNB0315500-JITENDER SING
1,123,NEFT-UTIB0CCH274-VIRENDER KUMA
1,123,NEFT-UTIB0CCH274-SUNITA DEVI
1,123,NEFT-PUNB0315500-AMLASH KUMAR
1,123,NEFT-PUNB0109800-FARIDUDDEN
1,123,NEFT-PUNB0109800-IDREESH
1,123,NEFT-PUNB0315500-BUDDHU
1,123,NEFT-UTIB0CCH274-SAKIL AHAMAD
1,123,NEFT-UTIB0CCH274-NAIM AHAMAD
1,123,NEFT-UTIB0CCH274-SALIM AHAMAD
1,123,NEFT-UTIB0CCH274-NADIM AHAMAD'''

fileobj = pd.compat.StringIO(data) # or 'path/to/csv'
df = pd.read_csv(fileobj)
df = df.replace('[A-Za-z]','N', regex=True).replace('\d','D', regex=True)
print(df)
</code></pre>

<p>Returns:</p>

<pre><code>    D    N                    TransDetails
0   1  NNN  NNNN-NNNNDDDDDDD-NNNNNNNN NNNN
1   1  DDD  NNNN-NNNNDNNNDDD-NNNNNNNN NNNN
2   1  DDD    NNNN-NNNNDNNNDDD-NNNNNN NNNN
3   1  DDD   NNNN-NNNNDDDDDDD-NNNNNN NNNNN
4   1  DDD     NNNN-NNNNDDDDDDD-NNNNNNNNNN
5   1  DDD        NNNN-NNNNDDDDDDD-NNNNNNN
6   1  DDD         NNNN-NNNNDDDDDDD-NNNNNN
7   1  DDD   NNNN-NNNNDNNNDDD-NNNNN NNNNNN
8   1  DDD    NNNN-NNNNDNNNDDD-NNNN NNNNNN
9   1  DDD   NNNN-NNNNDNNNDDD-NNNNN NNNNNN
10  1  DDD   NNNN-NNNNDNNNDDD-NNNNN NNNNNN
</code></pre>
","7386332","7386332","2018-06-01 12:01:58","7","1648","Anton vBR","2017-01-07 01:18:16","12659","1153","834","480","50642636","50642800","2018-06-01 11:40:50","0","490","<p>I want to encode the column values in pandas dataframe, such as the all the letters should be converted to a single letter(eg., <code>'vault'</code> to <code>'NNNNN'</code> , <code>'Nan123'</code> to <code>'NNNDDD'</code>).</p>

<p>I'm thinking of something like this:</p>

<pre><code>df['TransDetails'] = df['TransDetails'].str.replace('A', 'N')
</code></pre>

<p>My data:</p>

<pre><code>    TransDetails
0   NEFT-PUNB0315500-JITENDER SING
1   NEFT-UTIB0CCH274-VIRENDER KUMA
2   NEFT-UTIB0CCH274-SUNITA DEVI
3   NEFT-PUNB0315500-AMLASH KUMAR
4   NEFT-PUNB0109800-FARIDUDDEN
5   NEFT-PUNB0109800-IDREESH
6   NEFT-PUNB0315500-BUDDHU
7   NEFT-UTIB0CCH274-SAKIL AHAMAD
8   NEFT-UTIB0CCH274-NAIM AHAMAD
9   NEFT-UTIB0CCH274-SALIM AHAMAD
10  NEFT-UTIB0CCH274-NADIM AHAMAD
</code></pre>

<p>How can I convert all the column values in such codes? Thanks in advance</p>
","9594648","5003756","2018-06-01 11:47:59","I want to encode the column values in pandas dataframe","<python><pandas><dataframe>","2","3","866"
"50642811","2018-06-01 11:51:34","1","","<p><a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.contains.html"" rel=""nofollow noreferrer"">Series.str.contains</a> is the function you need. It returns a boolean Series which can be used to filter the DataFrame when used as its index. For multiple names, separate the names with <code>|</code> (pipe symbol). </p>

<pre><code>names_list = ['A', 'B']
my_names = '|'.join(names_list)  # my_names = ""A|B""
df = pd.read_csv('some.xlsx', index_col=False)
df1 = df1[df[""Name""].str.contains(my_names)]
df1 = df1[['Name', 'address', 'post', 'city']]    
df1.to_csv(""filtered.csv"")
</code></pre>
","8278951","8278951","2018-06-02 11:26:08","2","621","Rahul Goswami","2017-07-09 11:06:31","386","64","96","18","50642204","50642283","2018-06-01 11:15:54","0","83","<p>I have a csv sheet and it looks like below , each column has  some data.
Want to filter data by Name . I know the names, from which I have to filter. </p>

<pre><code>Name  gender  address  age  post  city 
A      M       abc     20   dd     ASD
C      F       xyz     21   ll     KLM
B      M       lmn     22   mm     NOP
</code></pre>

<p>Want output like.</p>

<pre><code>Name    address     post   city 
A         abc        dd     ASD
B         lmn        mm     NOP
</code></pre>

<p>The code below obviously overwrite the first data in csv. How to avoid that and fix this. Also, want to get rid of index column from excel. Any better way to code this ? New to Panda here .   </p>

<pre><code>for i in ['A','B']: #The names list is huge , taking 2 as example
        df=pd.read_csv('some.xlsx',index_col=False)
        df1= df.loc[df['Name'] == i, ['Name','address','post','city']]
df1.to_csv('ABC.csv')
</code></pre>
","9838554","9838554","2018-06-01 18:02:59","write panda data to csv","<python><python-3.x><python-2.7><pandas><csv>","2","0","928"
"50642820","2018-06-01 11:52:07","3","","<p>Nope, there is absolutely no such garantee. You have two solutions here: either write a single task that will do all three things at once, or use a shared filesystem to store your files.  </p>
","41316","","","1","196","bruno desthuilliers","2008-11-27 10:40:14","57630","7809","2022","2444","50642661","50642820","2018-06-01 11:42:19","2","290","<p>Are chained celery tasks guaranteed to run on the same node worker?</p>

<p>I know that I can build a queue for dedicated tasks, however my chain of tasks involves creating png files which need to be sent to S3. The Creation of png files is a different task in the chain so if they run in different workers the next tasks might not find the png file.</p>

<p>I don't want to build a seperate queue for it because otherwise I would need to run all the tasks in that particular queue and nowhere else.</p>

<pre><code> result = (process_diode.s() | plot_results.s() | send_to_s3.s()).apply_async()
</code></pre>

<p>In the above code if the <strong>plot_results</strong> task and <strong>send_to_s3</strong> task run in different workers than there is no guarantee that the png file will be there.</p>

<p>Having the guarantee that all tasks inside a chain run in the same worker node is good enough for me. Is that the case?</p>
","5520429","","","Are chained celery tasks guaranteed to run on the same node worker?","<python><django><rabbitmq><celery>","1","0","931"
"50642832","2018-06-01 11:52:39","3","","<p>You have defined a custom method, <code>partList</code>, but it is not being called from anywhere. The method is pointless and you should delete it.</p>

<p>If you want to add data to the template context in a class-based view, you need to define <code>get_context_data</code>. However there is no reason to do that here as it would just do what ListView does for you anyway. You should use the variable that the view automatically populates, which is <code>object_list</code>.</p>

<pre><code>{% for object in object_list %}
    {{ object.Party_number }}
{% endfor %}
</code></pre>
","104349","","","0","586","Daniel Roseman","2009-05-10 12:36:13","489411","52610","12851","10717","50642728","50642832","2018-06-01 11:46:52","0","49","<p>I have a <em>models.py</em>:</p>

<pre><code>class Part(models.Model):
    Party_number = models.CharField(max_length=10)
    Film = models.CharField(max_length=5)
</code></pre>

<p><em>viesws.py</em>:</p>

<pre><code>from django.shortcuts import render
from django.views.generic import ListView
from description.models import Part

class PartyNumView(ListView):
    template_name = 'part_list.html'
    model = Part

    def partList(self, request):
        my_var = Part.objects.all()
        return render(request, 'part_list.html', {""my_var"": my_var})
</code></pre>

<p>And HTML template <em>part_list.html</em>:</p>

<pre><code>{% extends 'base.html' %}
{% load staticfiles %}
{% load static %}

{% block activeButton %}
    &lt;li class=""active""&gt;&lt;a href=""/parties""&gt;Описание партий&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=""/measures""&gt;Ic и QE&lt;/a&gt;&lt;/li&gt;
{% endblock %}

{% block tableName %}
    Список партий
{% endblock %}

{% block content %}
    {% for object in my_var %}
        {{object.Party_number}}
    {% endfor %}
{% endblock content%}
</code></pre>

<p>My question is why part of code which consist cycle ""for"" does not working? i.e. objects of Party_number does not displayed on html page</p>

<p><strong>UPDATE</strong>
I change <em>object.Party_number</em> to <em>{{object.Party_number}}</em>, but whatever it does not working</p>
","8698126","8698126","2018-06-01 11:57:44","Does not work cycle ""for""","<python><django>","1","3","1382"
"50642834","2018-06-01 11:52:43","1","","<p>I believe the function np.nansum is not appropriate in your case. If I understand your question correctly, you wish to do an element-wise addition of two matrices with a little of logic regarding the NaN values.</p>

<p>Here is the full example on how to do it:</p>

<pre><code>import numpy as np

a = np.array([  [np.nan, 2],
                [3, np.nan]])

b = np.array([  [3, np.nan],
                [1, np.nan]])

result = np.add(a,b)

a_is_nan = np.isnan(a)
b_is_nan = np.isnan(b)

result_is_nan = np.isnan(result)

mask_a = np.logical_and(result_is_nan, np.logical_not(a_is_nan))
result[mask_a] = a[mask_a]

mask_b = np.logical_and(result_is_nan, np.logical_not(b_is_nan))
result[mask_b] = b[mask_b]

print(result)
</code></pre>

<p>A little bit of explanation:</p>

<p>The first operation is np.add(a,b). This adds both matrices and any NaN element will produce a result of NaN also.</p>

<p>To select the NaN values from either arrays, we use a logical mask:</p>

<pre><code># result_is_nan is a boolean array containing True whereve the result is np.NaN. This occurs when any of the two element were NaN
result_is_nan = np.isnan(result)

# mask_a is a boolean array which 'flags' elements that are NaN in result but were not NaN in a !
mask_a = np.logical_and(result_is_nan, np.logical_not(a_is_nan))
# Using that mask, we assign those value to result
result[mask_a] = a[mask_a]
</code></pre>

<p>There you have it ! </p>
","7569728","","","0","1434","W. Laroche","2017-02-15 15:17:49","46","5","2","0","50642414","","2018-06-01 11:27:29","1","416","<p>I have two 4D matrices, which I would like to add. The matrices have the exact same dimension and number of elements, but they both contain randomly distributed NaN values. </p>

<p>I would prefer to add them as below using numpy.nansum.<br>
(1) if two values are added I want the sum to be a value,<br>
(2) if a value and a NaN are added I want the sum to be the value and<br>
(3) if two NaN are added I want the sum to be NaN.  </p>

<p>Herewith what I tried</p>

<pre><code>a[6x7x180x360]
b[6x7x180x360]

C=np.nansum[(a,b)]
C=np.nansum(np.dstack((a,b)),2)
</code></pre>

<p>But I am unable to get the resultant matrix with same dimension as input. It means resultant matrix C should be in [6x7x180x360].
Anyone can help in this regard. Thank you in advance.</p>
","8129826","","","Avoid NaN values and add two matrices element wise in python","<python><numpy>","3","0","768"
"50642872","2018-06-01 11:54:58","0","","<p>Try something like </p>

<pre><code>y_train = []
for _ in range(len(dataframe)):
    string = dataframe.at[_, 'Product Categorization Tier 1'].strip()
    number = category_list.index(string)   
    # saving as category vector
    vector = [0] * 25 
    vector[number] = 1
    y_train.append(vector)
</code></pre>

<p>And make sure you get a 2d-int array and not an array of objects</p>
","9280994","","","2","390","Jonathan R","2018-01-28 21:47:13","1615","128","51","6","50641631","","2018-06-01 10:40:56","2","150","<p>When trying to train my tensorflow graph im getting the error message:</p>

<blockquote>
  <p>ValueError: setting an array element with a sequence</p>
</blockquote>

<p>happening in this line of code, in the feed_dict function:</p>

<pre><code># run the session and train the model
        _, c = sess.run([optimizer, cost], feed_dict = {input_x: x_train_v, output_y: y_train})
</code></pre>

<p>It seems to be a problem with my output variable (y_train). It is a List of size(25) inside a pandas dataframe. 
Already checked if every list has the same length with </p>

<pre><code>print(y_train.shape) #(23904,)
print(y_train.apply(type)[0]) #&lt;class 'list'&gt;

n = len(y_train[0])
if all(len(x) == n for x in y_train):
    print(""true"")  #true  
</code></pre>

<p>The variable is created with following code:</p>

<pre><code>dataframe['category_number'] = """"
for _ in range(len(dataframe)):
    string = dataframe.at[_, 'Product Categorization Tier 1'].strip()
    number = category_list.index(string)   
    # saving as category vector
    vector = [0] * 25 
    vector[number] = 1
    dataframe.at[_,'category_number'] = vector

y_train = train_df[""category_number""]
</code></pre>

<p><strong>Edit:</strong>
Cost Function and Optimizer</p>

<pre><code>prediction = neural_network_model(input_x )
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=output_y))
optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)
</code></pre>

<p>Full Error Message:
<a href=""https://i.stack.imgur.com/IRVCV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IRVCV.png"" alt=""enter image description here""></a></p>
","4841848","4841848","2018-06-01 11:39:12","Tensorflow, ValueError: setting an array element with a sequence","<python><pandas><tensorflow>","1","6","1681"
"50642877","2018-06-01 11:55:12","1","","<p>You need to use the <code>fillvalue</code> keyword argument of <code>zip_longest</code>:</p>

<pre><code>ziplongest(..., ..., fillvalue=('', ''))
</code></pre>

<p>Otherwise the default is <code>None</code> and <code>None</code> cannot fill a 2-tuple for example <code>(username, usrValue)</code>.</p>

<p>Aside from that, since dictionaries are not ordered the <code>zip</code> operation will return random pairs...</p>
","1388292","1388292","2018-06-01 12:35:56","0","424","Jacques Gaudin","2012-05-10 23:01:38","8213","656","1988","169","50642650","50642877","2018-06-01 11:41:36","1","204","<p>I am developing a python script to analyse a txt file and then save it to a osv file. I am trying to use ""zip_longest"" from the module ""itertools"". When one of my dictionaries no longer has i value i want it to paste a empty space while the other dictionary continues to paste its values.
My code looks like this:</p>

<pre><code>def csvExport(self):
        exportYN = input(""Would you like to export the document to a CSV file? (Y/N):"")
        if (exportYN == ""Y"" or exportYN == ""y""):
            with open('data.csv', 'w', encoding=""utf-8"") as csvfile:
                csvfile.write(""Username;Repeated;Password;Repeated"")
                for (username, usrValue), (password, passValue) in itertools.zip_longest(self.usernames.items(), self.passwords.items()):
                    csvfile.write(str(username) + "";"" + str(usrValue) + "";"" + str(password) + "";"" + str(passValue))
</code></pre>

<p>And the error code looks like this:</p>

<pre><code>for (username, usrValue), (password, passValue) in itertools.zip_longest(self.usernames.items(), self.passwords.items()):
TypeError: 'NoneType' object is not iterable
</code></pre>

<p>I think it is related to zip_longest because the two dictionaries i use is not the same length.</p>

<p>Hope you can help :)</p>
","8670650","1222951","2018-06-01 11:44:58","TypeError: 'NoneType' object is not iterable when using zip_longest","<python><python-3.x>","1","3","1267"
"50642891","2018-06-01 11:55:52","0","","<p>This code is not optimized (it certainly can be done in more performant, higher order ways), but have a look at <a href=""https://repl.it/repls/DependentBubblyAssignment"" rel=""nofollow noreferrer"">https://repl.it/repls/DependentBubblyAssignment</a></p>

<p>I think a very clear and beginner-friendly approach is to create a second list which reorders the columns as rows - then search the columns for matches in the same way as in the rows.</p>

<p>Details to the repl.it code:</p>

<p><code>crime in row or crime in row[::-1]</code> - checks if a <code>crime</code> is present either in ltr or rtl <code>row</code></p>

<p><code>verticals = ['' for char in slist[0]]</code> - creates a list filled with empty strings corresponding to the length of the first row (excl. whitespaces thanks to the previous <code>line.split(' ')</code></p>

<p><code>verticals[col] += slist[row][col]</code> - takes the char from the original string from the <code>col</code> at <code>row</code> and appends it to the current vertical <code>col</code> index</p>

<p>For your example data, it finds <code>['LOITERING', 'SMOKING', 'REDRUM', 'BADSINGING', 'BEINGSMELLY', 'JAYWALKING']</code></p>
","3820185","3820185","2018-06-01 12:11:59","7","1176","wiesion","2014-07-09 11:21:21","2008","165","89","27","50642307","50642891","2018-06-01 11:21:18","2","94","<p>I have an assignment where we have to find words in a word search (minus diagonals).                                     </p>

<pre><code>M L G Y J U G D T W W I S F P
G Y O H I K O P V F J B J J H
N B I M T M Y W R D J E C A I
I Y M X C U E U Z G U I J Y C
K D P S L R W J I N C N S W T
O P S D I D A B Z I D G D A B
M X D H K E B H U G T S I L G
S G D U Y R V D G N D M L K M
S P X K T W E F P I G E J I T
B U L B C M K I F S I L F N W
Z Q L X H G C J N D I L B G C
M T B W Z L A D A A X Y O K X
A E C Z K F Y V F B U V G A W
Y G O Z E A W J R N S Q J E A
L O I T E R I N G H F I P G R
</code></pre>

<p>This is the <code>15x15</code> word search that we are given as a text file. We must identify the criminal words within the search out of the list: </p>

<pre><code>[""JAYWALKING"", ""BURGLARY"", ""LAUNDERING"", ""BADSINGING"", ""REDRUM"", ""SMOKING"", ""BEINGSMELLY"",""CONNING"", ""SCAMS"", ""LOITERING""]
</code></pre>

<p>I have gone through the word search and have identified that the words needing to be found are: redrum, badsinging, beingsmelly, jaywalking, and loitering. </p>

<p>However, in my code, I only get a result of one from each direction: loitering, and jaywalking. I was hoping someone could give me some advice on how to make it fully functional. </p>

<pre><code>def crossword():
empty_list = []
crimes = [""JAYWALKING"", ""BURGLARY"", ""LAUNDERING"", ""BADSINGING"", ""REDRUM"", ""SMOKING"", ""BEINGSMELLY"",
          ""CONNING"", ""SCAMS"", ""LOITERING""]
confession = open(""confession.txt"", ""r"")
for line in confession:
    line = line.strip(""\n"")
    new_list = line.split(' ')
    empty_list.append(new_list)

#horizontal
words = []

for i in range(len(empty_list)):
    string_1 = """"
    string_2 = """"
    m = len(empty_list[i])
    for j in range(len(empty_list[i])):
        string_1 = string_1 + empty_list[i][j]
        string_2 = string_2 + empty_list[i][m-1-j]
    for k in range(len(crimes)):
        if crimes[k] in string_1 or crimes[k] in string_2:
            words.append(crimes[k])


#vertical
for i in range(len(empty_list)):
    string_1 = """"
    string_2 = """"
    m = len(empty_list[i])
    for j in range(len(empty_list[i])):
        string_1 = string_1 + empty_list[j][i]
        string_2 = string_2 + empty_list[j][m-1-i]
    for k in range(len(crimes)):
        if crimes[k] in string_1 or crimes[k] in string_2:
            words.append(crimes[k])
            return words

print(""Gentleman GoGo is guilty of:"")
print(crossword())
</code></pre>
","7789101","7505411","2018-06-01 11:24:40","How to make a proper word searcher with Python?","<python><python-3.x><list><file><loops>","1","2","2466"
"50642947","2018-06-01 11:59:40","1","","<p>I think the easiest way is to use <code>np.where</code></p>

<pre><code>result = np.where(
    np.isnan(a+b),
    np.where(np.isnan(a), b, a), 
    a+b
)
</code></pre>

<p>This reads as:
if <code>a+b</code> is not <code>nan</code>, use <code>a+b</code>, else use <code>a</code>, unless it is <code>nan</code>, then use <code>b</code>. Whether or <code>b</code> is <code>nan</code> is of little consequence then.</p>

<p>Alternatively, you can use it like this:</p>

<pre><code>result2 = np.where(
    np.isnan(a) &amp; np.isnan(b),
    np.nan,
    np.nansum(np.stack((a,b)), axis=0)
)
</code></pre>

<p><code>np.testing.assert_equal(result, result2)</code> passes</p>
","1562285","","","0","671","Maarten Fabré","2012-07-30 07:57:02","5780","346","203","47","50642414","","2018-06-01 11:27:29","1","416","<p>I have two 4D matrices, which I would like to add. The matrices have the exact same dimension and number of elements, but they both contain randomly distributed NaN values. </p>

<p>I would prefer to add them as below using numpy.nansum.<br>
(1) if two values are added I want the sum to be a value,<br>
(2) if a value and a NaN are added I want the sum to be the value and<br>
(3) if two NaN are added I want the sum to be NaN.  </p>

<p>Herewith what I tried</p>

<pre><code>a[6x7x180x360]
b[6x7x180x360]

C=np.nansum[(a,b)]
C=np.nansum(np.dstack((a,b)),2)
</code></pre>

<p>But I am unable to get the resultant matrix with same dimension as input. It means resultant matrix C should be in [6x7x180x360].
Anyone can help in this regard. Thank you in advance.</p>
","8129826","","","Avoid NaN values and add two matrices element wise in python","<python><numpy>","3","0","768"
"50643000","2018-06-01 12:02:31","1","","<p>I had an issue awhile back on one of my projects when switching from mac to windows. Turns out I just had a more recent version of the language on my windows machine that didn't like the way I did some things, I would first check the versions of your IDE as well as Language, then if they arent matching compiling your program on the linux machine and transferring the compiled version to your windows machine. If it runs fine like that, try to get matching versions of the language.  </p>
","9869959","","","1","493","Striker","2018-05-30 11:34:01","223","98","34","3","50642909","50643000","2018-06-01 11:57:03","-2","56","<p>I am extending a program that run without any issue in my Linux machine but not in Windows. I need to make it in Windows because the Linux machine has some issues in maintaining the Bluetooth connection.</p>

<p>This the error I am getting:</p>

<pre><code>Traceback (most recent call last):
  File ""myProject.py"", line 106, in &lt;module&gt;
    application = Server(options, args)
  File ""myProject.py"", line 42, in __init__
    super(Server, self).__init__(options, args)
  File ""C:\Users\admin\Documents\Python\Server\lib\kernel.py"", line 30, in __init__
    inject.configure(self.__configure_dependencies)
  File ""C:\Program Files\Python35\lib\site-packages\inject.py"", line 102, in configure
    _INJECTOR = Injector(config)
  File ""C:\Program Files\Python35\lib\site-packages\inject.py"", line 230, in __init__
    config(binder)
  File ""C:\Users\admin\Documents\Python\Server\lib\kernel.py"", line 58, in __configure_dependencies
    module = importlib.import_module(module_source, False)
  File ""C:\Program Files\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""&lt;frozen importlib._bootstrap&gt;"", line 985, in _gcd_import
  File ""&lt;frozen importlib._bootstrap&gt;"", line 968, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 955, in _find_and_load_unlocked
ImportError: No module named 'src\\myProject_config\\module'
</code></pre>

<p>I am sure the problem is with the environment not with the code that is why I did not include any. </p>
","5482015","","","Python - programs works in Linux but not in windows","<python><python-3.x><dependency-injection><pip>","1","1","1562"
"50643082","2018-06-01 12:07:07","2","","<p>You are printing Unicode to the <code>sys.stdout</code> handle (which is the default file object <code>print()</code> writes to). That object then has to encode your data again, but it has to do so based on the environment that it is connected to.</p>

<p>When you run <code>python3 ./test.py</code> then you connected to your terminal or console, and it is usually configured to tell scripts what codec is appropriate. On POSIX systems (Linux, Mac) you can run the <code>locale</code> command to see what that configuration is. In your console locale there is no problem displaying a non-ASCII codepoint like <code>ë</code>.</p>

<p>But when running as a CGI script connected to a webserver, there is no such language configuration present, and Python almost certainly has fallen back to the lowest common denominator instead: ASCII. And when this is the case, trying to print non-Unicode text will result in an exception:</p>

<pre><code>$ LC_ALL=""en_US.UTF-8"" python3 -c ""print(b'Chlo\xc3\xab'.decode())""
Chloë
$ LC_ALL=""C"" python3 -c ""print(b'Chlo\xc3\xab'.decode())""  # C =&gt; ""no locale set""
Traceback (most recent call last):
  File ""&lt;string&gt;"", line 1, in &lt;module&gt;
UnicodeEncodeError: 'ascii' codec can't encode character '\xeb' in position 4: ordinal not in range(128)
</code></pre>

<p>Because the exception takes place only <em>after</em> producing headers and all the other output, you don't see an HTTP error code. The exception should have been logged in your server error logs, however.</p>

<p>If your script is to output UTF-8 to the browser, as configured in the Content-Type header you emit, <em>replace</em> <code>sys.stdout</code> to force that codec:</p>

<pre><code>import sys
from io import TextIOWrapper

sys.stdout = TextIOWrapper(sys.stdout.buffer.detach(), encoding='utf8')
</code></pre>

<p>In Python 3, text files like those used for the <code>sys.stdout</code> stream, contain a buffer object, which in turn contains a binary file object that takes care of the actual binary data writing. The outer text file object is only responsible for encoding on write, really. The above replaces that outer object with a different one that always encodes to UTF-8.</p>
","100297","100297","2018-06-01 12:28:59","0","2205","Martijn Pieters","2009-05-03 14:53:57","770256","252083","5762","19510","50642908","50643082","2018-06-01 11:57:00","1","157","<p>I have a very short sample code:</p>

<pre><code>print(""Content-Type: text/plain; charset=utf-8"")
print(""Access-Control-Allow-Origin: *"")
print()

x = 'Chloë'.encode()
print(x)
print(x.decode())
</code></pre>

<p>Notice non Ascii <strong>ë</strong>, which is the source of all problems.</p>

<p>Calling the script in bash using <code>python3 ./test.py</code> produces following (correct) input:</p>

<pre><code>Content-Type: text/plain; charset=utf-8
Access-Control-Allow-Origin: *

b'Chlo\xc3\xab'
Chloë
</code></pre>

<p>However calling it from the browser, the last line is not present (headers of course aren't visible, but they are present). So the only visible part is:</p>

<pre><code>b'Chlo\xc3\xab'
</code></pre>

<p>Do you know, where could be a problem?</p>
","2318602","100297","2018-06-01 11:59:07","Python 3 bytes decode with non-ascii characters in CGI script","<python><python-3.x><encoding><http-headers><decode>","1","0","772"
"50643083","2018-06-01 12:07:07","2","","<p>Since you have no header, the column names are the integer order in which they occur, i.e. the first column is <code>df[0]</code>.  To programmatically set the last column to be <code>int32</code>, you can read the first line of the file to get the width of the dataframe, then construct a dictionary of the integer types you want to use with the number of the columns as the keys.</p>

<pre><code>import numpy as np
import pandas as pd

with open('file.dat') as fp:
    width = len(fp.readline().strip().split(','))
    dtypes = {i: np.int8 for i in range(width)}
    # update the last column's dtype
    dtypes[width-1] = np.int32

    # reset the read position of the file pointer
    fp.seek(0)
    df = pd.read_csv(fp, sep=',', engine='c', header=None, 
                     na_filter=False, dtype=dtypes, low_memory=False)
</code></pre>
","5003756","5003756","2018-06-01 12:14:57","0","846","James","2015-06-12 15:12:01","17092","1119","652","157","50642777","50643113","2018-06-01 11:49:40","0","6254","<p>I have a large csv file (~10GB), with around 4000 columns. I know that most of data i will expect is int8, so i set:</p>

<pre><code>pandas.read_csv('file.dat', sep=',', engine='c', header=None, 
                na_filter=False, dtype=np.int8, low_memory=False)
</code></pre>

<p>Thing is, the final column (4000th position) is int32, is there away can i tell read_csv that use int8 by default, and at column 4000th, use int 32?</p>

<p>Thank you</p>
","552279","5003756","2018-06-01 12:02:26","Set data type for specific column when using read_csv from pandas","<python><pandas>","2","1","454"
"50643113","2018-06-01 12:08:31","3","","<p>If you are certain of the number you could recreate the dictionary like this:</p>

<pre><code>dtype = dict(zip(range(4000),['int8' for _ in range(3999)] + ['int32']))
</code></pre>

<p>Considering that this works:</p>

<pre><code>import pandas as pd
import numpy as np
​
data = '''\
1,2,3
4,5,6'''
​
fileobj = pd.compat.StringIO(data)
df = pd.read_csv(fileobj, dtype={0:'int8',1:'int8',2:'int32'}, header=None)
​
print(df.dtypes)
</code></pre>

<p>Returns:</p>

<pre><code>0     int8
1     int8
2    int32
dtype: object
</code></pre>

<p>From the docs:</p>

<blockquote>
  <p>dtype : Type name or dict of column -> type, default None</p>
  
  <p>Data type for data or columns. E.g. {‘a’: np.float64, ‘b’: np.int32}
  Use str or object to preserve and not interpret dtype. If converters
  are specified, they will be applied INSTEAD of dtype conversion.</p>
</blockquote>
","7386332","7386332","2018-06-01 12:14:50","0","874","Anton vBR","2017-01-07 01:18:16","12659","1153","834","480","50642777","50643113","2018-06-01 11:49:40","0","6254","<p>I have a large csv file (~10GB), with around 4000 columns. I know that most of data i will expect is int8, so i set:</p>

<pre><code>pandas.read_csv('file.dat', sep=',', engine='c', header=None, 
                na_filter=False, dtype=np.int8, low_memory=False)
</code></pre>

<p>Thing is, the final column (4000th position) is int32, is there away can i tell read_csv that use int8 by default, and at column 4000th, use int 32?</p>

<p>Thank you</p>
","552279","5003756","2018-06-01 12:02:26","Set data type for specific column when using read_csv from pandas","<python><pandas>","2","1","454"
"50643121","2018-06-01 12:09:01","0","","<p>You can try to remove every <code>\n</code> in <code>S</code> like so: </p>

<pre><code>df[""S""] = df[""S""].apply(lambda x: float(str(x).replace(""\n"", """")))
</code></pre>

<p>But by the way: <code>df[df[""S""]]</code> will look for the values of <code>S</code> in the index. With your construction of the dataframe, there will probably not be any value of <code>S</code> in the index. </p>
","6503432","","","8","389","rongon","2016-06-23 09:35:07","470","56","51","4","50642551","","2018-06-01 11:35:31","0","51","<p>I am trying to access a column of a data frame I created out of two lists and do some filtering. However, there seems to be an additional space for every 12th element in my dataframe. How do I deal with this?   </p>

<pre><code>import pandas as pd

df = pd.DataFrame(
    {'S': s,
     'K': k})
</code></pre>

<p>I created the dataframe with the code as above. Also, for some weird reason, it was stored in scientific notation with type float. I used df.round(4) before I could actually figure out what the problem was. </p>

<hr>

<pre><code>KeyError                                  Traceback (most recent call last)
&lt;ipython-input-14-2e289598b460&gt; in &lt;module&gt;()
----&gt; 1 df[df['S']]

~\Anaconda\lib\site-packages\pandas\core\frame.py in __getitem__(self, key)
   1956         if isinstance(key, (Series, np.ndarray, Index, list)):
   1957             # either boolean or fancy integer index
-&gt; 1958             return self._getitem_array(key)
   1959         elif isinstance(key, DataFrame):
   1960             return self._getitem_frame(key)

~\Anaconda\lib\site-packages\pandas\core\frame.py in _getitem_array(self, key)
   2000             return self.take(indexer, axis=0, convert=False)
   2001         else:
-&gt; 2002             indexer = self.loc._convert_to_indexer(key, axis=1)
   2003             return self.take(indexer, axis=1, convert=True)
   2004 

~\Anaconda\lib\site-packages\pandas\core\indexing.py in _convert_to_indexer(self, obj, axis, is_setter)
   1229                 mask = check == -1
   1230                 if mask.any():
-&gt; 1231                     raise KeyError('%s not in index' % objarr[mask])
   1232 
   1233                 return _values_from_object(indexer)

KeyError: '[-0.65 -0.6  -0.6  -0.6  -0.55 -0.55 -0.55 -0.55 -0.55 -0.55 -0.5  -0.5\n -0.5  -0.5  -0.5  -0.5  -0.5  -0.5  -0.45 -0.45 -0.45 -0.45 -0.45 -0.45\n -0.45 -0.45 -0.45 -0.45 -0.4  -0.4  -0.4  -0.4  -0.4  -0.4  -0.4  -0.4\n -0.4  -0.4  -0.4  -0.4  -0.35 -0.35 -0.35 -0.35 -0.35 -0.35 -0.35 -0.35\n -0.35 -0.35 -0.35 -0.35 -0.35 -0.35 -0.3  -0.3  -0.3  -0.3  -0.3  -0.3\n -0.3  -0.3  -0.3  -0.3  -0.3  -0.3  -0.3  -0.3  -0.3  -0.3  -0.25 -0.25\n -0.25 -0.25 -0.25 -0.25 -0.25 -0.25 -0.25 -0.25 -0.25 -0.25 -0.25 -0.25\n -0.25 -0.25 -0.25 -0.25 -0.2  -0.2  -0.2  -0.2  -0.2  -0.2  -0.2  -0.2\n -0.2  -0.2  -0.2  -0.2  -0.2  -0.2  -0.2  -0.2  -0.2  -0.2  -0.2  -0.15\n -0.15 -0.15 -0.15 -0.15 -0.15 -0.15 -0.15 -0.15 -0.15 -0.15 -0.15 -0.15\n -0.15 -0.15 -0.15 -0.15 -0.15 -0.15 -0.15 -0.1  -0.1  -0.1  -0.1  -0.1\n -0.1  -0.1  -0.1  -0.1  -0.1  -0.1  -0.1  -0.1  -0.1  -0.1  -0.1  -0.1\n -0.1  -0.1  -0.1  -0.1  -0.05 -0.05 -0.05 -0.05 -0.05 -0.05 -0.05 -0.05\n -0.05 -0.05 -0.05 -0.05 -0.05 -0.05 -0.05 -0.05 -0.05 -0.05 -0.05 -0.05\n -0.05 -0.05  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n  0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05\n  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05  0.05\n  0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1\n  0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1   0.1\n  0.1   0.15  0.15  0.15  0.15  0.15  0.15  0.15  0.15  0.15  0.15  0.15\n  0.15  0.15  0.15  0.15  0.15  0.15  0.15  0.15  0.15  0.15  0.15  0.15\n  0.15  0.15  0.2   0.2   0.2   0.2   0.2   0.2   0.2   0.2   0.2   0.2\n  0.2   0.2   0.2   0.2   0.2   0.2   0.2   0.2   0.2   0.2   0.2   0.2\n  0.2   0.2   0.2   0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25\n  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25  0.25\n  0.25  0.25  0.25  0.25  0.25  0.3   0.3   0.3   0.3   0.3   0.3   0.3\n  0.3   0.3   0.3   0.3   0.3   0.3   0.3   0.3   0.3   0.3   0.3   0.3\n  0.3   0.3   0.3   0.3   0.3   0.3   0.35  0.35  0.35  0.35  0.35  0.35\n  0.35  0.35  0.35  0.35  0.35  0.35  0.35  0.35  0.35  0.35  0.35  0.35\n  0.35  0.35  0.35  0.35  0.35  0.35  0.35  0.4   0.4   0.4   0.4   0.4\n  0.4   0.4   0.4   0.4   0.4   0.4   0.4   0.4   0.4   0.4   0.4   0.4\n  0.4   0.4   0.4   0.4   0.4   0.4   0.4   0.4   0.45  0.45  0.45  0.45\n  0.45  0.45  0.45  0.45  0.45  0.45  0.45  0.45  0.45  0.45  0.45  0.45\n  0.45  0.45  0.45  0.45  0.45  0.45  0.45  0.45  0.45  0.5   0.5   0.5\n  0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5\n  0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.55  0.55  0.55\n  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55\n  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.55  0.6   0.6   0.6   0.6\n  0.6   0.6   0.6   0.6   0.6   0.6   0.6   0.6   0.6   0.6   0.6   0.6\n  0.6   0.6   0.6   0.6   0.6   0.6   0.65  0.65  0.65  0.65  0.65  0.65\n  0.65  0.65  0.65  0.65  0.65  0.65  0.65  0.65  0.65  0.65  0.65  0.65\n  0.65  0.65  0.65  0.7   0.7   0.7   0.7   0.7   0.7   0.7   0.7   0.7\n  0.7   0.7   0.7   0.7   0.7   0.7   0.7   0.7   0.7   0.7   0.7   0.75\n  0.75  0.75  0.75  0.75  0.75  0.75  0.75  0.75  0.75  0.75  0.75  0.75\n  0.75  0.75  0.75  0.75  0.75  0.8   0.8   0.8   0.8   0.8   0.8   0.8\n  0.8   0.8   0.8   0.8   0.8   0.8   0.8   0.8   0.8   0.8   0.85  0.85\n  0.85  0.85  0.85  0.85  0.85  0.85  0.85  0.85  0.85  0.85  0.85  0.85\n  0.85  0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9   0.9\n  0.9   0.9   0.95  0.95  0.95  0.95  0.95  0.95  0.95  0.95  0.95  0.95\n  0.95  1.    1.    1.    1.    1.    1.    1.    1.    1.    1.05  1.05\n  1.05  1.05  1.05  1.05  1.1   1.1   1.1   1.1   1.15  1.3   1.3   1.3\n  1.3   1.3   1.35  1.35  1.35  1.35  1.35  1.35  1.35  1.35  1.35  1.35\n  1.4   1.4   1.4   1.4   1.4   1.4   1.4   1.4   1.4   1.4   1.4   1.4\n  1.45  1.45  1.45  1.45  1.45  1.45  1.45  1.45  1.45  1.45  1.45  1.45\n  1.45  1.45  1.45  1.45  1.45] not in index'
</code></pre>
","8300107","8300107","2018-06-01 11:58:28","Removing a tab every nth column from pandas df","<python><python-3.x><pandas><dataframe>","1","4","5847"
"50643145","2018-06-01 12:10:23","10","","<p>The <code>permission denied</code> error is raised because you've already borked your virtual environment by installing with <code>sudo</code>. Run</p>

<pre><code>$ sudo chown -R david:staff /Users/david/Documents/projects/uptimeapp/env
</code></pre>

<p>to fix the permissions. Maybe it's even wise to fix the permissions for the whole home dir, should you have other permission issues:</p>

<pre><code>$ sudo chown -R david:staff /Users/david/
</code></pre>

<p>Now reinstalling packages should work again:</p>

<pre><code>$ source /Users/david/Documents/projects/uptimeapp/env/bin/activate
$ (env) pip uninstall -y fabric
$ (env) pip install fabric
</code></pre>

<blockquote>
  <p><code>'libssh2.h' file not found</code></p>
</blockquote>

<p>means that before installing <code>ssh-python</code>, you need to install the according lib first:</p>

<pre><code>$ brew install libssh2
</code></pre>
","2650249","","","4","903","hoefling","2013-08-04 10:33:30","21122","1974","2259","216","50639973","50643145","2018-06-01 09:14:00","11","35607","<p>I think I have some issues with either Python and/or pip on my Mac. I have Python 2.7 installed globally and then I normally setup virtualenvs and install Python3.6.4 but in the last day or so Ive been getting problems with packages such as Fabric and SSH2 where I have either not been able to install them with various errors or with Fabric it throws when I try to import the package.</p>

<p>Im now trying to remove Fabric and install Fabric3 and its throwing errors like this:</p>

<pre><code>Could not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/Users/david/Documents/projects/uptimeapp/env/lib/python3.6/site-packages/Fabric3-1.14.post1.dist-info'
Consider using the `--user` option or check the permissions.

(env) Davids-MacBook-Air:uptimeapp david$ pip install fabric3 --user
Can not perform a '--user' install. User site-packages are not visible in this virtualenv.
</code></pre>

<p>If I do <code>sudo pip install fabric</code>  then it installs but with this warning:</p>

<pre><code>The directory '/Users/david/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
The directory '/Users/david/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
</code></pre>

<p>But I thought it was not advised to pip install with sudo?</p>

<p>These are the errors I get when I try to <code>pip install ssh2-python</code></p>

<pre><code>ssh2/agent.c:569:10: fatal error: 'libssh2.h' file not found
    #include ""libssh2.h""
             ^~~~~~~~~~~
    1 error generated.
    error: command 'clang' failed with exit status 1

    ----------------------------------------
Command ""/Users/david/Documents/projects/uptimeapp/env/bin/python3.6 -u  -c ""import setuptools,   tokenize;__file__='/private/var/folders/bl/97vt48j97zd2sj05zmt4xst00000gn/T  /pip-install-mpyq41q4/ssh2-python/setup.py';f=getattr(tokenize, 'open',   open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record   /private/var/folders/bl/97vt48j97zd2sj05zmt4xst00000gn/T/pip-record-qul_k3kq/install-record.txt --single-version-externally-managed --compile -  -install-headers /Users/david/Documents/projects/uptimeapp/env/bin/../include/site/python3.6  /ssh2-python"" failed with error code 1 in /private/var/folders/bl/97vt48j97zd2sj05zmt4xst00000gn/T/pip-install-mpyq41q4/ssh2-python/
</code></pre>

<p>I have managed to remove Fabric and install Fabric3 with the sudo command but I would rather not do that.</p>

<p>I should add that Ive not had any other problems with installing other packages either globally in Python2.7 or in envs.</p>
","7266376","2650249","2019-07-30 16:06:12","Pip problems - Could not install packages due to an EnvironmentError","<python><python-3.x><macos><pip>","2","0","2950"
"50643168","2018-06-01 12:11:20","3","","<p>Here is the solution I propose:</p>

<pre><code>def remove_id(data):
    if isinstance(data, List):
        return [remove_id(sub_data) for sub_data in data]

    if isinstance(data, Dict):
        return {key: remove_id(value) for key, value in data.items()
                if key != 'id'}

    return data
</code></pre>

<p>And the result:</p>

<pre><code>{'snapshot_id': None,
 'snapshots': [{'volumes': [{'snapshot_id': '3ddc7ddd-02ca-4669-a0cb-fb0d56a4a6f5',
                             'snapshots': []},
                            {'snapshot_id': '3ddc7ddd-02ca-4669-a0cb-fb0d56a4a6f5',
                             'snapshots': [{'volumes': [{'snapshot_id': 'd637f6ea-4a41-448c-874f-ffe624ddc597',
                                                         'snapshots': []}]}]},
                            {'snapshot_id': '3ddc7ddd-02ca-4669-a0cb-fb0d56a4a6f5',
                             'snapshots': []}]}]}
</code></pre>
","4870915","4870915","2018-06-01 22:56:26","3","937","Gelineau","2015-05-06 13:55:15","1462","118","1019","92","50642922","","2018-06-01 11:58:08","0","128","<p>I have below dictionary (generated from a report, so the structure can change).
I need to go to the depth of the dictionary, find the <code>id</code> which in this case is <code>'id': u'ef3c8cf1-0987-4e56-a6d5-763c42be1f75'</code>, (there can be more than 1), delete that id and then move to one level up and repeat the same till I get to the top <code>id</code> which I finally delete. Since there is a dependency, I need to delete the orphan <code>id</code> first and then move to the top. </p>

<p>Any help is appreciable. If any other file/information is needed, please let me know. </p>

<pre><code>{ 
'id': u'4c31d813-a989-47dd-b01b-9a27b8db2dfc',                                                                                                                      
'snapshots': 
    [
        {
            'id': u'3ddc7ddd-02ca-4669-a0cb-fb0d56a4a6f5', 
            'volumes': 
                [
                    {  
                        'id': u'5488de90-50dc-4d72-a6aa-c995422fa179', 
                        'snapshots': [], 
                        'snapshot_id': u'3ddc7ddd-02ca-4669-a0cb-fb0d56a4a6f5'
                    }, 
                    {
                        'id': u'e566645f-4fb3-4778-be67-447a5bdd678d', 
                        'snapshots': 
                            [
                                { 
                                    'id': u'd637f6ea-4a41-448c-874f-ffe624ddc597', 
                                    'volumes': 
                                        [
                                            { 
                                                'id': u'ef3c8cf1-0987-4e56-a6d5-763c42be1f75', 
                                                'snapshots': [], 
                                                'snapshot_id': u'd637f6ea-4a41-448c-874f-ffe624ddc597'
                                            }
                                        ]
                                }
                            ], 
                        'snapshot_id': u'3ddc7ddd-02ca-4669-a0cb-fb0d56a4a6f5'}, 
                    {
                        'id': u'196483ee-4f21-4d83-8e15-8caea532b2ab', 
                        'snapshots': [], 
                        'snapshot_id': u'3ddc7ddd-02ca-4669-a0cb-fb0d56a4a6f5'
                    }
                ]
        }
    ], 
'snapshot_id': None
}
</code></pre>

<p>Python code</p>

<pre><code>oh=openstack_helper.OpenstackHelper()

def get_objects(item):
    items=None
    if item == 'stacks':
        items=oh.get_stacks()
    if item == 'volumes':
        items=oh.get_volumes()
    if item == 'snapshots':
        items=oh.get_snapshots()
    return items


def dep_graph(volumes,snapshots,snapshot_id=None):
    vol_list=[]

    for volume in volumes:
        if volume.snapshot_id == snapshot_id:
            info={'id':volume.id,'snapshot_id':volume.snapshot_id,'snapshots':[],
                  }
            vol_list.append(info)
    for snapshot in snapshots:
        for volume in vol_list:
            snap_list=[]
            if snapshot.volume_id == volume['id']:
               info={'id':snapshot.id, 'volumes':[]}
               info['volumes'].extend(dep_graph(volumes,snapshots,snapshot.id))
               volume['snapshots'].append(info)
    return vol_list

if __name__ == '__main__':

    volumes = get_objects('volumes')
    snapshots = get_objects('snapshots')
    output = dep_graph(volumes, snapshots)
    print output
</code></pre>
","8017535","8017535","2018-06-01 12:12:58","Traverse nested dictionary from depth and move to top","<python><python-2.7><dictionary><openstack>","1","4","3475"
"50643191","2018-06-01 12:12:44","1","","<p>change your line 7 and 8 </p>

<pre><code>for i in user_file:
    users_list.append(i)
</code></pre>

<p>to </p>

<pre><code>for i in user_file:
    users_list.append(i.strip())
</code></pre>

<p>and it should work as expected.</p>

<p>It is because <code>i</code> is a line from <code>user_file</code> and it ends with <code>\n</code>. <code>i.strip()</code> removes the trailing newline. </p>
","8278951","","","1","398","Rahul Goswami","2017-07-09 11:06:31","386","64","96","18","50642819","50643191","2018-06-01 11:52:01","0","42","<p>The script is adding a line break right after the curly braces while writing both strings right after the username.  I would think this is because of my source text file has encoding in it adding the breaks but as far as I can tell it does not.  Am I misunderstanding how write works?  Something simple I'm missing.  Been looking at this too long and need a new set of eyes.</p>

<pre><code>users_list = []

users = input(""File of User's "")

user_file = open(users  + '.txt', 'r')

for i in user_file:
    users_list.append(i)

sql_file = open('sql.txt', 'w')

sql_file.write(""SELECT MACHINE.NAME AS SYSTEM_NAME, SYSTEM_DESCRIPTION, 
MACHINE.IP, MACHINE.MAC, MACHINE.ID as TOPIC_ID FROM MACHINE WHERE 
((MACHINE.USER_NAME = '{}') OR "".format(users_list[0]))

for i in users_list:
    sql_file.write(""(MACHINE.USER_NAME = '{}')"".format(i))
    sql_file.write("" OR "")
</code></pre>

<p>The output of the file looks like this:</p>

<pre>
SELECT MACHINE.NAME AS SYSTEM_NAME, SYSTEM_DESCRIPTION, MACHINE.IP, MACHINE.MAC, MACHINE.ID as TOPIC_ID FROM MACHINE WHERE ((MACHINE.USER_NAME = 'hnelson
') OR (MACHINE.USER_NAME = 'hnelson
') OR (MACHINE.USER_NAME = 'snery
') OR (MACHINE.USER_NAME = 'jherman
</pre>
","2905001","3204551","2018-06-01 12:14:03","Script adding line break in middle of string","<python><string><python-3.x>","1","2","1205"
"50643192","2018-06-01 12:12:48","2","","<p>Something like this (not sure how you want to squeeze the result from 2D down to 1D?):</p>

<pre><code>&gt;&gt;&gt; np.isin(large,small)
array([[False,  True, False, False,  True, False,  True, False, False,
        False, False, False, False, False, False, False],
       [False,  True, False, False,  True, False,  True, False, False,
        False, False, False, False, False, False, False]], dtype=bool)

&gt;&gt;&gt; np.where(np.isin(large,small)) # tuple of arrays
(array([0, 0, 0, 1, 1, 1]), array([1, 4, 6, 1, 4, 6]))

# And generalizing, if you really want that as 2x2x3 array of indices:
idxs = array(np.where(np.isin(large,small)))
idxs.reshape( (2,) + small.shape )

array([[[0, 0, 0],
        [1, 1, 1]],

       [[1, 4, 6],
        [1, 4, 6]]])
</code></pre>
","202229","202229","2018-06-03 09:49:17","0","776","smci","2009-11-04 00:51:30","17377","5826","7432","373","50642883","50643192","2018-06-01 11:55:30","0","30","<p>I have a ""large"" numpy array like follows:</p>

<pre><code>from numpy import array
large = array([[-0.047391  , -0.10926778, -0.00899118,  0.07461428, -0.07667476,
         0.06961918,  0.09440736,  0.01648382, -0.04102225, -0.05038805,
        -0.00930337,  0.3667651 , -0.02803499,  0.02597451, -0.1218804 ,
         0.00561949],
       [-0.00253788, -0.08670117, -0.00466262,  0.07330351, -0.06403728,
         0.00301005,  0.12807456,  0.01198117, -0.04290793, -0.06138136,
        -0.01369276,  0.37094407, -0.03747804,  0.04444246, -0.01162705,
         0.00554793]])
</code></pre>

<p>And a ""small"" array that was subsetted from <code>large</code>. </p>

<pre><code>small = array([[-0.10926778, -0.07667476,  0.09440736],
       [-0.08670117, -0.06403728,  0.12807456]])
</code></pre>

<p>Without any other information, how could we identify the column indices in <code>large</code> from which the <code>small</code> array was generated?</p>

<p>In this case, the answer is 1, 4, 6 (starting at 0 as done in python).</p>

<p>What would be a generalizable way to determine this?</p>
","8364914","202229","2018-06-03 09:43:23","Identifying column indices that a subsetted np array came from in larger np array","<python><arrays><numpy><slice>","1","1","1092"
"50643226","2018-06-01 12:15:22","1","","<p>You can't assign in a list comprehension.</p>

<p>A <code>[...for...]</code> list comprehension isn't faster than a regular <code>for</code> loop, anyways. It's a <code>for</code> loop either way whether it's on one line or two. The two-line version has the advantage of being syntactically valid.</p>

<p>Stick with the original loop.</p>
","68587","68587","2018-06-01 12:21:54","2","343","John Kugelman supports Monica","2009-02-19 19:47:36","265299","16893","8847","4495","50642759","","2018-06-01 11:48:36","0","434","<pre><code>wordnum = [14, 1, 7, 0, 0, 11]
sentNum = [4, 2, 8, 6, 5, 8]
findtext = [u'I', u'our', u'it', u'The villa', u'It', u'the large main pool']
sentss2 = [
    ['When', 'planning', 'our', 'return', 'trip', 'to', 'Kauai', ',', 'husband', 'and', 'I', 'were', 'happy', 'to', 'read', 'that', 'all', 'the', 'renovations', 'to', 'the', 'Koloa', 'Landing', 'Resort', 'were', 'completed', '.'],
    ['We', 'then', 'went', 'ahead', 'and', 'booked', 'a', 'deluxe', 'studio', 'for', 'mid-October', '.'],
    ['Upon', 'our', 'check', 'in', 'after', 'a', 'long', 'flight', 'and', 'dark', 'rainy', 'drive', 'from', 'the', 'airport', ',', 'we', 'were', 'greeted', 'warmly', 'at', 'the', 'front', 'desk', '.'],
    ['While', 'we', 'are', 'platinum', 'status', ',', 'we', 'were', ""n't"", 'expecting', 'a', 'room', 'upgrade', ',', 'but', 'much', 'to', 'our', 'delight', ',', 'was', 'advised', 'by', 'the', 'clerk', 'that', 'we', 'received', 'one', '.'],
    ['However', ',', 'it', 'was', ""n't"", 'until', 'we', 'got', 'to', 'our', 'room', ',', 'or', 'should', 'I', 'say', 'villa', ',', 'how', 'nice', 'the', 'upgrade', 'was', '.'],
    ['It', 'was', 'a', 'one', 'bedroom', ',', '1', '1/2', 'bath', ',', 'almost', '1000', 'square', 'feet', 'corner', 'villa', 'with', '2', 'full', 'walls', 'of', 'windows', 'in', 'the', 'lovely', 'living', 'room', 'alone', '.'],
    ['The', 'villa', 'was', 'beautiful', '.'],
    ['Our', 'view', 'of', 'the', 'smaller', 'family', 'pool', 'and', 'soccer', 'field', 'was', 'so', 'nice', '.'],
    ['Really', 'enjoyed', 'being', 'in', 'this', 'area', 'as', 'it', ""'s"", 'quieter', 'than', 'the', 'large', 'main', 'pool', '.']
]
</code></pre>

<p>I have a for loop to assign values and replace existing values in sentss2 with the case lowered first value of <code>findtext</code>. I wrote a for loop to do this and it works perfectly:</p>

<pre><code>for aa,bb,cc in zip(sentNum,findtext,wordnum):
    sentss2[aa][cc]=findtext[0].lower()
</code></pre>

<p>Now I want to write it within a single inline statement to execute this time consuming for loop faster. If I write the inline loop like below:</p>

<pre><code>[(sentss2[aa][cc]=findtext[0],findtext[0].lower())[0] for aa,cc in zip(sentNum,wordnum)]
</code></pre>

<p>I get an invalid syntax error:</p>

<pre><code> File ""&lt;ipython-input-142-82f373e08c0a&gt;"", line 1
    [(sentss2[aa][cc]=findtext[0],findtext[0].lower())[0] for aa,cc in zip(sentNum,wordnum)]
                     ^
SyntaxError: invalid syntax
</code></pre>

<p>Any help how can I write this inline <code>for</code> loop?</p>
","4566277","68587","2018-06-01 12:21:35","SyntaxError: invalid syntax for Python inline for loop","<python><python-2.7><for-loop>","1","0","2562"
"50643303","2018-06-01 12:20:47","1","","<p>Indeed, the function is expecting a 1D tensor, and you've got a 2D tensor.</p>

<ul>
<li>Keras does have the <code>keras.backend.squeeze(x, axis=-1)</code> function.    </li>
<li>And you can also use <code>keras.backend.reshape(x, (-1,))</code></li>
</ul>

<p>If you need to go back to the old shape after the operation, you can both:</p>

<ul>
<li><code>keras.backend.expand_dims(x)</code>   </li>
<li><code>keras.backend.reshape(x,(-1,1))</code></li>
</ul>
","2097240","","","3","462","Daniel Möller","2013-02-21 21:35:32","48862","7561","2736","22","50642676","50643303","2018-06-01 11:43:29","0","560","<p>
I am implementing an OCR with Keras, Tensorflow backend.</p>

<p>I want to use <code>keras.backend.ctc_decode</code> implementation.</p>

<p>I have a model class :</p>

<pre class=""lang-python prettyprint-override""><code>import keras


def ctc_lambda_func(args):
    y_pred, y_true, input_x_width, input_y_width = args
    # the 2 is critical here since the first couple outputs of the RNN
    # tend to be garbage:
    # y_pred = y_pred[:, 2:, :]
    return keras.backend.ctc_batch_cost(y_true, y_pred, input_x_width, input_y_width)


class ModelOcropy(keras.Model):
    def __init__(self, alphabet: str):
        self.img_height = 48
        self.lstm_size = 100
        self.alphabet_size = len(alphabet)

        # check backend input shape (channel first/last)
        if keras.backend.image_data_format() == ""channels_first"":
            input_shape = (1, None, self.img_height)
        else:
            input_shape = (None, self.img_height, 1)

        # data input
        input_x = keras.layers.Input(input_shape, name='x')

        # training inputs
        input_y = keras.layers.Input((None,), name='y')
        input_x_widths = keras.layers.Input([1], name='x_widths')
        input_y_widths = keras.layers.Input([1], name='y_widths')

        # network
        flattened_input_x = keras.layers.Reshape((-1, self.img_height))(input_x)
        bidirectional_lstm = keras.layers.Bidirectional(
            keras.layers.LSTM(self.lstm_size, return_sequences=True, name='lstm'),
            name='bidirectional_lstm'
        )(flattened_input_x)
        dense = keras.layers.Dense(self.alphabet_size, activation='relu')(bidirectional_lstm)
        y_pred = keras.layers.Softmax(name='y_pred')(dense)

        # ctc loss
        ctc = keras.layers.Lambda(ctc_lambda_func, output_shape=[1], name='ctc')(
            [dense, input_y, input_x_widths, input_y_widths]
        )

        # init keras model
        super().__init__(inputs=[input_x, input_x_widths, input_y, input_y_widths], outputs=[y_pred, ctc])

        # ctc decoder
        top_k_decoded, _ = keras.backend.ctc_decode(y_pred, input_x_widths)
        self.decoder = keras.backend.function([input_x, input_x_widths], [top_k_decoded[0]])
        # decoded_sequences = self.decoder([test_input_data, test_input_lengths])
</code></pre>

<p>My use of <code>ctc_decode</code> comes from another post : <a href=""https://stackoverflow.com/questions/45601597/keras-using-lambda-layers-error-with-k-ctc-decode"">Keras using Lambda layers error with K.ctc_decode</a></p>

<p>I get an error :</p>

<p><code>ValueError: Shape must be rank 1 but is rank 2 for 'CTCGreedyDecoder' (op: 'CTCGreedyDecoder') with input shapes: [?,?,7], [?,1].</code></p>

<p>I guess I have to squeeze my <code>input_x_widths</code>, but Keras does not seem to have such function (it always outputs something like <code>(batch_size, 1)</code>)</p>
","2628726","","","Keras ctc_decode shape must be rank 1 but is rank 2","<python><tensorflow><keras>","2","0","2889"
"50643367","2018-06-01 12:24:14","1","","<p>Your code doesn't seem very clear. What are the <code>mean_X</code> and <code>mean_Y</code> variables ? </p>

<p>EDIT : Added variable declaration.</p>

<p>Anyhow, here's a simple suggestion :</p>

<pre><code>import numpy as np
def predict(X, Y, df):
    mean_X = np.mean(X)
    mean_Y = np.mean(Y)
    beta1 = sum([(X[i] - mean_X)*(Y[i] - mean_Y) for i in range(len(X))]) / sum([(X[i] - mean_X)**2 for i in range(len(X))])
    beta0 = mean_Y - beta1 * mean_X
    y_hat = [beta0 + beta1*X[i] for i in range(len(X))]
    df['prediction'] = y_hat
    return df
</code></pre>

<p>A cleverer way to proceed would be to use the <a href=""https://pandas.pydata.org/pandas-docs/version/0.21/generated/pandas.DataFrame.apply.html"" rel=""nofollow noreferrer"">apply()</a> function called on your DataFrame.</p>
","9787713","9787713","2018-06-01 12:38:44","0","802","Charles","2018-05-14 10:43:46","316","25","81","6","50643005","50643367","2018-06-01 12:02:41","1","149","<p>I wrote function to estimate parameters of simple linear regression. The function produces several outputs. Function inputs are two <strong>lists</strong>. Also, I have initial DataFrame df from where I derived two lists.</p>

<p>I want to add some outputs from function to the initial DataFrame as a new columns or either have new lists outside to function.</p>

<p>for example:</p>

<pre><code>def predict(X,Y):
     beta1 = sum([(X[i] - mean_X)*(Y[i] - mean_Y) for i in range(len(X))]) / sum([(X[i] - mean_X)**2 for i in range(len(X))])
     beta0 = mean_Y - beta1 * mean_X

     y_hat = [beta0 + beta1*X[i] for i in range(len(X))]

     return df.assign(prediction = y_hat)
</code></pre>

<p>Here, mean_X and mean_Y is sample average for list X and list Y, respectively.</p>

<p>Also I tried <strong>numpy.insert()</strong> to add y_hat into not initial DataFrame but into X which I converted into numpy array.</p>

<p>I have no success to achieve desired result so can someone help me?</p>
","6836950","6836950","2018-06-01 12:27:47","Add new column to Pandas Dataframe from functions' output","<python><pandas><numpy><linear-regression>","2","0","998"
"50643372","2018-06-01 12:24:18","0","","<p>you are question is make little confusions,you wrote dict but telling that you want list. so here I tried a solution with lists like you told :</p>

<pre><code>from collections import Counter
List = ['a','a','ab','ab','ab']
cnt = Counter(List)
main_list = []
for i in set(List):
    for j in range(1, cnt[i]+1):
        main_list.append([j, i])
print(main_list)
</code></pre>
","8393004","","","0","379","Vikas Damodar","2017-07-31 09:22:41","2264","383","137","99","50643209","","2018-06-01 12:14:07","-4","39","<p>I want to create a kind of dictionary list from exisiting list in the below format?</p>

<pre><code>List = {'a','a','ab',ab','ab'}
</code></pre>

<p>to</p>

<pre><code>{[1,'a'],[2,'a'],[1,'ab'],[2,'ab'],[3,'ab']}
</code></pre>

<p>Please let me know how to achieve this in python?</p>
","9881141","5079316","2018-06-01 12:20:19","Python List duplicate to specific value map creator","<python><list>","4","5","288"
"50643374","2018-06-01 12:24:19","0","","<p>I've got a similar problem, but I solved it with <code>set length 0.</code>
I share my code, I hope to help you.</p>

<pre><code>extrm_X460 = {
    'device_type': 'extreme',
    'ip':   ip,
    'username': 'username',
    'password': 'password',
    'port' : 22,          
}

try:
    # Connect
    print ""Trying to connect to: "" + ip
    net_connect = ConnectHandler(**extrm_X460)

    # Get info
    print ""Connected to: "" + ip
    net_connect.send_command('set length 0')
    output = net_connect.send_command('show mac port ge.*.*')
    print output

except (IOError, EOFError, SSHException, NetMikoTimeoutException, NetMikoAuthenticationException) as e:
    print e
    continue
</code></pre>
","8334684","","","0","701","alexfrancow","2017-07-19 23:28:04","28","13","43","0","47920905","","2017-12-21 08:30:43","1","1284","<p>I am working as a network engineer and I have tried to automate routine tasks with some python scripts. We are using cisco so I thought it would be nice to implement netmiko library.
Here is the part of the script which is used to connect to device and edit access-lists:</p>

<pre><code>def Connection (DevParameters, device, HostAddress):
    try:
        net_connect = ConnectHandler(**DevParameters)
    except:
        print device[0] + ': Authentication failed or device unavailable'
        return
    access_class = net_connect.send_command(""sh run | inc access-class"")
    if access_class == '':
        print device[0] + ': No access-class configured.' 
        net_connect.disconnect()
        return
    access_class = access_class.splitlines()[0].split(' ')[-2]
    acl_type = net_connect.send_command(""sh ip access-list "" + access_class).splitlines()[0].split(' ')[0].lower()
    net_connect.send_command('configure terminal')
    net_connect.send_command('ip access-list ' + acl_type + access_class)
    if acl_type == 'extended':
        net_connect.send_command('permit tcp host' + HostAddress + ' any eq 22')
    elif acl_type == 'standard':
        net_connect.send_command('permit ' + HostAddress )
    else:
        print device[0] + ': Unexpected ACL type. Connection closed.'
       net_connect.disconnect()
        return
    print device[0] + ': Configured.'
    net_connect.disconnect
    return
</code></pre>

<p>It works nicely from IDLE writing command line by line but when executing script it fails with:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\User\Desktop\MAT.py"", line 145, in &lt;module&gt;
    Connection (DevParameters, device, HostAddress)
  File ""C:\Users\User\Desktop\MAT.py"", line 90, in Connection
    net_connect.send_command('configure terminal')
  File ""C:\Program Files\Python27\lib\site-packages\netmiko\base_connection.py"", line 827, in send_command
    search_pattern))
IOError: Search pattern never detected in send_command_expect: HQ\_F3\_2960\_D\-5\#
</code></pre>

<p>I tried to implement sleep() after send_command() to no avail. What could be the issue?</p>
","9126093","9126093","2017-12-21 10:10:17","Netmiko library returns pattern never detected in send_command_expect","<python><paramiko><cisco>","1","1","2144"
"50643376","2018-06-01 12:24:40","0","","<p>This is one easy way: </p>

<pre><code>List = ['a','a','ab','ab','ab']

out = []
for i in range(len(List)):
    out.append([List[:i].count(List[i])+1, List[i]])

print(out)
# [[1, 'a'], [2, 'a'], [1, 'ab'], [2, 'ab'], [3, 'ab']]
</code></pre>

<p>Or a better way using <code>Counter</code>:</p>

<pre><code>from collections import Counter

List = ['a','a','ab','ab','ab']

def count_times(List):
    counter = Counter()
    for x in List:
        counter[x] += 1
        yield counter[x], x

print(list(count_times(List)))
# [(1, 'a'), (2, 'a'), (1, 'ab'), (2, 'ab'), (3, 'ab')]
</code></pre>
","8472377","8472377","2018-06-01 12:33:44","2","596","Austin","2017-08-16 11:54:23","18860","1780","141","2099","50643209","","2018-06-01 12:14:07","-4","39","<p>I want to create a kind of dictionary list from exisiting list in the below format?</p>

<pre><code>List = {'a','a','ab',ab','ab'}
</code></pre>

<p>to</p>

<pre><code>{[1,'a'],[2,'a'],[1,'ab'],[2,'ab'],[3,'ab']}
</code></pre>

<p>Please let me know how to achieve this in python?</p>
","9881141","5079316","2018-06-01 12:20:19","Python List duplicate to specific value map creator","<python><list>","4","5","288"
"50643383","2018-06-01 12:24:58","0","","<p>I have found a solution for the case <code>double *</code>. First of all, the  file <code>fib.pxd</code> is useless. 
Then we need a new <code>fib.pyx</code> file: </p>

<pre><code># distutils: language = c++
# distutils: sources = fib.cpp


import numpy as np 
cimport numpy as cnp 



cdef extern from ""fib.hpp"":
     double fib(int n)
     double add (double a, double b)
     double* mult(double a)



def make_mult(double a):
    cdef double[:] mv = &lt;double[:4]&gt; mult(a) # the 4 stands for
    return np.asarray(mv)   # the dimension of the array defined in fib.cpp 
</code></pre>

<p>In the case of a function which returns a nrows*ncols matrix; such as <code>double* make_mat(int nrows, int ncols)</code> , the second line of the  function <code>make_mult</code> has to be rewritten as:</p>

<pre><code>cdef double[:,:] mv=&lt;double[:nrows,:ncols]&gt; make_mat(nrows, ncols) 
</code></pre>

<p>Regrettably, If I have a  function <code>double** make_mat(int nrows, int ncols)</code> which always returns a matrix, then the previous code rises the error :</p>

<blockquote>
  <p>Pointer base type does not match cython.array base type</p>
</blockquote>
","9865833","9865833","2018-06-01 13:47:38","0","1168","Giovanni Paolinelli","2018-05-29 17:30:19","11","1","0","0","50610334","","2018-05-30 17:22:02","2","326","<p>I have a C++ function which returns a pointer <code>double**</code> - a high dimensional  matrix in particular - and I'd like to wrap it to some python code using Cython. How should I act? </p>

<p>Here an example  with a function with a <code>double*</code> pointer for sake of simplicity.</p>

<p>My C++ <code>fib.cpp</code> code:</p>

<pre><code>double add(double a, double b)
{
 return a+b;
}

double p[]= {1,2,3,4}; 

double* mult(double a)    
{
p[0]=p[0]*a;
p[1]=p[1]*a;
return p; 
}
</code></pre>

<p>Then there is the <code>fib.hpp</code> file:</p>

<pre><code>double add(double a,double b);
double* mult(double a);
</code></pre>

<p>Then the pxd file <code>fib.pxd</code>:</p>

<pre><code>cdef extern from ""fib.hpp"":
     double add(double a,double b);
     double* mult(double a);
</code></pre>

<p>In the end the pyx file</p>

<pre><code># distutils: language = c++
# distutils: sources = fib.cpp 

cimport fib 

def add(a,b):
    return fib.add(a,b)
def mult(a):           # dropping these lines
    return fib.mult(a) # the code works without the double* function
</code></pre>

<p>Everything  is compiled with the rather standard <code>setup.py</code> :</p>

<pre><code>from distutils.core import setup, Extension
from Cython.Build import cythonize

ext = Extension(""fib2cpp"",
            sources=[""fib.pyx"", ""fib.cpp""],
            language=""c++"")

setup(name=""fib"",
ext_modules=cythonize(ext))
</code></pre>

<p>When I compile the code:</p>

<pre><code>setup.py build_ext -if
</code></pre>

<blockquote>
  <p>Cannot convert '<code>double *</code>' to python object. </p>
</blockquote>

<p>When I try with a <code>double**</code> function I get the same error.
What should I do?</p>
","9865833","4850040","2018-05-30 17:30:28","Wrap a double** c++ function with cython","<python><c++><pointers>","1","0","1702"
"50643398","2018-06-01 12:25:54","2","","<p>Directory structure should be like this:</p>

<ol>
<li><code>../src/cython/class1.pyx</code></li>
<li><code>../src/cython/__init__.pxd</code></li>
<li><code>../src/cython/class1.pxd</code></li>
<li><code>../src/cython/class2.pyx</code></li>
</ol>

<p>In class2.pyx:</p>

<pre><code>from class1 cimport Class1
</code></pre>

<p>The setup.py should have:</p>

<pre><code>extensions = [Extension('my_package.cython.class1',
                        &lt;..&gt;
              ),
              Extension('my_package.cython.class2',
                        &lt;..&gt;,
              ),
]
</code></pre>

<p>The 'no module' error is because the directory names do not match the extension name, in addition to the directory needing to be defined as a package. </p>

<p><code>__init__.pxd</code> is the equivalent to <code>__init__.py</code> for <code>cimport</code>.</p>
","4165324","","","0","863","danny","2014-10-21 09:50:35","3669","253","172","52","50631966","50643398","2018-05-31 20:03:22","1","318","<p>I got 3 files in cython that represent 2 classes:</p>

<pre><code>1. ../src/cython/class1.pyx
2. ../src/cython/class1.pxd
3. ../src/cython/class2.pyx
</code></pre>

<p>I want to import a class defined in <code>class1</code> to <code>class2</code> to be able to use cython typing for a custom class.</p>

<p>Inside <code>class2</code> if I import <code>Class1</code> like this, I can't compile:</p>

<pre><code>from src.cython.class1 cimport Class1
</code></pre>

<p>Inside <code>class2</code> if I import <code>Class1</code> like below, I can compile but I get error <code>No module named 'src.cython.class1'</code> on execution:</p>

<pre><code>from class1 cimport Class1
</code></pre>

<p>I am running <code>setup.py</code> from <code>../</code></p>

<p>This question was marked as a possible duplicate of a very different issue. I was getting these errors only because I missed the <code>__init__.py</code> in the <code>cython</code> folder.</p>
","501054","501054","2018-06-01 15:32:44","Difficulty with cython paths to compile working version of pyx pxd files","<python><cython><setuptools>","1","6","952"
"50643399","2018-06-01 12:25:55","1","","<p>Assuming you want your input and output to be lists, and thus use the correct syntax <code>[...]</code>, then this can be achieved with <code>itertools.grouby</code> and <code>enumerate</code>. The kind of <em>dictionary-list</em> you want to obtain as output sounds like a list of tuples.</p>

<h3>Code</h3>

<pre><code>from itertools import groupby

def enumerate_groups(l):
    output = []
    for _, group in groupby(l):
        output.extend(enumerate(group, start=1))
    return output
</code></pre>

<h3>Example</h3>

<pre><code>l = ['a','a','ab', 'ab','ab', 'a']
print(enumerate_groups(l))
# prints: [(1, 'a'), (2, 'a'), (1, 'ab'), (2, 'ab'), (3, 'ab'), (1, 'a')]
</code></pre>
","5079316","5079316","2018-06-01 12:31:22","1","689","Olivier Melançon","2015-07-04 00:30:37","15837","1134","1990","511","50643209","","2018-06-01 12:14:07","-4","39","<p>I want to create a kind of dictionary list from exisiting list in the below format?</p>

<pre><code>List = {'a','a','ab',ab','ab'}
</code></pre>

<p>to</p>

<pre><code>{[1,'a'],[2,'a'],[1,'ab'],[2,'ab'],[3,'ab']}
</code></pre>

<p>Please let me know how to achieve this in python?</p>
","9881141","5079316","2018-06-01 12:20:19","Python List duplicate to specific value map creator","<python><list>","4","5","288"
"50643400","2018-06-01 12:26:02","0","","<p>first don not define a list with using ""{ }"". it means set not array. this code maybe help you:</p>

<pre><code>List = ['a','a','ab','ab','ab']

count = 1
current = List[0]
archive = [[count,current]]
for item in List:
    if current == item :
        count = count+1
        current = item 
        archive.append([count,item ])

    else:
        count = 0
        current = item 

print archive
</code></pre>
","9852050","","","0","415","Saeed Bolhasani","2018-05-26 18:16:38","430","87","43","0","50643209","","2018-06-01 12:14:07","-4","39","<p>I want to create a kind of dictionary list from exisiting list in the below format?</p>

<pre><code>List = {'a','a','ab',ab','ab'}
</code></pre>

<p>to</p>

<pre><code>{[1,'a'],[2,'a'],[1,'ab'],[2,'ab'],[3,'ab']}
</code></pre>

<p>Please let me know how to achieve this in python?</p>
","9881141","5079316","2018-06-01 12:20:19","Python List duplicate to specific value map creator","<python><list>","4","5","288"
"50643413","2018-06-01 12:26:44","1","","<p>Try this, let me know if this solves your issue,</p>

<pre><code>pip3 install -trusted-host=pypi.org -trusted-host=python.pypi.org -trusted-host=files.pythonhosted.org pyinput
</code></pre>
","4237254","","","3","193","BcK","2014-11-10 21:01:26","1519","137","63","23","50643166","","2018-06-01 12:11:11","0","473","<p>When I install packages with pip it always gives the same error, even if the package doesn't exist. I cant make any sense of the error and doesnt give me a error code. The package I am installing is called pyinput.</p>

<pre><code>$ pip install pyinput
Collecting pyinput
From cffi callback &lt;function _verify_callback at 0x7f79def81758&gt;:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/dist-packages/OpenSSL/SSL.py"", line 315, in wrapper
    _lib.X509_up_ref(x509)
AttributeError: 'module' object has no attribute 'X509_up_ref'
  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLError(""bad handshake: Error([('SSL routines', 'ssl3_get_server_certificate', 'certificate verify failed')],)"",),)': /simple/pyinput/
From cffi callback &lt;function _verify_callback at 0x7f79d92bd848&gt;:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/dist-packages/OpenSSL/SSL.py"", line 315, in wrapper
    _lib.X509_up_ref(x509)
AttributeError: 'module' object has no attribute 'X509_up_ref'
  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLError(""bad handshake: Error([('SSL routines', 'ssl3_get_server_certificate', 'certificate verify failed')],)"",),)': /simple/pyinput/
From cffi callback &lt;function _verify_callback at 0x7f79d92bd938&gt;:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/dist-packages/OpenSSL/SSL.py"", line 315, in wrapper
    _lib.X509_up_ref(x509)
AttributeError: 'module' object has no attribute 'X509_up_ref'
  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLError(""bad handshake: Error([('SSL routines', 'ssl3_get_server_certificate', 'certificate verify failed')],)"",),)': /simple/pyinput/
From cffi callback &lt;function _verify_callback at 0x7f79d92bd500&gt;:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/dist-packages/OpenSSL/SSL.py"", line 315, in wrapper
    _lib.X509_up_ref(x509)
AttributeError: 'module' object has no attribute 'X509_up_ref'
^COperation cancelled by user
From cffi callback &lt;function _verify_callback at 0x7f79d92bde60&gt;:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/dist-packages/OpenSSL/SSL.py"", line 315, in wrapper
    _lib.X509_up_ref(x509)
AttributeError: 'module' object has no attribute 'X509_up_ref'
Could not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError(SSLError(""bad handshake: Error([('SSL routines', 'ssl3_get_server_certificate', 'certificate verify failed')],)"",),)) - skipping
</code></pre>

<p>If anyone could help me with this i would appreciate it.
Thank you.</p>
","8358438","","","Pip gives same error when installing packages","<python><pip>","1","2","2880"
"50643489","2018-06-01 12:30:23","1","","<p>You need to first convert to dataframe :</p>

<pre><code>X = pd.concat([X,pd.DataFrame([[data['first occurrence of \'AB\''],data['similarity to \'AB\'']]],columns=['first occurrence of \'AB\'','similarity to \'AB\''])], ignore_index=True)
y = pd.concat([y,pd.DataFrame([data['Class']], columns=['Class'])], ignore_index=True)
</code></pre>

<p>EDIT : add ignore_index=True</p>
","3824723","3824723","2018-06-01 13:44:32","7","380","CoMartel","2014-07-10 09:13:03","2041","236","570","12","50643118","50643489","2018-06-01 12:08:57","0","2048","<p>This seems like it should be so much simpler yet here I am.</p>

<p>I'm trying to add a row to a data frame (2 data frames to be exact) from another data frame, but I get the following error:</p>

<pre><code>TypeError: cannot concatenate object of type ""&lt;class 'numpy.float64'&gt;""; only pd.Series, pd.DataFrame, and pd.Panel (deprecated) objs are valid
</code></pre>

<p>My code</p>

<pre><code>for i in range(0,len(k_means_labels_unique)):
    X = pd.DataFrame(columns=['first occurrence of \'AB\'','similarity to \'AB\''])
    y = pd.DataFrame(columns=['Class'])
    for row in result.iterrows():
        data=row[1]
        if data['cluster ID'] == i:
            X = pd.concat([X,data['first occurrence of \'AB\''],data['similarity to \'AB\'']])
            y = pd.concat([y,data['Class']])
</code></pre>

<p>Do I have to transform <code>data['first occurrence of \'AB\''],data['similarity to \'AB\'']</code> into another data frame? This seems horribly inefficient</p>

<p>EDIT: I tried <code>y = pd.concat([y,pd.Series(data['Class'])])</code> but that appended the data as a new column, example for <code>y</code>:</p>

<p><a href=""https://i.stack.imgur.com/y6O4p.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/y6O4p.png"" alt=""columns""></a></p>
","6187682","6187682","2018-06-01 12:24:51","Concatenating a row in a pandas Dataframe","<python><pandas><dataframe>","1","4","1278"
"50643558","2018-06-01 12:33:55","0","","<p>Looking through the _E object constructor I found     <code>__add__</code> and     <code>__mul__</code> methods.</p>

<pre><code>lambda i: (i.__add__(-mi)).__mul__(ratio)# doesnt work but 
lambda i: (i * ratio).__add__(-mi*ratio)# works
</code></pre>

<p>The first one returns </p>

<blockquote>
  <p>ValueError: illegal expression.</p>
</blockquote>

<p>I didn't want to struggle with that so I tried different way to implement my operation and finally the second one did work.</p>
","9876780","","","0","486","Edern Le Hyaric","2018-05-31 15:16:44","1","13","0","0","50627941","","2018-05-31 15:32:49","0","44","<p>I have a database of grayscale 16bits tif images, whose pixels range approx from 768 to 1280 (very dark). I want to strech the data from the extrema value and save the result in a 8bits jpg.</p>

<pre><code>from PIL import Image
image = Image.open(inp_16bits)
image.mode = 'I'
mi, ma = image.getextrema()
ratio = 256.0 / (ma - mi)
mapping = lambda i: (i-mi) * ratio
image.point(mapping).convert('L').save(out_8bits)
</code></pre>

<p>My mapping function is working, but doesn't work as a parameter for point, my code return </p>

<blockquote>
  <p>""TypeError: unsupported operand type(s) for -: '_E' and 'int'"".</p>
</blockquote>

<p>How to properly implement my function as to make it work with Image.point ? or is there an easy way to create a lookup table from my function mapping ?</p>
","9876780","3893262","2018-05-31 15:44:19","python PIL manually convert 16bits TIF to 8bits JPG","<python><image><python-imaging-library>","1","0","793"
"50643594","2018-06-01 12:36:19","0","","<p>You could give your <code>Worker</code> objects methods that call the appropriate <code>Broker</code> methods with their <code>id</code>s</p>

<pre><code>class Worker
    def __init__(self, broker, id):
        self.broker = broker
        self.id = id

    def closeAllPositions(self):
        return self.broker.closeAllPositions(self.id)
</code></pre>

<p>Then instead of <code>self.broker.closeAllPositions(""id"")</code>, you would just invoke  <code>self.closeAllPositions()</code> instead</p>
","6779307","","","0","501","Patrick Haugh","2016-08-31 14:38:46","36209","4654","2715","1256","50643324","","2018-06-01 12:21:48","0","58","<p>I'm working on a project and I'm stuck with a problem where the instance of one a class is passed to many workers objects. </p>

<pre><code>broker = Broker(...)

worker1 = Worker(broker, ""001"")
worker2 = Worker(broker, ""002"")
worker3 = Worker(broker, ""003"")
</code></pre>

<p>All these workers are now calling functions from the Broker object they got passed as an argument. As the workers are all managing different Broker-accounts which are identified by an ID, the <code>id</code> arguments (""001"", ""002"", ""003"") need to be passed from the workers to the broker on each function call so that they can identify themselves and the Broker knows onto which Account he has to sent the instructions.</p>

<p>An action that resolves out of the internal algorithmic logic could be to close all positions for worker1:</p>

<pre><code>worker1.closeAllPositions()
</code></pre>

<p>The functional implementation for the Worker-Class is done in such a way that the instruction is passed to the Broker-Class. But as the Broker does need to know the ID of the worker currently calling it, its <code>id</code> is passed as an argument:</p>

<pre><code>class Worker:

    def __init__(self, broker, id):
         self.id = id
         self.broker = broker

    def closeAllPositions(self):
        self.broker.closeAllPositions(self.id)

    def getSummary(self):
        self.broker.getSummary(self.id)

    def getBalance(self):
        self.broker.getBalance(self.id)
</code></pre>

<p>All Broker interfaces are now equipped with the <code>id</code> parameter, so that any function call from a worker to the Broker is identified with an account. But as the Broker functions have sub-dependencies (id passed to first who no has to pass the id to any further broker function in the chain) the <code>id</code>-problem gets out of control and ID's are passed everywhere quickly which looks very unclean to me. </p>

<pre><code>class Broker:

   def __init__(self, ...):
        ...

    def getSummary(self, id):
        self.setCallerID(id)                    &lt;-!!! Ugly
        ....

    def getBalance(self, id, returnType):
        self.setCallerID(id)                    &lt;-!!! Ugly
        ....
</code></pre>

<p>Instead of passing this information on any function call I'm looking for an <em>under the hood</em> method to resolve the callers ID. Something like a hook being invoked to set the caller ID, that runs whenever a Broker method is called. </p>

<p><strong>The order in which Workers call Broker functions and which Worker functions is not determinable and the order in which Workers want to interact with the single broker is completely random.</strong></p>

<p><strong>-> Therefore any call onto the Worker-Broker-interface needs some kind of identifier.</strong></p>

<p>Is there an elegant way in Python how an object can identify itself towards an other object without passing an ID parameter with any function call?</p>
","1280289","1280289","2018-06-02 08:55:42","Python object identification for many to one relationships","<python>","1","3","2937"
"50643635","2018-06-01 12:38:39","5","","<p>Well, you can create your own function to check it:</p>

<pre><code>def isOneToOne(df, col1, col2):
    first = df.groupby(col1)[col2].count().max()
    second = df.groupby(col2)[col1].count().max()
    return first + second == 2

isOneToOne(df, 'A', 'B')
#True
isOneToOne(df, 'A', 'C')
#False
isOneToOne(df, 'B', 'C')
#False
</code></pre>

<p>In case you data is more like this:</p>

<pre><code>df = pd.DataFrame({'A': [0, 1, 2, 0],
                   'C': [""'apple'"", ""'banana'"", ""'apple'"", ""'apple'""],
                   'B': [""'a'"", ""'b'"", ""'c'"", ""'a'""]})
df
#   A    B         C
#0  0  'a'   'apple'
#1  1  'b'  'banana'
#2  2  'c'   'apple'
#3  0  'a'   'apple'
</code></pre>

<p>Then you can use:</p>

<pre><code>def isOneToOne(df, col1, col2):
    first = df.drop_duplicates([col1, col2]).groupby(col1)[col2].count().max()
    second = df.drop_duplicates([col1, col2]).groupby(col2)[col1].count().max()
    return first + second == 2
</code></pre>
","5811078","5811078","2018-06-01 12:44:41","1","959","zipa","2016-01-19 14:52:58","19592","990","923","278","50643386","50643635","2018-06-01 12:25:15","2","1377","<p>Working with data in Python 3+ with pandas. It seems like there should be an easy way to check if two columns have a one-to-one relationship (regardless of column type), but I'm struggling to think of the best way to do this.</p>

<p>Example of expected output:</p>

<pre><code>A    B     C
0    'a'   'apple'
1    'b'   'banana'
2    'c'   'apple'
</code></pre>

<p>A &amp; B are one-to-one? TRUE</p>

<p>A &amp; C are one-to-one? FALSE</p>

<p>B &amp; C are one-to-one? FALSE</p>
","1895076","","","Easy Way to See if Two Columns are One-to-One in Pandas","<python><pandas><one-to-one>","4","3","485"
"50643647","2018-06-01 12:39:16","0","","<p>If you're question is about <code>%tags</code> look at Geany's manual at <a href=""https://www.geany.org/manual/current/index.html#substitutions-in-commands-and-working-directories"" rel=""nofollow noreferrer"">Substitutions in commands and working directories</a> section:</p>

<blockquote>
  <p>The first occurrence of each of the following character sequences in
  each of the command and working directory fields is substituted by the
  items specified below before the command is run.</p>

<pre><code>%d - substituted by the absolute path to the directory of the current file.
%e - substituted by the name of the current file without the extension or path.
%f - substituted by the name of the current file without the path.
%p - if a project is open, substituted by the base path from the project.
%l - substituted by the line number at the current cursor position.
</code></pre>
  
  <p><em>Note</em>: If the basepath set in the project preferences is not an absolute path, then it is taken as relative to the directory of the
  project file. This allows a project file stored in the source tree to
  specify all commands and working directories relative to the tree
  itself, so that the whole tree including the project file, can be
  moved and even checked into and out of version control without having
  to re-configure> the build menu.</p>
</blockquote>
","6709630","","","0","1365","freezed","2016-08-12 13:37:14","570","90","361","5","49788801","","2018-04-12 05:41:29","0","1293","<p>I've been using PyCharm for a month but decided to give geany a try.</p>

<p>I've spend hours looking at all the installation and setup tutorials on the web and youtube and even in my book it tells you how to set it up. They all say the same thing and as far as I know, I've done exactly what they said. (I found a question on here by someone with the exact same problem but it was never answered.)</p>

<p>go to Build>Set Build Commands, in the box next to Execute, put in the path to where python is located.</p>

<p>Here is my path</p>

<pre><code>C:\Users\{username}\AppData\Local\Programs\Python\Python36-32\python.exe
</code></pre>

<p>I don't know why my python installed in such an unusual place. Everyone else's look like this: <code>C:\Python35\python</code>. Is that the problem? When I go into <code>C:\Users\{username}</code>, the <code>AppData</code> folder is not visible. Is this for security? Is Geany not able to access those folders cause they are invisible?</p>

<p>I'm pretty sure you don't need the <code>.exe</code> so I deleted it.</p>

<pre><code>C:\Users\{username}\AppData\Local\Programs\Python\Python36-32\python ""%f""
</code></pre>

<p>When I hit execute, I can see the command window appear for a split second and then disappear, probably because it tries, but can't find python so closes.</p>

<p>I don't know what the <code>%f</code> is but everyone has it anyways.
Before I entered all that into the text box, by default in that text box, there was <code>.\%e</code> I don't know what that is all about.</p>
","9410831","6709630","2018-06-01 11:21:47","Geany do not find path to python","<python><windows><geany>","1","5","1543"
"50643652","2018-06-01 12:39:38","1","","<p>It's easy to <a href=""https://stackoverflow.com/questions/1074212/how-can-i-see-the-raw-sql-queries-django-is-running"">check the queries that Django is running</a> for yourself.</p>

<p>When I tried it, it appeared that <code>obj.things.exists()</code> did not cause any additional queries when <code>things</code> was prefetched.</p>
","113962","113962","2018-06-01 14:27:08","2","338","Alasdair","2009-05-28 20:26:55","203443","9896","5891","530","50642806","50643652","2018-06-01 11:51:14","3","407","<p>I am using <code>prefetch_related</code> when querying a model that have several m2m relationships:</p>

<pre><code>qs = context.mymodel_set.prefetch_related('things1', 'things2', 'things3')
</code></pre>

<p>So that when I do this there is no need to perform an additional query to get <code>things1</code>, they should have been fetched already:</p>

<pre><code>r = list(qs)
r[0].things1.all()
</code></pre>

<p>But what if I do <code>r[0].things1.exists()</code>? Will this generate a new query? Or will it use the prefetched information? If it generates a new query, does that mean that going for <code>r[0].things1.all()</code> for the purposes of existence checking is more efficient?</p>

<p>PS: cached information being in desync with the database does not worry me for this particular question.</p>
","3120489","","","Django prefetch related and exists","<python><django><django-1.8>","1","0","811"
"50643671","2018-06-01 12:40:45","-1","","<p>There been a cookies issue try it</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>from selenium import webdriver
import time
driver = webdriver.Chrome(executable_path = ""PATH OF CHROMEDRIVER"")
driver.maximize_window()
driver.get(""http://https://www.amazon.in"")
driver.get(""https://www.amazon.in/dp/B010RWAIAC/ref=twister_B07D5KL6TC?_encoding=UTF8&amp;th=1"")
time.sleep(5)
driver.find_element_by_id('color_name_1_price').click()
time.sleep(3)
driver.find_element_by_id('add-to-cart-button').click()</code></pre>
</div>
</div>
</p>
","9631120","","","0","696","bhupathi turaga","2018-04-11 14:15:17","163","130","19","2","50643610","50643671","2018-06-01 12:37:12","-3","36","<p>I am trying to automatically add this blue Jbl speaker in the cart so there are the steps first go to <a href=""https://www.amazon.in/dp/B010RWAIAC/ref=twister_B07D5KL6TC?_encoding=UTF8&amp;th=1"" rel=""nofollow noreferrer"">link of the Jbl speaker</a> and click on click on blue jbl speaker and click on add to cart button</p>

<p>But I can't reach to the Url saying error This site cant be reached</p>
","9880994","2386774","2018-06-01 12:58:40","Find and click on a element using selenium,python","<python><selenium-webdriver>","1","3","403"
"50643694","2018-06-01 12:42:09","0","","<p>You can modify the DNN placeholders to take the production input placeholder by default. Thus, you can still feed them custom values anytime you want.</p>

<pre><code>session = tf.Session()
production_input = tf.placeholder(tf.float32, [50])

# Own placeholders
dnn_1_input = tf.placeholder_with_default(production_input, [50])
dnn_2_input = tf.placeholder_with_default(production_input, [50])

# Production environment call
result = session.run([dnn_1_input, dnn_2_input],
                     feed_dict={production_input: np.arange(50)})
</code></pre>
","2628369","","","0","557","Kilian Batzner","2013-07-28 21:23:34","3025","219","149","6","50641374","","2018-06-01 10:25:29","0","360","<p>The following situation:
There is a production server (C++) running tensorflow. The code there expects to have exactly one input <code>Placeholder</code>, that gets fed similar to <code>feed_dict:{'input':...}</code>. I can't change the server code. I can only provide a new graph, that must have a <code>Placeholder</code> called <code>input</code>.</p>

<p>Using tensorflow, I have some code that creates two identical neural networks with different scope:</p>

<pre><code>dnn1 = DNN(scope='dnn1')
dnn2 = DNN(scope='dnn2')
</code></pre>

<p>Both DNNs have an input <code>Placeholder</code>: 
<code>&lt;tf.Tensor 'dnn1/input:0' shape=(50) dtype=float32&gt;</code>
and
<code>&lt;tf.Tensor 'dnn2/input:0' shape=(50) dtype=float32&gt;</code></p>

<p>Both networks should get the same input.
How can I make the tensor <code>input</code> flow to both, <code>dnn1/input</code> and <code>dnn2/input</code> within the graph? I also can't change the <code>DNN</code> class, so in the end my graph will have <code>dnn1/input</code>,<code>dnn2/input</code> and <code>input</code> as placeholder, but only <code>input</code> will be supplied by the production code.</p>
","3110740","3110740","2018-06-01 13:11:02","Feed Placeholder with another Placeholder using tensorflow","<python><tensorflow>","2","6","1162"
"50643717","2018-06-01 12:43:14","0","","<p>Your question would have been clearer if you had shown the actual JSON response.</p>

<p>However, it is clear from what you have posted that <code>id</code> and <code>name</code> are indeed not top-level keys, but keys inside nested dictionaries inside a list assigned to the <code>list</code> key. So you should get them from there:</p>

<pre><code>for item in jsonResponse['list']:
    print(item['id'], item['name'])
</code></pre>
","104349","","","2","437","Daniel Roseman","2009-05-10 12:36:13","489411","52610","12851","10717","50643654","50643717","2018-06-01 12:39:43","0","41","<p>I am trying to make some (JSON) API calls to our Wi-Fi controller and obtain some info. When I store the JSON response into a <code>dict</code> somehow it only see's a few keys, namely: </p>

<pre><code>   dict_keys(['totalCount', 'hasMore', 'firstIndex', 'list'])
</code></pre>

<p>and items:</p>

<pre><code>   dict_items([('totalCount', 32), ('hasMore', False), ('firstIndex', 0), 
   ('list', [{'id': 'ehgfhafgf', 'name': 'fasdfsd 
   xxxx'}, {'id': 'efasfsfas', 
   'name': 'zxcva'}])])
</code></pre>

<p>I removed a lot of items so It would make some sense otherwise it would be too much text.</p>

<p>So as you can see the <code>dict</code> recognizes the wrong variables as keys. Because as keys I need <code>id</code> and <code>name</code>. Is there a way to manually assign <code>dict</code> keys or a trick to simulate this?</p>

<p>My piece of code:</p>

<pre><code>#Method that retrieves all zones
def getZones():
    r = requests.get(""url.."", verify=False, cookies=cookie)
    print(type(r.json()))
    jsonResponse = r.json()
    print(""items: \n"") 
    print(jsonResponse.items())
    print(""\nkeys: \n"")
    print(jsonResponse.keys())
    print(jsonResponse.get('id'))
    return r
</code></pre>

<p>doing a lot of prints for debugging reasons.</p>
","4269350","4089949","2018-06-01 16:54:06","Parsing JSON object in python","<python><json><dictionary>","1","1","1269"
"50643746","2018-06-01 12:44:44","2","","<p>As far as I understood your question, you want to use your function in your existing/new column. If that is case, here is one way to do it. If not, then Let me know, I will remove the answer. Thanks</p>

<pre><code>import pandas as pd

def Somefunction(x, y):
  a = 2 *x
  b = 3 * y
  return pd.Series([a, b], index= ['YourColumn1', 'YourColumn2'])





df = pd.read_csv('YourFile')

df = df.join(df.apply(lambda x: 
  Somefunction(x['ColumnYouWantToApplyFunctionReturnValue a'], 
  x['ColumnYouWantToApplyFunctionReturnValue B']), axis=1))
</code></pre>
","3280146","3280146","2018-06-01 12:58:49","4","558","user3280146","2014-02-06 15:06:09","776","131","908","314","50643005","50643367","2018-06-01 12:02:41","1","149","<p>I wrote function to estimate parameters of simple linear regression. The function produces several outputs. Function inputs are two <strong>lists</strong>. Also, I have initial DataFrame df from where I derived two lists.</p>

<p>I want to add some outputs from function to the initial DataFrame as a new columns or either have new lists outside to function.</p>

<p>for example:</p>

<pre><code>def predict(X,Y):
     beta1 = sum([(X[i] - mean_X)*(Y[i] - mean_Y) for i in range(len(X))]) / sum([(X[i] - mean_X)**2 for i in range(len(X))])
     beta0 = mean_Y - beta1 * mean_X

     y_hat = [beta0 + beta1*X[i] for i in range(len(X))]

     return df.assign(prediction = y_hat)
</code></pre>

<p>Here, mean_X and mean_Y is sample average for list X and list Y, respectively.</p>

<p>Also I tried <strong>numpy.insert()</strong> to add y_hat into not initial DataFrame but into X which I converted into numpy array.</p>

<p>I have no success to achieve desired result so can someone help me?</p>
","6836950","6836950","2018-06-01 12:27:47","Add new column to Pandas Dataframe from functions' output","<python><pandas><numpy><linear-regression>","2","0","998"
"50643759","2018-06-01 12:45:26","1","","<p>You may use <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binned_statistic.html"" rel=""nofollow noreferrer""><code>scipy.stats.binned_statistic</code></a> to get the mean of the data in each bin. The bins would best be created via <code>numpy.logspace</code>. You may then plot those means e.g. as horiziontal lines spanning the bin width or as scatter at the mean position.</p>

<pre><code>import numpy as np; np.random.seed(42)
from scipy.stats import binned_statistic
import matplotlib.pyplot as plt

x = np.logspace(0,5,300)
y = np.logspace(0,5,300)+np.random.rand(300)*1.e3


fig, ax = plt.subplots()
ax.scatter(x,y, s=9)

s, edges, _ = binned_statistic(x,y, statistic='mean', bins=np.logspace(0,5,6))

ys = np.repeat(s,2)
xs = np.repeat(edges,2)[1:-1]
ax.hlines(s,edges[:-1],edges[1:], color=""crimson"", )

for e in edges:
    ax.axvline(e, color=""grey"", linestyle=""--"")

ax.scatter(edges[:-1]+np.diff(edges)/2, s, c=""limegreen"", zorder=3)

ax.set_xscale(""log"")
ax.set_yscale(""log"")
plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/um85r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/um85r.png"" alt=""enter image description here""></a></p>
","4124317","4124317","2018-06-01 15:45:44","2","1209","ImportanceOfBeingErnest","2014-10-09 07:50:43","173272","28712","2299","3162","50640339","","2018-06-01 09:32:16","-1","594","<p><a href=""https://i.stack.imgur.com/tmh2b.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tmh2b.png"" alt=""""></a></p>

<p>I have two arrays of corresponding data (x and y) that I plot as above on a log-log plot. The data is currently too granular and I would like to bin them to get a smoother relationship. Could I get some guidance on how I can bin along the x-axis, in <strong>exponential</strong> bin sizes, so that it appears linear on the log-log scale?</p>

<p>For example, if the first bin is of range x = 10^0 to 10^1, I want to collect all y-values with corresponding x in that range and average them into one value for that bin. I don't think np.hist or plt.hist quite does the trick, since they do binning by counting occurrences.</p>

<p>Edit: For context, if it helps, the above plot is an assortativity plot that plots the in vs out degree of a certain network. </p>
","9246732","","","How to bin a 2D data along the x-axis with Python","<python><numpy><matplotlib><histogram><binning>","2","0","902"
"50643775","2018-06-01 12:45:57","2","","<p>The problem comes in at that line.</p>

<pre><code>baseScore = baseScore * exp(-8 * x * x)
</code></pre>

<p>Since <code>x</code> only takes values in days, it will always be an integer. Now if <code>x == 0</code>, then you get <code>exp(-8 * x * x) == 1</code>, but as soon as <code>x == 1</code>, then it gets very close to <code>0</code>. Bottom line: <em>your function is not continuous</em>.</p>

<p>What you want is to gradually decrease the <em>hotness</em> of a post. This can be achieved by letting <code>x</code> take values between <code>0</code> and <code>1</code>. One way would be to take your time delta in minutes and thus allow for fractional days.</p>

<pre><code>timeDiff = (now - self.post.date).minutes / 1440
</code></pre>

<p>Then posts would stay hot for a few hours.</p>
","5079316","5079316","2018-06-01 14:04:31","0","799","Olivier Melançon","2015-07-04 00:30:37","15837","1134","1990","511","50643551","","2018-06-01 12:33:37","4","88","<p>I'm trying to replicate <a href=""https://stackoverflow.com/questions/11653545/hot-content-algorithm-score-with-time-decay/11654547"">reddit's hot algortithm</a> for sorting my posts. Here's my function:</p>

<pre><code>def hot(self):
    s = self.upvotes
    baseScore = log(max(s, 1))
    now = datetime.now()

    timeDiff = (now - self.post.date).days
    if (timeDiff &gt; 1):
        x = timeDiff - 1
        baseScore = baseScore * exp(-8 * x * x)
    print('Final:', baseScore) #always prints 0
    return baseScore
</code></pre>

<p>basically, <code>exp(-8 * x * x)</code> always makes the number 0. So i'm curious how I'm supposed to make this algorithm work.</p>

<p>Any idea?</p>
","6733153","","","""Hot"" algorithm always returns 0","<python><django><algorithm><sorting>","2","5","693"
"50643827","2018-06-01 12:49:37","2","","<p>You can use <code>random.choices</code> for this from Python 3.6, which includes a <code>weights</code> parameter:</p>

<pre><code>&gt;&gt;&gt; from random import choices
&gt;&gt;&gt; prob = [0.1, 0.3, 0.4, 0.2]
&gt;&gt;&gt; choices(range(len(prob)), weights=prob)
[2]
&gt;&gt;&gt; choices(range(len(prob)), weights=prob)
[3]
&gt;&gt;&gt; choices(range(len(prob)), weights=prob, k=4)
[1, 2, 2, 2]
</code></pre>
","6260170","","","0","414","Chris_Rands","2016-04-27 07:30:21","19982","2918","1860","2071","50643760","50643827","2018-06-01 12:45:32","1","48","<p>I have a vector of probabilities (which of course sum 1):</p>

<pre><code>prob = [0.1, 0.3, 0.4, 0.2]
</code></pre>

<p>Now I need to generate a random index for this vector (a number between 0 and 3 both included) but I want that the probability of each index is given by prob</p>

<pre><code>0 will be generated with prob 0.1
1 will be generated with prob 0.3
2 will be generated with prob 0.4
3 will be generated with prob 0.2
</code></pre>

<p>I know that I can do this by calculating the cumsum</p>

<pre><code>cumsum = [0.1, 0.4, 0.8, 1.0]
</code></pre>

<p>Then generating a random number between 0 and 1:</p>

<pre><code>rand_num = np.random.random()
</code></pre>

<p>And finally use <code>np.digitize</code> to check in which bin my random number falls.</p>

<pre><code>idx = np.digitize([rand_num], cumsum)
</code></pre>

<p>This works and I'm happy with this, digitize even accepts a list of numbers and classifies them into the bins, so I can create my own function to generate indexes given a probability distribution.</p>

<p>My question is: This is a common problem, so doesn't a function already exist that does this? (And that will be more efficient than doing it myself)</p>

<p>Thanks</p>
","2261062","","","Random index with custom probability distribution","<python><numpy><random><distribution>","1","1","1212"
"50643844","2018-06-01 12:50:50","0","","<p>Stripe does not support JSON payloads for the parameters. Instead, they require <code>application/x-www-form-urlencoded</code>.</p>

<p>At the moment, you are sending <code>metadata</code> as a hash and you are not encoding it properly so Stripe is rejecting it.</p>

<p>The best solution here is to avoid doing this yourself and instead rely on Stripe's official Python library that you can find here: <a href=""https://github.com/stripe/stripe-python"" rel=""nofollow noreferrer"">https://github.com/stripe/stripe-python</a></p>
","1606729","","","3","530","koopajah","2012-08-17 11:14:56","15693","1641","1809","58","50641691","","2018-06-01 10:44:08","3","265","<p>I am doing stripe payment integration using python and use the following data: </p>

<pre><code>import requests
import json
pos = requests.post
url = ""https://api.stripe.com/v1/sources""
headers = {'AUTHORIZATION': 'Bearer sk_test_NXht3wZpuYWRIWpMDDqT3RG2'}
data = {
    'type': 'alipay',
    'owner[email]': 'abc@xyz.com',
    'redirect[return_url]': 'https://www.google.com',
    'amount': '500',
    'currency': 'USD',
    'metadata': {
        'data': 'data'
    }
}
pos(url, data=data, headers=headers).text
json.loads(pos(url, data=data, headers=headers).text)
</code></pre>

<p>When give the metadata it gives error <strong>'{\n  ""error"": {\n ""message"": ""Invalid hash"",\n    ""param"": ""metadata"",\n    ""type"": ""invalid_request_error""\n  }\n}\n'</strong>
but according to stripe documentation metadata can be used( <a href=""https://stripe.com/docs/api/curl#create_source-metadata"" rel=""nofollow noreferrer"">https://stripe.com/docs/api/curl#create_source-metadata</a>)</p>

<p>Can anyone tell the solution why it gives that error. </p>
","8211382","4442714","2018-06-03 20:32:22","Metadeta error on stripe transaction using python","<python><python-3.x><stripe-payments><stripe-connect>","2","0","1042"
"50643848","2018-06-01 12:51:04","0","","<p>Two things I see from looking at your network:</p>

<ol>
<li>Sigmoid activation in the hidden layers is usually a bad choice. The sigmoid function saturates for large (positive or negative) inputs, resulting in the gradient becoming smaller and smaller as it is backpropagated through the networks. This is commonly referred to as the ""vanishing gradient"" problem. It could be that the gradient for variables near the output is ""healthy"" and thus the upper layers are learning, however if the lower layers don't receive any gradient they will simply keep returning random values that the higher layers can't work with. You could try replacing the sigmoid activations with e.g. <code>tf.nn.relu</code>. Sigmoid in the output layer is okay (and kinda necessary if you want your outputs to be 0/1), however consider using cross entropy instead of squared error as a loss function instead.</li>
<li>Your weight initialization likely results in excessively large weights. Standard deviation of 1.0 is way too high. This can lead to numerical issues as well as saturating the activations even more (since due to the large weights you can expect to have large activation values from the start). Try something like an std of 0.1, and consider using <code>truncated_normal</code> instead to prevent outliers (or use a uniform random initalization).</li>
</ol>

<p>It's difficult to say whether this will fix your issues, however I believe both of these are things you should definitely change about your network as it is right now.</p>
","9393102","","","1","1530","xdurch0","2018-02-21 20:26:51","4479","386","399","1167","50641866","52658859","2018-06-01 10:53:48","3","461","<p>I am training a classifier that takes a RGB input (so three 0 to 255 values) and returns whether black or white (0 or 1) font would fit best with that colour. After training, my classifier always returns 0.5 (or there about) and never gets any more accurate than that.</p>

<p>The code is below:</p>

<pre><code>import tensorflow as tf
import numpy as np
from tqdm import tqdm

print('Creating Datasets:')

x_train = []
y_train = []

for i in tqdm(range(10000)):
    x_train.append([np.random.uniform(0, 255), np.random.uniform(0, 255), np.random.uniform(0, 255)])

for elem in tqdm(x_train):
    if (((elem[0] + elem[1] + elem[2]) / 3) / 255) &gt; 0.5:
        y_train.append(0)
    else:
        y_train.append(1)

x_train = np.array(x_train)
y_train = np.array(y_train)

graph = tf.Graph()

with graph.as_default():

    x = tf.placeholder(tf.float32)
    y = tf.placeholder(tf.float32)

    w_1 = tf.Variable(tf.random_normal([3, 10], stddev=1.0), tf.float32)
    b_1 = tf.Variable(tf.random_normal([10]), tf.float32)
    l_1 = tf.sigmoid(tf.matmul(x, w_1) + b_1)

    w_2 = tf.Variable(tf.random_normal([10, 10], stddev=1.0), tf.float32)
    b_2 = tf.Variable(tf.random_normal([10]), tf.float32)
    l_2 = tf.sigmoid(tf.matmul(l_1, w_2) + b_2)

    w_3 = tf.Variable(tf.random_normal([10, 5], stddev=1.0), tf.float32)
    b_3 = tf.Variable(tf.random_normal([5]), tf.float32)
    l_3 = tf.sigmoid(tf.matmul(l_2, w_3) + b_3)

    w_4 = tf.Variable(tf.random_normal([5, 1], stddev=1.0), tf.float32)
    b_4 = tf.Variable(tf.random_normal([1]), tf.float32)
    y_ = tf.sigmoid(tf.matmul(l_3, w_4) + b_4)

    loss = tf.reduce_mean(tf.squared_difference(y, y_))

    optimizer = tf.train.AdadeltaOptimizer().minimize(loss)

    with tf.Session() as sess:

        sess.run(tf.global_variables_initializer())

        print('Training:')

        for step in tqdm(range(5000)):
            index = np.random.randint(0, len(x_train) - 129)
            feed_dict = {x : x_train[index:index+128], y : y_train[index:index+128]}
            sess.run(optimizer, feed_dict=feed_dict)
            if step % 1000 == 0:
                print(sess.run([loss], feed_dict=feed_dict))

        while True:
            inp1 = int(input(''))
            inp2 = int(input(''))
            inp3 = int(input(''))
            print(sess.run(y_, feed_dict={x : [[inp1, inp2, inp3]]}))
</code></pre>

<p>As you can see, I start by importing the modules I will be using. Next I generate my input x dataset and desired output y dataset. The x_train dataset consists of 10000 random RGB values, while the y_train dataset consists of 0's and 1's, with a 1 corresponding to an RGB value with a mean lower than 128 and a 0 corresponding to an RGB value with a mean higher than 128 (this ensures bright backgrounds get dark font and vice versa).</p>

<p>My neural net is admittedly overly complex (or so i assume), but as far as I am aware it is a pretty standard feed forward net, with an Adadelta optimiser and the default learning rate.</p>

<p>The training of the net is normal as far as my limited knowledge informs me, but nonetheless the model always spits out 0.5.</p>

<p>The last block of code allows the user to input values and see what they turn into when passed to the neural net.</p>

<p>I have messed around with different activation functions, losses, methods of initialising biases etc. But to no avail. Some times when I tinker with the code, the model always returns 1 or 0 respectively, but this is still just as inaccurate as being indecisive and returning 0.5 over and over. I have not been able to find a suitable solution to my problem online. Any advice or suggestions are welcome.   </p>

<p>Edit:</p>

<p>The loss, weights, biases and the output don't change much over the course of training (the weights and biases only change by hundredths and thousandths every 1000 iterations, and the loss fluctuates around 0.3). Also, the output sometimes varies f depending on the input (as you would expect), but other times is constant. One run of the program lead to constant 0.7's as output, while another always returned 0.5 apart from very near to zero, where it returned 0.3 or 0.4 type values. Neither of the aforementioned are the desired output. What should happen is that (255, 255, 255) should map to 0 and (0, 0, 0) should map to 1 and (128, 128, 128) should map to either 1 or 0, as in the middle the font colour doesn't really matter.</p>
","9858492","9858492","2018-10-04 17:48:05","Binary classifier always returns 0.5","<python><tensorflow><machine-learning>","2","3","4444"
"50643856","2018-06-01 12:51:59","0","","<p>one way to solve this ,</p>

<pre><code>df['A to B']=df.groupby('B')['A'].transform(lambda x:x.nunique()==1)
df['A to C']=df.groupby('C')['A'].transform(lambda x:x.nunique()==1)
df['B to C']=df.groupby('C')['B'].transform(lambda x:x.nunique()==1)
</code></pre>

<p>Output:</p>

<pre><code>   A  B       C  A to B  A to C  B to C
0  0  a   apple    True   False   False
1  1  b  banana    True    True    True
2  2  c   apple    True   False   False
</code></pre>

<p>To check column by column:</p>

<pre><code>print (df['A to B']==True).all()
print (df['A to C']==True).all()
print (df['B to C']==True).all()

True
False
False
</code></pre>
","4684861","","","1","644","Mohamed Thasin ah","2015-03-18 10:49:38","4803","833","1351","258","50643386","50643635","2018-06-01 12:25:15","2","1377","<p>Working with data in Python 3+ with pandas. It seems like there should be an easy way to check if two columns have a one-to-one relationship (regardless of column type), but I'm struggling to think of the best way to do this.</p>

<p>Example of expected output:</p>

<pre><code>A    B     C
0    'a'   'apple'
1    'b'   'banana'
2    'c'   'apple'
</code></pre>

<p>A &amp; B are one-to-one? TRUE</p>

<p>A &amp; C are one-to-one? FALSE</p>

<p>B &amp; C are one-to-one? FALSE</p>
","1895076","","","Easy Way to See if Two Columns are One-to-One in Pandas","<python><pandas><one-to-one>","4","3","485"
"50643870","2018-06-01 12:52:28","3","","<p>It's because of the first and last brackets.</p>

<p>You have to escape <code>{</code> and <code>}</code>.</p>

<pre><code>""{{\r\n \""name\"": \""{}\"",\r\n \""id\"":\""{}\""}}"".format(1,2)
</code></pre>
","7200715","","","2","199","Arount","2016-11-23 14:51:24","5875","559","82","60","50643808","50643876","2018-06-01 12:48:25","2","333","<p>I want to send dynamic data in my Payload,</p>

<pre><code>payload = ""{\r\n \""name\"": \""{0}\"",\r\n \""id\"":\""{1}\""}"".format(1,2)
*** KeyError: '\r\n ""name""'
</code></pre>

<p>But when i try to add static value it's working fine :</p>

<p><code>payload = ""{\r\n \""name\"": \""just\"",\r\n \""id\"":\""32\""}""</code></p>

<p>How can i add dynamic data on it?</p>

<p>Thanks in advance.</p>
","8231270","","","How to concatenate strings in Python using Format?","<python><django><python-3.x><python-2.7><django-rest-framework>","4","0","383"
"50643871","2018-06-01 12:52:29","2","","<p><strong>In short</strong>: you have created an <em>extreme ice storm</em> in which posts simply freeze to death after 48 hours.</p>

<p>There is nothing ""wrong"" with your algorithm, but you let the scores ""cool down"" too fast.</p>

<p>Imagine that a post is two days old (then the <code>if</code> clause) is triggered. In that case <code>x = 1</code>, and in that case the <code>exp(..)</code> will result in:</p>

<pre><code>&gt;&gt;&gt; exp(-8)
0.00033546262790251185
</code></pre>

<p>That's right. <code>0.00033...</code>, or <code>0.03%</code>. So that means if your post got 10 000 votes, the base score is <code>9.21</code>, and after this multiplication, only:</p>

<pre><code>&gt;&gt;&gt; log(10000) * exp(-8)
0.003089724985059729
</code></pre>

<p>Yes, the cooling scheme should ensure that eventually everything cools down, but not by putting the posts into an ice storm.</p>

<p>You can for example remove the <code>8*</code> factor. This means that the second day, we multiply the score with ~<code>0.37</code> or 36.79%. You can experiment a bit with the factor or some other parts of the cooling scheme and thus let the posts cool down nicely.</p>

<p>Another aspect is that the time is quite <em>descritized</em>: you count the number of days. But that means that as long as the second day is not entirely over, the value is 1. But from the moment the second day is over, the ""<em>temperature</em>"" of the post makes a gigantic drop. You could use the number of seconds and divide by 86'400 instead:</p>

<pre><code>timeDiff = (now - self.post.date).total_seconds() / 86400  # continuum
</code></pre>
","67579","","","0","1620","Willem Van Onsem","2009-02-17 21:39:52","199073","27204","11746","1497","50643551","","2018-06-01 12:33:37","4","88","<p>I'm trying to replicate <a href=""https://stackoverflow.com/questions/11653545/hot-content-algorithm-score-with-time-decay/11654547"">reddit's hot algortithm</a> for sorting my posts. Here's my function:</p>

<pre><code>def hot(self):
    s = self.upvotes
    baseScore = log(max(s, 1))
    now = datetime.now()

    timeDiff = (now - self.post.date).days
    if (timeDiff &gt; 1):
        x = timeDiff - 1
        baseScore = baseScore * exp(-8 * x * x)
    print('Final:', baseScore) #always prints 0
    return baseScore
</code></pre>

<p>basically, <code>exp(-8 * x * x)</code> always makes the number 0. So i'm curious how I'm supposed to make this algorithm work.</p>

<p>Any idea?</p>
","6733153","","","""Hot"" algorithm always returns 0","<python><django><algorithm><sorting>","2","5","693"
"50643876","2018-06-01 12:52:40","2","","<p>Your problem is that when you use <code>format()</code>, the brace character is special (because of <code>{0}</code>) and needs to be escaped by doubling the brace that is not special:</p>

<pre><code>&gt;&gt;&gt; payload = ""{{\r\n \""name\"": \""{0}\"",\r\n \""id\"":\""{1}\""}}"".format(""just"",32)
&gt;&gt;&gt; payload
'{\r\n ""name"": ""just"",\r\n ""id"":""32""}'
</code></pre>
","2084384","","","0","368","BoarGules","2013-02-18 18:26:42","10770","849","166","27","50643808","50643876","2018-06-01 12:48:25","2","333","<p>I want to send dynamic data in my Payload,</p>

<pre><code>payload = ""{\r\n \""name\"": \""{0}\"",\r\n \""id\"":\""{1}\""}"".format(1,2)
*** KeyError: '\r\n ""name""'
</code></pre>

<p>But when i try to add static value it's working fine :</p>

<p><code>payload = ""{\r\n \""name\"": \""just\"",\r\n \""id\"":\""32\""}""</code></p>

<p>How can i add dynamic data on it?</p>

<p>Thanks in advance.</p>
","8231270","","","How to concatenate strings in Python using Format?","<python><django><python-3.x><python-2.7><django-rest-framework>","4","0","383"
"50643906","2018-06-01 12:54:57","0","","<p>The problem comes from the { and } that you are using. You should double them:</p>

<pre><code>payload = ""{{\r\n \""name\"": \""{0}\"",\r\n \""id\"":\""{1}\""}}"".format(1,2)
</code></pre>
","4870915","","","0","183","Gelineau","2015-05-06 13:55:15","1462","118","1019","92","50643808","50643876","2018-06-01 12:48:25","2","333","<p>I want to send dynamic data in my Payload,</p>

<pre><code>payload = ""{\r\n \""name\"": \""{0}\"",\r\n \""id\"":\""{1}\""}"".format(1,2)
*** KeyError: '\r\n ""name""'
</code></pre>

<p>But when i try to add static value it's working fine :</p>

<p><code>payload = ""{\r\n \""name\"": \""just\"",\r\n \""id\"":\""32\""}""</code></p>

<p>How can i add dynamic data on it?</p>

<p>Thanks in advance.</p>
","8231270","","","How to concatenate strings in Python using Format?","<python><django><python-3.x><python-2.7><django-rest-framework>","4","0","383"
"50643908","2018-06-01 12:55:05","0","","<p>From the following <a href=""https://stackoverflow.com/a/49431561/1066110"">answer</a>, the solution is:</p>

<pre><code> mask = df.pipe(lambda x: (x['T'].isin(['N/A', ''])) | (x['T'].isna()),)
 df.drop(df[mask].index, inplace=True)
</code></pre>

<p>this allow to provide different lambdas</p>
","1066110","","","0","296","Agus","2011-11-25 18:16:29","1030","98","264","12","50634988","50643908","2018-06-01 01:42:28","0","227","<p>I have dataframe in wich I have to drop row if some of values.</p>

<p>for instance,</p>

<pre><code>x not in ['N/A', ''] where x is columns
</code></pre>

<p>is there a way like, apply?</p>

<pre><code> df[x] = df[x].apply(lambda x: x.lower())
</code></pre>

<p>I am think in something like:</p>

<pre><code>df.drop.apply(lambda x: X not in ['N/A', ''])???
</code></pre>

<p>My DF</p>

<pre><code>     F   T   l
0    0   ""0""   ""0""
1    1   """"   ""1""
2    2   ""2""   """"
</code></pre>

<p>drop row if T == """" or l == """"</p>

<pre><code>     F   T   l
0    0   ""0""   ""0""
</code></pre>

<p>I could not use </p>

<pre><code>df.drop(df.T == """") since the condition ("""") depend on runtime data
</code></pre>
","1066110","1066110","2018-06-01 02:01:53","how to drop row base on lambda","<python><dataframe>","2","1","703"
"50643960","2018-06-01 12:57:11","2","","<p>This is the short version of your code I guess, still I am not sure what kind of <em>reduction</em> you want.</p>

<pre><code>a = [random.randint(0, 100) for i in range(30)]
b = [random.randint(0, 100) for i in range(30)]

print(set(a) &amp; set(b))
# {100, 91, 45, 78, 50, 51, 23, 59}
</code></pre>
","4237254","","","3","303","BcK","2014-11-10 21:01:26","1519","137","63","23","50643780","50644275","2018-06-01 12:46:30","0","50","<p>So I have created a program that will create 2 lists of random length filled with random numbers, and then tell me if there is a match between the 2 lists or not. I am quite new to this but just wanted to know if there is a way to maybe reduce all those random. functions when making these lists?</p>

<pre><code>c = []
d = []

e = random.randrange(1,10)
f = random.randrange(e,100)
g = random.randrange(1,10)
h = random.randrange(1,10)

#print(""This is g:"", g)
#print(""This is h:"", h)

while g &lt; 14:
    f = random.randrange(e, 124)
    c.append(f)
    g += 1

c.sort()
print(c)

while h &lt; 14:
    f = random.randrange(e, 124)
    d.append(f)
    h += 1

d.sort()
print(d)

n = set(c) &amp; set(d)

#print(bool(n))

if bool(n) == True:
    print(""The following values are a match:"", n)
elif bool(n) == False:
    print(""No match"")
</code></pre>

<p>Output</p>

<pre><code>[14, 36, 80, 80, 120]
[14, 15, 28, 45, 52, 53, 63, 71, 83, 104, 110, 115]
The following values are a match: {14}
</code></pre>
","9041256","8278951","2018-06-02 21:04:50","Reducing code for random lists","<python><python-3.x>","2","6","1009"
"50644000","2018-06-01 12:59:20","2","","<p>If you want to normalize you data you can do as you suggest and simply calculate:</p>

<pre><code>z_i=\frac{x_i-min(x)}{max(x)-min(x)}
</code></pre>

<p>(Sorry but i cannot post images yet but you can visit <a href=""https://chart.googleapis.com/chart?cht=tx&amp;chl=z_i%3D%5Cfrac%7Bx_i-min(x)%7D%7Bmax(x)-min(x)%7D"" rel=""nofollow noreferrer"">this</a> )</p>

<p>where zi is your i-th normalized time data, and xi is your absolute data.</p>

<p>An example using numpy:</p>

<pre><code>import numpy

x = numpy.random.rand(10) # generate 10 random values
normalized = (x-min(x))/(max(x)-min(x))

print(x,normalized)
</code></pre>
","9709009","","","1","629","Nikolaos Paschos","2018-04-27 07:28:39","149","17","8","1","50643572","50644000","2018-06-01 12:34:43","0","282","<p>I have read the following sentence:</p>

<blockquote>
  <p>Figure 3 depicts how the pressure develops during a touch event. It
  shows the mean over all button touches from all users. To account for
  the different hold times of the touch events, the time axis has been
  normalized before averaging the pressure values.</p>
</blockquote>

<p>They have measured the touch pressure over touch events and made a plot. I think normalizing the time axis means to scale the time axis to 1s, for example. But how can this be made? Let's say for example I have a measurement which spans 3.34 seconds (1000 timestamps and 1000 measurements). How can I normalize this measurement?</p>
","1684118","","","Normalizing time series measurements","<python><time-series><normalization>","1","0","679"
"50644024","2018-06-01 13:00:29","3","","<p>Instead of manipulating the JSON in string use the JSON module.</p>

<p><strong>Ex:</strong></p>

<pre><code>import json
payload = {""name"": '',""id"":''}
payload[""name""] = 1
payload[""id""] = 2

payload = json.dumps(payload)
print( payload )
print( type(payload) )
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>{""name"": 1, ""id"": 2}
&lt;type 'str'&gt;
</code></pre>
","532312","","","0","377","Rakesh","2010-12-06 13:07:54","56694","5302","758","1508","50643808","50643876","2018-06-01 12:48:25","2","333","<p>I want to send dynamic data in my Payload,</p>

<pre><code>payload = ""{\r\n \""name\"": \""{0}\"",\r\n \""id\"":\""{1}\""}"".format(1,2)
*** KeyError: '\r\n ""name""'
</code></pre>

<p>But when i try to add static value it's working fine :</p>

<p><code>payload = ""{\r\n \""name\"": \""just\"",\r\n \""id\"":\""32\""}""</code></p>

<p>How can i add dynamic data on it?</p>

<p>Thanks in advance.</p>
","8231270","","","How to concatenate strings in Python using Format?","<python><django><python-3.x><python-2.7><django-rest-framework>","4","0","383"
"50644039","2018-06-01 13:01:31","0","","<p>You might be interested in a stacked area plot. This should work on your DataFrame, named <code>df</code>:</p>

<pre><code>df.drop(columns='age').plot(kind='area', stacked=True)
</code></pre>

<p>One issue is that legend items will show up in reverse order compared to the vertical ordering of the plot areas. To fix this, you can manually reverse the legend handles and labels:</p>

<pre><code>ax = plt.gca()
leg_handles, leg_labels = ax.get_legend_handles_labels()
ax.legend(leg_handles[::-1], leg_labels[::-1])
</code></pre>

<p>Here's some example data (post text, not images, so we can easily copy-paste and experiment :)):</p>

<pre><code>df = pd.DataFrame({'age': [1, 2, 3], 
                   'Class1': [22, 14, 26], 
                   'Class2': [14, 15, 14], 
                   'Class3': [64, 71, 60]
                  })
</code></pre>

<p>Output:
<a href=""https://i.stack.imgur.com/ELhFQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ELhFQ.png"" alt=""Stackplot demo""></a></p>

<p>To reverse the vertical order in the plot so that Class 1 ends up at the top, sort the columns (<code>axis=1</code>) in descending order before plotting:</p>

<pre><code>df.drop(columns='age').sort_index(axis=1, ascending=False)plot(kind='area', stacked=True)
</code></pre>
","8008776","","","0","1290","Peter Leimbigler","2017-05-14 01:39:16","5876","736","465","96","50642098","50644579","2018-06-01 11:09:32","2","5109","<p>I'm currently working on the below dataframe.</p>

<p><a href=""https://i.stack.imgur.com/JVcwo.png"" rel=""nofollow noreferrer"">Dataframe</a></p>

<p>To summarize the contents, there is an ""age"" column, which relates to an age group (i.e. 16-25, 26-32) - and then 8 class % values, these are percentage values which tell me what percentage of a certain age group are in that specific social class. So in this example, there are 10.81%(rounded) of the people in age group with the ID of 1 that are also in the social class with an ID of 1. For the same age group, there are 22.34% in the social class with an ID of 2, and so on, so forth. Each row totals to 100%.</p>

<p>I am looking to create a line graph, which has one line representing each age group. So this graph should have a total of 5 lines.</p>

<p>The X-Axis should represent the Social classes (so ranging 1 through 8), and the Y-Axis should represent the percentage of people in that class.</p>

<p>I'm looking for the graph in this format to make it clear to see for each distinct age group, the patterns in how many people are in each social class, and how this changes as you get older.</p>

<p>Any help with this would be appreciated, I'm not even sure where to start? I've tried some examples online but nothing seems to work. Even a starter would be great.</p>

<p>Thanks.</p>
","6329328","","","Plot line graph from Pandas dataframe (with multiple lines)","<python><pandas><dataframe><graph><line>","3","0","1348"
"50644067","2018-06-01 13:03:00","4","","<p>It is possible in one sort, by applying a custom function within the 'sorted' method as a User described above. I have tried a simplified version for the same. The default 'sorted' method does the wonder with a little tweak. Hope it resolves your query.</p>

<pre><code>import re

input = ""12 I have car 8 200 a""
splitted = input.split()
s_lst=sorted(splitted, key=lambda a:int(a) if a.isdigit() else a.lower())

count_nos = re.findall(r'\d+',' '.join(s_lst))
str_index = len(count_nos)
no_index = 0
result=[]
for i in range(0,len(splitted)):
    if splitted[i].isdigit():
        result.append(s_lst[no_index])
        no_index+=1
    else:
        result.append(s_lst[str_index])
        str_index+=1
print ' '.join(result)
</code></pre>
","681481","681481","2018-06-01 16:32:55","2","743","yogi","2011-03-29 06:24:35","198","21","9","0","47311720","50622681","2017-11-15 15:48:55","8","701","<p>Say I have a string a.</p>

<pre><code>a = ""12 I have car 8 200 a""
</code></pre>

<p>I need to sort this string in such a way that the output should be</p>

<pre><code>8 a car have 12 200 I
</code></pre>

<p>ie, Sort the string in such a way that all words are in alphabetical order and all integers are in numerical order. Furthermore, if the nth element in the string is an integer it must remain an integer, and if it is a word it must remain a word.</p>

<p>This is what I tried.</p>

<pre><code>a = ""12 I have car 8 200 a""


def is_digit(element_):
    """"""
    Function to check the item is a number. We can make using of default isdigit function
    but it will not work with negative numbers.
    :param element_:
    :return: is_digit_
    """"""
    try:
        int(element_)
        is_digit_ = True
    except ValueError:
        is_digit_ = False

    return is_digit_



space_separated = a.split()

integers = [int(i) for i in space_separated if is_digit(i)]
strings = [i for i in space_separated if i.isalpha()]

# sort list in place
integers.sort()
strings.sort(key=str.lower)

# This conversion to iter is to make use of next method.
int_iter = iter(integers)
st_iter = iter(strings)

final = [next(int_iter) if is_digit(element) else next(st_iter) if element.isalpha() else element for element in
         space_separated]

print "" "".join(map(str, final))
# 8 a car have 12 200 I
</code></pre>

<p>I am getting the right output. But I am using two separate sorting function for sorting integers and the words(which I think is expensive). </p>

<p>Is it possible to do the entire sorting using a single sort function?.</p>
","6699447","6699447","2018-05-31 08:23:24","Sort string with integers and words without any change in their positions","<python><python-2.7><performance><sorting><iterator>","6","0","1641"
"50644099","2018-06-01 13:04:44","0","","<p>For anyone that runs into the same issue, I had to add a sendline() and set the prompt between pxssh.pxssh() and the login.</p>

<p>Like this:</p>

<pre><code>...
s = pxssh.pxssh()
s.PROMPT = ""\S+#|\S+# |(\S+)$|(\S+) $|\S+&gt;$|\S+&gt; $""
s.login(router,'username','password', auto_prompt_reset=False)
s.sendline()
pattern_index = s.expect([r'\S+#$', r'\S+# $', r'\S+&gt; ', r'\S+&gt;$'])
if pattern_index == 0 or pattern_index == 1:
...
</code></pre>
","2901454","","","0","455","123troy","2013-10-21 01:03:58","3","7","0","0","50628672","","2018-05-31 16:17:15","0","146","<p>I am trying to use pxssh to connect to several devices and collect data from them.  The devices have one of the following prompts:</p>

<pre><code>routerA#
rp/0/rps0/cpu0:routerB#
routerC&gt; (enable)
routerD&gt;
</code></pre>

<p>Devices with a ""#"" have one set of commands, devices with ""> (enable)"" has a different set and those with just "">"" get a third set of commands.  Given that we do not know what the prompt is until after login, we need to have one script that differentiates based on the prompt.</p>

<p>With my code below, routerC and routerD work, but routerA and routerB do not.</p>

<pre><code>#!/usr/local/bin/python3

import sys
import pexpect
from pexpect import pxssh
import time
import subprocess
import pdb

routerFile = open('test.hosts','r')
routeServers = [i for i in routerFile]
commandFile = open('standard.commands','r')
commands = [i for i in commandFile]
pdb.set_trace()
for router in routeServers:
  router = router.strip()
  print('####connecting to####', router)
  for i in range(max_retries):
    try:
      s = pxssh.pxssh()
      s.login(router,'username','password', original_prompt=""\S+#$|\S+# $|\(\S+\)$|\(\S+\) $|\S+&gt;$|\S+&gt; $"", auto_prompt_reset=False)
      print(s.before)
      print(s.after)
      pattern_index = s.expect([r'\S+#$', r'\S+# $', r'\S+&gt; ', r'\S+&gt;$'])
      print(s.before)
      print(s.after)
      if pattern_index == 0 or pattern_index == 1:
        print(""#"")        
      elif pattern_index == 2:
        print(""catos"")
       if pattern_index == 3:
        print(""enable"")
    except Exception as e:
      print ('!!!!!ERROR!!!!! Issue with host %s', router)
      print (str(e))
      s = None
</code></pre>

<p>Here is the debug output that I get.</p>

<pre><code>&gt; pxssh_test.py(21)&lt;module&gt;()
-&gt; for router in routeServers:
(Pdb) n
&gt; pxssh_test.py(22)&lt;module&gt;()
-&gt; router = router.strip()
(Pdb)
&gt; pxssh_test.py(23)&lt;module&gt;()
-&gt; print('####connecting to####', router)
(Pdb)
####connecting to#### routerA
&gt; pxssh_test.py(24)&lt;module&gt;()
-&gt; for i in range(max_retries):
(Pdb)
&gt; pxssh_test.py(25)&lt;module&gt;()
-&gt; try:
(Pdb)
&gt; pxssh_test.py(26)&lt;module&gt;()
-&gt; s = pxssh.pxssh()
(Pdb)
&gt; pxssh_test.py(27)&lt;module&gt;()
-&gt; s.login(router,'username','password', original_prompt=""\S+#$|\S+# $|\(\S+\)$|\(\S+\) $|\S+&gt;$|\S+&gt; $"", auto_prompt_reset=False)
(Pdb)
&gt; directory/pxssh_test.py(28)&lt;module&gt;()
-&gt; print(s.before)
(Pdb)
b' \r\n....data removed....\r\n\r'
&gt; directory/pxssh_test.py(29)&lt;module&gt;()
-&gt; print(s.after)
(Pdb)
b'routerA# '
&gt; pxssh_test.py(30)&lt;module&gt;()
-&gt; pattern_index = s.expect([r'\S+#$', r'\S+# $', r'\S+&gt; ', r'\S+&gt;$'])
(Pdb)
pexpect.exceptions.TIMEOUT: Timeout exceeded.
&lt;pexpect.pxssh.pxssh object at 0x7f9f19f8efd0&gt;
command: /bin/ssh
args: ['/bin/ssh', '-q', '-l', 'intelliden', 'routerA']
buffer (last 100 chars): b''


after: &lt;class 'pexpect.exceptions.TIMEOUT'&gt;
match: None
match_index: None
exitstatus: None
flag_eof: False
pid: 42618
child_fd: 7
closed: False
timeout: 30
delimiter: &lt;class 'pexpect.exceptions.EOF'&gt;
logfile: None
logfile_read: None
logfile_send: None
maxread: 2000
ignorecase: False
searchwindowsize: None
delaybeforesend: 0.05
delayafterclose: 0.1
delayafterterminate: 0.1
searcher: searcher_re:
    0: re.compile(""b'\\S+#$'"")
    1: re.compile(""b'\\S+# $'"")
    2: re.compile(""b'\\S+&gt; '"")
    3: re.compile(""b'\\S+&gt;$'"")
&gt; pxssh_test.py(30)&lt;module&gt;()
-&gt; pattern_index = s.expect([r'\S+#$', r'\S+# $', r'\S+&gt; ', r'\S+&gt;$'])
(Pdb)
&gt; pxssh_test.py(106)&lt;module&gt;()
-&gt; except pexpect.EOF:
(Pdb)
&gt; pxssh_test.py(113)&lt;module&gt;()
-&gt; except pexpect.TIMEOUT:
(Pdb)
</code></pre>

<p>Any help on why the # will not work would be greatly appreciated.</p>
","2901454","","","Python pxssh expect pattern is not matching '#'","<python><pexpect><pxssh>","1","1","3844"
"50644110","2018-06-01 13:05:12","4","","<p>The error is happening because the function <code>handle_uploaded_file(f)</code> is trying to open an already opened file. </p>

<p>The value of <code>request.FILES['file']</code> is a <code>InMemoryUploadedFile</code> and can be used like a normal file. You don't need to open it again.</p>

<p>To fix, just remove the line that tries to open the file:</p>

<pre><code>def handle_uploaded_file(f):
    for x in f:
        if x.startswith('newick;'):
            print('')
    return cutFile(x)
</code></pre>
","2091925","","","2","512","Will Keeling","2013-02-20 15:40:04","13987","872","566","3","50638374","50644110","2018-06-01 07:37:23","4","3217","<p>I have a method to read a Newick file and return a String in Django framework which is the following:</p>

<pre><code>def handle_uploaded_file(f):
    output = "" ""
    for chunk in f.chunks():
        output += chunk.decode('ascii')
    return output.replace(""\n"", """").replace(""\r"", """")


def post(self, request):
    form = HomeForm(request.POST, request.FILES)
    if form.is_valid():
        input = handle_uploaded_file(request.FILES['file'])
        treeGelezen = Tree(input, format=1)
        script, div = mainmain(treeGelezen)
        form = HomeForm()
    args = {'form': form, 'script': script, 'div': div}
    return render(request, self.template_name, args)
</code></pre>

<p>Which works fine for normal Newick files but i also have some files which have a string at the beginning of the file. I'm trying to make another method which checks if the file has the following String before it (which is the case in some files): ""newick;"" and removes the string if found. It works locally but i can't seem to merge them. This is how it locally looks like:</p>

<pre><code>def removeNewick(tree_with_newick):
    for x in tree_with_newick:
        if x.startswith('newick;'):
            print('')
    return x


filepath = ""C:\\Users\\msi00\\Desktop\\ncbi-taxanomy.tre""
tree_with_newick = open(filepath)
tree = Tree(newick=removeNewick(tree_with_newick), format=1)
</code></pre>

<p>which works perfectly when i specify the path just in python so i tried combining them in Django like this: </p>

<pre><code>def handle_uploaded_file(f):
    tree_with_newick = open(f)
    for x in tree_with_newick:
        if x.startswith('newick;'):
            print('')
    return cutFile(x)


def cutFile(f):
    output = "" ""
    for chunk in f.chunks():
        output += chunk.decode('ascii')
    return output.replace(""\n"", """").replace(""\r"", """")


def post(self, request):
    form = HomeForm(request.POST, request.FILES)
    if form.is_valid():
        input = handle_uploaded_file(request.FILES['file'])
        treeGelezen = Tree(input, format=1)
        script, div = mainmain(treeGelezen)
        form = HomeForm()
    args = {'form': form, 'script': script, 'div': div}
    return render(request, self.template_name, args)
</code></pre>

<p>Which doesn't work and it gives the following error:</p>

<pre><code>expected str, bytes or os.PathLike object, not InMemoryUploadedFile
</code></pre>

<p>I've been working on it for two days already and couldn't figure out why the error is popping up.</p>
","","","2018-06-01 07:45:54","expected str, bytes or os.PathLike object, not InMemoryUploadedFile","<python><django><ncbi><ete3>","2","2","2504"
"50644132","2018-06-01 13:06:18","0","","<p>Something that people often forget with in-place operators is that their use always involves an assignment. You can see this intuitively in <code>Alpha.content</code> (or any <code>int</code> or <code>str</code> really): integers are immutable, but the operation works. It's much easier to forget about this step for something like <code>Alpha</code> or <code>list</code>, where the in-place operator just returns <code>self</code>. Just remember that the operator can return anything at all, and the result has to be bound to the original name. What's happening here is basically this:</p>

<pre><code>x = beta.value
x = operator.iadd(x, 2)      # Totally fine
beta.value = x # You can  imagine how this would be a problem...
</code></pre>

<p>A direct consequence of this is that you'll see the changes in <code>beta.value</code> despite the error.</p>

<p>You're always welcome to bypass the reassignment by first assigning to a temporary variable, i.e., running the first two lines shown above explicitly. Just remember that while in your case <code>Alpha</code> is mutable and really modifies itself in-place, that's not a requirement for the general case:</p>

<pre><code>x = beta.value
x += 2
</code></pre>

<p>works as intended. However,</p>

<pre><code>x = beta.value.content
x += 2
</code></pre>

<p>does not, since <code>int.__iadd__</code> inevitably returns a new reference.</p>
","2988730","2988730","2018-06-01 13:18:15","0","1395","Mad Physicist","2013-11-13 16:56:40","47060","10150","10088","2033","50643742","50644803","2018-06-01 12:44:28","1","48","<p><a href=""https://stackoverflow.com/questions/11987949"">This question</a> deals with <code>__iadd__</code> on Python read-write properties. However, I'm struggling to find the solution for read-only properties.</p>

<p>In my MWE we have a read-only property <code>Beta.value</code>, returning an <code>Alpha</code> instance. I imagine I should be able to use <code>__iadd__</code> on <code>Beta.value</code> because the <em>returned</em> value is mutated in-place, and no change is made to <code>Beta</code> itself, much like the ""<code>beta.value.content +=</code>"" line preceding it. However the following code crashes with an <code>AttributeError: can't set attribute</code>.</p>

<p><strong>Is it possible to use <code>__iadd__</code> on read-only properties?</strong></p>

<pre><code>class Alpha:
    def __init__( self, content : int ) -&gt; None:
        self.content : int = content


    def __iadd__( self, other : int ) -&gt; ""Alpha"":
        self.content += other
        return self


class Beta:
    def __init__( self ):
        self.__value: Alpha = Alpha(1)


    @property
    def value( self ) -&gt; Alpha:
        return self.__value


beta = Beta()
beta.value.content += 2
beta.value += 2
</code></pre>
","3601660","2988730","2018-06-01 13:20:40","__iadd__ in Python with read only property","<python>","2","3","1226"
"50644174","2018-06-01 13:08:27","1","","<p><strong>conda</strong> is a packaging tool and installer that aims to do more than what pip can do; handle library dependencies outside of the Python packages as well as the Python packages themselves. Conda also creates a virtual environment, like virtualenv does. For creating virtualenv with conda, use the following command:-</p>

<pre><code>conda create -n yourenvname python=x.x anaconda
</code></pre>

<p>Use the following to activate the virtualenv in conda</p>

<pre><code>source activate yourenvname
</code></pre>

<p>Then, you can install the packages in virtualenv using conda as:-</p>

<pre><code>conda install -n yourenvname [package]
</code></pre>

<p>To Deactivate use:-</p>

<pre><code>source deactivate
</code></pre>

<p>And to delete a no longer needed virtualenv, use :-</p>

<pre><code>conda remove -n yourenvname -all
</code></pre>
","2122230","","","0","857","nandal","2013-03-01 04:48:31","1806","191","19","10","50643691","50644376","2018-06-01 12:42:06","0","2562","<p>I want to work with the python package <a href=""https://holopy.readthedocs.io/en/3.1.1/users/install.html"" rel=""nofollow noreferrer"">holopy</a>. Apparently you have to use conda to install it, so I first installed Anaconda 4.2.0 (since I'm using Python 3.5). I opened the virtual environment I normally use and installed holopy as they recommend on the official site:</p>

<pre><code>conda install -c conda-forge holopy
</code></pre>

<p>Afterwards, when in the virtual environment I type <code>conda list</code>, holopy shows up. But when I type <code>python3</code> and then <code>import holopy</code>, it says package not found. It does however work when I leave the virtual environment. I need it in the virtual environment though, how can I do that?</p>
","5522601","","","Importing a package installed with anaconda in virtual environment","<python><virtualenv><conda>","3","0","762"
"50644196","2018-06-01 13:09:22","0","","<p>This is one way:</p>

<pre><code># perform vectorised calculation
arr = df.values / df1.loc[:, df.columns.map(db[0].get)].values

# feed result into dataframe
res = pd.DataFrame(arr, columns=df.columns)

print(res)

      Green      Blue       Red    Yellow
0  0.500000  3.000000  0.333333  1.666667
1  4.000000  3.500000  2.000000  2.500000
2  2.666667  1.333333  4.000000  8.000000
</code></pre>
","9209546","9209546","2018-06-01 14:10:29","3","401","jpp","2018-01-12 14:47:22","109049","18235","7890","3496","50644011","50644196","2018-06-01 12:59:55","1","41","<p>I have three DataFrames: one containing a codification, one containing values and the third containing divisors.</p>

<p>I would like to divide the second dataframe by the third based on the columns codification, without looping through. How can I do it?</p>

<pre><code>names=['Green','Blue','Red','Yellow']
values=numpy.random.randint(1,10,[3,4])
attributes=['Small','Small','Medium','Large']
attributes1=['Small','Medium','Large']    
divisors=numpy.random.randint(1,5,(3,3))

db=pandas.DataFrame(attributes,index=names)
df=pandas.DataFrame(values,columns=names)
df1=pandas.DataFrame(divisors,columns=attributes1)

values_divided=pandas.DataFrame(values,columns=names)

for name in values_divided.columns:
    values_divided[name]=df[name]/df1[db[0][name]]
</code></pre>
","9870355","9870355","2018-06-01 13:49:39","Divide dataframe rows if column name has certain attribute in pandas","<python><pandas>","1","0","777"
"50644212","2018-06-01 13:10:04","0","","<p>A possible solution to create the line graph as you requested could be (using a dummy dataset):</p>

<pre><code>import matplotlib.pyplot as plt
import pandas as pd

df=pd.DataFrame({""age"":[1,2,3,4,5],""class1"":[0.1,0.2,0.3,0.3,0.6],""class2"":[0.4,0.1,0.2,0.3,0.6],""class3"":[0.1,0.7,0.8,0.3,0.5]})
df=df.set_index(""age"")
for i in range(len(df)):
    plt.plot([k for k in df.columns],[df[y].iloc[i] for y in df.columns])
plt.legend(df.index,loc=""upper left"")
plt.show()
</code></pre>

<p>Output:
<a href=""https://i.stack.imgur.com/5QVPQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5QVPQ.png"" alt=""enter image description here""></a>
Probably not the most pythonic way though.</p>
","9758553","","","0","701","Dav2357","2018-05-08 12:49:41","134","9","1","0","50642098","50644579","2018-06-01 11:09:32","2","5109","<p>I'm currently working on the below dataframe.</p>

<p><a href=""https://i.stack.imgur.com/JVcwo.png"" rel=""nofollow noreferrer"">Dataframe</a></p>

<p>To summarize the contents, there is an ""age"" column, which relates to an age group (i.e. 16-25, 26-32) - and then 8 class % values, these are percentage values which tell me what percentage of a certain age group are in that specific social class. So in this example, there are 10.81%(rounded) of the people in age group with the ID of 1 that are also in the social class with an ID of 1. For the same age group, there are 22.34% in the social class with an ID of 2, and so on, so forth. Each row totals to 100%.</p>

<p>I am looking to create a line graph, which has one line representing each age group. So this graph should have a total of 5 lines.</p>

<p>The X-Axis should represent the Social classes (so ranging 1 through 8), and the Y-Axis should represent the percentage of people in that class.</p>

<p>I'm looking for the graph in this format to make it clear to see for each distinct age group, the patterns in how many people are in each social class, and how this changes as you get older.</p>

<p>Any help with this would be appreciated, I'm not even sure where to start? I've tried some examples online but nothing seems to work. Even a starter would be great.</p>

<p>Thanks.</p>
","6329328","","","Plot line graph from Pandas dataframe (with multiple lines)","<python><pandas><dataframe><graph><line>","3","0","1348"
"50644250","2018-06-01 13:12:08","5","","<pre><code>class Partner(models.Model):
    _inherit = ""res.partner""

   my_field = fields.Boolean()
</code></pre>

<p><strong>Note:</strong> Give 'base' as dependencies in manifest of your custom module.</p>
","9117204","","","3","209","Bhoomi Vaishnani","2017-12-19 09:11:20","620","226","16","15","50638548","51527195","2018-06-01 07:48:57","2","1094","<p>I'm working on a custom module and i need to add field to res.partner model. I've add some field to this model but since 1 week, when i try to add a new one i got this error :</p>

<pre><code>ERROR: column res_partner.my_field does not exist
</code></pre>

<p>Other field works good but not this one :</p>

<pre><code>my_field = fields.Boolean(default=False)
</code></pre>

<p>I really don't unterstand why i have this issue. I've try to add 'contacts' dependencies to my module, it have work on my local version but not on my online verison</p>

<p>If somone has any idea it could be really nice</p>

<p>Thanks for your help</p>

<p>Edit
res.partner classe</p>

<pre><code>from odoo import api, fields, models

class ResPartner(models.Model):
    _inherit = 'res.partner'

badge_ids = fields.One2many('mymodule.badge','client_i
sub_ids = fields.One2many('mymodule.subs','client_id')
#field that doesn't work
my_field = fields.Boolean(default=False)
</code></pre>

<p>Manifest dependencies</p>

<pre><code>'depends': ['base', 'sale', 'website', 'calendar','contacts', 'point_of_sale', 'base_automation'],
</code></pre>
","8249226","8249226","2018-06-05 13:28:45","Error column doesn't exist on custom module Odoo","<python><odoo-11>","4","3","1122"
"50644262","2018-06-01 13:12:50","3","","<p>Accounting for ties:</p>

<pre><code>val = sorted(d.values(), reverse=True)[4]
res = {k: v for k, v in d.items() if v &gt;= val}

print(res)

{'3': 5, '5': 8, '7': 3, '4': 8, '10': 7}
</code></pre>

<p><strong>Explanation</strong></p>

<ul>
<li>Calculate the 5th highest value using <code>sorted</code> with <code>reverse=True</code>. Remember indexing begins at <code>0</code> so index with <code>[4]</code>.</li>
<li>Use a dictionary comprehension to select all items from your dictionary where value is greater than the calculated value.</li>
</ul>

<p><strong>Optimisation</strong></p>

<p>A more efficient method, as pointed out by @Chris_Rands, is to use <code>heapq</code> to calculate the 5th highest value:</p>

<pre><code>import heapq

val = heapq.nlargest(5, d.values())[-1]
res = {k: v for k, v in d.items() if v &gt;= val}
</code></pre>
","9209546","9209546","2018-06-01 13:37:54","1","853","jpp","2018-01-12 14:47:22","109049","18235","7890","3496","50644186","50644262","2018-06-01 13:08:51","2","84","<p>I have dictionary as below. Is there a way to output a dictionary with the 5 highest values?</p>

<p>If there are ties for the 5th highest value, I need to include those keys.</p>

<p><strong>Input dictionary</strong>:</p>

<pre><code>{
    ""1"": 1,
    ""12"": 1,
    ""13"":2,
    ""3"": 5,
    ""5"":8,
    ""7"":3,
    ""4"":8,
    ""10"":7
}
</code></pre>

<p><strong>Desired result:</strong></p>

<pre><code>{
    ""3"": 5,
    ""5"":8,
    ""7"":3,
    ""4"":8,
    ""10"":7
}
</code></pre>
","9342041","9209546","2018-06-01 13:25:45","Filter dictionary and remove lowest values","<python><python-3.x><dictionary>","2","4","476"
"50644263","2018-06-01 13:12:55","1","","<p>From the inputs you give, it can be understood that its not the problem with pjsip wrapper. Transport configurations looks fine. </p>

<p>Looking in to the 'create_transport' error, the program is not able to create the connection because 5060 port is already occupied with some other program. </p>

<p>For that you are killing that process and you are able to run the program with out any error. And you say it only on restart, so on your system restart some program is occupying the port. </p>

<p>You can try like this </p>

<pre><code>sudo netstat -nlp|grep 5060
</code></pre>

<p>in your case it will give like </p>

<blockquote>
  <blockquote>
    <p>1137/ProgramName</p>
  </blockquote>
</blockquote>

<p>go to the 'ProgramName' in your startup configurations and make modifications such that it wont pickup the port. </p>
","7460588","","","7","833","DKA","2017-01-24 02:00:45","38","6","0","0","50604849","50644263","2018-05-30 12:35:54","0","454","<p>I am trying to make calls using PJSIP module in python. For setup of SIP transport, I am doing like </p>

<pre><code>trans_cfg = pj.TransportConfig()
# port for VoIP communication
trans_cfg.port = 5060
# local system address
trans_cfg.bound_addr = inputs.client_addr
transport = lib.create_transport(pj.TransportType.UDP,trans_cfg)
</code></pre>

<p>when I finish the call I am clearing the transport setup as, <code>transport = None</code>. </p>

<p>I am able to make call to user by running my program. But every time I restart my PC alone, I get an error while I run my python program </p>

<pre><code>File ""pjsuatrail_all.py"", line 225, in &lt;module&gt;
   main()
File ""pjsuatrail_all.py"", line 169, in main
   transport = transport_setup()
File ""pjsuatrail_all.py"", line 54, in transport_setup
   transport = lib.create_transport(pj.TransportType.UDP,trans_cfg)
File ""/usr/local/lib/python2.7/dist-packages/pjsua.py"", line 2304, in 
   create_transport
   self._err_check(""create_transport()"", self, err)
File ""/usr/local/lib/python2.7/dist-packages/pjsua.py"", line 2723, in _err_check
   raise Error(op_name, obj, err_code, err_msg)
pjsua.Error: Object: Lib, operation=create_transport(), error=Address already in use
Exception AttributeError: ""'NoneType' object has no attribute 'destroy'"" in &lt;bound method Lib.__del__ of &lt;pjsua.Lib instance at 0x7f8a4bbb6170&gt;&gt; ignored
</code></pre>

<p>For this currently I am doing like </p>

<pre><code>$sudo lsof -t -i:5060
&gt;&gt; 1137
$sudo kill 1137
</code></pre>

<p>Then I run my code it works fine. 
By instance from error, I can understand that somewhere I am not closing my transport configuration properly. Can anyone help in this regards. 
<a href=""https://github.com/chakrit/pjsip/blob/master/pjsip-apps/src/python/samples/call.py"" rel=""nofollow noreferrer"">Reference code used</a></p>
","7457101","","","pjsua.error, error = address already in use","<python><pjsip><telephony>","1","0","1859"
"50644275","2018-06-01 13:13:17","2","","<p><a href=""https://stackoverflow.com/users/4237254/bck"">BcK's</a> previous answer is fine, except it doesn't generate lists with random size (both have size = 30).
The code should be</p>

<pre><code>a = [random.randint(0, 100) for i in range(random.randint(0, 30))]
b = [random.randint(0, 100) for i in range(random.randint(0, 30))]
common = set(a) &amp; set(b)
if common:
    print(*common)
else:
    print(""No match"")
</code></pre>
","8278951","364696","2018-06-01 13:40:06","2","435","Rahul Goswami","2017-07-09 11:06:31","386","64","96","18","50643780","50644275","2018-06-01 12:46:30","0","50","<p>So I have created a program that will create 2 lists of random length filled with random numbers, and then tell me if there is a match between the 2 lists or not. I am quite new to this but just wanted to know if there is a way to maybe reduce all those random. functions when making these lists?</p>

<pre><code>c = []
d = []

e = random.randrange(1,10)
f = random.randrange(e,100)
g = random.randrange(1,10)
h = random.randrange(1,10)

#print(""This is g:"", g)
#print(""This is h:"", h)

while g &lt; 14:
    f = random.randrange(e, 124)
    c.append(f)
    g += 1

c.sort()
print(c)

while h &lt; 14:
    f = random.randrange(e, 124)
    d.append(f)
    h += 1

d.sort()
print(d)

n = set(c) &amp; set(d)

#print(bool(n))

if bool(n) == True:
    print(""The following values are a match:"", n)
elif bool(n) == False:
    print(""No match"")
</code></pre>

<p>Output</p>

<pre><code>[14, 36, 80, 80, 120]
[14, 15, 28, 45, 52, 53, 63, 71, 83, 104, 110, 115]
The following values are a match: {14}
</code></pre>
","9041256","8278951","2018-06-02 21:04:50","Reducing code for random lists","<python><python-3.x>","2","6","1009"
"50644286","2018-06-01 13:13:42","1","","<p>Joblib uses <code>pickle</code>. The <code>pickle</code> can store any arbitrary Python object to disk and restore it into another process afterwards. But if that Python object is, or contains, an instance of a class that is defined in the code that does the dump, then that class definition needs to be available in the code that does the load. </p>

<p>And if that class is defined in a library that was <em>imported by</em> the code that does the dump, then it also needs to import that library at load time. You don't have to do the import: <code>pickle</code> will do that for you. But it must be available for import.</p>

<p>I understand that you don't think that the object you are trying to load needs the class. But <code>pickle</code> does think that.</p>
","2084384","","","7","770","BoarGules","2013-02-18 18:26:42","10770","849","166","27","50643983","50644286","2018-06-01 12:58:12","0","373","<p>I am saving an object with <code>joblib.dump()</code>. When I try to open it using another Python instance (but same version), joblib complains it can't load a particular module:</p>

<pre><code>ImportError: No module named some_module
</code></pre>

<p>Now, this module <code>some_module</code> is indeed not available in that Python instance.
The point is, however, that the object I was trying to load does not need that module at all.</p>

<p>So my question is, why does joblib think it needs this package?</p>

<p>Does it somehow include all module that were active at the time of dumping?</p>
","2312926","","","Joblib ImportError. Tries to load a library I think it shouldn't be looking for in the first place","<python><python-2.7><joblib>","1","0","602"
"50644311","2018-06-01 13:15:02","1","","<pre><code>from  collections import Counter
dict(Counter(your_dict).most_common(5))
</code></pre>

<p>OUTPUT:</p>

<pre><code>{'10': 7, '3': 5, '4': 8, '5': 8, '7': 3}
</code></pre>
","2695448","2695448","2018-06-01 13:17:06","5","182","mad_","2013-08-19 07:13:37","6342","678","153","349","50644186","50644262","2018-06-01 13:08:51","2","84","<p>I have dictionary as below. Is there a way to output a dictionary with the 5 highest values?</p>

<p>If there are ties for the 5th highest value, I need to include those keys.</p>

<p><strong>Input dictionary</strong>:</p>

<pre><code>{
    ""1"": 1,
    ""12"": 1,
    ""13"":2,
    ""3"": 5,
    ""5"":8,
    ""7"":3,
    ""4"":8,
    ""10"":7
}
</code></pre>

<p><strong>Desired result:</strong></p>

<pre><code>{
    ""3"": 5,
    ""5"":8,
    ""7"":3,
    ""4"":8,
    ""10"":7
}
</code></pre>
","9342041","9209546","2018-06-01 13:25:45","Filter dictionary and remove lowest values","<python><python-3.x><dictionary>","2","4","476"
"50644353","2018-06-01 13:17:09","1","","<p>MechanicalSoup does not do JavaScript. The website you are trying to browse has:</p>

<pre><code>&lt;form id=""websitevariantsearch""
      action=""""
      onsubmit=""if ...""&gt;
</code></pre>

<p>There's no action in the sense of traditional HTML forms, but there's a piece of JavaScript executed on submission. MechanicalSoup won't help here. Selenium may work: <a href=""http://mechanicalsoup.readthedocs.io/en/stable/faq.html#how-does-mechanicalsoup-compare-to-the-alternatives"" rel=""nofollow noreferrer"">http://mechanicalsoup.readthedocs.io/en/stable/faq.html#how-does-mechanicalsoup-compare-to-the-alternatives</a></p>
","4830165","","","1","624","Matthieu Moy","2015-04-24 20:12:54","8096","892","291","35","50628120","50644353","2018-05-31 15:43:10","2","248","<p>I am a novice in web-scraping and web-things in general (but pretty much used to Python), and I'd like to understand how it works to integrate a website search in a bioinformatics research tool.</p>

<p>Goal: retrieve the output of the form on <a href=""http://www.lovd.nl/3.0/search"" rel=""nofollow noreferrer"">http://www.lovd.nl/3.0/search</a></p>

<pre><code>import mechanicalsoup

# Connect to LOVD
browser = mechanicalsoup.StatefulBrowser()
browser.open(""http://www.lovd.nl/3.0/search"")

# Fill-in the search form
browser.select_form('#websitevariantsearch')
browser[""variant""] = ""chr15:g.40699840C&gt;T""
browser.submit_selected()

# Display the results
print(browser.get_current_page())
</code></pre>

<p>In the output I get the very same page ( <a href=""http://www.lovd.nl/3.0/search"" rel=""nofollow noreferrer"">http://www.lovd.nl/3.0/search</a>). I tried with standard requests but I get another kind of error:</p>

<pre><code>from requests import get, Session

url=""http://www.lovd.nl/3.0/search""
formurl = ""http://www.lovd.nl/3.0/ajax/search_variant.php""
client = Session()

#get the csrf
soup = BeautifulSoup(client.get(url).text, ""html.parser"")
csrf = soup.select('form input[name=""csrf_token""]')[0]['value']

form_data = {
    ""search"": """",
    ""csrf_token"": csrf,
    ""build"": ""hg19"",
    ""variant"": ""chr15:g.40699840C&gt;T""
}

response = get(formurl, data=form_data)
html=response.content
return html
</code></pre>

<p>...and this returns only an </p>

<pre><code>alert(""Error while sending data."");
</code></pre>

<p>The form_data fields were took from the XHR request (from developer -> network tab).</p>

<p>I can see that the data is sent asynchronously via ajax but I do not understand the practical implications of this information.</p>

<p>Need some guidance</p>
","2261984","","","Form request with mechanicalsoup not showing expected results","<python><http><web-scraping><python-requests><mechanicalsoup>","1","0","1785"
"50644360","2018-06-01 13:17:24","2","","<p>Each call <code>sm.OLS(y,X)</code> creates a new model instance, each call to <code>.fit()</code> creates a new results instance with a reference to the underlying model. Instances are independent of each other, that is they don't share any attributes except for possibly the underlying data.</p>

<p>However in your example you assign the same name <code>ols</code> to each of the regression results, so the name <code>ols</code> only refers to the last instance.</p>

<p>more details:</p>

<p>Creating a model like <code>sm.OLS(y,X)</code> does not copy the data y and X if the copy is not needed. Specifically, if y and X are numpy ndarrays, then no copy is needed. (Technically, conversion and copy behavior depends on np.asarray(y) and np.asarray(X))</p>

<p>Repeated calls to a <code>fit</code> method creates a new results instance each time, but they hold a reference to the same model instance. For example, we can call fit with different cov_type options which will create the covariance of the parameter estimates using different assumptions. </p>

<pre><code>model = sm.OLS(y,X)
ols_nonrobust = model.fit()
ols_hc = model.fit(cov_type=""HC3"")
</code></pre>

<p>In most models all the relevant information from the fit is attached to the results instance. In the above case we can look at both results instances at the same time, e.g. comparing the parameter standard errors</p>

<pre><code>ols_nonrobust.bse
ols_hc.bse
</code></pre>

<p>statsmodels still has a few cases in RLM and some time series models where some fit options might change the underlying model. In that case, only the last results instance created by fit will have the correct model attributes.
Those cases are fine if we fit in a loop where we only need the last instances, but might show incorrect results if several results instances are used at the same time and they refer to the same underlying model instance.
<a href=""http://www.statsmodels.org/devel/pitfalls.html#repeated-calls-to-fit-with-different-parameters"" rel=""nofollow noreferrer"">http://www.statsmodels.org/devel/pitfalls.html#repeated-calls-to-fit-with-different-parameters</a></p>
","333700","","","1","2134","Josef","2010-05-05 17:37:25","15319","1655","364","6","50641585","50644360","2018-06-01 10:38:54","1","319","<p>I am trying to fit a linear regression model implemented in <code>statsmodels</code> library.</p>

<p>I have a doubt regarding the <code>fit()</code> method. Let's say I have data sample of size 15 and I broke it down into 3 parts and fit the model. Does call to each <code>fit()</code> will fit the model properly or will it overwrite previous values.</p>

<pre><code>import numpy as np
import statsmodels.api as sm

# First call
X = [377, 295, 457, 495, 9] # independent variable
y = [23, 79, 16, 41, 40]    # dependent variable
X = sm.add_constant(X)
ols = sm.OLS(y,X).fit()
#print(ols.summary())

# Second call
X = [243, 493, 106, 227, 334]
y = [3, 5, 1, 62, 92]
X = sm.add_constant(X)
ols = sm.OLS(y,X).fit()
#print(ols.summary())

# Third call
X = [412, 332, 429, 96, 336] 
y = [30, 1, 99, 4, 33]
X = sm.add_constant(X)
ols = sm.OLS(y,X).fit()
#print(ols.summary())

scores = [9, 219, 200, 134, 499]
scores = sm.add_constant(scores)
print(ols.predict(scores))
</code></pre>
","9483941","","","Understanding statsmodels linear regression","<python><linear-regression><statsmodels>","1","0","983"
"50644371","2018-06-01 13:17:57","0","","<p>Possibly you want to use an additional column and let the ""core"" plot span all 3 rows?</p>

<pre><code>ax = plt.subplot2grid((3,3),(0,0), rowspan = 3, colspan = 2, aspect = 'equal',
                      autoscale_on = False, xlim = (-51.2,51.2), ylim = (-50.4,50.4))
ax1 = plt.subplot2grid((3,3),(1,2))
</code></pre>

<p><a href=""https://i.stack.imgur.com/vsxnk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vsxnk.png"" alt=""enter image description here""></a></p>
","4124317","","","2","488","ImportanceOfBeingErnest","2014-10-09 07:50:43","173272","28712","2299","3162","50644118","50644371","2018-06-01 13:05:30","1","62","<p>I am trying to code a simple nuclear fission simulation and thus far I've got the simulation of the core itself working properly. Now what I am trying to do is to have a second graph near the simulation, that tells the user how much power is being outputted from the core.</p>

<p>With that being said, I am trying to use <code>subtplot2grid</code> but I can't seem to find the right fitting measurements for my program. I also add a patch of <code>plt.Rectangle</code> into the core simulation that I use as bounds to my core, I tried running the program with and without the patch and it seems like it is the problem. Even though, I would like that rectangle to stay, please help me find the right measurements and explain why are the dimensions different with and without the patch.</p>

<p>here's my code: </p>

<pre><code>import matplotlib.pyplot as plt
import matplotlib.animation as animation

BOUNDS = [-20,20,-20,20]

fig = plt.figure()

ax = plt.subplot2grid((3,2),(0,0), rowspan = 2, colspan = 2, aspect = 'equal', autoscale_on = False,
                          xlim = (-51.2,51.2), ylim = (-50.4,50.4))
ax1 = plt.subplot2grid((3,2),(2,1))

ax1.set_xlabel('Time')
ax1.set_ylabel('Jouls')

rect = plt.Rectangle(BOUNDS[::2], #Creates the frame of the board (black rectangle)
                 BOUNDS[1] - BOUNDS[0],
                 BOUNDS[3] - BOUNDS[2],
                 ec='black', lw=2, fc='none')


ax.add_patch(rect)
ax.axis('off')
plt.show()
</code></pre>

<p>If you will run the program, you will see this: 
<a href=""https://i.stack.imgur.com/DIKDf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DIKDf.png"" alt=""enter image description here""></a></p>

<p>Like I said before, I would like the power graph to be near the simulation, which will take up most of the figure, like so:</p>

<p><a href=""https://i.stack.imgur.com/BTRO2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BTRO2.png"" alt=""enter image description here""></a></p>

<p>I would appreciate any help, thanks!</p>
","8752338","","","Matplotlib subplot2grid doesn't work properly","<python><python-3.x><animation><matplotlib><graph>","1","0","2035"
"50644376","2018-06-01 13:18:12","1","","<p>I'm not sure how well anaconda and virtual environments i.e.<code>venv</code> work together. If you're using anaconda anyway then I highly recommend using anaconda environments. Please go through this short tutorial about <a href=""https://conda.io/docs/user-guide/tasks/manage-environments.html"" rel=""nofollow noreferrer"">anaconda environments</a> - you won't regret it.</p>

<h3>Why it didn't work for you?</h3>

<p>The <code>conda</code> command is available only in the base anaconda environment. So when you run the command - <code>conda insall -c conda-forge holopy</code>, it installed <code>holopy</code> in the base anaconda environment and it won't be available to you in your <code>venv</code>.</p>

<p>After looking at the documentation of <code>holopy</code> it seems probable that when they said <code>virtual environment</code> they actually meant <code>anaconda virtual environment</code>. Therefore the solution is to first create an <code>anaconda virtual environment</code> called <code>holopy-env</code> and then run the command <code>conda install -n holopy-env -c conda-forge holopy</code>.</p>

<h2>A better way of doing things with Anaconda</h2>

<p>I will also give you a quick and clean example of how to create an environment using anaconda. If you're using Anaconda then it would be wise to use it's <a href=""https://conda.io/docs/user-guide/tasks/manage-environments.html"" rel=""nofollow noreferrer"">environment management</a> tools. Create an <code>environment.yml</code> file with the following contents:</p>

<h3>environment.yml using conda-forge/holopy &amp; python 3.6</h3>

<pre><code>name: holopy-env      # any name for the environment
channels:
- conda-forge
dependencies:         # everything under this, installed by conda
- python=3.6
- holopy
- pip:                # everything under this, installed by pip
  - future
</code></pre>

<h3>How to install the environment?</h3>

<p><code>conda create --force -f environment.yml</code></p>

<h3>How to activate the environment?</h3>

<p><code>source activate opencv-env</code></p>

<h3>After activating the environment</h3>

<ul>
<li>You should be able to <code>import holopy</code></li>
<li>Install pip packages using <code>pip install &lt;package&gt;</code></li>
<li>Install conda packages using <code>conda install -n holopy-env -c CHANNEL &lt;package&gt;</code></li>
</ul>
","2598661","","","4","2365","nitred","2013-07-19 08:33:12","2640","92","89","0","50643691","50644376","2018-06-01 12:42:06","0","2562","<p>I want to work with the python package <a href=""https://holopy.readthedocs.io/en/3.1.1/users/install.html"" rel=""nofollow noreferrer"">holopy</a>. Apparently you have to use conda to install it, so I first installed Anaconda 4.2.0 (since I'm using Python 3.5). I opened the virtual environment I normally use and installed holopy as they recommend on the official site:</p>

<pre><code>conda install -c conda-forge holopy
</code></pre>

<p>Afterwards, when in the virtual environment I type <code>conda list</code>, holopy shows up. But when I type <code>python3</code> and then <code>import holopy</code>, it says package not found. It does however work when I leave the virtual environment. I need it in the virtual environment though, how can I do that?</p>
","5522601","","","Importing a package installed with anaconda in virtual environment","<python><virtualenv><conda>","3","0","762"
"50644396","2018-06-01 13:19:05","0","","<p>You cannot lock processes like this, you have tu use <a href=""https://docs.python.org/3/library/multiprocessing.html#managers"" rel=""nofollow noreferrer""><code>multiprocessing.Manager</code></a>:</p>

<blockquote>
  <p>Managers provide a way to create data which can be shared between
  different processes, including sharing over a network between
  processes running on different machines. A manager object controls a
  server process which manages shared objects. Other processes can
  access the shared objects by using proxies.</p>
</blockquote>

<hr>

<pre><code>manager = multiprocessing.Manager()
lock = manager.Lock()
</code></pre>

<p>Instead of</p>

<pre><code>lock = Lock()
</code></pre>
","7200715","","","0","702","Arount","2016-11-23 14:51:24","5875","559","82","60","50643381","","2018-06-01 12:24:55","2","46","<p>Given the following code  </p>

<pre><code>import time
from multiprocessing import Pool
from threading import Lock
import multiprocessing

PROCESSES = 2
WORKER_CALLS = 2
lock = Lock()

def run(num):
    lock.acquire()
    print(""enter""+str(num))
    time.sleep(2)
    print(""exit"" + str(num))
    lock.release()


if __name__ == ""__main__"":
    pool = multiprocessing.Pool(processes=PROCESSES)
    pool_outputs = pool.map(run, range(WORKER_CALLS))
    pool.close()
    pool.join()
    print('Pool:', pool_outputs)
</code></pre>

<p>Expected output is, since assume the lock part only one process can be in:</p>

<pre><code>enter0
exit0
enter1
exit1
Pool: [None, None]
</code></pre>

<p>but actual output is:</p>

<pre><code>enter0
enter1
exit0
exit1
Pool: [None, None]
</code></pre>

<p>What's the problem and how to resolve this?</p>
","1497720","789671","2018-06-01 12:35:30","Python Locking does not work","<python><python-3.x>","1","2","838"
"50644403","2018-06-01 13:19:18","0","","<p>Assuming you are using Windows, write path name as:</p>

<pre><code>img = cv2.imread('C:\\Users\\Desktop\\x.jpg', 0)
</code></pre>

<p>to load the image correctly.</p>
","8278951","","","1","171","Rahul Goswami","2017-07-09 11:06:31","386","64","96","18","50644073","","2018-06-01 13:03:24","-3","1542","<p>I am writing this simple code but this is showing error saying size.width>0&amp;&amp;size.height>0 in function imshow()<code>enter code here</code></p>

<pre><code>import numpy as np
import cv2
img = cv2.imread('C:/Users/Desktop/x.jpg',0)
cv2.namedWindow('image',cv2.WINDOW_NORMAL)
cv2.imshow('image',img)
cv2.namedWindow('image',cv2.WINDOW_NORMAL)
cv2.waitKey(0)
cv2.destroyAllWindows()
</code></pre>
","9869143","","","Opencv Error:(-215)","<python><opencv>","1","6","405"
"50644424","2018-06-01 13:20:34","1","","<p>Complete fix :</p>

<pre><code>    # ctc decoder
    flattened_input_x_width = keras.backend.reshape(input_x_widths, (-1,))
    top_k_decoded, _ = keras.backend.ctc_decode(y_pred, flattened_input_x_width)
    self.decoder = keras.backend.function([input_x, flattened_input_x_width], [top_k_decoded[0]])
    # decoded_sequences = self.decoder([input_x, flattened_input_x_width])
</code></pre>
","2628726","","","0","395","Arsleust","2013-07-29 02:06:48","15","8","0","0","50642676","50643303","2018-06-01 11:43:29","0","560","<p>
I am implementing an OCR with Keras, Tensorflow backend.</p>

<p>I want to use <code>keras.backend.ctc_decode</code> implementation.</p>

<p>I have a model class :</p>

<pre class=""lang-python prettyprint-override""><code>import keras


def ctc_lambda_func(args):
    y_pred, y_true, input_x_width, input_y_width = args
    # the 2 is critical here since the first couple outputs of the RNN
    # tend to be garbage:
    # y_pred = y_pred[:, 2:, :]
    return keras.backend.ctc_batch_cost(y_true, y_pred, input_x_width, input_y_width)


class ModelOcropy(keras.Model):
    def __init__(self, alphabet: str):
        self.img_height = 48
        self.lstm_size = 100
        self.alphabet_size = len(alphabet)

        # check backend input shape (channel first/last)
        if keras.backend.image_data_format() == ""channels_first"":
            input_shape = (1, None, self.img_height)
        else:
            input_shape = (None, self.img_height, 1)

        # data input
        input_x = keras.layers.Input(input_shape, name='x')

        # training inputs
        input_y = keras.layers.Input((None,), name='y')
        input_x_widths = keras.layers.Input([1], name='x_widths')
        input_y_widths = keras.layers.Input([1], name='y_widths')

        # network
        flattened_input_x = keras.layers.Reshape((-1, self.img_height))(input_x)
        bidirectional_lstm = keras.layers.Bidirectional(
            keras.layers.LSTM(self.lstm_size, return_sequences=True, name='lstm'),
            name='bidirectional_lstm'
        )(flattened_input_x)
        dense = keras.layers.Dense(self.alphabet_size, activation='relu')(bidirectional_lstm)
        y_pred = keras.layers.Softmax(name='y_pred')(dense)

        # ctc loss
        ctc = keras.layers.Lambda(ctc_lambda_func, output_shape=[1], name='ctc')(
            [dense, input_y, input_x_widths, input_y_widths]
        )

        # init keras model
        super().__init__(inputs=[input_x, input_x_widths, input_y, input_y_widths], outputs=[y_pred, ctc])

        # ctc decoder
        top_k_decoded, _ = keras.backend.ctc_decode(y_pred, input_x_widths)
        self.decoder = keras.backend.function([input_x, input_x_widths], [top_k_decoded[0]])
        # decoded_sequences = self.decoder([test_input_data, test_input_lengths])
</code></pre>

<p>My use of <code>ctc_decode</code> comes from another post : <a href=""https://stackoverflow.com/questions/45601597/keras-using-lambda-layers-error-with-k-ctc-decode"">Keras using Lambda layers error with K.ctc_decode</a></p>

<p>I get an error :</p>

<p><code>ValueError: Shape must be rank 1 but is rank 2 for 'CTCGreedyDecoder' (op: 'CTCGreedyDecoder') with input shapes: [?,?,7], [?,1].</code></p>

<p>I guess I have to squeeze my <code>input_x_widths</code>, but Keras does not seem to have such function (it always outputs something like <code>(batch_size, 1)</code>)</p>
","2628726","","","Keras ctc_decode shape must be rank 1 but is rank 2","<python><tensorflow><keras>","2","0","2889"
"50644448","2018-06-01 13:21:55","0","","<p>Use <code>AWS_S3_CUSTOM_DOMAIN</code> in Djangp Storage there is option to specify CloudFront URL</p>
","1485355","","","0","105","Hardik Gajjar","2012-06-27 10:39:53","816","180","104","17","34455382","","2015-12-24 16:17:30","2","591","<p>I have configured my django app's default file storage to use <code>boto</code>. </p>

<pre><code>DEFAULT_FILE_STORAGE = 'storages.backends.s3boto.S3BotoStorage
</code></pre>

<p>I also have a model that stores uploaded images to s3</p>

<pre><code>...
profile_pic = models.ImageField(upload_to=get_upload_path, null=True)
...
</code></pre>

<p>However, when I reference this field, it shows up with an S3 url. 
How do I configure this to return a cloudfront address?</p>
","2001811","","","Django: ImageField - Upload to S3 but read from cloudfront?","<python><django><amazon-s3><boto><amazon-cloudfront>","1","1","475"
"50644457","2018-06-01 13:22:19","2","","<p>Try this:</p>

<pre><code>import pandas as pd

Dict1 = {'str1A':'file1', 'str1B':'file1', 'str1C':'file1', 'str1D':'file1', 'str2A':'file2', 'str2B':'file2', 'str2C':'file2', 'str2D':'file2', 'str2D':'file2', 'str3A':'file3', 'str3B':'file3','str3C':'file3', 'str3D':'file3', 'str3D':'file3' , 'str4A':'file4', 'str4B':'file4', 'str4C':'file4', 'str4D':'file4', 'str4E':'file4'}
Dict2 = {'str1A':'jump', 'str1B':'fly', 'str1C':'swim', 'str2A':'jump', 'str2B':'fly', 'str2C':'swim', 'str2D':'run', 'str3A':'jump', 'str3B':'fly', 'str3C':'swim', 'str3D':'run'}
Dict3 = {'str1A':'90', 'str1B':'60', 'str1C':'30', 'str2A':'70', 'str2B':'30', 'str2C':'60', 'str2D':'40', 'str3A':'10', 'str3B':'90', 'str3C':'70', 'str3D':'90'}

data = {}
for k, col in Dict2.items():
    if k not in Dict1 or k not in Dict3:
        continue
    data.setdefault(col, {})[Dict1[k]] = Dict3[k]
df = pd.DataFrame(data, index=sorted(set(Dict1.values())), columns=sorted(set(Dict2.values())))

print(df)
</code></pre>

<p>Output:</p>

<pre><code>       fly jump  run swim
file1   60   90  NaN   30
file2   30   70   40   60
file3   90   10   90   70
file4  NaN  NaN  NaN  NaN
</code></pre>
","1782792","1782792","2018-06-05 10:29:09","6","1166","jdehesa","2012-10-29 11:43:40","37080","2442","2562","26","50644315","50644457","2018-06-01 13:15:11","1","62","<p>In a dictionary with information about a string in a text file, where keys are the strings and values are the names of the files. </p>

<pre><code>Dict1 = {'str1A':'file1', 'str1B':'file1', 'str1C':'file1', 'str1D':'file1', 'str2A':'file2', 'str2B':'file2', 'str2C':'file2', 'str2D':'file2', 'str2D':'file2', 'str3A':'file3', 
</code></pre>

<p>'str3B':'file3','str3C':'file3', 'str3D':'file3', 'str3D':'file3' , 'str4A':'file4', 'str4B':'file4', 'str4C':'file4', 'str4D':'file4', 'str4E':'file4'}</p>

<p>Another dictionary contains information about the best match for the strings from the text.</p>

<pre><code>Dict2 = {'str1A':'jump', 'str1B':'fly', 'str1C':'swim', 'str2A':'jump', 'str2B':'fly', 'str2C':'swim', 'str2D':'run', 'str3A':'jump', 'str3B':'fly', 'str3C':'swim', 'str3D':'run'}
</code></pre>

<p>The third dictionary contains information about the percentage of occurrence of the string in the text.</p>

<pre><code>Dict3 = {'str1A':'90', 'str1B':'60', 'str1C':'30', 'str2A':'70', 'str2B':'30', 'str2C':'60', 'str2D':'40', 'str3A':'10', 'str3B':'90', 'str3C':'70', 'str3D':'90'}
</code></pre>

<p>Now my aims are to use the information of these different dictionaries to generate a dataframe like this:</p>

<pre><code>       jump     fly     swim    run
file1   90      60      30      NA
file2   70      30      60      40
file3   10      90      70      90
</code></pre>

<p>To this, I started the script but I am stuck:</p>

<pre><code>col_file = ['str', 'file']
df_origin = pd.DataFrame(Dict1.items(), columns=col_file)
#print df_origin

col_bmatch = ['str', 'text']
df_bmatch =  pd.DataFrame(Dict2.items(), columns=col_bmatch)
#print df_bmatch

col_percent = ['str', 'percent']
df_percent = pd.DataFrame(Dict3.items(), columns=col_percent)
#print df_percent
</code></pre>

<p>This block was removed from script: </p>

<blockquote>
<pre><code>df_origin['text'] = df_origin['str'].map(df_bmatch.set_index('str')['text'])

df_origin['percent'] = df_origin['str'].map(df_percent.set_index('str')['percent'])
</code></pre>
</blockquote>

<p>And substituted to: </p>

<pre><code>data = {}
for k, col in Dict1.items():
    if k in Dict1 and k not in Dict3:
        data.setdefault(k, {})[col] = ""NA""
    elif k in Dict1 and k in Dict3:
        data.setdefault(k, {})[col] = Dict3[k]

    df = pd.DataFrame(data)

print(df)
</code></pre>

<p>But the final result was not very exact:</p>

<pre><code>      str1A str1B str1C str1D str2A str2B str2C str2D str3A str3B  \
file1     90     60     30     NO    NaN    NaN    NaN    NaN    NaN    NaN   
file2    NaN    NaN    NaN    NaN     70     30     60     40    NaN    NaN   
file3    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN     10     90   
file4    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   

      str3C str3D str4A str4B stre4C str4D str4E  
file1    NaN    NaN    NaN    NaN    NaN    NaN    NaN  
file2    NaN    NaN    NaN    NaN    NaN    NaN    NaN  
file3     70     90    NaN    NaN    NaN    NaN    NaN  
file4    NaN    NaN     NO     NO     NO     NO     NO  
</code></pre>

<p>But the expected table is:</p>

<pre><code>         jump   fly    swim   run   sit
file1    90     60     30     NA    NA
file2    70     30     60     40    NA
file3    10     90     70     90    NA
file4    NA     NA     NA     NA    NA
</code></pre>

<p>Where the string in file4 where not detected.</p>

<p>Blosk removed</p>

<blockquote>
<pre><code>print df_origin

#          str   file  text percent
#    0   str2B  file2   fly      30
#    1   str2C  file2  swim      60
#    2   str3C  file3  swim      70
#    3   str3B  file3   fly      90
#    4   str3D  file3   run      90
#    5   str2D  file2   run      40
#    6   str3A  file3  jump      10
#    7   str1D  file1   NaN     NaN
#    8   str1C  file1  swim      30
#    9   str1B  file1   fly      60
#    10  str1A  file1  jump      90
#    11  str2A  file2  jump      70
</code></pre>
</blockquote>

<p>Here relies the problem</p>

<pre><code>print pd.get_dummies(df_origin.set_index('file')['text']).max(level=0).max(level=0, axis=1)
</code></pre>

<p>But the only result that I get is this:</p>

<pre><code>       fly  jump  run  swim
file                       
file2    1     1    1     1
file3    1     1    1     1
file1    1     1    0     1
</code></pre>

<p>As I can understand, pd.getdummies groups the field 'file' from my df_origin and uses 'text' to check their presence. </p>

<p>How can I redirect the command to plot the columns 'percent' in my df_origin dataframe?</p>
","5775504","5775504","2018-06-04 13:57:51","Merge dictionaries to dataframe get_dummies","<python><pandas><dictionary><dataframe>","2","0","4553"
"50644475","2018-06-01 13:23:01","2","","<p>The C14N serialisation was only ever implemented as an experimental extension to the original ElementTree stand-alone library developed by Fredrik Lundh, and was never part of an official release.</p>

<p>When the 1.3 branch of the library was imported into the Python standard library, the support code for the optional library was included, but the C14N serialisation never was. Later on the optional-component import code <a href=""https://github.com/python/cpython/commit/e6a951b83e30b3b9c809a441041fb0f21f72b168"" rel=""nofollow noreferrer"">was removed altogether</a>, but several mentions of C14N remain in the code. However, the documentation never mentioned C14N and the remaining mentions should just be removed from Python.</p>

<p>However, if you must make it work, you could always <a href=""https://bitbucket.org/effbot/et-2009-provolone/raw/35607128f5bb113cc1d08f70998b5f8543386f8e/elementtree/elementtree/ElementC14N.py"" rel=""nofollow noreferrer"">download the original <code>ElementC14N.py</code> module</a>, and update this to work with the current ElementTree library. This does require some work; the codebase is Python 2 specific and will need to be updated to handle Unicode instead.</p>
","100297","","","0","1207","Martijn Pieters","2009-05-03 14:53:57","770256","252083","5762","19510","43527996","50644475","2017-04-20 19:12:09","3","435","<p>The ElementTree class in the Python <code>xml.etree</code> API has a <code>write()</code> method that documents the optional <code>method</code> argument:</p>

<pre><code>*method* -- either ""xml"" (default), ""html, ""text"", or ""c14n""
</code></pre>

<p>At least with Python 3.5 (on Fedora 25) just writing with this method</p>

<pre><code>import xml.etree.ElementTree as ET
# ... create some elements ...
tree = ET.ElementTree(root)
tree.write(filename, method='c14n')
</code></pre>

<p>just throws a ValueError:</p>

<pre><code>File ""/usr/lib64/python3.5/xml/etree/ElementTree.py"", line 751, in write
  raise ValueError(""unknown method %r"" % method)
ValueError: unknown method 'c14n'
</code></pre>

<p>And the ElementTree code contains this note:</p>

<pre><code>_serialize = {
    ""xml"": _serialize_xml,
    ""html"": _serialize_html,
    ""text"": _serialize_text,
# this optional method is imported at the end of the module
#   ""c14n"": _serialize_c14n,
}
</code></pre>

<p>What is that supposed to mean, exactly?</p>

<p>How to serialize a ElementTree to a c14n XML file in Python?</p>
","427158","5377805","2017-04-22 23:51:54","Write ElementTree as canonicalized XML (c14n)","<python><xml><fedora-25><c14n>","1","0","1086"
"50644484","2018-06-01 13:23:19","1","","<p>when you make it :</p>

<pre><code>font = font.SysFont(""candara"", 35)   
</code></pre>

<p>you rewrite <code>font</code> variable from <code>pygame.font</code> try it for example</p>

<pre><code>candara_font = font.SysFont(""candara"", 35)
</code></pre>

<p>and after this go on with it</p>

<pre><code>arial_font = font.SysFont(""arial"", 25) 
</code></pre>
","7003620","","","0","358","Druta Ruslan","2016-10-11 19:45:06","4826","569","1886","582","50644295","","2018-06-01 13:14:20","0","29","<p>I'm working with pygame and I have text through out my code using </p>

<pre><code>font = font.SysFont(""candara"", 35)   
</code></pre>

<p>using this font, the program is working smoothly and the text shows up  </p>

<p>but when I added another font </p>

<pre><code>font2 = font.SysFont(""arial"", 25) 
</code></pre>

<p>I'm getting an error and the text won't work. </p>

<pre><code>builtins.AttributeError: 'pygame.font.Font' object has no attribute 'SysFont'
</code></pre>

<p>This is where I'm using my new font </p>

<pre><code>def button4(X):                                                
        draw.rect(X,PINK,(50,70,200,50))
        buttonText = font2.render(""Home Country"",False,(0,0,0))
        screen.blit(buttonText,(55,80))
</code></pre>

<p>and this is the beginning of my code; </p>

<pre><code>from pygame import *
import os
os.environ['SDL_VIDEO_WINDOW_POS'] = ""%d, %d"" %(0, 0)
init()
</code></pre>

<p>Any suggestions would be greatly appreciated, thank you.</p>
","9881379","","","Receiving error builtins.AttributeError: 'pygame.font.Font' object has no attribute 'SysFont' for font and can't figure out why?","<python><text><fonts><pygame>","1","0","988"
"50644523","2018-06-01 13:25:40","0","","<p>Can't comment on Mirian's answer because I don't have enough reputation, but from looking at Miriam's link, sklearn actually calls scipy's <code>linalg.svd</code> which is doesn't seem to be the same as np.linalg.svd <a href=""https://stackoverflow.com/questions/32569188/scipy-svd-vs-numpy-svd"">(discussion here)</a></p>

<p>So it may be better to use <code>U, S, V = scipy.linalg.svd(a, full_matrices=True)</code></p>
","7100714","","","0","422","tjann","2016-11-01 16:38:37","21","11","19","0","45865069","","2017-08-24 15:06:43","1","618","<p><a href=""https://en.wikipedia.org/wiki/Singular_value_decomposition"" rel=""nofollow noreferrer"">Singular value decomposition</a> of matrix <code>M</code> of size <code>(M,N)</code> means factoring </p>

<p><a href=""https://i.stack.imgur.com/1cuCL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1cuCL.png"" alt=""enter image description here""></a></p>

<p>How to obtain all three matrices from <code>scikit-learn</code> and <code>numpy</code> package?</p>

<p>I think I can obtain <code>Sigma</code> with <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"" rel=""nofollow noreferrer""><code>PCA</code> model</a>:</p>

<pre><code>import numpy as np
from sklearn.decomposition import PCA

model = PCA(N, copy=True, random_state=0)
model.fit(X)

Sigma = model.singular_values_
Sigma = np.diag(singular_values)
</code></pre>

<p>What about other matrices?</p>
","258483","4909087","2017-08-24 15:14:07","How to get all three SVD matrices with sklearn?","<python><numpy><scikit-learn><pca><svd>","2","0","911"
"50644536","2018-06-01 13:26:32","0","","<pre><code>print(MAMA_element.text)
</code></pre>

<p>You may want to strip it</p>

<pre><code>print(MAMA_element.text.strip())
</code></pre>
","7200715","","","1","142","Arount","2016-11-23 14:51:24","5875","559","82","60","50644508","","2018-06-01 13:24:31","1","598","<p>I am trying to get this code to print the values for Ehlers MAMA by scrapping data from the tradingview website. here is my code. It returns a blank array when I run it.</p>

<pre><code>from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

# example option: add 'incognito' command line arg to options
option = webdriver.ChromeOptions()
option.add_argument(""--incognito"")

# create new instance of chrome in incognito mode
browser = webdriver.Chrome(chrome_options=option)

# go to website of interest
browser.get(""https://www.tradingview.com/chart/vKzVQllW/#"")

# wait up to 10 seconds for page to load
timeout = 10
try:
    WebDriverWait(browser, timeout).until(EC.visibility_of_element_located((By.XPATH, ""/html/body/div[1]"")))
except TimeoutException:
    print(""Timed out waiting for page to load"")
    browser.quit()

# get all of the titles for the financial values
MAMA_element = browser.find_elements_by_xpath(""/html/body/div[1]/div[1]/div/div[1]/div[2]/table/tbody/tr[1]/td[2]/div/div[3]/div[3]/div/span[3]/span"")

print(MAMA_element)
</code></pre>

<p>this is a sample of what i'm trying to print the ""7498.984"". but like I said I get a blank array returned</p>

<pre><code>&lt;span class=""pane-legend-item-value pane-legend-line xh-highlight"" style=""color: rgb(255, 0, 255);""&gt;7498.984&lt;/span&gt;
</code></pre>
","9879211","","","data from tradingview with selenium python","<python><selenium>","1","0","1529"
"50644560","2018-06-01 13:28:17","0","","<pre><code>C:\Users\UserName\AppData\Roaming\nltk_data\corpora
</code></pre>

<p>I used Anaconda Platform, with conda environment... my corpora location</p>
","7473616","2087247","2018-06-01 17:45:00","1","157","Kiran Maharjan","2017-01-26 12:05:35","37","2","8","0","17335928","17350294","2013-06-27 06:24:53","0","1983","<p>I am using the Natural Language Toolkit for python to write a program. In it I am trying to load a corpus of my own files. To do that I am using code to the following effect:</p>

<pre><code>from nltk.corpus import PlaintextCorpusReader
corpus_root=(insert filepath here)
wordlists=PlaintextCorpusReader(corpus_root, '.*')
</code></pre>

<p>Let's say my file is called reader.py and my corpus of files is located in a directory called 'corpus' in the same directory as reader.py. I would like to know a way to generalize finding the filepath above, so that my code could find the path for the 'corpus' directory for any location for anyone using the code. I have tried these posts, but they only allow me to get absolute file paths:
<a href=""https://stackoverflow.com/questions/5137497/find-current-directory-and-files-directory"">Find current directory and file&#39;s directory</a></p>

<p>Any help would be greatly appreciated!</p>
","1927885","-1","2017-05-23 12:11:16","Finding path for corpus in NLTK","<python><nltk><filepath>","2","0","936"
"50644579","2018-06-01 13:29:40","1","","<p>You can do it in two lines. Firstly you could simply transpose your dataset so that it's in a shape that you want to plot it:</p>

<pre><code>df_plot = df.set_index('age').T
</code></pre>

<p>this produces(numbers are randomly generated and differ from the ones you've provided):</p>

<pre><code>age          1       2       3       4       5
class1  0.5377  0.2147  0.4837  0.8682  0.3429
class2  0.8350  0.0544  0.4314  0.6592  0.6475
class3  0.9382  0.0283  0.7152  0.0962  0.3012
class4  0.7277  0.1523  0.3124  0.0077  0.4039
class5  0.7580  0.4149  0.1352  0.5068  0.2955
class6  0.3243  0.3346  0.2820  0.8481  0.9782
class7  0.2298  0.0522  0.7307  0.9851  0.8681
class8  0.3283  0.0562  0.9052  0.6320  0.6140
</code></pre>

<p>Then produce a plot by calling the inbuilt plot function:</p>

<pre><code>df_plot.plot(figsize=(10,6), xticks=range(0, 8)).legend(title='age', bbox_to_anchor=(1, 1))
</code></pre>

<p>this results in:
<a href=""https://i.stack.imgur.com/24tbR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/24tbR.png"" alt=""enter image description here""></a></p>
","4357954","","","0","1104","user59271","2014-12-13 19:43:17","171","15","67","0","50642098","50644579","2018-06-01 11:09:32","2","5109","<p>I'm currently working on the below dataframe.</p>

<p><a href=""https://i.stack.imgur.com/JVcwo.png"" rel=""nofollow noreferrer"">Dataframe</a></p>

<p>To summarize the contents, there is an ""age"" column, which relates to an age group (i.e. 16-25, 26-32) - and then 8 class % values, these are percentage values which tell me what percentage of a certain age group are in that specific social class. So in this example, there are 10.81%(rounded) of the people in age group with the ID of 1 that are also in the social class with an ID of 1. For the same age group, there are 22.34% in the social class with an ID of 2, and so on, so forth. Each row totals to 100%.</p>

<p>I am looking to create a line graph, which has one line representing each age group. So this graph should have a total of 5 lines.</p>

<p>The X-Axis should represent the Social classes (so ranging 1 through 8), and the Y-Axis should represent the percentage of people in that class.</p>

<p>I'm looking for the graph in this format to make it clear to see for each distinct age group, the patterns in how many people are in each social class, and how this changes as you get older.</p>

<p>Any help with this would be appreciated, I'm not even sure where to start? I've tried some examples online but nothing seems to work. Even a starter would be great.</p>

<p>Thanks.</p>
","6329328","","","Plot line graph from Pandas dataframe (with multiple lines)","<python><pandas><dataframe><graph><line>","3","0","1348"
"50644591","2018-06-01 13:30:34","0","","<pre><code>def compare(one,two):
    if set(one.keys()) != set(two.keys()):
        main_key_added = set(two.keys()) - set(one.keys())
        main_key_removed = set(one.keys()) - set(two.keys())
        print(""The main key {} where added"".format(main_key_added))
        print(""The main key {} where removed"".format(main_key_removed))
        return False

    for mainkey in one:
        if set(one[mainkey].keys()) != set(two[mainkey].keys()):
            second_key_added = set(two[mainkey].keys()) - set(one[mainkey].keys())
            second_key_removed = set(one[mainkey].keys()) - set(two[mainkey].keys())
            print(""The second key {} where added for main key {}"".format(second_key_added, mainkey))
            print(""The second key {} where removed for main key"".format(second_key_removed, mainkey))
            return False

        for subkey in one[mainkey]:
            if not set(one[mainkey][subkey]) ^ set(two[mainkey][subkey]):
                return False

    return True
</code></pre>
","7529716","7529716","2018-06-01 14:11:58","3","1014","Yassine Faris","2017-02-07 15:47:46","735","56","111","96","50643953","50655934","2018-06-01 12:57:01","1","101","<p>Have this dict -> dict -> list structure</p>

<p>Want to compare 2 structures of this kind.</p>

<pre><code>one = {""1iG5NDGVre"": {""118"": [""test1"", ""test2"", ""test3"", ""tcp"", ""22"", ""Red"", ""0.0.0.0/0""]}}
two = {""1iG5NDGVre"": {""118"": [""test1"", ""test2"", ""test3"", ""tcp"", ""22"", ""Red"", ""Blue"", ""0.0.0.0/0""]}}
</code></pre>

<p>This code works well:</p>

<pre><code>def compare(one,two):
    for mainkey in one:
        for subkey in one[mainkey]:
            return set(one[mainkey][subkey]) ^ set(two[mainkey][subkey])
</code></pre>

<p>However when dict -> dict have more or less keys it should be returned by a function that key was added or removed along with all list values.</p>

<p>Also if list was modified, it should be returned by the program that the list was modified.</p>

<p>Anybody can help on this?</p>

<p>It is pretty much used to compare two JSONs, I want to see when keys were removed, added or its values modified.</p>

<p><strong>Update 1:</strong></p>

<p>I am still learning Python</p>

<p>For this structure:</p>

<pre><code>one = {""1iG5NDGVre"": {""118"": [""test1"", ""test2"", ""test3"", ""tcp"", ""22"", ""Red"", ""0.0.0.0/0""]}}
two = {""1iG5NDGVre"": {""118"": [""test1"", ""test2"", ""test3"", ""tcp"", ""22"", ""Red"", ""Blue"", ""0.0.0.0/0""]},""119"": [""test10"",""test11""]}
</code></pre>

<p>It does not work.</p>

<p>It should print as output:</p>

<pre class=""lang-none prettyprint-override""><code>118 was modified. New values Blue. 119 was added with values test10 test11
</code></pre>

<p>and for these scenarios:</p>

<p>1.</p>

<pre><code>one = {""1iG5NDGVre"": {""118"": [""test1"", ""test2"", ""test3"", ""tcp"", ""22"", ""Red"", ""0.0.0.0/0""]}}
two = {""1iG5NDGVre"": }
</code></pre>

<p>Should print as output:</p>

<pre class=""lang-none prettyprint-override""><code>118 was removed with values test1 test2 test3 tcp 22 Red 0.0.0.0/0
</code></pre>

<p>2.</p>

<pre><code>one = {""1iG5NDGVre"": {""118"": [""test1"", ""test2"", ""test3"", ""tcp"", ""22"", ""Red"", ""0.0.0.0/0""]}}
two = {""1iG5NDGVre"": {""118"": [""test100"", ""test200"", ""test3"", ""tcp"", ""22"", ""Red"", ""Blue"", ""0.0.0.0/0""]},""119"": [""test10"",""test11""]}
</code></pre>

<p>Should print as output:</p>

<pre class=""lang-none prettyprint-override""><code>118 was modifed. New values test100 test200
</code></pre>

<p>I want to cover all possible cases. I do this as I said for JSON comparison.</p>
","5376493","5376493","2018-06-01 17:31:21","Dictionary Structure (dict -> dict) with a list in it comparison","<python><python-2.7><dictionary>","2","3","2313"
"50644642","2018-06-01 13:33:08","1","","<p>simple remove <code>%f</code></p>

<pre><code>dt.strftime('%H:%M:%S')
</code></pre>
","7003620","","","0","87","Druta Ruslan","2016-10-11 19:45:06","4826","569","1886","582","50644610","","2018-06-01 13:31:22","0","158","<p>How to round time in format <strong>%HH:%MM:%SS.%f</strong> to <strong>%HH:%MM:%SS</strong> ?</p>

<p>e.g: <code>23:45:21.630</code> to <code>23:45:22</code></p>
","9740447","","","Round time in Python","<python><datetime><time>","2","0","165"
"50644646","2018-06-01 13:33:20","1","","<p>use <code>strftime()</code> formatting function. For example:</p>

<pre><code>datetime.datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")
</code></pre>
","9305895","","","0","149","Kajbo","2018-02-02 15:23:19","320","53","1581","31","50644610","","2018-06-01 13:31:22","0","158","<p>How to round time in format <strong>%HH:%MM:%SS.%f</strong> to <strong>%HH:%MM:%SS</strong> ?</p>

<p>e.g: <code>23:45:21.630</code> to <code>23:45:22</code></p>
","9740447","","","Round time in Python","<python><datetime><time>","2","0","165"
"50644755","2018-06-01 13:40:29","1","","<p>You can unnest your string with <code>Series</code> after <code>str.split</code></p>

<pre><code>df.set_index('id').t.str.split(r'\n').apply(pd.Series).stack().reset_index(level=1,drop=True).to_frame('t')
Out[177]: 
                 t
id                
1   very long text
1      text line 2
1      text line 3
2       short text
</code></pre>
","7964527","","","0","347","WeNYoBen","2017-05-04 16:45:29","164847","15327","4764","689","50644066","","2018-06-01 13:02:56","6","3167","<p>Good day,</p>

<p>Is it possible to get multi line cell output when using pandas DataFrame in a shell? I understand the whole row will have height more than 1 char, but that's what I actually want.</p>

<p>Example:</p>

<pre><code>data = [
       {'id': 1, 't': 'very long text\ntext line 2\ntext line 3'},
       {'id': 2, 't': 'short text'}
       ]
df = pd.DataFrame(data)
df.set_index('id', inplace=True)
print(df)
</code></pre>

<p>and want to get output:</p>

<pre>
id                  t
1      very long text
       text line 2
       text line 3
2      short text
</pre>

<p>instead of</p>

<pre>
id                                         t
1     very long text\ntextline 2\ntext line3
2                                 short text
</pre>
","179859","","","pandas dataframe and multi line values","<python><pandas><dataframe>","3","0","750"
"50644762","2018-06-01 13:40:55","2","","<p>It seems the problem is only the number of coefficients of the filter: 5 was too low.</p>

<p>With </p>

<pre><code>b = signal.firwin(101, cutoff=1000, fs=sr, pass_zero=False)
</code></pre>

<p>it works far better.</p>

<p>Note: the audio will be nearly zero during the first 101 samples, so we should probably zero-pad the WAV file at the beginning and end, then apply the filter, then crop the file to remove the zero-padded parts.</p>
","1422096","","","4","441","Basj","2012-05-28 16:27:42","8040","2782","1843","359","50644578","","2018-06-01 13:29:38","2","1515","<p>I'm trying to apply a high-pass filter (cutoff: 1000 Hz) to a mono 16-bit 44.1 Khz WAV file with <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.firwin.html"" rel=""nofollow noreferrer""><code>scipy.signal.firwin</code></a>:</p>

<pre><code>from scipy.io import wavfile
from scipy import signal
import numpy as np

sr, x = wavfile.read('test.wav')      # 16-bit mono 44.1 khz

b = signal.firwin(5, cutoff=1000, fs=sr, pass_zero=False)

x = signal.lfilter(b, [1.0], x)

wavfile.write('test2.wav', sr, x.astype(np.int16))
</code></pre>

<p>The result is totally similar to the input (but not exactly equal), i.e. no high-pass has been applied. </p>

<p><strong>What should I modify to make this high-pass with <code>firwin</code> work?</strong></p>
","1422096","1422096","2018-06-01 13:33:01","Apply a high-pass filter to a WAV file with scipy.signal.firwin","<python><audio><scipy><signal-processing>","1","5","774"
"50644769","2018-06-01 13:41:16","0","","<p>Okay, you're on the right track. The div should be reloading correctly, if not try wrapping it in adding body and head tags, that helped me one time. It is now up to you to return any data you want to the webpage. Say you want to display time:
In python import the datetime module using : <code>import datetime</code>
and make it the  index function with:</p>

<pre><code>def index():
    return render_template('index.html', title='Estat', time = str(datetime.datetime.now()))
</code></pre>

<p>then modify your div to be:
    <code>&lt;div id=""display""&gt;&lt;h1&gt;{{ time}}&lt;/h1&gt;&lt;/div&gt;</code></p>

<p>now your webpage should be returning the time, and refreshing every second</p>
","7705970","","","5","698","Michael Ilie","2017-03-14 01:02:53","369","80","58","11","50644215","","2018-06-01 13:10:13","0","52","<p>I'm trying to use jquery .load in my flash application to update the div, but that does not work.</p>

<p>routes.py</p>

<pre><code>@app.route('/index', methods=['GET', 'POST'])
@app.route('/', methods=['GET', 'POST'])
def red():  
    return redirect('/index.html')

@app.route('/index.html', methods=['GET', 'POST'])
def index():
    return render_template('index.html', title='Estat')
</code></pre>

<p>index.html</p>

<pre><code> &lt;script&gt;
     function run() {
    $( ""#display"" ).load( ""/index.html #display"" );
    }

    setInterval(run, 1000);
    &lt;/script&gt;

&lt;div id=""display""&gt;&lt;h1&gt;word 2&lt;/h1&gt;&lt;/div&gt; 
</code></pre>

<p>I want each time when changing the template file in a time interval the content of the div block changes. Help me pls.</p>
","9881305","","","Jquery .load for update fragment on flask\python","<javascript><python><jquery><html><flask>","1","2","788"
"50644803","2018-06-01 13:43:02","3","","<p>It can be tricked by adding a special setter for the property that only accepts the original <em>object</em>.</p>

<p>Class <code>Beta</code> would become:</p>

<pre><code>class Beta:
    def __init__( self ):
        self.__value: Alpha = Alpha(1)

    def _get_val( self ) -&gt; Alpha:
        return self.__value
    def _set_val( self, val: Alpha):
        if not (val is self.__value):            # only accept the existing object
            raise AttributeError(""can't set attribute"")
    value = property(_get_val, _set_val)
</code></pre>

<p>With that hack/trick on, you can successfully use:</p>

<pre><code>&gt;&gt;&gt; beta = Beta()
&gt;&gt;&gt; beta.value.content
1
&gt;&gt;&gt; beta.value = Alpha(2)               # property IS read only
Traceback (most recent call last):
  File ""&lt;pyshell#86&gt;"", line 1, in &lt;module&gt;
    beta.value = Alpha(2)
  File ""&lt;pyshell#78&gt;"", line 9, in _set_val
    raise AttributeError(""can't set attribute"")
AttributeError: can't set attribute
&gt;&gt;&gt; beta.value.content                  # and was not changed by an assignment attempt
1
&gt;&gt;&gt; beta.value += 2                     # but accepts augmented assignment
&gt;&gt;&gt; beta.value.content
3
</code></pre>
","3545273","","","0","1234","Serge Ballesta","2014-04-17 12:25:02","90494","5432","1346","480","50643742","50644803","2018-06-01 12:44:28","1","48","<p><a href=""https://stackoverflow.com/questions/11987949"">This question</a> deals with <code>__iadd__</code> on Python read-write properties. However, I'm struggling to find the solution for read-only properties.</p>

<p>In my MWE we have a read-only property <code>Beta.value</code>, returning an <code>Alpha</code> instance. I imagine I should be able to use <code>__iadd__</code> on <code>Beta.value</code> because the <em>returned</em> value is mutated in-place, and no change is made to <code>Beta</code> itself, much like the ""<code>beta.value.content +=</code>"" line preceding it. However the following code crashes with an <code>AttributeError: can't set attribute</code>.</p>

<p><strong>Is it possible to use <code>__iadd__</code> on read-only properties?</strong></p>

<pre><code>class Alpha:
    def __init__( self, content : int ) -&gt; None:
        self.content : int = content


    def __iadd__( self, other : int ) -&gt; ""Alpha"":
        self.content += other
        return self


class Beta:
    def __init__( self ):
        self.__value: Alpha = Alpha(1)


    @property
    def value( self ) -&gt; Alpha:
        return self.__value


beta = Beta()
beta.value.content += 2
beta.value += 2
</code></pre>
","3601660","2988730","2018-06-01 13:20:40","__iadd__ in Python with read only property","<python>","2","3","1226"
"50644812","2018-06-01 13:43:27","0","","<p>You don't talk to Python classes from AngularJS. Your Python app and your Angular app are two separate programs that don't even run on the same computer.</p>

<p>The challenge here is to invent a ""language"" that these two programs can communicate. There's a good amount of ways you could design such a language, HTTP is a great default pick.</p>

<p>Your back-end is using Django and Django is quite good at understanding and responding to HTTP requests. Take a day or two to go over the Django tutorial, figure out how to make a Django app, then think what HTTP endpoint your app needs to be able to handle. Add it to your backend, test it a little, then move over to the Angular app.</p>

<blockquote>
  <p>Careful though - Django apps normally don't rely on global state stored in a class like <code>class Production</code> and instead store the state in databases.</p>
</blockquote>

<p>Once you have the backend in place, use Angular's <a href=""https://docs.angularjs.org/api/ng/service/$http"" rel=""nofollow noreferrer""><code>$http</code> service</a> to send requests to the address you just added. </p>

<p>I'm a bit vague (sorry) but I'm assuming here that you're new to Django at the moment, and the tutorial will be more helpful than I could possibly be. Good luck!</p>
","399317","","","1","1282","Kos","2010-07-22 16:02:26","52562","4785","2727","173","50644641","","2018-06-01 13:33:05","0","38","<p>I am currently working on a web based application, for which the back end has been developed using Django &amp; Python, and the front end has been developed using AngularJS.</p>

<p>There is a form on one of the pages in the front end, which the user can use to export data that has been recorded &amp; stored in the back end to a spreadsheet.</p>

<p>All of the data recorded in the back end is 'time sensitive', and at present, there is only one time zone set in the back end (UTC). I want to 'create' settings for adding multiple new time zones, so that the data recorded can be exported in different time zones.</p>

<p>The time zone is set in project/apps/appAbc/abc/settings.py with:</p>

<pre><code>class Production(Settings):
    ...
    TIME_ZONE = 'UTC'
    ...
</code></pre>

<p>The 'Export' form has a number of fields (preset, type of data, sort data by quantity/ period, file type to export to), and I have just added a 'Timezone' drop down to it:</p>

<pre><code>&lt;section class=""panel panel-brand export-page panel-no-border-radius""&gt;
&lt;div class=""panel-body panel-body-fixed-md""&gt;
    ...
    &lt;div class=""divider""&gt;&lt;/div&gt;
    &lt;div class=""row""&gt;
        &lt;label data-i18n=""Timezone:"" class=""col-sm-3""&gt;&lt;/label&gt;
        &lt;div ul-timezone-picker callback=""selectTimezone"" class=""col-sm-9"" default-label={{presetTimezone}} btn-class=""btn-xs btn-default""&gt;&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=""divider""&gt;&lt;/div&gt;
    &lt;div class=""row"" ul-scroll-to-me=""scrollToFtype""&gt;
        &lt;label data-i18n=""File type:"" class=""col-sm-3""&gt;&lt;/label&gt;
        &lt;div ul-options-menu class=""col-sm-9"" options=""ftypeOptions""
             callback=""selectFType"" warning=""{{ftypeWarning}}""
             btn-class=""btn-xs btn-default"" default-idx=""0"" default-label=""{{presetFtype}}""&gt;&lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=""divider divider-lg""&gt;&lt;/div&gt;
    &lt;div class=""row""&gt;
        &lt;div class=""col-sm-4""&gt;&lt;/div&gt;
        &lt;div class=""col-sm-4""&gt;
            &lt;button class=""btn btn-sm btn-block btn-brand"" ng-hide=""!btnDisabled"" ng-click=""setDownloadUrl(true)""&gt;
                &lt;i class=""ti-export""&gt;&lt;/i&gt;
                &lt;span data-i18n=""Export""&gt;&lt;/span&gt;
            &lt;/button&gt;
            &lt;a class=""btn btn-sm btn-block btn-brand"" role=""button"" target=""_blank"" href=""{{downloadURL}}"" ng-hide=""btnDisabled""&gt;
                &lt;i class=""ti-export""&gt;&lt;/i&gt;
                &lt;span data-i18n=""Export""&gt;&lt;/span&gt;
            &lt;/a&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
</code></pre>

<p></p>

<p>and in Pages/directive.js, I have added multiple timezones to the <code>ulTimezonePicker</code> controller:</p>

<pre><code>.directive('ulTimezonePicker', function() {
    return {
        ...
        controller: function ($scope) {
            $scope.options = [{label: 'UTC', value: 0},
            // Add in other timezones
            {label: 'GMT', value: 1},
            {label: 'BST', value: 2},
        ];
        ...
    ...
})
</code></pre>

<p>The <code>selectTimezone()</code> function that is called by the <code>ul-timezone-picker</code> field on the form is defined in project/libs/front/app/scripts/app/pages/exportCtrl.js with:</p>

<pre><code>angular.module('app.pages').controller('ExportCtrl',
    function(...) {
        ...
        $scope.selectTimezone = function(option){
            if(option) {
                $scope.timezone = option;
                if(option.label != $scope.presetTimezone) {
                    $scope.presetTimezone = undefined;
                }
            }
        };
    ...
    }
}
</code></pre>

<p>So, my question is, how do I get the <code>selectTimezone()</code> function to actually change the value of the <code>TIME_ZONE</code> variable defined in settings.py?</p>

<p>I tried declaring an instance of the variable within the function with:</p>

<pre><code>Production.TIME_ZONE = $scope.timezone;
</code></pre>

<p>But when I do this, and view the page that has the form on it in a browser, the browser console gives me an error stating:</p>

<blockquote>
  <p>ReferenceError: Production is not defined</p>
</blockquote>

<p>How do I actually get hold of that Python class from within the AngularJS function, to change the value of one of its variables?</p>
","1841758","","","AngularJS- how to change value of backend Python variable from AngularJS frontend function?","<javascript><python><angularjs><variables><web-frontend>","1","5","4384"
"50644830","2018-06-01 13:44:02","0","","<p>OK, looks like the web services documentation is wrong, and metadata simply needs to be sent as parameters. Moreover, I found in another request that you shouldn't set the header. So I was starting from a wrong example.</p>
","9640238","","","0","227","mrgou","2018-04-13 07:46:49","123","12","1","0","50642442","","2018-06-01 11:29:36","0","44","<p>I'm trying to code the upload of a file to a web service through a REST API in Python. The service's documentation shows a example using curl as client:</p>

<pre><code>curl -X POST -H \
-H ""Content-Type: multipart/form-data"" \
-F ""file=filename.ext"" \
-F ""property1=value1"" \
-F ""property2=value2"" \
-F ""property3=value3"" \
https://domain/api/endpoint
</code></pre>

<p>The difficulty for me is that this syntax doesn't match multipart form-data examples I found, including the <code>requests</code> documentation. I tried this, which doesn't work (rejected by the API):</p>

<pre><code>import requests

file_data = [
    (""file"", ""filename.ext""),
    (""property1"", ""value1""),
    (""property2"", ""value2""),
    (""property3"", ""value3""),
]

response = requests.post(""https://domain/api/endpoint"",
    headers={""Content-Type"": ""multipart/form-data""}, files=file_data)
</code></pre>

<p>With the error: ""org.apache.commons.fileupload.FileUploadException: the request was rejected because no multipart boundary was found""</p>

<p>Can anybody help in transposing that curl example to proper Python code?</p>

<p>Thanks!</p>

<p>R.</p>
","9640238","9640238","2018-06-01 13:20:48","Posting a file with a REST API: from a curl example to Python code","<python><rest><http><post>","1","0","1132"
"50644881","2018-06-01 13:46:53","0","","<p>Assuming that your gdb is attached to a target process (as is usually the case), you are looking at the wrong memory space.
<code>id(a)</code> gives object address in gdb's memory space, while <code>x</code> command looks into target memory space.</p>

<p>If your case, the target seems to also be a 32 bit process, which is why your addresses are truncated.</p>
","638752","","","0","366","theamk","2011-03-01 04:15:37","1266","58","38","14","50607748","","2018-05-30 14:53:15","1","69","<p>I'm trying to view python variable with gdb.</p>

<pre><code>(gdb) python print(hex(id(a)))
0x7f3ca4f68c20
(gdb) x 0x7f3ca4f68c20
0xa4f68c20: Cannot access memory at address 0xa4f68c20
</code></pre>

<p>Could someone explain me why gdb is trying to access <code>0xa4f68c20</code> when I asked it to look at <code>0x7f3ca4f68c20</code>?</p>

<p>Thanks.</p>
","2281274","2281274","2018-05-30 15:08:57","How to force gdb to view at a given address","<python><gdb><heap-memory>","1","9","359"
"50644903","2018-06-01 13:48:02","1","","<pre><code># standardize datetime format for comparison
datadf['WeekEnding'] = pd.to_datetime(datadf.WeekEnding, format='%m/%d/%Y %H:%M')
datadf['ReleaseDate'] = pd.to_datetime(datadf.ReleaseDate, format='%m/%d/%Y')

# replace weekending with release date if smaller
datadf['WeekEnding'] = datadf['WeekEnding'].where(
    datadf['WeekEnding'] &gt; datadf['ReleaseDate'], datadf['ReleaseDate']
)

datadf.groupby(
    ['TitleCode', 'ReleaseDate', 'WeekEnding']
).TotalUnits.sum().reset_index()
</code></pre>

<p><a href=""https://i.stack.imgur.com/N4zY0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/N4zY0.png"" alt=""enter image description here""></a></p>
","4983450","","","0","673","Psidom","2015-06-07 13:40:28","137976","6288","3736","111","50644615","50644903","2018-06-01 13:31:43","0","28","<p>I have the following dataframes:</p>

<pre><code>import pandas as pd

from io import StringIO
data = StringIO(""""""TitleCode,ReleaseDate,WeekEnding,TotalUnits
A,12/16/2017,12/2/2017 0:00,5
A,12/16/2017,12/9/2017 0:00,10
A,12/16/2017,12/16/2017 0:00,2
A,12/16/2017,12/23/2017 0:00,5
A,12/16/2017,12/30/2017 0:00,4
B,1/6/2018,1/13/2017 0:00,4
B,1/6/2018,1/20/2017 0:00,2
"""""")


result = StringIO(""""""TitleCode,ReleaseDate,WeekEnding,TotalUnits
A,12/16/2017,12/16/2017 0:00,17
A,12/16/2017,12/23/2017 0:00,5
A,12/16/2017,12/30/2017 0:00,4
B,1/6/2018,1/13/2017 0:00,4
B,1/6/2018,1/13/2017 0:00,2
"""""")
datadf = pd.read_csv(data, parse_dates=True)
resultdf = pd.read_csv(result, parse_dates=True)

datadf
    TitleCode   ReleaseDate WeekEnding  TotalUnits
0   A   12/16/2017  12/2/2017 0:00  5
1   A   12/16/2017  12/9/2017 0:00  10
2   A   12/16/2017  12/16/2017 0:00 2
3   A   12/16/2017  12/23/2017 0:00 5
4   A   12/16/2017  12/30/2017 0:00 4
5   B   1/6/2018    1/13/2017 0:00  4
6   B   1/6/2018    1/13/2017 0:00  2

resultdf
    TitleCode   ReleaseDate WeekEnding  TotalUnits
0   A   12/16/2017  12/16/2017 0:00 17
1   A   12/16/2017  12/23/2017 0:00 5
2   A   12/16/2017  12/30/2017 0:00 4
3   B   1/6/2018    1/13/2017 0:00  4
4   B   1/6/2018    1/20/2017 0:00  2
</code></pre>

<p>The datadf dataframe shows item sales by week, and the release date of the item. I want to group together all pre-sell sales, that is, sales that occur before the release date (resultdf).</p>

<p>The only way I can think of doing it is by looping over the dataframe but there must be a more efficient way of doing this.</p>

<p>Thanks!</p>
","5405545","","","Grouping items before a certain date","<python><pandas>","1","0","1627"
"50644916","2018-06-01 13:48:55","4","","<p>Use <code>&amp;</code> to have multiple conditions with <code>np.where</code>:</p>

<pre><code>array[np.where((inf &lt; array) &amp; (array &lt; sup))[0]] -= 10
</code></pre>

<p>Or without <code>np.where</code>:</p>

<pre><code>array[(inf &lt; array) &amp; (array &lt; sup)] -= 10
</code></pre>
","8472377","8472377","2018-06-01 14:23:52","2","299","Austin","2017-08-16 11:54:23","18860","1780","141","2099","50644639","50644916","2018-06-01 13:32:49","2","262","<p>Given an array of unsorted points I need to replace those in a given interval. Easiest way I think of is</p>

<pre><code>import numpy as np

def v1(array,inf,sup):
    for i in range(len(array)):
        if inf&lt;array[i]&lt;sup:
            array[i]-=10
    return array
</code></pre>

<p>I was suggested to use <code>np.where</code>. It works smoothly if there is only one boolean condition:</p>

<pre><code>def v2(array,sup):
    array[np.where(array &lt; sup)[0]]-=10
    return array
</code></pre>

<p>But the same setup with <code>inf</code> and <code>sup</code> value, i.e. </p>

<pre><code>array[np.where(inf &lt; array &lt; sup)[0]]-=10
</code></pre>

<p>will raise an error</p>

<pre><code>ValueError: The truth value of an array with more than one element is ambiguous.
Use a.any() or a.all().
</code></pre>

<p>I have to do something clumsy like using <code>np.where</code> twice with two conditions and intersecate the two resulting indexes arrays...</p>

<pre><code>def v2(array,inf,sup):
    i=list(set.intersection(set(np.where(array&gt;inf)[0]),set(np.where(array&lt;sup)[0])))
    array[i]-=10
    return array
</code></pre>

<p>Suggestions?</p>
","3487233","","","Find elements in array such that inf < element < sup in Python","<python><arrays><numpy>","2","1","1168"
"50644991","2018-06-01 13:53:00","1","","<p>Two parts to this answer:</p>

<ol>
<li><p>Python unittests are pretty explicit. To test multiple views, you need to make multiple requests. One request per view. You could put the reverse names in a list and loop through them and do an assert for each in the loop.</p></li>
<li><p>You shouldn't test more than one individual view at a time. Their called unittests for a reason. You're testing an individual unit for an individual behavior. I would set it up where each view has it's own TestCase class. Then, for each of those test cases, test that that individual view redirects if not logged in. So you'll end up having a TestCase with a test method for each view and, therfore, you'll have one or more test methods that tests login validation for each view.</p></li>
</ol>

<p>The philosophy behind unittests is that each test should only test a tiny chunk of behavior. To ensure you're doing this, you should only have 1 assertion per test method. Only in extreme cases should you have more than one, and if so, they should all be very connected to each other logically.</p>

<p>---------------------------------------Edit-------------------------------------</p>

<p>If you want to know what views don't have the login_required/LoginRequiredMixin implementation, I don't think there's anything that can help you in that beyond searching via the editors search engine. I could be wrong so I'm not saying this for certain, but I think you're just gonna have to use file search.</p>

<p>You could write a script that looks through all url.py files and loops through every url:</p>

<pre><code>for url in app1.urls.urlpatterns: # loop through urls in ""urlpatterns = [url(...), url(...), ...]""
    url_name = &lt;get_url_name_from_pattern&gt;
    response = # request to url without login
    if response.status_code = 302: print ""this view has login required""
    else: print ""this view doesn't have login required""

for url in app2.urls.urlpatterns: # loop through urls in ""urlpatterns = [url(...), url(...), ...]""
    url_name = &lt;get_url_name_from_pattern&gt;
    response = # request to url without login
    if response.status_code = 302: print ""this view has login required""
    else: print ""this view doesn't have login required""

for url in app3.urls.urlpatterns: ...
</code></pre>

<p>You'll have to look up how to get the reverse name for a urlpattern. I've never done that.</p>
","4452004","4452004","2018-06-01 20:39:08","1","2396","Ian Kirkpatrick","2015-01-14 05:33:06","936","102","111","4","50643767","","2018-06-01 12:45:43","0","153","<p>Can I test multiple views against one condition? For example if all of these views uses <code>LoginRequiredMixin</code> or <code>@login_required</code> decorator?</p>

<p>This is just for one url which calls <code>IndexView</code>.</p>

<pre><code>class IndexTest(TestCase):
    def setUp(self):
        self.user = User.objects.create(username='testuser')

    def test_login_required(self):
        response = self.client.get(reverse(""profiles:user_filter""))
        self.assertRedirects(response,reverse(""account_login"")+""?next={}"".format(reverse(""profiles:user_filter"")))
</code></pre>

<p>I would like to test if all views except one or two has <code>LoginRequiredMixin</code> so if I create new view in future, tests will fail if I forgot to use <code>LoginRequiredMixin</code>.</p>
","2607447","","","Django - test multiple views at once","<python><django><unit-testing><django-views><django-testing>","1","0","792"
"50644999","2018-06-01 13:53:39","3","","<p>i'm sorry but maybe you need to import in this way ?</p>

<pre><code>from bs4 import BeautifulSoup
</code></pre>

<p><a href=""https://www.crummy.com/software/BeautifulSoup/bs4/doc/#"" rel=""nofollow noreferrer"">docs</a></p>
","7003620","","","0","225","Druta Ruslan","2016-10-11 19:45:06","4826","569","1886","582","50644849","50644999","2018-06-01 13:45:03","0","63","<p>I installed anaconda to <code>C:\Users\chris\Anaconda3</code>. When I type <code>conda list</code> it verifies that I have <code>BeautifulSoup4</code> installed. </p>

<p>However when I start <code>C:\Users\chris\Anaconda3\python.exe</code> and try to import BeautifulSoup it doesn't work:</p>

<pre><code>&gt;&gt;&gt; import BeautifulSoup
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ModuleNotFoundError: No module named 'BeautifulSoup'
</code></pre>

<p>When I'm in the Anaconda Navigator it also lists the package but when I try to start <code>base(root)/Open with python</code> I can't import my package. Neither can Spyder that I installed.
What do I have to do, to fix this?</p>
","25282","","","Python doesn't find packages that are installed with Conda","<python><anaconda><conda>","1","0","728"
"50645005","2018-06-01 13:53:58","2","","<p>Using <code>pivot</code>.</p>

<pre><code>Dict1 = {'str1A':'file1', 'str1B':'file1', 'str1C':'file1', 'str1D':'file1', 'str2A':'file2', 'str2B':'file2', 'str2C':'file2', 'str2D':'file2', 'str2D':'file2', 'str3A':'file3', 'str3B':'file3','str3C':'file3', 'str3D':'file3', 'str3D':'file3'}
Dict2 = {'str1A':'jump', 'str1B':'fly', 'str1C':'swim', 'str2A':'jump', 'str2B':'fly', 'str2C':'swim', 'str2D':'run', 'str3A':'jump', 'str3B':'fly', 'str3C':'swim', 'str3D':'run'}
Dict3 = {'str1A':'90', 'str1B':'60', 'str1C':'30', 'str2A':'70', 'str2B':'30', 'str2C':'60', 'str2D':'40', 'str3A':'10', 'str3B':'90', 'str3C':'70', 'str3D':'90'}

col_file = ['str', 'file']
df_origin = pd.DataFrame.from_dict(Dict1, orient=""index"")
df_bmatch = pd.DataFrame.from_dict(Dict2, orient=""index"")
df_percent = pd.DataFrame.from_dict(Dict3, orient=""index"")

df_temp = pd.concat([df_origin, df_bmatch, df_percent], axis=1)
df_temp.columns = [""col1"", ""col2"", ""col3""]

        col1    col2    col3
str1A   file1   jump    90
str1B   file1   fly     60
str1C   file1   swim    30
str1D   file1   NaN     NaN
str2A   file2   jump    70
str2B   file2   fly     30
str2C   file2   swim    60
str2D   file2   run     40
str3A   file3   jump    10
str3B   file3   fly     90
str3C   file3   swim    70
str3D   file3   run     90

df_temp.pivot(values=""col3"", columns=""col2"", index=""col1"").drop([np.nan], axis=1)

col2   fly  jump  run   swim
col1                    
file1  60   90    None  30
file2  30   70    40    60
file3  90   10    90    70
</code></pre>
","4829258","","","0","1533","Tai","2015-04-24 15:18:28","5199","574","766","17","50644315","50644457","2018-06-01 13:15:11","1","62","<p>In a dictionary with information about a string in a text file, where keys are the strings and values are the names of the files. </p>

<pre><code>Dict1 = {'str1A':'file1', 'str1B':'file1', 'str1C':'file1', 'str1D':'file1', 'str2A':'file2', 'str2B':'file2', 'str2C':'file2', 'str2D':'file2', 'str2D':'file2', 'str3A':'file3', 
</code></pre>

<p>'str3B':'file3','str3C':'file3', 'str3D':'file3', 'str3D':'file3' , 'str4A':'file4', 'str4B':'file4', 'str4C':'file4', 'str4D':'file4', 'str4E':'file4'}</p>

<p>Another dictionary contains information about the best match for the strings from the text.</p>

<pre><code>Dict2 = {'str1A':'jump', 'str1B':'fly', 'str1C':'swim', 'str2A':'jump', 'str2B':'fly', 'str2C':'swim', 'str2D':'run', 'str3A':'jump', 'str3B':'fly', 'str3C':'swim', 'str3D':'run'}
</code></pre>

<p>The third dictionary contains information about the percentage of occurrence of the string in the text.</p>

<pre><code>Dict3 = {'str1A':'90', 'str1B':'60', 'str1C':'30', 'str2A':'70', 'str2B':'30', 'str2C':'60', 'str2D':'40', 'str3A':'10', 'str3B':'90', 'str3C':'70', 'str3D':'90'}
</code></pre>

<p>Now my aims are to use the information of these different dictionaries to generate a dataframe like this:</p>

<pre><code>       jump     fly     swim    run
file1   90      60      30      NA
file2   70      30      60      40
file3   10      90      70      90
</code></pre>

<p>To this, I started the script but I am stuck:</p>

<pre><code>col_file = ['str', 'file']
df_origin = pd.DataFrame(Dict1.items(), columns=col_file)
#print df_origin

col_bmatch = ['str', 'text']
df_bmatch =  pd.DataFrame(Dict2.items(), columns=col_bmatch)
#print df_bmatch

col_percent = ['str', 'percent']
df_percent = pd.DataFrame(Dict3.items(), columns=col_percent)
#print df_percent
</code></pre>

<p>This block was removed from script: </p>

<blockquote>
<pre><code>df_origin['text'] = df_origin['str'].map(df_bmatch.set_index('str')['text'])

df_origin['percent'] = df_origin['str'].map(df_percent.set_index('str')['percent'])
</code></pre>
</blockquote>

<p>And substituted to: </p>

<pre><code>data = {}
for k, col in Dict1.items():
    if k in Dict1 and k not in Dict3:
        data.setdefault(k, {})[col] = ""NA""
    elif k in Dict1 and k in Dict3:
        data.setdefault(k, {})[col] = Dict3[k]

    df = pd.DataFrame(data)

print(df)
</code></pre>

<p>But the final result was not very exact:</p>

<pre><code>      str1A str1B str1C str1D str2A str2B str2C str2D str3A str3B  \
file1     90     60     30     NO    NaN    NaN    NaN    NaN    NaN    NaN   
file2    NaN    NaN    NaN    NaN     70     30     60     40    NaN    NaN   
file3    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN     10     90   
file4    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   

      str3C str3D str4A str4B stre4C str4D str4E  
file1    NaN    NaN    NaN    NaN    NaN    NaN    NaN  
file2    NaN    NaN    NaN    NaN    NaN    NaN    NaN  
file3     70     90    NaN    NaN    NaN    NaN    NaN  
file4    NaN    NaN     NO     NO     NO     NO     NO  
</code></pre>

<p>But the expected table is:</p>

<pre><code>         jump   fly    swim   run   sit
file1    90     60     30     NA    NA
file2    70     30     60     40    NA
file3    10     90     70     90    NA
file4    NA     NA     NA     NA    NA
</code></pre>

<p>Where the string in file4 where not detected.</p>

<p>Blosk removed</p>

<blockquote>
<pre><code>print df_origin

#          str   file  text percent
#    0   str2B  file2   fly      30
#    1   str2C  file2  swim      60
#    2   str3C  file3  swim      70
#    3   str3B  file3   fly      90
#    4   str3D  file3   run      90
#    5   str2D  file2   run      40
#    6   str3A  file3  jump      10
#    7   str1D  file1   NaN     NaN
#    8   str1C  file1  swim      30
#    9   str1B  file1   fly      60
#    10  str1A  file1  jump      90
#    11  str2A  file2  jump      70
</code></pre>
</blockquote>

<p>Here relies the problem</p>

<pre><code>print pd.get_dummies(df_origin.set_index('file')['text']).max(level=0).max(level=0, axis=1)
</code></pre>

<p>But the only result that I get is this:</p>

<pre><code>       fly  jump  run  swim
file                       
file2    1     1    1     1
file3    1     1    1     1
file1    1     1    0     1
</code></pre>

<p>As I can understand, pd.getdummies groups the field 'file' from my df_origin and uses 'text' to check their presence. </p>

<p>How can I redirect the command to plot the columns 'percent' in my df_origin dataframe?</p>
","5775504","5775504","2018-06-04 13:57:51","Merge dictionaries to dataframe get_dummies","<python><pandas><dictionary><dataframe>","2","0","4553"
"50645010","2018-06-01 13:54:23","0","","<p>Create a module named <code>airflow_local_settings</code> that can be imported globally. Airflow's <a href=""https://github.com/apache/incubator-airflow/blob/master/airflow/settings.py#L224"" rel=""nofollow noreferrer""><code>airflow.settings</code> attempts to load the module</a> during startup.</p>
","174652","","","1","301","joeb","2009-09-16 22:12:21","2392","203","134","15","50603904","50645010","2018-05-30 11:48:25","0","274","<p>Is there any way how I can execute some code only once after Airflow has started? I would like to configure how the logging is handled, but I can't find a way to initialize this configuration only once...</p>

<pre><code>from utils import logger
logger.initialize()
</code></pre>

<p>It's basically these to lines (utils is a locally defined package). It works when I put these lines into one of the DAGs, but then the function is executed multiple times which is unnecessary...</p>
","2883254","","","Execute code on startup in Airflow","<python><airflow>","1","2","486"
"50645035","2018-06-01 13:55:35","1","","<p>Upon examining the network log, credits: @P.hunter, I saw that a request was being sent to <code>http://www.kidzee.com/wp-admin/admin-ajax.php?action=state</code> whenever I pressed the Select Country button. So I wrote the following code to fetch the data: </p>

<pre><code>    from urllib.parse import urlencode
    from urllib.request import Request, urlopen

    url = 'http://www.kidzee.com/wp-admin/admin-ajax.php?action=state'
    fields = {'state': '1'}  

    r = Request(url, urlencode(fields).encode())
    json = urlopen(r).read().decode()
    print(json)
</code></pre>

<p>And now I am getting the required states in the country. </p>

<p>Thanks for the right direction @P.hunter. </p>
","9839478","","","1","702","Vizag","2018-05-24 08:27:20","477","78","85","15","50637847","","2018-06-01 07:04:42","2","263","<p>I am working with RoboBrowser on Python and I am trying to fill up a form on a website (<a href=""http://www.kidzee.com/admissions-at-kidzee/"" rel=""nofollow noreferrer"">here</a>). Now the thing is that the form is dynamic. If you open up the link, you'll see a row for 'Country', 'State/District', 'City' and 'Location'.<br> </p>

<p>The State/District row depends on which Country you select out of the drop down menu, City depends on State/District and Location depends on City. I want to be able to extract for example 'values' of State/District for a particular value of the country. For example when I write the following code:</p>

<pre><code>&gt;&gt;&gt; import re
&gt;&gt;&gt; from robobrowser import RoboBrowser
&gt;&gt;&gt; browser = RoboBrowser()
&gt;&gt;&gt; browser.open('http://http://www.kidzee.com/admissions-at-kidzee/')
&gt;&gt;&gt; form = browser.get_form()
&gt;&gt;&gt; form
&lt;RoboForm siteid=, adunit=, fname=, lname=, email=, mobile=,country=,state=,
city=, location=0, 6_letters_code=, admission_submit=Submit&gt;
&gt;&gt;&gt; form['country'].options
['','1','2'] #The options presented are India and Nepal.
&gt;&gt;&gt;form['country'].value = '1'
&gt;&gt;&gt;form['state'].options
['']
</code></pre>

<p>Now what I want to do it get all the possible values for <code>form['state'].value</code> with <code>form['country'].value = '1'</code> (because the list of states depends on the value of country). How do I go about doing that?</p>

<p>Thanks for reading my question.</p>
","9839478","9839478","2018-06-01 10:37:52","Filling up a dynamic online form in Python","<python><python-3.x><forms><web-scraping><robobrowser>","1","3","1504"
"50645042","2018-06-01 13:55:59","0","","<p>Alternatively, you could also use a list comprehension as the index and use comparison chaining for each element in the comprehension.</p>

<pre><code>&gt;&gt;&gt; A = np.array([i*10 for i in range(10)])
&gt;&gt;&gt; inf, sup = 30, 70
&gt;&gt;&gt; A[[inf &lt; x &lt; sup for x in A]] *= 10
&gt;&gt;&gt; A
array([  0,  10,  20,  30, 400, 500, 600,  70,  80,  90])
</code></pre>

<p>In the given example, using <code>&amp;</code> as in the other answer works just as well, if not better, but this might be more universally applicable, e.g. for checking some function on each value.</p>
","1639625","","","0","587","tobias_k","2012-08-31 20:34:34","63214","4895","5994","655","50644639","50644916","2018-06-01 13:32:49","2","262","<p>Given an array of unsorted points I need to replace those in a given interval. Easiest way I think of is</p>

<pre><code>import numpy as np

def v1(array,inf,sup):
    for i in range(len(array)):
        if inf&lt;array[i]&lt;sup:
            array[i]-=10
    return array
</code></pre>

<p>I was suggested to use <code>np.where</code>. It works smoothly if there is only one boolean condition:</p>

<pre><code>def v2(array,sup):
    array[np.where(array &lt; sup)[0]]-=10
    return array
</code></pre>

<p>But the same setup with <code>inf</code> and <code>sup</code> value, i.e. </p>

<pre><code>array[np.where(inf &lt; array &lt; sup)[0]]-=10
</code></pre>

<p>will raise an error</p>

<pre><code>ValueError: The truth value of an array with more than one element is ambiguous.
Use a.any() or a.all().
</code></pre>

<p>I have to do something clumsy like using <code>np.where</code> twice with two conditions and intersecate the two resulting indexes arrays...</p>

<pre><code>def v2(array,inf,sup):
    i=list(set.intersection(set(np.where(array&gt;inf)[0]),set(np.where(array&lt;sup)[0])))
    array[i]-=10
    return array
</code></pre>

<p>Suggestions?</p>
","3487233","","","Find elements in array such that inf < element < sup in Python","<python><arrays><numpy>","2","1","1168"
"50645049","2018-06-01 13:56:39","2","","<p>I think the problem here is the second one, which is</p>

<pre><code>b = 90.005
print('%.2f' % b)
</code></pre>

<p>Now as @divibisan said, if the digit is between 0-4, it's rounded down, if it's between 5-9, it's rounded up. So why is <code>90.005</code> not rounded up ?</p>

<p>Because computers can not represent the floating points precisely. If you look at it closely you will see that <code>90.005</code> is represented as,</p>

<pre><code>&gt;&gt;&gt; print('%.50f' % b)
90.00499999999999545252649113535881042480468750000000
</code></pre>

<p>That's the reason it is being rounded down, while others behave normally.</p>

<pre><code>&gt;&gt;&gt; print ('%.50f'%a)
0.00500000000000000010408340855860842566471546888351
# 0.01

&gt;&gt;&gt; print ('%.50f'%c)
90.01500000000000056843418860808014869689941406250000
# 90.02
</code></pre>
","4237254","","","6","843","BcK","2014-11-10 21:01:26","1519","137","63","23","50644626","50645049","2018-06-01 13:32:15","0","62","<pre><code>a=0.005
print ('%.2f'%a)
b=90.005
print('%.2f'%b)
c=90.015
print('%.2f'%c)
</code></pre>

<p>Above code is written in python3 and the output is following:</p>

<pre><code> 0.01
 90.00
 90.02
</code></pre>

<p>Is this any kind of computaional error or m missing a point, please explain.
This question might appear similar to <a href=""https://stackoverflow.com/questions/10825926/python-3-x-rounding-behavior"">this</a> but isn't and i'm looking for possible solution</p>
","7530221","7530221","2018-06-01 13:56:14","Rounding numbers in Python","<python><python-3.x>","1","7","480"
"50645089","2018-06-01 13:58:11","0","","<h1>Approach 1: Distinguishing characters</h1>

<p><a href=""https://en.wikipedia.org/wiki/Wikipedia:Language_recognition_chart#Spanish_(Espa%C3%B1ol)"" rel=""nofollow noreferrer"">Spanish</a> and <a href=""https://en.wikipedia.org/wiki/Wikipedia:Language_recognition_chart#Catalan_(Catal%C3%A0)"" rel=""nofollow noreferrer"">Catalan</a> (note: there will be exceptions for proper names and loanwords e.g. <em>Barça</em>):</p>

<pre><code>esp_chars = ""ñÑáÁýÝ""
cat_chars = ""çÇàÀèÈòÒ·ŀĿ""
</code></pre>

<p>Example:</p>

<pre><code>def containsAny(seq, aset):
    """""" Check whether seq contains ANY of the items in aset. """"""
    for c in seq:
        if c in aset: return True
    return False

sample_texts = [""El año que es abundante de poesía, suele serlo de hambre."",
                ""Cal no abandonar mai ni la tasca ni l'esperança.""]

for text in sample_texts:
    if containsAny(esp_chars, text):
        print(""Spanish: {}"".format(text))
    elif containsAny(cat_chars, text):
        print(""Catalan: {}"".format(text))
</code></pre>

<p></p>

<pre><code>&gt;&gt;&gt; Spanish: El año que es abundante de poesía, suele serlo de hambre.
    Catalan: Cal no abandonar mai ni la tasca ni l'esperança.
</code></pre>

<p>If this isn't sufficient, you could expand this logic to search for language exclusive digraphs, letter combinations, or words:</p>

<p><strong>Catalan only digraphs/letter combinations:</strong></p>

<ul>
<li><code>d'</code> <code>l'</code> (initial)</li>
<li><code>ss</code> <code>tj</code> <code>qü</code> <code>l·l</code> <code>l.l</code></li>
<li><code>ig</code> (terminal)</li>
</ul>

<p><strong>Catalan letter combinations that only marginally appear in Spanish</strong></p>

<ul>
<li><code>tx</code></li>
<li><code>tg</code>          <sup>(Es. exceptions <em>postgrado, postgraduado, postguerra</em>)</sup></li>
<li><code>ny</code>          <sup>(Es. exceptions mostly prefixed <em>in-, en-, con-</em> + <em>y-</em>)</sup></li>
<li><code>ll</code> (terminal) <sup>(Es. exceptions <em>detall, nomparell</em>)</sup></li>
</ul>

<p><strong>Common Spanish only words:</strong> </p>

<ul>
<li><code>como</code> <code>y</code> <code>su</code> <code>con</code> <code>él</code> <code>otro</code></li>
</ul>

<p><strong>Common Catalan only words:</strong> </p>

<ul>
<li><code>com</code> <code>i</code> <code>seva</code> <code>amb</code> <code>ell</code> <code>altre</code></li>
</ul>

<hr>

<hr>

<h1>Approach 2: googletrans module</h1>

<p>You could also use the <a href=""https://pypi.org/project/googletrans/"" rel=""nofollow noreferrer"">googletrans</a> module to detect the language:</p>

<pre><code>from googletrans import Translator

translator = Translator()

for text in sample_texts:
    lang = translator.detect(text).lang
    print(lang, "":"", text)
</code></pre>

<p></p>

<pre><code>&gt;&gt;&gt; es : El año que es abundante de poesía, suele serlo de hambre.
    ca : Cal no abandonar mai ni la tasca ni l'esperança.
</code></pre>
","9067615","9067615","2018-08-30 13:17:03","1","2951","ukemi","2017-12-07 13:32:15","3409","280","1249","84","45672720","","2017-08-14 10:40:45","1","206","<p>I'm working on a text mining script in python. I need to detect language of a natural language field from dataset.</p>

<p>The things is, 98% of the rows are on Spanish and Catalan, I tried using some algorithms like the stopwords one or the langdetect library, but this languages share a lot of words so they fail a lot.</p>

<p>I'm looking for some ideas to improve this algorithm.</p>

<p>One I thought is, write a dictionary with some words that are specific for spanish and catalan, so if one text has any of this words, is tagged for that language no matter what.</p>

<p>Thanks.</p>
","8461643","8461643","2017-08-14 11:58:17","Ideas to improve language detection between Spanish and Catalan","<python><language-detection>","1","0","593"
"50645097","2018-06-01 13:58:54","3","","<p>You can create a dictionary from the string's contents with <code>re</code>:</p>

<pre><code>import re
s = 'DATE: 7/25/2017 DATE OPENED: 7/25/2017 RETURN DATE: 7/26/2017 NUMBER: 201707250008754 RATE: 10.00'
results = re.findall('[a-zA-Z\s]+(?=:)|[\d/\.]+', s)
d = dict([re.sub('^\s+', '', results[i]), results[i+1]] for i in range(0, len(results), 2))
for i in ['DATE', 'RETURN DATE', 'NUMBER']:
   print(d[i])
</code></pre>

<p>Output:</p>

<pre><code>7/25/2017
7/26/2017
201707250008754
</code></pre>
","7326738","","","1","506","Ajax1234","2016-12-21 16:39:57","49079","3709","2930","360","50645034","","2018-06-01 13:55:30","3","55","<p>I wanted to extract the date from the given string on the basis of tag. </p>

<p>My string is - </p>

<pre><code>DATE: 7/25/2017 DATE OPENED: 7/25/2017 RETURN DATE: 7/26/2017 
NUMBER: 201707250008754 RATE:  10.00
</code></pre>

<p>I want something like this - 
If I give ""DATE"" it should return <code>7/25/2017</code> only </p>

<p>if I give ""RETURN DATE"" it should return <code>7/26/2017</code></p>

<p>if I give the ""NUMBER"" it should return <code>201707250008754</code>
and so on.</p>

<p>How we can achieve this in Python 2.7 (Note: Dates and numbers are always random in string""</p>
","6504894","789671","2018-06-01 17:05:14","How to extract the file Data in python","<python><python-2.7>","2","2","591"
"50645181","2018-06-01 14:02:44","10","","<ul>
<li>UAS (Unlabelled Attachment Score) and LAS (Labelled Attachment Score) are standard metrics to evaluate dependency parsing. UAS is the proportion of tokens whose head has been correctly assigned, LAS is the proportion of tokens whose head has been correctly assigned with the right dependency label (subject, object, etc).</li>
<li><code>ents_p</code>, <code>ents_r</code>, <code>ents_f</code> are the precision, recall and <a href=""https://en.wikipedia.org/wiki/Precision_and_recall"" rel=""noreferrer"">fscore</a> for the NER task.</li>
<li><code>tags_acc</code> is the POS tagging accuracy.</li>
<li><code>token_acc</code> seems to be the precision for token segmentation.</li>
</ul>
","9730177","","","1","692","mcoav","2018-05-02 11:59:32","1406","56","35","0","50644777","50645181","2018-06-01 13:41:32","7","1104","<p>I'm evaluating a custom NER model that I built using Spacy. I'm evaluating the training sets using Spacy's Scorer class.</p>

<pre><code>    def Eval(examples):
    # test the saved model
    print(""Loading from"", './model6/')
    ner_model = spacy.load('./model6/')

    scorer = Scorer()
    try:
        for input_, annot in examples:
            doc_gold_text = ner_model.make_doc(input_)
            gold = GoldParse(doc_gold_text, entities=annot['entities'])
            pred_value = ner_model(input_)
            scorer.score(pred_value, gold)
    except Exception as e: print(e)

    print(scorer.scores)
</code></pre>

<p>It works fine but I don't understand the output. Here's what I get for each training set.</p>

<pre><code>{'uas': 0.0, 'las': 0.0, 'ents_p': 90.14084507042254, 'ents_r': 92.7536231884058, 'ents_f': 91.42857142857143, 'tags_acc': 0.0, 'token_acc': 100.0}

{'uas': 0.0, 'las': 0.0, 'ents_p': 91.12227805695142, 'ents_r': 93.47079037800687, 'ents_f': 92.28159457167091, 'tags_acc': 0.0, 'token_acc': 100.0}

{'uas': 0.0, 'las': 0.0, 'ents_p': 92.45614035087719, 'ents_r': 92.9453262786596, 'ents_f': 92.70008795074759, 'tags_acc': 0.0, 'token_acc': 100.0}

{'uas': 0.0, 'las': 0.0, 'ents_p': 94.5993031358885, 'ents_r': 94.93006993006993, 'ents_f': 94.76439790575917, 'tags_acc': 0.0, 'token_acc': 100.0}

{'uas': 0.0, 'las': 0.0, 'ents_p': 92.07920792079209, 'ents_r': 93.15525876460768, 'ents_f': 92.61410788381743, 'tags_acc': 0.0, 'token_acc': 100.0}
</code></pre>

<p>Does anyone know what the keys are? I've looked over Spacy's documentation and could not find anything.</p>

<p>Thanks!</p>
","7128500","","","Understanding Spacy's Scorer Output","<python><spacy><ner>","1","0","1628"
"50645212","2018-06-01 14:04:10","4","","<p>I believe need <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.resample.Resampler.transform.html"" rel=""nofollow noreferrer""><code>Resampler.transform</code></a>:</p>

<pre><code>df['day_high']= df.resample('D').high.transform('max')
df['day_low']= df.resample('D').low.transform('min')
print (df)
                      open   high    low  close   vol  day_high  day_low
date_time                                                               
2018-05-13 18:00:00  70.54  70.60  70.42  70.55  2665     70.70    70.42
2018-05-13 18:15:00  70.55  70.59  70.53  70.58   378     70.70    70.42
2018-05-13 18:30:00  70.58  70.70  70.57  70.69  1470     70.70    70.42
2018-05-13 18:45:00  70.68  70.68  70.63  70.65   427     70.70    70.42
2018-05-14 00:00:00  70.46  70.47  70.40  70.41  1276     70.48    70.38
2018-05-14 00:15:00  70.41  70.45  70.38  70.39  1356     70.48    70.38
2018-05-14 00:30:00  70.39  70.48  70.39  70.46  1161     70.48    70.38
2018-05-14 00:45:00  70.46  70.47  70.43  70.46   359     70.48    70.38
</code></pre>
","2901002","","","0","1071","jezrael","2013-10-20 20:27:26","427380","89269","18260","743","50645134","50645212","2018-06-01 14:00:29","4","61","<p>Let I have a dataframe with datetime index like this:</p>

<pre><code>date_time           open    high    low     close   vol
2018-05-13 18:00:00 70.54   70.60   70.42   70.55   2665
2018-05-13 18:15:00 70.55   70.59   70.53   70.58   378
2018-05-13 18:30:00 70.58   70.70   70.57   70.69   1470
2018-05-13 18:45:00 70.68   70.68   70.63   70.65   427
...
2018-05-14 00:00:00 70.46   70.47   70.40   70.41   1276
2018-05-14 00:15:00 70.41   70.45   70.38   70.39   1356
2018-05-14 00:30:00 70.39   70.48   70.39   70.46   1161
2018-05-14 00:45:00 70.46   70.47   70.43   70.46   359
</code></pre>

<p>I need two more columns with DAILY High and DAILY low values.
I'm trying make this:</p>

<pre><code>df['day_high']= x.resample('D').high.max()
df.day_high = x.day_high.fillna(method='ffill')
</code></pre>

<p>It is works perfectly on days, where 00:00:00 datatime exist. Thus, 14.05.2018 I have a value with datetime 00:00:00 and my code works. But 2018-05-13 day began at 18:00 and my code return ""NaN"" value (I know, why, but I don't know how to write correct code).</p>

<p>Could you help me? Thanks.</p>
","4679622","","","Daily High/Low on intraday data","<python><pandas><dataframe>","1","0","1112"
"50645249","2018-06-01 14:06:13","0","","<p>You don't need to pickle it, h2o provides its own persistence methods: <a href=""http://docs.h2o.ai/h2o/latest-stable/h2o-docs/save-and-load-model.html"" rel=""nofollow noreferrer"">http://docs.h2o.ai/h2o/latest-stable/h2o-docs/save-and-load-model.html</a></p>

<pre><code># build the model
model = H2ODeepLearningEstimator(params)
model.train(params)

# save the model
model_path = h2o.save_model(model=model, path=""/tmp/mymodel"", force=True)

print(model_path)
# outputs: /tmp/mymodel/DeepLearning_model_python_1441838096933

# load the model
saved_model = h2o.load_model(model_path)
</code></pre>

<p>be warned though, persisted models are <strong><em>NOT</em></strong> compatible between even the most minor version changes i.e. if you train and save a model in 3.18.0.1 you won't be able to load it in 3.18.0.2</p>
","1011724","","","1","819","Dan","2011-10-24 21:37:14","38664","4068","4813","447","50642358","","2018-06-01 11:23:55","0","666","<p>I am currently trying to serialize a h2o gb model into a pickle object and reuse it. Due to some constraints, I can't use the default method or POJO and MOJO given at - <a href=""http://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html"" rel=""nofollow noreferrer"">http://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html</a>. The model gets pickled, however on unpickling(pickle.loads), the following error comes up -</p>

<pre><code>__new__() missing 1 required positional argument: 'keyvals'
</code></pre>

<p>The code below for reference-</p>

<pre><code>import h2o as h2o
import pickle as pickle
from h2o.estimators.gbm import H2OGradientBoostingEstimator
h2o.init()

csv_url = ""https://h2o-public-test-data.s3.amazonaws.com/smalldata/wisc/wisc-diag-breast-cancer-shuffled.csv""
data = h2o.import_file(csv_url)
y = 'diagnosis'
x = data.columns
del x[0:1]
train, test = data.split_frame(ratios=[0.75], seed=1)


model = H2OGradientBoostingEstimator(distribution='bernoulli',
                                ntrees=100,
                                max_depth=4,
                                learn_rate=0.1)
model.train(x=x, y=y, training_frame=train, validation_frame=test)

loaded_model = pickle.loads(saved_model)
perf = loaded_model.model_performance(test)
perf.auc()
</code></pre>

<p>I tried to understand the pickle module and make some changes, but it didn't work. Any workaround/help will be highly appreciated. Thanks. </p>
","6062480","6062480","2018-06-03 02:05:03","Serializing a h2o model with pickle - python","<python><serialization><machine-learning><h2o>","1","0","1461"
"50645286","2018-06-01 14:07:56","0","","<p>you might want to display <code>address</code> right after</p>

<pre><code>address = data['results'][n]['vicinity']
</code></pre>

<p>before you edit it with</p>

<pre><code>address = address[: address.find("", "")]
</code></pre>

<p>It might have to do with the <code>,</code> Characters in the text </p>

<blockquote>
  <p>C, 2976 Ask-Kay Dr, Smyrna</p>
</blockquote>
","7774231","","","1","371","Bodo Hugo Barwich","2017-03-27 12:53:34","44","18","14","0","50644983","50645286","2018-06-01 13:52:30","1","48","<p>I'm trying to parse a JSON file from Google Places API. I am trying to gather the vicinity address, the lng/lat coordinates, as well as the Place ID. On the page with the JSON file, my code is capturing more of the locations' information, but for some reason it is not getting the data from one of the locations. My code is:</p>

<pre><code>response = urllib.urlopen(url)
data = json.loads(response.read())
for n in range(len(data['results'])):
  location = []
  address = data['results'][n]['vicinity']
  address = address[: address.find("", "")]
  coords_lat = str(data['results'][n]['geometry']['location']['lat'])
  coords_lat = coords_lat[:-4]
  coords_lng = str(data['results'][n]['geometry']['location']['lng'])
  coords_lng = coords_lng[:-4]
  place_id = data['results'][n]['place_id']
  location = [address, coords_lat, coords_lng, place_id]
  print location
</code></pre>

<p>But for one of the locations, my program is returning:
<code>...[u'C', '33.87', '-84.53', u'ChIJOUmV_jgX9YgRdN1sudI8yOI']...</code> instead of the full vicinity address for this location, <code>'C, 2976 Ask-Kay Dr, Smyrna'</code>.</p>

<p><a href=""https://i.stack.imgur.com/d0kAx.png"" rel=""nofollow noreferrer"">This is my python program</a></p>

<p><a href=""https://i.stack.imgur.com/UvFJf.png"" rel=""nofollow noreferrer"">This is the API JSON page with one of the results I'm referring to</a></p>

<p><a href=""https://i.stack.imgur.com/QUHA1.png"" rel=""nofollow noreferrer"">These are some of the lines my program is printing</a></p>
","9881545","355230","2018-06-01 13:57:04","Parsing a JSON file but not getting everything returned","<python><json><parsing><google-places-api>","1","5","1518"
"50645308","2018-06-01 14:09:01","0","","<p>I still don't have a full picture of the problem, but I assume your problem is related to the downgrading of the mask.</p>

<p>When going from a high-resolution boolean mask to a lower resolution, there are three different strategies you can follow. The choice depends on your problem.</p>

<p>I'll illustrate the effect with the mask that you have provided, showing a zoom-in:</p>

<p><a href=""https://i.stack.imgur.com/FN7ft.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FN7ft.png"" alt=""enter image description here""></a></p>

<h3>Downgrading a mask with bool type</h3>

<p>You can simply downgrade the <em>boolean</em> mask, giving you the left panel in the figure below. Note that small patches will turn out unmasked in this procedure.</p>

<pre><code>map_lowres_bool = hp.ud_grade(mask_highres.astype(bool), nside=16)
</code></pre>

<h3>Downgrading a mask with float type</h3>

<p>If you instead downgrade your mask of type <code>float</code>, you'll get a continuous map with weights instead (middle panel below). I assume this produces the effect you're seeing.</p>

<pre><code>map_lowres_float = hp.ud_grade(mask_highres.astype(float), nside=16)
</code></pre>

<h3>Properly downgrading a mask with bool type</h3>

<p>You can also downgrade a boolean mask such that all masked regions remain masked. Due to the lower resolution, your masked patches will be larger than in the original mask.</p>

<pre><code>mask_lowres_proper = hp.ud_grade(mask.astype(float), nside=16).astype(float)
mask_lowres_proper = np.where(mask_lowres_proper == 1., True, False).astype(bool)
</code></pre>

<p>This produces the right panel below.</p>

<p>Hope you found this helpful, let me know if you have questions!</p>

<p><a href=""https://i.stack.imgur.com/uB1nP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uB1nP.png"" alt=""enter image description here""></a></p>
","4874233","","","0","1896","Daniel Lenz","2015-05-07 09:29:12","1405","147","487","31","50498612","","2018-05-23 22:58:01","0","126","<p>I am trying to mask my healpy map for pixels that have no data, however when I apply the healpy mask with <code>badval=-1.6375e+30</code>, there seems to be a border around my mask.</p>

<pre><code>import healpy as hp
import numpy as np

# load the mask
mask = hp.read_map('mask_nvss_S20-S1000_Ns64.fits')

# degrade the mask to NSIDE = 16
mask16 = np.round(hp.ud_grade(mask, 16))

# apply it to the map
masked_map = hp.ma(map1)
masked_map.mask = np.logical_not(mask16)

# masked map
nvss_map = np.round(masked_map)
</code></pre>

<p>Can anyone help me remove this border effect?</p>

<p><a href=""https://i.stack.imgur.com/WvHFt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WvHFt.png"" alt=""enter image description here""></a></p>
","6594821","4874233","2018-05-24 10:30:31","Healpy mask has border","<python><healpy>","1","8","754"
"50645338","2018-06-01 14:10:28","3","","<p>If need compare each value separately convert <code>table2</code> to <code>1d</code> array:</p>

<pre><code>a = table1.isin(table2.values.ravel())
print (a)
       a      b
0   True   True
1   True   True
2  False  False
3   True   True
4   True   True
</code></pre>

<p>If need compare each row separately:</p>

<pre><code>a = (table1.apply(tuple, 1).isin(table2.apply(tuple, 1)))
</code></pre>

<p>Or:</p>

<pre><code>a = (table1.astype(str).apply('###'.join, 1).isin(table2.astype(str).apply('###'.join, 1).))


print (a)
0     True
1     True
2    False
3     True
4     True
dtype: bool
</code></pre>

<p>For better explanation input data are changed:</p>

<pre><code>table1 = pd.DataFrame({'a':[1, 2, 5, 3, 4],
              'b':['d', 'b', 'e', 'c', 'd']})
table2 = pd.DataFrame({'a':[1, 4, 3, 6, 2],
              'b':['a', 'd', 'c', 'f', 'b']})

print (table1)
   a  b
0  1  d -&gt; changed to d
1  2  b
2  5  e
3  3  c
4  4  d

print (table2)
   a  b
0  1  a
1  4  d
2  3  c
3  6  f
4  2  b
</code></pre>

<hr>

<pre><code>a = table1.isin(table2.values.ravel())
print (a)
       a      b
0   True   True  d exist in table2, so True
1   True   True
2  False  False
3   True   True
4   True   True

a = (table1.apply(tuple, 1).isin(table2.apply(tuple, 1)))
print (a)
0    False -&gt; comparing 1-a with 1-b return False
1     True
2    False
3     True
4     True
dtype: bool
</code></pre>
","2901002","2901002","2018-06-01 14:54:04","10","1400","jezrael","2013-10-20 20:27:26","427380","89269","18260","743","50645297","50645672","2018-06-01 14:08:23","2","619","<p>I'm trying to check if rows exist in another dataframe. I'm not joining/merging because of issues with it creating duplication, and then needing to filter out that duplication might also filter out actual duplication that I <em>want</em> to keep.</p>

<p><em>example:</em></p>

<pre><code>table1 = pd.DataFrame({'a':[1, 2, 5, 3, 4],
              'b':['a', 'b', 'e', 'c', 'd']})
table2 = pd.DataFrame({'a':[1, 4, 3, 6, 2],
              'b':['a', 'd', 'c', 'f', 'b']})


table1.isin(table2)

       a      b
0   True   True
1  False  False
2  False  False
3  False  False
4  False  False
</code></pre>

<p>I would like all of these to be <code>True</code> except at index 2 where row <code>5 e</code> doesn't exist in <code>table2</code>.</p>
","7237997","7237997","2018-06-01 14:24:09","How to use isin while ignoring index","<python><pandas>","2","0","746"
"50645363","2018-06-01 14:12:13","2","","<p>We can iterate over the attributes, and update the values, then we call the <code>.save()</code> function:</p>

<pre><code>def my_fancy_update(self, **kwargs):
    <b>for k, v in kwargs.items():
        setattr(self, k, v)</b>
    self.save()</code></pre>

<p>This will however set the attributes manually, so it is possible that there are some signals, triggers, etc. attached that will update and clean the values.</p>

<p>The <code>setattr(..)</code> is a Python function such that <code>setattr(x, 'y', z)</code> is equivalent to <code>x.y = z</code> (note that we pass <code>'y'</code> as a string in <code>setattr(..)</code>.</p>

<p>It is also impossible to update the attributes like <code>related_field__attribute</code>, etc.</p>
","67579","","","1","743","Willem Van Onsem","2009-02-17 21:39:52","199073","27204","11746","1497","50645301","50645363","2018-06-01 14:08:35","1","310","<p>I searched for a simple answer to this question but couldn't really find any.<br>
 <a href=""https://stackoverflow.com/questions/23138334/django-update-model-object"">This question</a> somewhat looks like it, but not quit the same so here it is.</p>

<p>Let's say I have a custom update method inside my Django model.</p>

<pre><code>class People(models.Model):
    identifier = models.CharField(max_length=255, primary_key = True)
    name = models.CharField(max_length=255)
    house = model.ForeignKey(House)
    salary = model.IntegerField()

    def my_fancy_update(self, **kwargs):
        # Here I want to do my update
        pass
</code></pre>

<p>What I want to do is use the <code>kwargs</code> to update my model.<br>
Right now I am using the following:</p>

<pre><code>def my_fancy_update(self, **kwargs):
    self.name = kwargs.get('name')
    self.house = kwargs.get('house')
    self.salary = kwargs.get('salary')
    self.save()
</code></pre>

<p>This is something that looks like what I really want to do:</p>

<pre><code>def my_fancy_update(self, **kwargs):
    self.update(**kwargs)
</code></pre>

<p><code>self.update</code> does not work for it raises an error:<br>
<code>AttributeError: 'People' object has no attribute 'update'</code></p>

<p>Also <code>self.objects.update</code> isn't possible due to the fact that the manager isn't accessible when using <code>self</code>.</p>

<p>My question now is, if this is possible and how this is done in Django.<br>
It saves me a lot of time listing every attribute in my actual models. </p>
","6434747","6434747","2018-06-04 07:05:24","Django - update fields inside django model object using kwargs","<python><django><django-models>","2","4","1561"
"50645364","2018-06-01 14:12:17","0","","<p>Look at this answer from @unsorted here <a href=""https://stackoverflow.com/questions/34322448/pretty-printing-newlines-inside-a-string-in-a-pandas-dataframe"">Pretty printing newlines inside a string in a Pandas DataFrame</a></p>

<p>here it is with you example:</p>

<pre><code>import pandas as pd
from IPython.display import display, HTML

def pretty_print(df):
    return display(HTML(df.to_html().replace(""\\n"",""&lt;br&gt;"")))

data = [
       {'id': 1, 't': 'very long text\ntext line 2\ntext line 3'},
       {'id': 2, 't': 'short text'}
       ]
df = pd.DataFrame(data)
df.set_index('id', inplace=True)

pretty_print(df)

         t
id  
1   very long text
    text line 2
    text line 3
2   short text
</code></pre>
","9177877","","","0","727","Chris","2018-01-05 14:16:38","3703","401","84","29","50644066","","2018-06-01 13:02:56","6","3167","<p>Good day,</p>

<p>Is it possible to get multi line cell output when using pandas DataFrame in a shell? I understand the whole row will have height more than 1 char, but that's what I actually want.</p>

<p>Example:</p>

<pre><code>data = [
       {'id': 1, 't': 'very long text\ntext line 2\ntext line 3'},
       {'id': 2, 't': 'short text'}
       ]
df = pd.DataFrame(data)
df.set_index('id', inplace=True)
print(df)
</code></pre>

<p>and want to get output:</p>

<pre>
id                  t
1      very long text
       text line 2
       text line 3
2      short text
</pre>

<p>instead of</p>

<pre>
id                                         t
1     very long text\ntextline 2\ntext line3
2                                 short text
</pre>
","179859","","","pandas dataframe and multi line values","<python><pandas><dataframe>","3","0","750"
"50645371","2018-06-01 14:12:29","3","","<p>Note that your current solution:</p>

<pre><code>def my_fancy_update(self, **kwargs):
    self.name = kwargs.get('name')
    self.house = kwargs.get('house')
    self.salary = kwargs.get('salary')
    self.save()
</code></pre>

<p>is very brittle as it can possibly set any of name, house and salary to <code>None</code>.</p>

<p>A first clean solution would be to use a for loop and settattr:</p>

<pre><code>def my_fancy_update(self, **kwargs):
    for name, value in kwargs.items():
        setattr(self, name, value)
    self.save()
</code></pre>

<p>but this is still brittle in that it doesn't check that the names passed are actually model field names. A safer implementation would use <code>self._meta.get_fields()</code> to check this:</p>

<pre><code>def my_fancy_update(self, **kwargs):
    fieldnames = set(f.name for f in self._meta.get_fields())
    for name, value in kwargs.items():
        if name not in fieldnames:
            raise ValueError(""%s is not a field of %s"" % (name, type(self).__name__))
        setattr(self, name, value)
    self.save()
</code></pre>

<p>and finally, you may want to avoid calling <code>save()</code> if nothing was updated, and only commit what was actually updated:</p>

<pre><code>def my_fancy_update(self, **kwargs):
    if not kwargs:
        return

    fieldnames = set(f.name for f in self._meta.get_fields())
    updated = []
    for name, value in kwargs.items():
        if name not in fieldnames:
            raise ValueError(""%s is not a field of %s"" % (name, type(self).__name__))
        setattr(self, name, value)
        updated.append(name)
    self.save(update_fields=updated)
</code></pre>
","41316","41316","2018-06-01 14:17:46","4","1664","bruno desthuilliers","2008-11-27 10:40:14","57630","7809","2022","2444","50645301","50645363","2018-06-01 14:08:35","1","310","<p>I searched for a simple answer to this question but couldn't really find any.<br>
 <a href=""https://stackoverflow.com/questions/23138334/django-update-model-object"">This question</a> somewhat looks like it, but not quit the same so here it is.</p>

<p>Let's say I have a custom update method inside my Django model.</p>

<pre><code>class People(models.Model):
    identifier = models.CharField(max_length=255, primary_key = True)
    name = models.CharField(max_length=255)
    house = model.ForeignKey(House)
    salary = model.IntegerField()

    def my_fancy_update(self, **kwargs):
        # Here I want to do my update
        pass
</code></pre>

<p>What I want to do is use the <code>kwargs</code> to update my model.<br>
Right now I am using the following:</p>

<pre><code>def my_fancy_update(self, **kwargs):
    self.name = kwargs.get('name')
    self.house = kwargs.get('house')
    self.salary = kwargs.get('salary')
    self.save()
</code></pre>

<p>This is something that looks like what I really want to do:</p>

<pre><code>def my_fancy_update(self, **kwargs):
    self.update(**kwargs)
</code></pre>

<p><code>self.update</code> does not work for it raises an error:<br>
<code>AttributeError: 'People' object has no attribute 'update'</code></p>

<p>Also <code>self.objects.update</code> isn't possible due to the fact that the manager isn't accessible when using <code>self</code>.</p>

<p>My question now is, if this is possible and how this is done in Django.<br>
It saves me a lot of time listing every attribute in my actual models. </p>
","6434747","6434747","2018-06-04 07:05:24","Django - update fields inside django model object using kwargs","<python><django><django-models>","2","4","1561"
"50645421","2018-06-01 14:15:24","1","","<p>Try this:</p>

<pre><code>outfile1.writelines(""{}\n"".format(ele) for ele in bigram)
</code></pre>
","1782792","1782792","2018-06-01 14:22:04","1","101","jdehesa","2012-10-29 11:43:40","37080","2442","2562","26","50645349","50645448","2018-06-01 14:11:31","3","84","<p>Bigram is a list which looks like-</p>

<pre><code>[('a', 'b'), ('b', 'b'), ('b', 'b'), ('b', 'c'), ('c', 'c'), ('c', 'c'), ('c', 'd'), ('d', 'd'), ('d', 'e')]
</code></pre>

<p>Now I am trying to wrote each element if the list as a separate line in a file with this code-</p>

<pre><code> bigram = list(nltk.bigrams(s.split()))
 outfile1.write(""%s"" % ''.join(ele) for ele in bigram)
</code></pre>

<p>but I am getting this error :</p>

<blockquote>
  <p>TypeError: write() argument must be str, not generator</p>
</blockquote>

<p>I want the result as in file-</p>

<pre><code>('a', 'b') 
('b', 'b')
('b', 'b')
('b', 'c')
('c', 'c')
......
</code></pre>
","8660280","8660280","2018-06-01 14:18:12","write the elements of list to file","<python><file>","3","2","658"
"50645446","2018-06-01 14:16:39","0","","<p>Since you are applying PCA in the kernel space, there is a strictly nonlinear relation with your original features and the features of the reduced data; the eigenvectors you calculate are in the kernel space to begin with. This obstructs a straightforward approach, but maybe you can do some kind of sensitivity analysis. Apply small perturbations to the original features and measure how the final, reduced features react to them. The Jacobian of the final features with respect to the original features can be also a good place to start.</p>
","1538049","","","0","547","Ufuk Can Bicici","2012-07-19 13:32:25","2073","431","102","1","50613294","","2018-05-30 21:00:34","1","68","<p>I am trying to implement Kernel PCA to my dataset which has both categorical (encoded with one hot encoder) and numeric features and decreases the number of dimensions from 22 to 3 dimensions in total. After that, I will continue with clustering implementation. I use Spyder as IDE.
In order to understand the structure of my yielded clusters from the algorithm, I want to interpret which features affect the derived principal components and how they affect them.
Is it possible? If so, how can I interpret this, is there any method?</p>
","8010873","","","Finding original features' effect to the principal components used as inputs in Kernel PCA","<python><machine-learning><cluster-analysis><pca><kernel-density>","1","0","541"
"50645448","2018-06-01 14:16:41","3","","<p>you're passing a generator comprehension to <code>write</code>, which needs strings.</p>

<p>If I understand correctly you want to write one representation of tuple per line.</p>

<p>You can achieve that with:</p>

<pre><code>outfile1.write("""".join('{}\n'.format(ele) for ele in bigram))
</code></pre>

<p>or</p>

<pre><code>outfile1.writelines('{}\n'.format(ele) for ele in bigram)
</code></pre>

<p>the second version passes a generator comprehension to <code>writelines</code>, which avoids to create the big string in memory before writing to it (and looks more like your attempt)</p>

<p>it produces a file with this content:</p>

<pre><code>('a', 'b')
('b', 'b')
('b', 'b')
('b', 'c')
('c', 'c')
('c', 'c')
('c', 'd')
('d', 'd')
('d', 'e')
</code></pre>
","6451573","","","2","763","Jean-François Fabre","2016-06-10 19:19:53","113106","37329","9248","14670","50645349","50645448","2018-06-01 14:11:31","3","84","<p>Bigram is a list which looks like-</p>

<pre><code>[('a', 'b'), ('b', 'b'), ('b', 'b'), ('b', 'c'), ('c', 'c'), ('c', 'c'), ('c', 'd'), ('d', 'd'), ('d', 'e')]
</code></pre>

<p>Now I am trying to wrote each element if the list as a separate line in a file with this code-</p>

<pre><code> bigram = list(nltk.bigrams(s.split()))
 outfile1.write(""%s"" % ''.join(ele) for ele in bigram)
</code></pre>

<p>but I am getting this error :</p>

<blockquote>
  <p>TypeError: write() argument must be str, not generator</p>
</blockquote>

<p>I want the result as in file-</p>

<pre><code>('a', 'b') 
('b', 'b')
('b', 'b')
('b', 'c')
('c', 'c')
......
</code></pre>
","8660280","8660280","2018-06-01 14:18:12","write the elements of list to file","<python><file>","3","2","658"
"50645455","2018-06-01 14:16:51","1","","<p>This is the operator precedence problem.</p>

<p>You want an expression like this:</p>

<pre><code>(""%s"" % ''.join(ele)) for ele in bigram
</code></pre>

<p>Instead, you get it interpreted like this, where the part in the parens is indeed a generator:</p>

<pre><code>""%s"" % (''.join(ele) for ele in bigram)
</code></pre>

<p>Use the explicit parentheses.</p>

<p>Please note that <code>(""%s"" % ''.join(ele)) for ele in bigram</code> is <em>itself</em> a generator. You need to call <code>write</code> on each element from it.</p>

<p>If you want to write each pair in a separate line, you have to add line separators explicitly. The easiest, to my mind, is an explicit loop:</p>

<pre><code>for pair in bigram:
  outfile.write(""(%s, %s)\n"" % pair)
</code></pre>
","223424","223424","2018-06-01 14:26:18","2","766","9000","2009-12-03 01:14:45","32123","2971","4620","18","50645349","50645448","2018-06-01 14:11:31","3","84","<p>Bigram is a list which looks like-</p>

<pre><code>[('a', 'b'), ('b', 'b'), ('b', 'b'), ('b', 'c'), ('c', 'c'), ('c', 'c'), ('c', 'd'), ('d', 'd'), ('d', 'e')]
</code></pre>

<p>Now I am trying to wrote each element if the list as a separate line in a file with this code-</p>

<pre><code> bigram = list(nltk.bigrams(s.split()))
 outfile1.write(""%s"" % ''.join(ele) for ele in bigram)
</code></pre>

<p>but I am getting this error :</p>

<blockquote>
  <p>TypeError: write() argument must be str, not generator</p>
</blockquote>

<p>I want the result as in file-</p>

<pre><code>('a', 'b') 
('b', 'b')
('b', 'b')
('b', 'c')
('c', 'c')
......
</code></pre>
","8660280","8660280","2018-06-01 14:18:12","write the elements of list to file","<python><file>","3","2","658"
"50645464","2018-06-01 14:17:08","1","","<p>Python is creating an instance of Excel, setting <code>Application.Visible=False</code> and doing its thing there. It's actually tying up the UI thread of that instance, but you can't see it. During that time, you are able to open another instance of Excel by clicking on the Start Menu link (or double clicking a file), and you can use the UI thread of that instance to do other things.</p>

<p>When <em>you</em> open Excel, by default, it sets <code>Application.Visible=True</code>. Your macro is running in that instance and blocking the UI. Since you're using the default UI instance, it's blocked and you don't get the option to create another instance.</p>
","2344413","","","3","666","FreeMan","2013-05-02 18:48:24","5158","671","588","20","50645044","50645464","2018-06-01 13:56:06","0","95","<p>Just curious if anybody knows why this happens. </p>

<p>I have python code that triggers and runs a macro in a workbook. The macro can take several hours to run. If I run the macro without the use of Python, my workbook is open and for the time it takes the macro to run I am unable to use excel until the macro is completed. </p>

<p>What I noticed when I run the macro via Python, the sheet doesn't open and not only that but I am able at the same time python runs the excel macro I can actually open up other workbooks. </p>

<p>How does this happen? and where is the python triggered workbook macro run from? It doesn't even pop up in the Task Manager?</p>

<p>If anybody knows how this happens I would love to know! </p>

<p>Regards,
T</p>
","9875416","5202456","2018-06-01 14:12:33","Running Macro through Python and still able open new excel sheets","<python><excel><vba><excel-vba>","1","0","749"
"50645518","2018-06-01 14:19:57","2","","<p>You could also write your own wrapper around sqlite3 to support <code>with</code>:</p>

<pre><code>class SQLite():
    def __init__(self, file='sqlite.db'):
        self.file=file
    def __enter__(self):
        self.conn = sqlite3.connect(self.file)
        self.conn.row_factory = sqlite3.Row
        return self.conn.cursor()
    def __exit__(self, type, value, traceback):
        self.conn.commit()
        self.conn.close()

with SQLite('test.db') as cur:
    print(cur.execute('select sqlite_version();').fetchall()[0][0])
</code></pre>

<p><a href=""https://docs.python.org/2.5/whatsnew/pep-343.html#SECTION000910000000000000000"" rel=""nofollow noreferrer"">https://docs.python.org/2.5/whatsnew/pep-343.html#SECTION000910000000000000000</a></p>
","6078370","","","0","754","DSchmidt","2016-03-17 16:52:03","625","75","14","10","19522505","19522634","2013-10-22 15:45:50","11","8817","<p>I was doing a tutorial and came across a way to handle connections with sqlite3,
Then I studied about the WITH keyword and found out that it is an alternative to try,except,finally way of doing things</p>

<p>It was said that in case of file-handling, 'WITH' automatically handles closing of files and I thought similar with the connection as said in zetcode tutorial:-</p>

<blockquote>
  <p>""With the with keyword, the Python interpreter automatically releases
  the resources. It also provides error handling."" <a href=""http://zetcode.com/db/sqlitepythontutorial/"" rel=""nofollow noreferrer"">http://zetcode.com/db/sqlitepythontutorial/</a></p>
</blockquote>

<p>so I thought it would be good to use this way of handling things, but I couldn't figure out why both (inner scope and outer scope) statements work? shouldn't the WITH release the connection?</p>

<pre><code>import sqlite3

con = sqlite3.connect('test.db')

with con:    
    cur = con.cursor()    

    cur.execute('SELECT 1,SQLITE_VERSION()')
    data = cur.fetchone()   
    print data        

cur.execute('SELECT 2,SQLITE_VERSION()')
data = cur.fetchone()
print data
</code></pre>

<p>which outputs</p>

<pre><code>(1, u'3.6.21')
(2, u'3.6.21')
</code></pre>

<p>I don't know what exactly the WITH is doing here(or does in general), so, if you will please elaborate on the use of WITH over TRY CATCH in this context.</p>

<p>And should the connections be opened and closed on each query? (I am formulating queries inside a function which I call each time with an argument) Would it be a good practice?</p>
","2303994","1638010","2018-10-15 17:53:58","using sqlite3 in python with ""WITH"" keyword","<python><python-2.7><sqlite>","3","1","1577"
"50645529","2018-06-01 14:20:25","0","","<p>How about something like this (in psuedo code)?</p>

<pre><code>for each col in array
    for each row in col
        if array[col,row] == 0 &amp;&amp; row&gt;0
            array[col,row] = array[col,row-1]
</code></pre>

<p><strong>edit:</strong> Combined with @ukemi, who has a quicker solution, but does not loop over the various columns. Also, you need to make sure to not try to index array[0][-1].</p>
","9466270","","","0","411","Hein Wessels","2018-03-09 07:37:00","678","67","53","8","50645325","50645732","2018-06-01 14:09:57","3","150","<p>Currently I have an array as follows:</p>

<pre><code>myArray = np.array(
    [[ 976.77 ,  152.95 ,  105.62 ,   53.44 ,   0 ],
    [ 987.61 ,  156.63 ,  105.53 ,   51.1  ,    0 ],
    [1003.74 ,  151.31 ,  104.435,   52.86 ,    0 ],
    [ 968.   ,  153.41 ,  106.24 ,   58.98 ,    0 ],
    [ 978.66 ,  152.19 ,  103.28 ,   57.97 ,    0 ],
    [1001.9  ,  152.88 ,  105.08 ,   58.01 ,    0 ],
    [1024.93 ,  146.59 ,  107.06 ,   59.94 ,    0 ],
    [1020.01 ,  148.05 ,  109.96 ,   58.67 ,    0 ],
    [1034.01 ,  152.69 ,  107.64 ,   59.74 ,    0 ],
    [   0.   ,  154.88 ,  102.   ,   58.96 ,    0 ],
    [   0.   ,  147.46 ,  100.69 ,   54.95 ,    0 ],
    [   0.   ,  149.7  ,  102.439,   53.91 ,    0 ]]
)
</code></pre>

<p>I would like the fill in the zeros in the first column with the previous last value (1034.01) however if the 0's start from index 0, for it to remain as 0. </p>

<p>Example of end result:</p>

<pre><code>myArrayEnd = np.array(
    [[ 976.77 ,  152.95 ,  105.62 ,   53.44 ,   0 ],
    [ 987.61 ,  156.63 ,  105.53 ,   51.1  ,    0 ],
    [1003.74 ,  151.31 ,  104.435,   52.86 ,    0 ],
    [ 968.   ,  153.41 ,  106.24 ,   58.98 ,    0 ],
    [ 978.66 ,  152.19 ,  103.28 ,   57.97 ,    0 ],
    [1001.9  ,  152.88 ,  105.08 ,   58.01 ,    0 ],
    [1024.93 ,  146.59 ,  107.06 ,   59.94 ,    0 ],
    [1020.01 ,  148.05 ,  109.96 ,   58.67 ,    0 ],
    [1034.01 ,  152.69 ,  107.64 ,   59.74 ,    0 ],
    [1034.01 ,  154.88 ,  102.   ,   58.96 ,    0 ],
    [1034.01 ,  147.46 ,  100.69 ,   54.95 ,    0 ],
    [1034.01 ,  149.7  ,  102.439,   53.91 ,    0 ]]
)
</code></pre>

<p>I would like the code to be applicable to any array not just this one, where the situation may be different. (Column 3 might be all 0's and Column 4 might have 0's in the middle which should be filled with the last previous value).</p>
","9801467","9209546","2018-06-01 21:09:47","Fill values in a numpy array given a condition","<python><arrays><numpy>","5","3","1852"
"50645535","2018-06-01 14:20:37","1","","<p>Assuming I've understood you correctly, this should do the trick:</p>

<pre><code>def fill_zeroes(array):
    temp_array = array
    for i in xrange(1, len(temp_array)):
        if temp_array[i][0] == 0:
            temp_array[i][0] = temp_array[i-1][0]
    return temp_array
</code></pre>
","9067615","","","0","293","ukemi","2017-12-07 13:32:15","3409","280","1249","84","50645325","50645732","2018-06-01 14:09:57","3","150","<p>Currently I have an array as follows:</p>

<pre><code>myArray = np.array(
    [[ 976.77 ,  152.95 ,  105.62 ,   53.44 ,   0 ],
    [ 987.61 ,  156.63 ,  105.53 ,   51.1  ,    0 ],
    [1003.74 ,  151.31 ,  104.435,   52.86 ,    0 ],
    [ 968.   ,  153.41 ,  106.24 ,   58.98 ,    0 ],
    [ 978.66 ,  152.19 ,  103.28 ,   57.97 ,    0 ],
    [1001.9  ,  152.88 ,  105.08 ,   58.01 ,    0 ],
    [1024.93 ,  146.59 ,  107.06 ,   59.94 ,    0 ],
    [1020.01 ,  148.05 ,  109.96 ,   58.67 ,    0 ],
    [1034.01 ,  152.69 ,  107.64 ,   59.74 ,    0 ],
    [   0.   ,  154.88 ,  102.   ,   58.96 ,    0 ],
    [   0.   ,  147.46 ,  100.69 ,   54.95 ,    0 ],
    [   0.   ,  149.7  ,  102.439,   53.91 ,    0 ]]
)
</code></pre>

<p>I would like the fill in the zeros in the first column with the previous last value (1034.01) however if the 0's start from index 0, for it to remain as 0. </p>

<p>Example of end result:</p>

<pre><code>myArrayEnd = np.array(
    [[ 976.77 ,  152.95 ,  105.62 ,   53.44 ,   0 ],
    [ 987.61 ,  156.63 ,  105.53 ,   51.1  ,    0 ],
    [1003.74 ,  151.31 ,  104.435,   52.86 ,    0 ],
    [ 968.   ,  153.41 ,  106.24 ,   58.98 ,    0 ],
    [ 978.66 ,  152.19 ,  103.28 ,   57.97 ,    0 ],
    [1001.9  ,  152.88 ,  105.08 ,   58.01 ,    0 ],
    [1024.93 ,  146.59 ,  107.06 ,   59.94 ,    0 ],
    [1020.01 ,  148.05 ,  109.96 ,   58.67 ,    0 ],
    [1034.01 ,  152.69 ,  107.64 ,   59.74 ,    0 ],
    [1034.01 ,  154.88 ,  102.   ,   58.96 ,    0 ],
    [1034.01 ,  147.46 ,  100.69 ,   54.95 ,    0 ],
    [1034.01 ,  149.7  ,  102.439,   53.91 ,    0 ]]
)
</code></pre>

<p>I would like the code to be applicable to any array not just this one, where the situation may be different. (Column 3 might be all 0's and Column 4 might have 0's in the middle which should be filled with the last previous value).</p>
","9801467","9209546","2018-06-01 21:09:47","Fill values in a numpy array given a condition","<python><arrays><numpy>","5","3","1852"
"50645568","2018-06-01 14:22:12","2","","<p>Your problem lies in this line: </p>

<pre><code>b = a 
</code></pre>

<p>This doesn't do what you think it does. In particular, it does <em>not</em> make a copy of <code>a</code>. After the assignment, both <code>b</code> and <code>a</code> refer to the same object. Thus, any change to <code>b</code> is reflected in <code>a</code> also.</p>

<p>One way to force a shall copy is to use the slice syntax:</p>

<pre><code>b = a[:]
</code></pre>
","8747","8747","2018-06-01 14:26:50","2","448","Robᵩ","2008-09-15 16:47:33","124214","7724","5618","743","50645444","","2018-06-01 14:16:35","-3","49","<p>As I don't know what title should be given to my this confusion so I'm putting it just a doubt</p>

<pre><code>a = [1,2,3,4,5]
b = a 

for i in range(len(a)):
    c = (i - 4)
    print(a)
    print(b)
    b[c] = a[i]
    print(a)
    print(b)
</code></pre>

<p>output</p>

<pre><code>[1, 2, 3, 4, 5]
[1, 2, 3, 4, 5]
[1, 1, 3, 4, 5]
[1, 1, 3, 4, 5]
[1, 1, 3, 4, 5]
[1, 1, 3, 4, 5]
[1, 1, 1, 4, 5]
[1, 1, 1, 4, 5]
...
</code></pre>

<p>why values of list <code>a</code> is getting in each step of loop?</p>
","5924737","2971485","2018-06-01 15:48:38","Why value of input list is getting changed?","<python><python-3.x>","1","3","508"
"50645573","2018-06-01 14:22:23","1","","<p>the main idea is order by numeric and then by char part of the label, i can't reproduce and test, but solution may looks like:</p>

<p>first here the sql:</p>

<pre><code>SELECT 
  (regexp_matches(short_label, '^\d+'))[1]::numeric AS ln,
  regexp_matches(short_label, '^\D+') as ls,
  short_label
FROM YOUR_APP_TABLENAME ORDER BY 1, 2, 3;
</code></pre>

<p>annotaion in the orm:</p>

<p>for first sql condition i create <a href=""https://docs.djangoproject.com/en/2.0/ref/models/expressions/#func-expressions"" rel=""nofollow noreferrer"">custom Func</a></p>

<pre><code>In [1]: from myapp.models import *

In [2]: from django.db.models import F, Func, Value
   ...: 
   ...: class StartNumeric(Func):
   ...:     function = 'REGEXP_MATCHES'
   ...:     template = ""(%(function)s(%(expressions)s, '^\d+'))[1]::int""
   ...: 
   ...: qs = Ingredient.objects.annotate(
   ...:     ln=StartNumeric('short_label'),
   ...:     ls=Func('short_label', Value('^\D+'), function='regexp_matches'),
   ...:     ).values('ln').order_by('ln', 'ls', 'short_label')
   ...: 
   ...:     

In [3]: print(qs.query)
SELECT (REGEXP_MATCHES(""myapp_ingredient"".""short_label"", '^\d+'))[1]::int AS ""ln"" FROM ""myapp_ingredient"" ORDER BY ""ln"" ASC, regexp_matches(""myapp_ingredient"".""short_label"", ^\D+) ASC, ""myapp_ingredient"".""short_label"" ASC

In [4]: data = qs.values_list('short_label', flat=True)
   ...: print(list(data))
   ...: 
   ...: 
['1', '2c', '3c', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32c', '33c', '34', '35c', '36', '37', '38', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '200c', '260', '261', '262', '263', '264', '265fs', '266fs', '267c', '268c', '269c', '273c', '274c', '275c', '276c', '302', '501', '502', '503', '504', '505', '506', '507', '508', '509', '510', '511', '512', '513', '514', '515', '516', '517', '518', '519', '520', '521', '522', '524', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '701', '702', '703', '703', '704', '705', '706', '707', '708', '709', '710', '801', '802', '803', '804', '805', '806', '807', '808', '809', '810', '901', '902', 'aaaa', 'ddd', 'ddeee', 'rrrrr', 'S1', 'S10', 'S11', 'S12', 'S13', 'S14', 'S15', 'S16', 'S17', 'S18', 'S19', 'S2', 'S20', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'vvvv', 'zzzz']
</code></pre>

<p>hope it help</p>
","8060120","8060120","2018-06-02 10:50:21","10","2887","Bear Brown","2017-05-24 14:25:13","13388","1962","2764","38","50645204","50645573","2018-06-01 14:03:59","3","118","<p>I have the following in Manager:</p>

<pre><code>class NullIf(Func):
template = ""NULLIF(%(expressions)s, '')""

class MySiteManager(models.Manager):

def get_queryset(self):
    qws = MySiteQuerySet(self.model, using=self._db).filter(
        some_id=settings.BASE_SOME_ID).annotate(
            # This is made for sorting by short labels as by numeric values
            short_label_numeric=Cast(
                NullIf(Func(
                    F('short_label'),
                    Value('^(\D+)|(\w+)'),
                    Value(''),
                    Value('g'),
                    function='regexp_replace')),
                models.BigIntegerField())
            ).order_by('short_label_numeric', 'short_label')

    for q in qws:
        print(q.short_label, end='\n')

    return qws
</code></pre>

<p>Output of print values looks like:</p>

<p>1
10
100
101
102
103
104
105
106
107
108
109
11
110
111
112
113
114
115
116
117
118
119
12
120
121
122
123
124
125
126
127
128
129
13
130
131
132
133
134
135
136
137
138
139
14
140
141
142
143
144
145
146
147
148
149
15
150
151
152
153
154
155
156
157
158
159
16
17
18
19
20
200c
21
22
23
24
25
26
260
261
262
263
264
265fs
266fs
267c
268c
269c
27
273c
274c
275c
276c
28
29
2c
30
302
31
32c
33c
34
35c
36
37
38
3c
4
5
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
524
6
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
7
701
702
703
704
705
706
707
708
709
710
8
801
802
803
804
805
806
807
808
809
810
9
901
902
S1
S10
S11
S12
S13
S14
S15
S16
S17
S18
S19
S2
S20
S3
S4
S5
S6
S7
S8
S9</p>

<p>And my question:
How to build queryset with output looks like e.g. 1 2 3 3c 4 5 6 6c ... 264 265fs 266fs 267c 268c 269c ... S1 S2 S3 S4 ??? Does someone have any assumptions?</p>
","6360146","","","Get sorted queryset by specified field with regex in django","<python><regex><django><django-models><django-queryset>","2","0","1816"
"50645617","2018-06-01 14:24:55","1","","<p>Use <code>dict</code> to map key (eg: 'DATE' ) to its value.</p>

<pre><code>import re
s = '''DATE: 7/25/2017 DATE OPENED: 7/25/2017 RETURN DATE: 7/26/2017 NUMBER: 201707250008754 RATE:  10.00'''

items = re.findall('\s*(.*?)\:\s*([0-9/.]*)',s)
#[('DATE', '7/25/2017'), ('DATE OPENED', '7/25/2017'), ('RETURN DATE', '7/26/2017'), ('NUMBER', '201707250008754'), ('RATE', '10.00')]

info = dict(items)
#{'DATE': '7/25/2017', 'DATE OPENED': '7/25/2017', 'RETURN DATE': '7/26/2017', 'NUMBER': '201707250008754', 'RATE': '10.00'}


for key in ['DATE', 'RETURN DATE', 'NUMBER']:
    print(info[key])
</code></pre>
","8524503","","","1","611","Abhijith Asokan","2017-08-27 19:23:17","1599","127","415","13","50645034","","2018-06-01 13:55:30","3","55","<p>I wanted to extract the date from the given string on the basis of tag. </p>

<p>My string is - </p>

<pre><code>DATE: 7/25/2017 DATE OPENED: 7/25/2017 RETURN DATE: 7/26/2017 
NUMBER: 201707250008754 RATE:  10.00
</code></pre>

<p>I want something like this - 
If I give ""DATE"" it should return <code>7/25/2017</code> only </p>

<p>if I give ""RETURN DATE"" it should return <code>7/26/2017</code></p>

<p>if I give the ""NUMBER"" it should return <code>201707250008754</code>
and so on.</p>

<p>How we can achieve this in Python 2.7 (Note: Dates and numbers are always random in string""</p>
","6504894","789671","2018-06-01 17:05:14","How to extract the file Data in python","<python><python-2.7>","2","2","591"
"50645630","2018-06-01 14:25:35","4","","<p>If you include the following code at the top of your script, <code>matplotlib</code> will run inline when in an <code>IPython</code> environment (like jupyter, hydrogen atom plugin...), and it will still work if you launch the script directly via command line (<code>matplotlib</code> won't run inline, and the charts will open in a pop-ups as usual).</p>

<pre><code>from IPython import get_ipython
ipy = get_ipython()
if ipy is not None:
    ipy.run_line_magic('matplotlib', 'inline')
</code></pre>
","2093076","","","0","504","Erwan Swak","2013-02-20 21:13:48","183","21","7","1","35595766","","2016-02-24 07:24:39","42","84940","<p>I try to run the following codes on Spyder (Python 2.7.11):</p>

<pre><code># -*- coding: utf-8 -*-

import numpy as np
import pandas as pd

%matplotlib inline

import matplotlib.pyplot as plt
import matplotlib.cm as cm

import tensorflow as tf

# settings
LEARNING_RATE = 1e-4
# set to 20000 on local environment to get 0.99 accuracy
TRAINING_ITERATIONS = 2000        

DROPOUT = 0.5
BATCH_SIZE = 50

# set to 0 to train on all available data
VALIDATION_SIZE = 2000

# image number to output
IMAGE_TO_DISPLAY = 10
</code></pre>

<p>But I got this error:</p>

<pre><code>line 10
    %matplotlib inline
    ^
SyntaxError: invalid syntax.
</code></pre>

<p>I appreciate if anybody gives me an explanation.</p>

<p>P.S. the code is from Kaggle competition project: Digit Recognizer</p>
","5972819","3005167","2018-05-24 13:29:07","%matplotlib line magic causes SyntaxError in Python script","<python><matplotlib><ipython><spyder>","7","3","786"
"50645649","2018-06-01 14:26:14","1","","<p>Used the render method of model views to bypass my problem:</p>

<pre><code>class UserView(ModelView):
    def render(self):
        if not self.can_create:
            flash(gettext(""You can't create other user""), 'error')

        return super(UserView, self).render()
</code></pre>
","7529716","","","0","288","Yassine Faris","2017-02-07 15:47:46","735","56","111","96","50525890","50645649","2018-05-25 09:29:25","1","485","<p>I am trying to use the flash system to display a message to the user in the admin view if he doesn't have certain right.<br>
The following almost work, but the message is only display the next time the user change the page.</p>

<pre><code>from flask_admin.contrib.sqla import ModelView
from flask_sqlalchemy import SQLAlchemy
from flask_admin import Admin
from flask import flash, Flask
from flask_admin.babel import gettext


db = SQLAlchemy()


class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(80), unique=True, nullable=False)
    email = db.Column(db.String(120), unique=True, nullable=False)

    def __repr__(self):
        return '&lt;User %r&gt;' % self.username




class UserView(ModelView):
    def check_can_create(self):
        can_create = False
        if can_create:
            print(""wat"")
            return True
        else:
            print(""flash"")
            flash(gettext(""You can't create other user""), 'error')
            return False

    can_create = property(check_can_create)



admin = Admin()
admin.add_view(UserView(User, db.session))

app = Flask(__name__)
app.secret_key = b'ninja'
db.init_app(app)
db.app = app
db.create_all()
admin.init_app(app)

app.run()  # Goto 127.0.0.1:5000/admin
</code></pre>

<p>I think it's du to the fact that flask first fetch all the flash message then check the can_create/delete flag of the model_view.<br>
Do someone have a solution?</p>
","7529716","7529716","2019-04-03 14:46:35","Flask admin view and flash message","<python><flask><flask-admin>","1","0","1473"
"50645662","2018-06-01 14:26:59","1","","<p>I'm using the <code>1.1.1</code> version of Allure Adaptor for Robot Framework and the severity is picked from the test case tags and added as a label under the <code>test-case</code> element of the report.</p>

<p>However, it seems that Allure <code>2.6.0</code> is also expecting a valid value for the <code>severity</code> attribute of the <code>test-case</code> element.</p>

<p>In order to use Allure2 with the current reports I have altered <code>AllureListener.py</code> to also add the severity to the test case:</p>

<pre><code>elif tag in SEVERITIES:
    test.severity = tag
    test.labels.append(TestLabel(
        name='severity',
        value=tag
    ))
</code></pre>
","19756","","","0","686","alexandrul","2008-09-20 20:28:04","9291","1282","6306","13","47031779","","2017-10-31 09:19:12","2","898","<p>I am using Ride (RobotFramework IDE) and I have imported Library <code>AllureReportLibrary</code> in my project. 
Using the <code>Set Output Dir</code>, I am creating a Directory <code>C:/AutomationLogs/Allure</code> and all the allure properties and xml files are getting generated in that path.</p>

<pre><code>Set Output Dir   C:/AutomationLogs/
</code></pre>

<p>Then I am using the ""allure serve C:\AutomationLogs\Allure"" command to try and generate the html report file in command prompt, but it shows the below error -</p>

<blockquote>
  <p>""Could not read result
  C:\AutomationLogs\Allure\f56f4796-d30a-47f3-a988-d17f6c4e13ca-testsuite.xml:
  {} com.fasterxml.jackson.databind.exc.InvalidFormatException: Cannot
  deserialize va lue of type
  <code>ru.yandex.qatools.allure.model.SeverityLevel</code> from String ""None"":
  value not one of declared Enum instance names: [trivial, blocker,
  minor, normal, critical]""</p>
</blockquote>

<p>The xml file ""<code>f56f4796-d30a-47f3-a988-d17f6c4e13ca-testsuite.xml</code>"" was generated using the AllureReportLibrary</p>

<p>Also the index.html file which is generated after the command opens after this command and shows Allure Report unknown 
unknown - unknown (Unknown)   0 test cases    NaN%</p>

<p>I am using the below -
Allure version - 2.4.1</p>

<p>Ride version - RIDE 1.5.2.1 running on Python 2.7.12. </p>

<p>I am new to Robot Framework and Allure. Please let me know whether I have implemented it correctly and why I am facing the above error.</p>

<p>-Ryan M</p>
","6670988","6152737","2017-11-01 09:31:43","Allure not able to read output.xml file generated by AllureReportLibrary in Robot Framework","<python><xml><robotframework><allure><robotframework-ide>","3","1","1535"
"50645672","2018-06-01 14:27:16","5","","<p>IIUC</p>

<pre><code>table1.stack().isin(table2.stack().values).unstack()
Out[207]: 
       a      b
0   True   True
1   True   True
2  False  False
3   True   True
4   True   True
</code></pre>

<p>If check the row bases</p>

<pre><code>table1.astype(str).sum(1).isin(table2.astype(str).sum(1))
</code></pre>

<p>By using <code>merge</code></p>

<pre><code>table1.merge(table2.assign(vec=True),how='left').fillna(False)
Out[232]: 
   a  b    vec
0  1  a   True
1  2  b   True
2  5  e  False
3  3  c   True
4  4  d   True
</code></pre>
","7964527","7964527","2018-06-01 15:08:50","7","539","WeNYoBen","2017-05-04 16:45:29","164847","15327","4764","689","50645297","50645672","2018-06-01 14:08:23","2","619","<p>I'm trying to check if rows exist in another dataframe. I'm not joining/merging because of issues with it creating duplication, and then needing to filter out that duplication might also filter out actual duplication that I <em>want</em> to keep.</p>

<p><em>example:</em></p>

<pre><code>table1 = pd.DataFrame({'a':[1, 2, 5, 3, 4],
              'b':['a', 'b', 'e', 'c', 'd']})
table2 = pd.DataFrame({'a':[1, 4, 3, 6, 2],
              'b':['a', 'd', 'c', 'f', 'b']})


table1.isin(table2)

       a      b
0   True   True
1  False  False
2  False  False
3  False  False
4  False  False
</code></pre>

<p>I would like all of these to be <code>True</code> except at index 2 where row <code>5 e</code> doesn't exist in <code>table2</code>.</p>
","7237997","7237997","2018-06-01 14:24:09","How to use isin while ignoring index","<python><pandas>","2","0","746"
"50645722","2018-06-01 14:30:08","0","","<p>The code below requires testing:</p>

<pre><code>values = myArray.to_list()    # don't remember if nd_array.to_list is a method or property
result = []
last = None
for i,item in enumerate(values):
    if i == 0 and item[0] == 0:
        last = item
    elif item[0] == 0 and last is not None:
        item[0] = last
    else:
        last = item[0]

    result.append(item)
</code></pre>
","4150857","","","0","391","Gomes J. A.","2014-10-16 17:46:47","85","16","9","0","50645325","50645732","2018-06-01 14:09:57","3","150","<p>Currently I have an array as follows:</p>

<pre><code>myArray = np.array(
    [[ 976.77 ,  152.95 ,  105.62 ,   53.44 ,   0 ],
    [ 987.61 ,  156.63 ,  105.53 ,   51.1  ,    0 ],
    [1003.74 ,  151.31 ,  104.435,   52.86 ,    0 ],
    [ 968.   ,  153.41 ,  106.24 ,   58.98 ,    0 ],
    [ 978.66 ,  152.19 ,  103.28 ,   57.97 ,    0 ],
    [1001.9  ,  152.88 ,  105.08 ,   58.01 ,    0 ],
    [1024.93 ,  146.59 ,  107.06 ,   59.94 ,    0 ],
    [1020.01 ,  148.05 ,  109.96 ,   58.67 ,    0 ],
    [1034.01 ,  152.69 ,  107.64 ,   59.74 ,    0 ],
    [   0.   ,  154.88 ,  102.   ,   58.96 ,    0 ],
    [   0.   ,  147.46 ,  100.69 ,   54.95 ,    0 ],
    [   0.   ,  149.7  ,  102.439,   53.91 ,    0 ]]
)
</code></pre>

<p>I would like the fill in the zeros in the first column with the previous last value (1034.01) however if the 0's start from index 0, for it to remain as 0. </p>

<p>Example of end result:</p>

<pre><code>myArrayEnd = np.array(
    [[ 976.77 ,  152.95 ,  105.62 ,   53.44 ,   0 ],
    [ 987.61 ,  156.63 ,  105.53 ,   51.1  ,    0 ],
    [1003.74 ,  151.31 ,  104.435,   52.86 ,    0 ],
    [ 968.   ,  153.41 ,  106.24 ,   58.98 ,    0 ],
    [ 978.66 ,  152.19 ,  103.28 ,   57.97 ,    0 ],
    [1001.9  ,  152.88 ,  105.08 ,   58.01 ,    0 ],
    [1024.93 ,  146.59 ,  107.06 ,   59.94 ,    0 ],
    [1020.01 ,  148.05 ,  109.96 ,   58.67 ,    0 ],
    [1034.01 ,  152.69 ,  107.64 ,   59.74 ,    0 ],
    [1034.01 ,  154.88 ,  102.   ,   58.96 ,    0 ],
    [1034.01 ,  147.46 ,  100.69 ,   54.95 ,    0 ],
    [1034.01 ,  149.7  ,  102.439,   53.91 ,    0 ]]
)
</code></pre>

<p>I would like the code to be applicable to any array not just this one, where the situation may be different. (Column 3 might be all 0's and Column 4 might have 0's in the middle which should be filled with the last previous value).</p>
","9801467","9209546","2018-06-01 21:09:47","Fill values in a numpy array given a condition","<python><arrays><numpy>","5","3","1852"
"50645732","2018-06-01 14:30:36","4","","<p>Here's a vectorised way with <code>pandas</code>. This is also possible with <code>numpy</code>. In any case, you should not need explicit loops for this task.</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame(myArray)\
       .replace(0, np.nan)\
       .ffill().fillna(0)

res = df.values

print(res)

[[  976.77    152.95    105.62     53.44      0.   ]
 [  987.61    156.63    105.53     51.1       0.   ]
 [ 1003.74    151.31    104.435    52.86      0.   ]
 [  968.      153.41    106.24     58.98      0.   ]
 [  978.66    152.19    103.28     57.97      0.   ]
 [ 1001.9     152.88    105.08     58.01      0.   ]
 [ 1024.93    146.59    107.06     59.94      0.   ]
 [ 1020.01    148.05    109.96     58.67      0.   ]
 [ 1034.01    152.69    107.64     59.74      0.   ]
 [ 1034.01    154.88    102.       58.96      0.   ]
 [ 1034.01    147.46    100.69     54.95      0.   ]
 [ 1034.01    149.7     102.439    53.91      0.   ]]
</code></pre>
","9209546","","","0","982","jpp","2018-01-12 14:47:22","109049","18235","7890","3496","50645325","50645732","2018-06-01 14:09:57","3","150","<p>Currently I have an array as follows:</p>

<pre><code>myArray = np.array(
    [[ 976.77 ,  152.95 ,  105.62 ,   53.44 ,   0 ],
    [ 987.61 ,  156.63 ,  105.53 ,   51.1  ,    0 ],
    [1003.74 ,  151.31 ,  104.435,   52.86 ,    0 ],
    [ 968.   ,  153.41 ,  106.24 ,   58.98 ,    0 ],
    [ 978.66 ,  152.19 ,  103.28 ,   57.97 ,    0 ],
    [1001.9  ,  152.88 ,  105.08 ,   58.01 ,    0 ],
    [1024.93 ,  146.59 ,  107.06 ,   59.94 ,    0 ],
    [1020.01 ,  148.05 ,  109.96 ,   58.67 ,    0 ],
    [1034.01 ,  152.69 ,  107.64 ,   59.74 ,    0 ],
    [   0.   ,  154.88 ,  102.   ,   58.96 ,    0 ],
    [   0.   ,  147.46 ,  100.69 ,   54.95 ,    0 ],
    [   0.   ,  149.7  ,  102.439,   53.91 ,    0 ]]
)
</code></pre>

<p>I would like the fill in the zeros in the first column with the previous last value (1034.01) however if the 0's start from index 0, for it to remain as 0. </p>

<p>Example of end result:</p>

<pre><code>myArrayEnd = np.array(
    [[ 976.77 ,  152.95 ,  105.62 ,   53.44 ,   0 ],
    [ 987.61 ,  156.63 ,  105.53 ,   51.1  ,    0 ],
    [1003.74 ,  151.31 ,  104.435,   52.86 ,    0 ],
    [ 968.   ,  153.41 ,  106.24 ,   58.98 ,    0 ],
    [ 978.66 ,  152.19 ,  103.28 ,   57.97 ,    0 ],
    [1001.9  ,  152.88 ,  105.08 ,   58.01 ,    0 ],
    [1024.93 ,  146.59 ,  107.06 ,   59.94 ,    0 ],
    [1020.01 ,  148.05 ,  109.96 ,   58.67 ,    0 ],
    [1034.01 ,  152.69 ,  107.64 ,   59.74 ,    0 ],
    [1034.01 ,  154.88 ,  102.   ,   58.96 ,    0 ],
    [1034.01 ,  147.46 ,  100.69 ,   54.95 ,    0 ],
    [1034.01 ,  149.7  ,  102.439,   53.91 ,    0 ]]
)
</code></pre>

<p>I would like the code to be applicable to any array not just this one, where the situation may be different. (Column 3 might be all 0's and Column 4 might have 0's in the middle which should be filled with the last previous value).</p>
","9801467","9209546","2018-06-01 21:09:47","Fill values in a numpy array given a condition","<python><arrays><numpy>","5","3","1852"
"50645793","2018-06-01 14:34:34","1","","<p>Just to add: in case the other answers do not work for you, try putting the static url before the other ones. Like so:</p>

<pre><code>urlpatterns = static(...) + [...]
</code></pre>

<p>What may be happening is that some of your patterns in the list prevent the request from reaching the static handlers. So putting the static handlers first solves this. Worked for me. </p>
","5904193","","","0","379","Seyi Shoboyejo","2016-02-09 16:07:42","51","12","17","0","36280056","36280131","2016-03-29 09:14:35","24","17253","<p>I am able to upload the files to media folder( <code>'/peaceroot/www/media/'</code>) that I have set up in <code>settings.py</code> as below</p>

<pre><code>MEDIA_ROOT = '/peaceroot/www/media/'
MEDIA_URL = '/media/'
</code></pre>

<p>But through admin I tried to access the uploaded image file</p>

<p><a href=""http://localhost:8000/media/items/1a39246c-4160-4cb2-a842-12a1ffd72b3b.jpg"" rel=""noreferrer"">http://localhost:8000/media/items/1a39246c-4160-4cb2-a842-12a1ffd72b3b.jpg</a></p>

<p>then I am getting 404 error. </p>

<p>The file exists at <code>peaceroot/www/media/items/1a39246c-4160-4cb2-a842-12a1ffd72b3b.jpg</code></p>
","1579374","","","Page not found 404 Django media files","<python><django><django-settings><django-media>","3","0","635"
"50645805","2018-06-01 14:35:11","1","","<p>In Python 2.7 this will raise a syntax error. It feels like abuse of <em>dictionary</em> (key-value storage) concept. Maybe you should rework your code and you could use <code>'Common'</code>, <code>'Rare'</code> as keys and values as ranges, i.e. <code>range(5,20)</code>, <code>range(20)</code>, etc.</p>
","2242445","","","0","310","rook","2013-04-03 21:41:46","3734","326","847","1","50645700","","2018-06-01 14:28:53","0","79","<p>Was wondering if something like the following was possible:</p>

<pre><code>rarity = {&gt;= 75: 'Common', &lt;= 20 : 'Rare', &gt;= 5: 'Legendary'}
</code></pre>
","9640428","2242445","2018-06-01 14:36:15","Python dict keys as statements","<python><python-3.x><dictionary><conditional>","3","6","164"
"50645807","2018-06-01 14:35:14","5","","<p>I think there is problem with the path.</p>

<p>I <strong>strongly recommend</strong> using virtual environment for all django development.</p>

<p>You can follow this process:</p>

<p><strong>Install pip3</strong></p>

<pre><code>sudo apt-get install python3-pip
</code></pre>

<p><strong>Install Virtual Environment for Python3</strong></p>

<pre><code>sudo pip3 install virtualenv
</code></pre>

<p><strong>Create a project directory</strong></p>

<pre><code>mkdir ~/newproject
cd ~/newproject
</code></pre>

<p><strong>Create a new virtual environemnt and activate it</strong></p>

<pre><code>virtualenv .venv
source .venv/bin/activate
</code></pre>

<p><strong>Now Install Django</strong></p>

<pre><code>pip install django
</code></pre>

<p>and then <strong>create project and start</strong> it,</p>

<pre><code>django-admin startproject my_project

cd my_project

python manage.py runserver
</code></pre>

<p>It should work this way.</p>
","6794568","6794568","2018-06-01 16:02:25","10","948","Astik Anand","2016-09-05 01:20:45","7759","1083","647","134","50645548","50645807","2018-06-01 14:21:18","3","1459","<p>I am starting up with Python-Django in Ubuntu 18.04.</p>

<p>I have python3 installed. </p>

<p><code>python3 --version</code> says <code>Python 3.5.2</code></p>

<p>After installing Python, I installed Django as below:</p>

<pre><code>sudo apt install python3-pip
pip3 install django
</code></pre>

<p>I also have Django installed. </p>

<p><code>django-admin --version</code> says <code>2.0.5</code></p>

<p>In my project, <strong>startproject</strong> worked successfully, but when I am trying to run the following command inside my project:</p>

<pre><code>python3 manage.py runserver
</code></pre>

<p>It gives following errors:</p>

<pre><code>Traceback (most recent call last):
  File ""manage.py"", line 8, in &lt;module&gt;
    from django.core.management import execute_from_command_line
ImportError: No module named 'django'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""manage.py"", line 14, in &lt;module&gt;
    ) from exc
ImportError: Couldn't import Django. Are you sure it's installed and available on your PYTHONPATH environment variable? Did you forget to activate a virtual environment?
</code></pre>

<p>Following command also gives error:</p>

<pre><code>python3 -c ""import django; print(django.__path__)""
</code></pre>

<p>Error is:</p>

<pre><code>python3 -c ""import django; print(django.__path__)""
Traceback (most recent call last):
  File ""&lt;string&gt;"", line 1, in &lt;module&gt;
ImportError: No module named 'django'
</code></pre>

<p><code>which django</code> gives blank output</p>

<p><code>echo $PYTHONPATH</code> gives blank output</p>

<p><code>python3 -m django --version</code> says <code>/usr/local/bin/python3: No module named django</code></p>

<p><code>echo $PATH</code> shows <code>/home/shobhit/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin</code></p>

<p>What is the problem and what is the solution here?</p>

<p><strong>Update</strong></p>

<p><code>ls /usr/local/lib | grep</code> python says</p>

<blockquote>
  <p>libpython3.5m.a </p>
  
  <p>python3.5</p>
  
  <p>python3.6</p>
</blockquote>
","6717660","6794568","2018-06-04 06:59:46","Startup with Django runserver error","<python><django><linux><python-3.x><ubuntu>","1","6","2175"
"50645856","2018-06-01 14:37:51","1","","<p>You are getting the <code>404</code> error because in your <code>urls.py</code> you have not added url for <code>/</code>.</p>

<p>Try in your browser: <code>localhost:8000/admin</code> and it should work.</p>
","6794568","6794568","2018-06-01 14:43:14","0","213","Astik Anand","2016-09-05 01:20:45","7759","1083","647","134","50645707","","2018-06-01 14:29:26","0","89","<p>I wanted to run official Django tutorial. I've installed Django using pip, and started the project. And when I run <code>python manage.py runserver</code> the console shows the same thing as in tutorial, but when I open the page <code>http://127.0.0.1:8000/</code> or <code>localhost:8000</code> I only see <code>404 not found</code>.</p>
","9881747","1007939","2018-06-01 15:24:24","Can't run Django project","<python><django>","1","5","342"
"50645869","2018-06-01 14:38:38","1","","<p>To answer your question: there is not alternative exposed by the S3 API.</p>

<p>Using multiple threads or asynchronous I/O are solid ways to reduce the real time required to make multiple requests, by doing them in parallel, as you mentioned.</p>

<p>A further enhancement that might be worth considering would be to wrap this logic up in an AWS Lambda function that you could invoke with a bucket name and a list of object keys as arguments.  Parallellize the bucket operations inside the Lambda function and return the results to the caller already parsed and interpeted, in one tidy response.  This would put most of the bandwidth usage between the function and S3 on the AWS network within the region, which should be the fastest possible place for it to happen.  Lambda functions are an excellent way to abstract away any AWS interaction that requires multiple API requests.  </p>

<p>This also allows your Lambda function to be written in a different language than the main project, if desired, because the language does not matter across that boundary -- it's just JSON crossing the border between the two.  Some AWS interactions are easier to do (or to execute in complex series/parallel fashion) in some languages than in others, in my opinion, so for example, your function could be written in Node.JS even though your project is written in python, and this would make no difference when it comes to invoking the funcrion and using the response it generates.</p>
","1695906","1695906","2018-06-01 14:43:48","0","1477","Michael - sqlbot","2012-09-25 00:40:52","109025","8343","2655","752","50638573","50645869","2018-06-01 07:49:59","0","74","<p>I have a list of S3 keys for the same bucket <code>my_s3_bucket</code>.</p>

<p>What is the most efficient way to figure out which of those keys actually exist in aws S3. By efficient I mean with low latency and hopefully low network bandwidth usage.</p>

<p>Note: the keys don't share the same prefix so filtering by a single prefix is not effective</p>

<p>The two suboptimal approaches I can think of:</p>

<ol>
<li>Check the existence of each key, one-by-one</li>
<li>List all keys in the bucket and check locally. This is not good if the total number of keys is large since listing the keys will still incur many network calls.</li>
</ol>

<p>Is there any better alternative?</p>
","1488945","1488945","2018-06-01 08:13:07","What is the most efficient way in python of checking the existence of multiple s3 keys in the same bucket?","<python><amazon-web-services><amazon-s3><boto3>","1","2","688"
"50645879","2018-06-01 14:39:02","1","","<p>This cannot be done with <code>dict</code> in python. You probably need an ordinary function for your task:</p>

<pre><code>def check(x):
    if x &gt;= 75:
        return 'Common'
    if x &lt;= 20:
        ...
</code></pre>

<p>Remember that order of checks and <code>return</code> statements matters.</p>
","2315573","2315573","2018-06-01 14:44:43","4","311","Ivan Vinogradov","2013-04-24 12:46:24","2337","361","1232","3","50645700","","2018-06-01 14:28:53","0","79","<p>Was wondering if something like the following was possible:</p>

<pre><code>rarity = {&gt;= 75: 'Common', &lt;= 20 : 'Rare', &gt;= 5: 'Legendary'}
</code></pre>
","9640428","2242445","2018-06-01 14:36:15","Python dict keys as statements","<python><python-3.x><dictionary><conditional>","3","6","164"
"50645887","2018-06-01 14:39:22","0","","<p>I have found a very easy way to Delete any <strong>folder(Even NOT Empty)</strong> or file on <strong>WINDOWS OS</strong>.</p>

<pre><code>os.system('powershell.exe  rmdir -r D:\workspace\Branches\*%s* -Force' %CANDIDATE_BRANCH)
</code></pre>
","7522029","7522029","2018-06-01 15:29:47","0","246","seremet","2017-02-06 08:12:51","88","10","3","0","303200","303225","2008-11-19 20:15:38","782","581339","<p>I am getting an 'access is denied' error when I attempt to delete a folder that is not empty. I used the following command in my attempt: <code>os.remove(""/folder_name"")</code>. </p>

<p>What is the most effective way of removing/deleting a folder/directory that is not empty?</p>
","37804","7851470","2019-09-23 09:35:37","How do I remove/delete a folder that is not empty?","<python><file>","18","2","284"
"50645895","2018-06-01 14:39:37","1","","<p>The <code>_id</code> field is not part of the payload, but you will have access to it if you use an <code>on_inserted_&lt;resource_name&gt;</code> database hook, since eve adds <code>_id</code> before making the database insert. Documentation here (<a href=""http://python-eve.org/features.html#database-event-hooks"" rel=""nofollow noreferrer"">http://python-eve.org/features.html#database-event-hooks</a>)</p>
","4950452","4950452","2018-06-03 21:28:04","1","411","gcw","2015-05-28 18:18:23","1146","132","1164","7","50608868","50645895","2018-05-30 15:48:41","1","132","<p>I have a registration endpoint where no authentication is required for POST requests, therefore also no AUTH_FIELD is added to the mongodb collection when an account is created.</p>

<p>As I need the user-restricted resource access also for this endpoint, I'm trying to add the field with a post event hook:</p>

<p>AUTH_FIELD = 'user_id'</p>

<pre><code>def adduserid(request, payload):
  data = json.loads(payload.get_data().decode('utf-8'))
  setid = data['_id']
  app.data.driver.db['accounts'].update({""username"" : username},{""$set"": {""user_id"": setid}})

app.on_post_POST_accounts += adduserid
</code></pre>

<p>I get a KeyError as result:</p>

<pre><code>File ""run.py"", line 30, in adduserid
setid = data['_id']
KeyError: '_id'
</code></pre>

<p>I have the feeling that I'm parsing this payload object in a wrong way, but I don't know what's wrong.
Maybe there is an easier way to do it at all?</p>
","2463796","","","Python Eve: Add _id value in auth_field with event hook","<python><eve>","1","2","909"
"50645922","2018-06-01 14:41:12","14","","<p>After spending many weeks on this and trying all the alternatives - PyInstaller, py2exe, cx_freeze,... - I created my own library: <a href=""https://build-system.fman.io/"" rel=""noreferrer"">https://build-system.fman.io/</a>. It is based on PyInstaller but solves many of its common pain points. It also lets you create native installers on Windows, Mac and Linux.</p>
","1839209","1839209","2018-06-02 07:01:38","0","369","Michael Herrmann","2012-11-20 15:06:23","2852","446","123","8","5888870","5916707","2011-05-04 19:32:07","43","55123","<p>I started to fiddle with PyQt, and made a ""beautiful"" script from the pyqt whitepaper example app (<a href=""http://pastebin.com/NZnpxv2F"" rel=""noreferrer"">pastebin</a>)</p>

<p>It works perfectly in Windows and Linux (with qt environment already installed on both).</p>

<p>Now my question is: Since I am trying to use Qt because it is compiled (at least pure old C++ based Qt), how can I compile some .exe file to run it on Windows, or a standalone executable for Linux.</p>

<p>The point is that I want the program to be compiled, because of speed and portability, instead of interpreted from source, which would require a previous setup on any machine. One of the goals, for example, is sending small gui scripts via email to coworkers who are not programmers at all.</p>
","401828","3878253","2016-03-21 01:56:44","How do I compile a PyQt script (.py) to a single standalone executable file for windows (.exe) and/or linux?","<python><qt4><compilation><executable><pyqt4>","6","5","778"
"50645929","2018-06-01 14:41:33","1","","<p>Functions take arguments and return values. Use those features. Send the line:</p>

<pre><code>def readFile():
    for line in file:
        test2(line)    #  &lt;-- send line to test2
        // do things...
</code></pre>
","389289","","","2","226","zvone","2010-07-12 08:25:02","11303","980","601","663","50645781","50646377","2018-06-01 14:34:03","-1","40","<p>My question is really simple. Let me explain my problem with an exemple code. I have developed a specific file parser and each line represents an object.</p>

<pre><code>file = open(""file.txt"", ""r"")

class Reader(object):
    def __init__(self):
        self.i = 1   // first line

    def readFile(self):
        for line in file:
            test2()
            // do things..


class Line():
    def printLine(self):
        f.readline() // Need to read the current line from the readFile() loop 


test()
</code></pre>

<p>So, what I would like to know is how can I get access to the current line from <code>readFile()</code> in <code>test2().printLine()</code> without reopening the file ?</p>
","9792948","8033585","2018-06-01 18:48:38","How to get a line file from a class in an other class?","<python><class><parsing>","2","6","702"
"50645935","2018-06-01 14:41:45","10","","<p>So, for those who would like to know the summary of that discussion. The final top scores for counting a 50 million-lengthed generator expression using: </p>

<ul>
<li><code>len(list(gen))</code>, </li>
<li><code>len([_ for _ in gen])</code>, </li>
<li><code>sum(1 for _ in gen),</code> </li>
<li><code>ilen(gen)</code> (from <a href=""https://pypi.org/project/more-itertools/"" rel=""noreferrer"">more_itertool</a>), </li>
<li><code>reduce(lambda c, i: c + 1, gen, 0)</code>, </li>
</ul>

<p>sorted by performance of execution (including memory consumption), will make you surprised:</p>

<p>```</p>

<h1>1: test_list.py:8: 0.492 KiB</h1>

<pre><code>gen = (i for i in data*1000); t0 = monotonic(); len(list(gen))
</code></pre>

<p>('list, sec', 1.9684218849870376)</p>

<h1>2: test_list_compr.py:8: 0.867 KiB</h1>

<pre><code>gen = (i for i in data*1000); t0 = monotonic(); len([i for i in gen])
</code></pre>

<p>('list_compr, sec', 2.5885991149989422)</p>

<h1>3: test_sum.py:8: 0.859 KiB</h1>

<pre><code>gen = (i for i in data*1000); t0 = monotonic(); sum(1 for i in gen); t1 = monotonic()
</code></pre>

<p>('sum, sec', 3.441088170016883)</p>

<h1>4: more_itertools/more.py:413: 1.266 KiB</h1>

<pre><code>d = deque(enumerate(iterable, 1), maxlen=1)

test_ilen.py:10: 0.875 KiB
gen = (i for i in data*1000); t0 = monotonic(); ilen(gen)
</code></pre>

<p>('ilen, sec', 9.812256851990242)</p>

<h1>5: test_reduce.py:8: 0.859 KiB</h1>

<pre><code>gen = (i for i in data*1000); t0 = monotonic(); reduce(lambda counter, i: counter + 1, gen, 0)
</code></pre>

<p>('reduce, sec', 13.436614598002052)
```</p>

<p>So, <code>len(list(gen))</code> is the most frequent and less memory consumable</p>
","4195172","4195172","2018-06-19 09:29:24","1","1695","Alex-Bogdanov","2014-10-29 17:13:51","557","75","43","4","393053","393059","2008-12-25 18:50:18","117","56878","<p>Python provides a nice method for getting length of an eager iterable, <code>len(x)</code> that is. But I couldn't find anything similar for lazy iterables represented by generator comprehensions and functions. Of course, it is not hard to write something like:</p>

<pre><code>def iterlen(x):
  n = 0
  try:
    while True:
      next(x)
      n += 1
  except StopIteration: pass
  return n
</code></pre>

<p>But I can't get rid of a feeling that I'm reimplementing a bicycle.</p>

<p>(While I was typing the function, a thought struck my mind: maybe there really is no such function, because it ""destroys"" its argument. Not an issue for my case, though).</p>

<p>P.S.: concerning the first answers - yes, something like <code>len(list(x))</code> would work too, but that drastically increases the usage of memory.</p>

<p>P.P.S.: re-checked... Disregard the P.S., seems I made a mistake while trying that, it works fine. Sorry for the trouble.</p>
","","","2008-12-25 19:12:51","Length of generator output","<python><generator><iterable>","9","2","953"
"50646012","2018-06-01 14:46:53","1","","<p>Ok, huge thanks to the Udemy course posted by Mark Winterbottom for the Django Rest Framework. I'll go ahead and leave this here for anybody else struggling with understanding some basic ideas in the Django Rest Framework.</p>

<p>JSON data is extracted by having the frontend hit urls determined by your api. So this becomes a question of ""how do I implement some search functionality that's in a url?"".</p>

<p>Django uses the Model, View, Controller pattern. A Model is what interacts with the database, and allows you to extract data from it without needing to understand how to query using actual SQL code (uses something called Object Relational Mapping to do this, or ORM, and your Models are in the model.py file). The Controller, how you interact with the pulled data to create/read/update/delete stuff in your api is saved in views.py (a bit counter intuitive, seeing as the View is what you would have in your templates folder [HTML pages and such]).</p>

<p>You can implement something called filters in your Controller (views.py) to allow you to search by specific information to get that <code>?search=whateveryouresearching</code> url, by including the following:</p>

<p><code>from rest_framework import filters</code></p>

<p>and adding this to your ViewSet you want to search:</p>

<pre><code>filter_backends = (filters.SearchFilter,) #allows for search functionality
search_fields = ('name','email') #which can be any Field in your viewset
</code></pre>

<p>This <code>?search=whateveryouresearching</code> created by the filter is how some front end device will access specific searched information (such as a specific user input linke English or Mandarin inside 'language').</p>
","8544781","","","0","1703","aalberti333","2017-08-31 18:00:39","415","43","145","0","50624345","","2018-05-31 12:28:09","1","403","<p>I've recently complete the Django Rest Framework api tutorial and am having a difficult time understanding specifically how it's used as a backend for an application I plan to develop (this is my first venture into backend development). To put more simply, I don't understand how querying will work from the front end. Navigating through the api with either the browser or httpie makes sense, but I'm at a loss for how a frontend extracts specified data from a model.</p>

<p>For example, let's say I have the following:</p>

<p><strong>Models</strong></p>

<pre><code>class Snippet(models.Model):
    created = models.DateTimeField(auto_now_add=True)
    title = models.CharField(max_length=100, blank=True, default='')
    code = models.TextField()
    linenos = models.BooleanField(default=False)
    language = models.CharField(choices=LANGUAGE_CHOICES, default='python', max_length=100)
    style = models.CharField(choices=STYLE_CHOICES, default='friendly', max_length=100)
    highlighted = models.TextField()
</code></pre>

<p><strong>Serializers</strong></p>

<pre><code>class SnippetSerializer(serializers.HyperlinkedModelSerializer):
    class Meta:
        model = Snippet
        fields = ('id', 'title', 'code', 'linenos', 'language', 'style', 'url', 'highlight')
</code></pre>

<p><strong>Views</strong></p>

<pre><code>class SnippetViewSet(viewsets.ModelViewSet):
    queryset = Snippet.objects.all()
    serializer_class = SnippetSerializer
</code></pre>

<p>If I'm a user on the other end of an application, how would I query 'language' inside of the Snippet model? How would I have access to whatever information is in 'language', and in what way would a frontend need to interact with my api to obtain this information?</p>

<p>My problem isn't necessarily how to build the api, but how to interact with it. Any help is greatly appreciated.</p>

<p>(Django 2.0, Python 3.5)</p>
","8544781","","","Extracting specific data field from Django Rest Framework api","<python><django><django-rest-framework>","1","0","1901"
"50646038","2018-06-01 14:48:24","1","","<p>I can't see a way to do this with better than O(k) performance, where <code>k</code> is the number of keys in your sort-of dict.</p>

<p>If you are not seeking <code>dict</code>'s O(1) performance, and just want a <code>dict</code>-like syntax, you can implement a mapping object yourself, like so: </p>

<pre><code>from collections.abc import Mapping

class CallDict(Mapping):
    def __init__(self, *pairs):
        self._pairs = pairs
    def __iter__(self):
        return iter(())
    def __len__(self):
        return len(self._pairs)
    def __getitem__(self, x):
        for func, value in self._pairs:
            if func(x):
                return value
        raise KeyError(""{} satisfies no condition"".format(x))

# Conditions copied directly from OP, but probably wrong.
cd = CallDict(
    ((lambda x: x &gt;= 75), ""Common""),
    ((lambda x: x &lt;= 20), ""Rare""),
    ((lambda x: x &gt;= 5), ""Legendary""),
)


assert cd[1] == 'Rare'
assert cd[10] == 'Rare'
assert cd[50] == 'Legendary'
assert cd[100] == 'Common'
</code></pre>
","8747","8747","2018-06-01 14:54:16","1","1044","Robᵩ","2008-09-15 16:47:33","124214","7724","5618","743","50645700","","2018-06-01 14:28:53","0","79","<p>Was wondering if something like the following was possible:</p>

<pre><code>rarity = {&gt;= 75: 'Common', &lt;= 20 : 'Rare', &gt;= 5: 'Legendary'}
</code></pre>
","9640428","2242445","2018-06-01 14:36:15","Python dict keys as statements","<python><python-3.x><dictionary><conditional>","3","6","164"
"50646039","2018-06-01 14:48:29","1","","<p>You're almost there. You need to return the value of x from your function, and reassign the value to that result. So:</p>

<pre><code>def parse_element(x)
    x = [y.text for y in x]
    x = "" "".join(x)
    return x
</code></pre>

<p>...</p>

<pre><code>description = driver.find_elements_by_id('some-id')
description = parse_element(description)
</code></pre>
","104349","","","1","364","Daniel Roseman","2009-05-10 12:36:13","489411","52610","12851","10717","50645836","50646039","2018-06-01 14:36:43","0","60","<p>I have the following code I'm repeating and wondered if anyone had some advise as to how to write this more efficiently:</p>

<pre><code>def get_description(links):
    for link in links:
        description = driver.find_elements_by_id('some-id')
        description = [x.text for x in description]
        description = "" "".join(description)
        title = driver.find_elements_by_id('different-id')
        title = [x.text for x in title]
        title = "" "".join(title)
        company = driver.find_elements_by_id('another-different-id')
        company = [x.text for x in company]
        company = "" "".join(company)
        location = driver.find_elements_by_id('location-id')
        location = [x.text for x in location]
        location = "" "".join(location)+ "" United Kingdom""
        salary = driver.find_elements_by_xpath(""//*[@id='randomly generated id']/div[3]/span[1]"")
        salary = [x.text for x in salary]
        salary = "" "".join(salary)
</code></pre>

<p>I tried defining a separate function called 'element_parse' as follows:</p>

<pre><code>def parse_element(x)
    x = [y.text for y in x]
    x = "" "".join(x)
</code></pre>

<p>then calling this the main function by doing:</p>

<pre><code>description = driver.find_elements_by_id('some-id')
parse_element(description)
</code></pre>

<p>But alas! No joy.</p>

<p>Not a show stopper as I've got it working but feel like there's a lot of repetition in here I want to clean up!</p>
","9801219","","","Is there a better way to format these elements_by_id?","<python><selenium><xpath><web-scraping>","2","2","1459"
"50646075","2018-06-01 14:50:34","2","","<p>Staying within <code>numpy</code>:</p>

<pre><code>for k, c in enumerate(myArray.T):
    idx = np.flatnonzero(c == 0)
    if idx.size &gt; 0 and idx[0] &gt; 0:
        myArray[idx, k] = myArray[idx[0] - 1, k]
</code></pre>
","8033585","8033585","2018-06-01 14:56:13","1","226","AGN Gazer","2017-05-18 21:39:33","6041","460","707","281","50645325","50645732","2018-06-01 14:09:57","3","150","<p>Currently I have an array as follows:</p>

<pre><code>myArray = np.array(
    [[ 976.77 ,  152.95 ,  105.62 ,   53.44 ,   0 ],
    [ 987.61 ,  156.63 ,  105.53 ,   51.1  ,    0 ],
    [1003.74 ,  151.31 ,  104.435,   52.86 ,    0 ],
    [ 968.   ,  153.41 ,  106.24 ,   58.98 ,    0 ],
    [ 978.66 ,  152.19 ,  103.28 ,   57.97 ,    0 ],
    [1001.9  ,  152.88 ,  105.08 ,   58.01 ,    0 ],
    [1024.93 ,  146.59 ,  107.06 ,   59.94 ,    0 ],
    [1020.01 ,  148.05 ,  109.96 ,   58.67 ,    0 ],
    [1034.01 ,  152.69 ,  107.64 ,   59.74 ,    0 ],
    [   0.   ,  154.88 ,  102.   ,   58.96 ,    0 ],
    [   0.   ,  147.46 ,  100.69 ,   54.95 ,    0 ],
    [   0.   ,  149.7  ,  102.439,   53.91 ,    0 ]]
)
</code></pre>

<p>I would like the fill in the zeros in the first column with the previous last value (1034.01) however if the 0's start from index 0, for it to remain as 0. </p>

<p>Example of end result:</p>

<pre><code>myArrayEnd = np.array(
    [[ 976.77 ,  152.95 ,  105.62 ,   53.44 ,   0 ],
    [ 987.61 ,  156.63 ,  105.53 ,   51.1  ,    0 ],
    [1003.74 ,  151.31 ,  104.435,   52.86 ,    0 ],
    [ 968.   ,  153.41 ,  106.24 ,   58.98 ,    0 ],
    [ 978.66 ,  152.19 ,  103.28 ,   57.97 ,    0 ],
    [1001.9  ,  152.88 ,  105.08 ,   58.01 ,    0 ],
    [1024.93 ,  146.59 ,  107.06 ,   59.94 ,    0 ],
    [1020.01 ,  148.05 ,  109.96 ,   58.67 ,    0 ],
    [1034.01 ,  152.69 ,  107.64 ,   59.74 ,    0 ],
    [1034.01 ,  154.88 ,  102.   ,   58.96 ,    0 ],
    [1034.01 ,  147.46 ,  100.69 ,   54.95 ,    0 ],
    [1034.01 ,  149.7  ,  102.439,   53.91 ,    0 ]]
)
</code></pre>

<p>I would like the code to be applicable to any array not just this one, where the situation may be different. (Column 3 might be all 0's and Column 4 might have 0's in the middle which should be filled with the last previous value).</p>
","9801467","9209546","2018-06-01 21:09:47","Fill values in a numpy array given a condition","<python><arrays><numpy>","5","3","1852"
"50646093","2018-06-01 14:51:45","1","","<p>As you already found out the data is put into the page via JS. However, you can still get that data, because the entire data over the comapany is always loaded with the page. You can access this data via <code>requests</code> +  <code>BeautifulSoup</code> +  <code>json</code> (+ <code>re</code>):</p>

<pre><code>import json
import re

import requests
from bs4 import BeautifulSoup

webpage = ""https://www.zippia.com/amazon-com-careers-487/""
page = requests.get(webpage)
soup = BeautifulSoup(page.content, 'lxml')

for script in soup.find_all('script', {'type': 'text/javascript'}):
    if 'getCompanyInfo' in script.text:
        match = re.search(""{[^\n]*}"", script.text)
        data = json.loads(match.group())
        print(data[""companyDiversity""][""languages""])

        json.dump(data, open(""test.json"", ""w""), indent=2) # Only if you want the data put in a readable format to a file (like if you want to find the path to an entry)
</code></pre>
","8472976","","","0","956","MegaIng","2017-08-16 13:56:11","3958","373","232","57","50645645","","2018-06-01 14:26:03","0","38","<p>I'm trying to extract the language proportion spoken at companies, using python's <code>BeautifulSoup</code>.</p>

<p>Yet, the information seems to come from a script, not from HTML, and I'm having some trouble.</p>

<p>For instance, from the following page, when I try</p>

<pre><code>webpage =""https://www.zippia.com/amazon-com-careers-487/""
page = requests.get(webpage)
soup = BeautifulSoup(page.content, 'lxml')

for links in soup.find_all('div', {'class':'companyEducationDegrees'}):
    raw_text = links.get_text()
    lines = raw_text.split('\n')
    print(lines)
    print('-------------------')
</code></pre>

<p>I don't get any result while the ideal result should be <code>Spanish 61.1%, French 9,7%, etc</code></p>
","8521859","1007939","2018-06-01 14:32:37","Webscraping from a script","<python><python-3.x>","1","4","730"
"50646131","2018-06-01 14:53:44","2","","<p>To install modules in a specific directory, you can try  <code>pip install module --target=.</code></p>

<p>By default python search for those modules in same directory as the script first, then, if not available, it will search for python install lib files.</p>
","4658340","659864","2018-06-01 15:01:29","0","266","Jishnunand P k","2015-03-11 12:10:45","107","39","4","0","50645396","50646131","2018-06-01 14:13:45","1","425","<p>I'm new to using python modules.</p>

<p>I'm currently working on a python 2.7 script that will be deployed to many remote computers (which have python 2.7 on them). The problem is that the script needs to use a module, which <em>I am <strong>not</strong> allowed to install</em> on those computers.</p>

<p>I'm wondering if it is possible to include the module files in the same package as my script (possibly have them compiled first), and then have the script import the library from that local folder, thus achieving a ""portable"" script.</p>

<p>If that is possible, how would I go about doing that?</p>

<hr>

<p>Specifics: I'm running 2.7.11 on Windows needing to use Paramiko.</p>

<p>I'm asking this question because the similar questions that I can find either do not answer mine, or expect me to be familiar with core python structures with which I am not. I also DON'T want to include the entirety of python and then install the module onto that, something I see is often called Portable Python. I just want to send my script and the module and nothing more.</p>

<hr>

<p>Many thanks!</p>
","659864","","","Portable Python Script with Module","<python><module><portability>","1","4","1104"
"50646154","2018-06-01 14:54:59","9","","<p><code>numpy.where</code> returns a tuple because each element of the tuple refers to a dimension.</p>

<p>Consider this example in 2 dimensions:</p>

<pre><code>a = np.array([[1, 2, 3, 4, 5, 6],
              [-2, 1, 2, 3, 4, 5]])

print(np.where(a &gt; 2))

(array([0, 0, 0, 0, 1, 1, 1], dtype=int64),
 array([2, 3, 4, 5, 3, 4, 5], dtype=int64))
</code></pre>

<p>As you can see, the first element of the tuple refers to the first dimension of relevant elements; the second element refers to the second dimension.</p>

<p>This is a convention <code>numpy</code> often uses. You will see it also when you ask for the shape of an array, i.e. the shape of a 1-dimensional array will return a tuple with 1 element:</p>

<pre><code>a = np.array([[1, 2, 3, 4, 5, 6],
              [-2, 1, 2, 3, 4, 5]])

print(a.shape, a.ndim)  # (2, 6) 2

b = np.array([1, 2, 3, 4, 5, 6])

print(b.shape, b.ndim)  # (6,) 1
</code></pre>
","9209546","","","0","919","jpp","2018-01-12 14:47:22","109049","18235","7890","3496","50646102","50646154","2018-06-01 14:52:09","8","3257","<p>When I run this code:</p>

<pre><code>import numpy as np
a = np.array([1, 2, 3, 4, 5, 6])
print(np.where(a &gt; 2))
</code></pre>

<p>it would be natural to get an array of indices where <code>a &gt; 2</code>, i.e. <code>[2, 3, 4, 5]</code>, but instead we get:</p>

<pre><code>(array([2, 3, 4, 5], dtype=int64),)
</code></pre>

<p>i.e. a tuple with empty second member.</p>

<p>Then, to get the the ""natural"" answer of <code>numpy.where</code>, we have to do:</p>

<pre><code>np.where(a &gt; 2)[0]
</code></pre>

<p><strong>What's the point in this tuple? In which situation is it useful?</strong></p>

<p>Note: I'm speaking here only about the use case <code>numpy.where(cond)</code> and not <code>numpy.where(cond, x, y)</code> that also exists (see documentation).</p>
","1422096","9209546","2018-06-01 16:59:52","What is the purpose of numpy.where returning a tuple?","<python><arrays><numpy>","3","5","776"
"50646155","2018-06-01 14:55:02","0","","<p>I ended up using a file structure like the following:</p>

<pre><code>src/
    main.py
    project/
        api/
            get_data_from_api.py
        util/
            log.py
    test/
</code></pre>

<p>That way, because I am running the <code>main.py</code> file, from anywhere in the program, I can just do <code>import project.&lt;package&gt;.&lt;module&gt;</code>. For example, I can just do this: </p>

<p><strong>get_data_from_api.py</strong></p>

<pre><code>import project.util.log

# Rest of code goes here...
</code></pre>

<p>And everything works!</p>

<p>If I'm completely shooting in the dark please let me know, I'd much rather be wrong now then hundreds of hours into the project!</p>
","3102725","","","0","706","agupta231","2013-12-14 17:23:11","493","63","21","1","50643670","50646155","2018-06-01 12:40:43","0","153","<p>I know this question has been asked in a lot of places, but none of them really seem to answer my question. </p>

<p>I am creating a standalone application in python, my current project structure is like so:</p>

<pre><code>Project/
    utils/
        log.py
    api/
        get_data_from_api.py
    main.py
</code></pre>

<p>I would like to have a set up similar to <a href=""https://github.com/django/django"" rel=""nofollow noreferrer"">django</a>, in which I can refer to any file by using a <code>Project.&lt;package&gt;</code> syntax. </p>

<p>For example, if I wanted to access my <code>log</code> module from my <code>get_data_from_api</code> module, I could do something like this:</p>

<p><strong>get_data_from_api.py</strong></p>

<pre><code>import Project.utils.log

# Rest of code goes here...
</code></pre>

<p>However, I can't seem to get that to work, even when I added an <code>__init__.py</code> file in the root directory.</p>

<p>I read somewhere that I should modify my <code>PYTHONPATH</code>, but I would like to prevent that, if possible. Additionally, django seemed to pull it off, as I couldn't find any <code>PYTHONPATH</code> modification code in there.</p>

<p>I really appreciate the help!</p>

<p><em>Post Note:</em> Where would <code>tests</code> fit in this file structure? I would like them to be separate, but also have access to the entire project really easily.</p>
","3102725","","","Best way to organize a python project to easily access all of the modules","<python><django><setup-project><project-structure>","2","0","1403"
"50646204","2018-06-01 14:57:16","2","","<p>For consistency: the length of the tuple matches the number of dimensions of the input array.</p>

<pre><code>&gt;&gt;&gt; np.where(np.ones((1)) &gt; 0)
(array([0]),)
&gt;&gt;&gt; np.where(np.ones((1,1)) &gt; 0)
(array([0]), array([0]))
&gt;&gt;&gt; np.where(np.ones((1,1,1)) &gt; 0)
(array([0]), array([0]), array([0]))
</code></pre>

<p>Making the 1-d case return an array instead of a tuple would cause inhomogeneous return types. If the caller code is dealing with input data of arbitrary shape, then the programmer would have to special-case handling for 1-d inputs in the return value.</p>
","674039","674039","2018-06-01 15:38:20","0","599","wim","2011-03-23 23:40:27","187587","12233","9064","5087","50646102","50646154","2018-06-01 14:52:09","8","3257","<p>When I run this code:</p>

<pre><code>import numpy as np
a = np.array([1, 2, 3, 4, 5, 6])
print(np.where(a &gt; 2))
</code></pre>

<p>it would be natural to get an array of indices where <code>a &gt; 2</code>, i.e. <code>[2, 3, 4, 5]</code>, but instead we get:</p>

<pre><code>(array([2, 3, 4, 5], dtype=int64),)
</code></pre>

<p>i.e. a tuple with empty second member.</p>

<p>Then, to get the the ""natural"" answer of <code>numpy.where</code>, we have to do:</p>

<pre><code>np.where(a &gt; 2)[0]
</code></pre>

<p><strong>What's the point in this tuple? In which situation is it useful?</strong></p>

<p>Note: I'm speaking here only about the use case <code>numpy.where(cond)</code> and not <code>numpy.where(cond, x, y)</code> that also exists (see documentation).</p>
","1422096","9209546","2018-06-01 16:59:52","What is the purpose of numpy.where returning a tuple?","<python><arrays><numpy>","3","5","776"
"50646205","2018-06-01 14:57:18","1","","<p>You can <code>drop_duplicates</code> and using <code>lookup</code> </p>

<pre><code>s=df2.drop_duplicates('ID').reset_index(drop=True)

df1.iloc[0,:]=s.lookup(df1.iloc[0,:]-1,['ID']*len(s))
df1
Out[222]: 
   Out1  Out2  Out3  Out4  Out5
0   777   999   101   555   111
1   100    50   200   300   200
</code></pre>
","7964527","","","1","318","WeNYoBen","2017-05-04 16:45:29","164847","15327","4764","689","50646120","","2018-06-01 14:52:58","0","87","<p>I have the following pandas data frames:</p>

<p>df1</p>

<pre><code>Out 1 Out 2 Out 3 Out 4 Out 5
3     1     2     4     5
100  50    200    300   200
</code></pre>

<p>The values in df1 represent the Nth unique values in df2$ID. So the 1st unique value is 999, the 3rd unique value is 777 etc.</p>

<pre><code>ID  ID2
999 888
101 801
777 666
777 666
555 100
555 100
111 100
</code></pre>

<p>So for a final df3 I need the following: Note I've only updated the first row for the example</p>

<pre><code>Out 1 Out 2 Out 3 Out 4 Out 5
777   999   101   555   111
100  50    200    300   200
</code></pre>

<p>I have tried using the answer by @Wen, but it creates the following error: I couldn't find an answer to this one, beyond it may be an indexing error. I have also made sure I converted relevant data types to int32 to no avail. Any help would be much appreciated - </p>

<pre><code>ValueError: Row labels must have same size as column labels
</code></pre>
","2107210","2107210","2018-06-07 13:35:25","Find the Nth unique value and return to dataframe in Python","<python><pandas><unique>","1","2","966"
"50646223","2018-06-01 14:58:37","0","","<p>OK... it turned out not to be a problem with the plotting, but with the function. I don't fully understand what went wrong, but I think some of the computations were running into ""huge integer"" problems. When I ran it to make the comparison in log space it started working.</p>

<p>I'll leave this here as it might help others who are trying to make similar grids of heatmaps, and maybe someone will answer with a better way of doing it.</p>

<p>Here's my annotated code to make the heatmap itself from a dataframe with columns <code>['i', 'j', 'n', 'p']</code>:</p>

<pre><code># The first two lines are defining the subplots, i.e. the heatmaps
# themselves. I am passing the dataframe, and specifying the names of the
# columns to use as axes. I say 'factor(i)' etc to treat the i column as
# discrete, not continuous (in my case it's integers).
gg = (ggplot(df, aes('factor(i)', 'factor(j)')) +
    geom_tile(aes(fill='fun')) +
# The last bit is to call facet_grid which applies the above code
# in a grid. The parameter 'n ~ p' specifies that I want the grid to be
# over the columns 'n' and 'p' from the dataframe. The labeller
# parameter is what makes the labels at the edges (see top and right in
# image below) show both the column name and the value).
    facet_grid('n ~ p', labeller='label_both'))
</code></pre>

<p>Here's the full corrected code with the fixed result:</p>

<pre><code>from plotnine import ggplot, aes, facet_grid, geom_tile
import pandas as pd
import numpy as np
import itertools

def fun((i, j, n, p)):
    if n &gt; j:
        return 1 if np.log10(p) * (3*n) &gt; np.log10(p+i) * (3*(n-j)) else 0

    return -1

ilist, jlist, nlist, plist = range(1,10), range(1,9), range(8,10), range(4,6)
rows = itertools.product(ilist, jlist, nlist, plist)
df = pd.DataFrame(list(rows))
df.columns = ['i','j','n','p']
df['fun'] = df.apply(fun, axis=1)

gg = (ggplot(df, aes('factor(i)', 'factor(j)')) +
    geom_tile(aes(fill='fun')) +
    facet_grid('n ~ p', labeller='label_both'))

gg.draw()
</code></pre>

<p><a href=""https://i.stack.imgur.com/03qA6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/03qA6.png"" alt=""Corrected heatmap""></a></p>
","8131703","","","0","2187","LangeHaare","2017-06-08 13:26:36","1155","111","216","4","50629537","","2018-05-31 17:12:50","0","123","<p>I am using plotnine in python 2 (but would be happy for a solution using matplotlib or any other plotting package). I have a function (slightly simplified below) with 4 arguments. I want to plot a grid of heatmaps, with ""super axes"" varying two of the parameters, and each heatmap varying the other two. A bit like this: </p>

<p><a href=""https://i.stack.imgur.com/dNNvl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dNNvl.png"" alt=""Heatmap grid skeleton diagram""></a></p>

<p>I came up with this code:</p>

<pre><code>from plotnine import ggplot, aes, facet_grid, geom_tile
import pandas as pd
import itertools

def fun((i, j, n, p)):
    if n &gt; j:
        return 1 if (p**(3*n)) &gt; ((p+i)**(3*(n-j))) else 0
    return -1

ilist, jlist, nlist, plist = range(1,10), range(1,9), range(8,10), range(4,6)
rows = itertools.product(ilist, jlist, nlist, plist)

df = pd.DataFrame(list(rows))
df.columns = ['i','j','n','p']
df['fun'] = df.apply(fun, axis=1)

(ggplot(df, aes('factor(i)', 'factor(j)')) +
 geom_tile(aes(fill='fun')) +
facet_grid('n ~ p', labeller='label_both'))
</code></pre>

<p>This produces the following:</p>

<p><a href=""https://i.stack.imgur.com/yvCVj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yvCVj.png"" alt=""Heatmap grid""></a></p>

<p>This (more or less) has the form I am going for, but the heatmaps seem wrong. (EDIT: I discovered the error was in the definition of <code>fun</code>, not in the plotting - see my answer).</p>
","8131703","8131703","2018-06-01 15:01:09","Plotting a function with 4 arguments in a grid of heatmaps","<python><matplotlib><plot><heatmap><plotnine>","1","0","1500"
"50646225","2018-06-01 14:58:43","0","","<pre><code>import tkinter
from tkinter import filedialog
from tkinter import *
from tkinter import messagebox
from tkinter.filedialog import askdirectory
import os
import matplotlib.pyplot as plt
import skimage.external.tifffile as tiff
import pandas as pd
from pandas import DataFrame
import numpy as np

def load_images_from_folder(folder):
    images=[]
    for filename in os.listdir(folder):
        if any([filename.endswith(x) for x in ['.tif']]):
            img=tiff.imread(os.path.join(folder, filename))
            if img is not None:
                images.append(img)
    return images

tkinter.Tk().withdraw()   
dirname = askdirectory(initialdir=""/"", title='Please select a directory')
os.chdir(dirname)

data = pd.DataFrame([])
ratiodata = []
foldernames = []
for folder in os.listdir(dirname):  
    if not folder.endswith('.xlsx'):
       images=load_images_from_folder(folder)
       green=[image[..., 1].mean() for image in images]
       red=[image[..., 0].mean() for image in images]
       ratios= [img[..., 1].mean() / img[..., 0].mean() for img in images]
       #PLOTS
       temp = pd.DataFrame({folder + "" ratios"" : ratios})
       data = pd.concat([data,temp],axis=1)
data.to_excel('test.xlsx', sheet_name='sheet1')
</code></pre>
","8371964","","","0","1260","JurgK","2017-07-26 18:30:14","6","22","0","0","50307820","","2018-05-12 15:25:21","0","65","<p>I need to export lists of pixel intensities derived from multiple images in multiple folders to an excel spreadsheet. Each folder contains a list of tiff files each representing a certain time point in a timelapse. I've managed to obtain the pixel intensities of each folder subset, but I'm struggling with the output to excel using DataFrames with pandas. The data frame only displays the list of values from the last folder, and I need the spreadsheet to display each list in a separate row. Here is what I have:</p>

<pre><code>import os
import matplotlib.pyplot as plt
import skimage.external.tifffile as tiff
import pandas as pd
from pandas import DataFrame

#to read images in each folder
def load_images_from_folder(folder):
images=[]
for filename in os.listdir(folder):
    if any([filename.endswith(x) for x in ['.tif']]):
        img=tiff.imread(os.path.join(folder, filename))
        if img is not None:
            images.append(img)
return images

folders = [
    'path to folder1',
    'path to folder2',
    'path to folder3',
]

for folder in folders:
    images=load_images_from_folder(folder)

#ratio the mean green to red signal in each image 
    ratios = [image[..., 1].mean() / image[..., 0].mean() for image in 
    images]
    plt.plot(range(len(images)), ratios)
    plt.show()


df=DataFrame({'Ratios':ratios})
df.to_excel('Ratios.xlsx', sheet_name='sheet1', index=0)
</code></pre>

<p>Printing out the ratios gives: 
Folder1:
[list of values]
Folder2:
[list of values]
Folder3:
[list of values 
etc.
But the data displayed by df (DataFrame) is only from the list in Folder3. So, what do I need to do differently to export values derived from multiple folders into Excel? I also made sure that each image is read as a ndarray and the type=uint8.  </p>
","8371964","","","Exporting image intensities from multiple files in multiple folders to excel in Python","<python><pandas><image-processing><scikit-image><numpy-ndarray>","2","1","1782"
"50646293","2018-06-01 15:02:29","0","","<p>The first two lines of your code render <code>x</code> and <code>y</code> as tuples which act different than numpy arrays to mathematical operators. All you need to do is:
<code>x,y=np.array(sorted(resultsDict.items())).T</code>
This makes a numpy array for you, then transpose it so it has 2 rows and n columns, so it can be unpacked onto <code>x,y</code>.</p>
","1245694","","","0","365","anishtain4","2012-03-02 18:03:48","1309","173","254","34","50644832","","2018-06-01 13:44:08","-1","601","<p>I have problem with approximation in python.
I have a function, which gives me a dict with results. For example <code>{1: 0.5, 2: 0.25}</code>. It means that <code>f(1)==0.5</code>, <code>f(2)==0.25</code>, etc.</p>

<p>Below is what I do after getting values from my function.</p>

<pre><code>lists = sorted(resultsDict.items())
x, y = zip(*lists)
startvalues = [0.5,1.0,0]
popt, pcov = curve_fit(func, x, y,p0)
function=func(x,popt[0],popt[1],popt[2])
plt.plot(x,y,'x',x,function,'r-')
plt.show()
</code></pre>

<p>And now I have answer. If I define function func in that way everything is OK.</p>

<pre><code>def func(x,a,b,c):
   return  a+b/x
</code></pre>

<p>If I define that way</p>

<pre><code>def func(x,a,b,c):
   return  a+b/x+c*x
</code></pre>

<p>I have error:</p>

<pre class=""lang-none prettyprint-override""><code>TypeError: 'numpy.float64' object cannot be interpreted as an integer
</code></pre>

<p>And if I define that way</p>

<pre><code>def func(x,a,b,c):
   return  a+b/x+c/(x*x) # or x**2
</code></pre>

<p>I have error:</p>

<pre class=""lang-none prettyprint-override""><code>TypeError: can't multiply sequence by non-int of type 'tuple'
</code></pre>

<p>I don't know, where is a problem especially in second one and I don't know how can I dodge problem in third one if I wanted to check function <code>1/x**2</code>.</p>
","9848825","355230","2018-06-01 13:53:18","Multiple values of tuples","<python><numpy><tuples><approximation>","1","4","1350"
"50646336","2018-06-01 15:05:04","3","","<p>There is an open (as of 2018.06.01) <a href=""https://github.com/pandas-dev/pandas/issues/21103"" rel=""nofollow noreferrer"">issue</a> for this bug in pandas 0.23.</p>

<p>You might want to downgrade to 0.22 which would work as expected.</p>
","4077912","","","1","242","Primer","2014-09-25 06:17:47","7341","149","478","5","50645445","","2018-06-01 14:16:35","0","723","<p>i always stored into my DB (SQL server) thousands of parameters until some days ago. 
I use spyder (Python 3.6).
I updated all packages with conda update --all some days ago and now im not able to import my dataframes into my DB.</p>

<p>--- I Don't want a workaround to split in a 2100- parameters DF ---</p>

<p>I would like to understand what is changed and why and how to come back to a working one.</p>

<p>this is a simple code:</p>

<pre><code>import pyodbc
import sqlalchemy
import numpy as np
import pandas as pd


c = pyodbc.connect(""Driver={SQL Server};Server=**;Trusted_Connection=no;Database=*;UID=*;PWD=*;"")
cursor = c.cursor()  
engine = sqlalchemy.create_engine('mssql+pyodbc://*:*/*?driver=SQL+Server')



df= pd.DataFrame(np.random.randn(5000))
df.to_sql('pr',engine,if_exists= 'append', index=False)
</code></pre>

<p>and this is the error:
ProgrammingError: (pyodbc.ProgrammingError) ('42000', '[42000] [Microsoft][ODBC SQL Server Driver][SQL Server]The incoming request has too many parameters. The server supports a maximum of 2100 parameters. Reduce the number of parameters and resend the request. (8003) (SQLExecDirectW)')</p>

<p>Thanks a lot</p>
","8889622","","","Python pandas to_sql maximum 2100 parameters","<python><pandas><pyodbc>","1","0","1176"
"50646377","2018-06-01 15:07:26","1","","<p>This code is neither really Pythonic nor even OO. After reading the original question I think that what you need should look like:</p>

<pre><code>file_name = ""file.txt""

class Reader(object):

    def __init__(self, name):
        self.name = name             # store file name as a data member

    def readFile(self):
        with open(self.name, ""r"") as file:  # with ensure proper close of file
                                            #   using data member name
            for line in file:               # iterate on lines
                Line(line).printline()      # pass the line to another object
                # or: test2(line)           # or to a function
                # do other things...

class Line:                       # an auxilliary class
    def __init__(self, line):
        self.line = line
    def printLine(self):
        # use self.line here

def test2(line):                  # a module level function
    # use line here

Reader(file_name).readFile()      # create a new Reader object and calls its readfile method
</code></pre>
","3545273","","","1","1070","Serge Ballesta","2014-04-17 12:25:02","90494","5432","1346","480","50645781","50646377","2018-06-01 14:34:03","-1","40","<p>My question is really simple. Let me explain my problem with an exemple code. I have developed a specific file parser and each line represents an object.</p>

<pre><code>file = open(""file.txt"", ""r"")

class Reader(object):
    def __init__(self):
        self.i = 1   // first line

    def readFile(self):
        for line in file:
            test2()
            // do things..


class Line():
    def printLine(self):
        f.readline() // Need to read the current line from the readFile() loop 


test()
</code></pre>

<p>So, what I would like to know is how can I get access to the current line from <code>readFile()</code> in <code>test2().printLine()</code> without reopening the file ?</p>
","9792948","8033585","2018-06-01 18:48:38","How to get a line file from a class in an other class?","<python><class><parsing>","2","6","702"
"50646379","2018-06-01 15:07:39","1","","<p>das-g provided a working solution!</p>

<p>Another way to do this would be</p>

<pre><code>fixed_dictionaries(dict(
    required=text(),
    optional=none()|text(),  # note None first, for shrinking
)).map(
    lambda d: {k: v for k, v in d.items() if v is not None}
)
</code></pre>
","9297601","","","0","286","Zac Hatfield-Dodds","2018-02-01 02:26:24","847","65","22","6","50603092","50603447","2018-05-30 11:06:28","2","491","<p>Currently I am using hypothesis fixed_dictionaries strategy to generate a dictionary with specific keys and data types that are considered valid for my application. I need a strategy which produces this fixed dictionary as well as others with specific keys removed. Or a dictionary with a certain minimal set of keys with optional additional ones, preferably in a way that produces the various combinations of these optional keys.</p>

<p>This is an example of the json schema that needs to be validated, with the 2 optional fields. I'd like to generate all possible valid data for this schema.</p>

<pre><code>'user_stub': {
    '_id':        {'type': 'string'},
    'username':   {'type': 'string'},
    'social':     {'type': 'string'},
    'api_name':   {'type':     'string',
                   'required': False},
    'profile_id': {'type':     'integer',
                   'required': False},
}
</code></pre>

<p>This is what I came up with but it is incorrect because it retains the keys but uses None as the value, and I want instead that the keys are removed.</p>

<pre><code>return st.fixed_dictionaries({
    '_id':        st.text(),
    'username':   st.text(),
    'social':     st.text(),
    'api_name':   st.one_of(st.none(),
                            st.text()),
    'profile_id': st.one_of(st.none(),
                            st.integers()),
})
</code></pre>

<p>EDIT: updated composite strategy -></p>

<p>Seems like it would be best to separate the additional optional dictionaries based on the type of data being returned, otherwise might get keys with mismatched values.</p>

<pre><code>@st.composite
def generate_data(draw):
    base_data = st.fixed_dictionaries({
        '_id':      st.text(),
        'username': st.text(),
        'social':   st.text(),
    })
    optional_strs = st.dictionaries(
        keys=st.just('api_name'),
        values=st.text()
    )
    optional_ints = st.dictionaries(
        keys=st.just('profile_id'),
        values=st.integers()
    )

    b = draw(base_data)
    s = draw(optional_strs)
    i = draw(optional_ints)
    return {**b, **s, **i}  # noice
</code></pre>
","196870","196870","2018-05-31 08:39:17","Strategy for dictionary with optional keys","<python><python-3.x><unit-testing><testing><python-hypothesis>","2","0","2139"
"50646383","2018-06-01 15:07:55","2","","<p>I got this working by using the following:</p>

<pre><code>r = None
while r is None:
    r = pyautogui.locateOnScreen('rbin.PNG', grayscale = True)
print icon_to_click + ' now loaded'
</code></pre>

<p>The key is to make grayscale = True.</p>
","5108710","","","0","246","feltersnach","2015-07-12 19:38:42","245","115","25","6","43702511","","2017-04-30 03:05:36","4","9220","<p>Here's the code that I'm trying to run:</p>

<pre><code>import pyautogui
r=pyautogui.locateOnScreen('C:\Users\David\Desktop\index.png',grayscale=False)
print r
</code></pre>
","6660758","46914","2017-05-04 02:35:21","Why PyAutoGui LocateOnScreen() only Returns None","<python><pyautogui>","8","4","177"
"50646440","2018-06-01 15:10:41","4","","<p>You will get your expected result with</p>

<pre><code>df.t.str.split(""\n"", expand=True).stack()
</code></pre>
","8187340","8187340","2019-10-28 09:34:48","0","114","J. Doe","2017-06-20 08:29:22","1545","144","476","4","50644066","","2018-06-01 13:02:56","6","3167","<p>Good day,</p>

<p>Is it possible to get multi line cell output when using pandas DataFrame in a shell? I understand the whole row will have height more than 1 char, but that's what I actually want.</p>

<p>Example:</p>

<pre><code>data = [
       {'id': 1, 't': 'very long text\ntext line 2\ntext line 3'},
       {'id': 2, 't': 'short text'}
       ]
df = pd.DataFrame(data)
df.set_index('id', inplace=True)
print(df)
</code></pre>

<p>and want to get output:</p>

<pre>
id                  t
1      very long text
       text line 2
       text line 3
2      short text
</pre>

<p>instead of</p>

<pre>
id                                         t
1     very long text\ntextline 2\ntext line3
2                                 short text
</pre>
","179859","","","pandas dataframe and multi line values","<python><pandas><dataframe>","3","0","750"
"50646530","2018-06-01 15:16:16","1","","<p>You need:</p>

<pre><code>import matplotlib.pyplot as plt

a_x_axis = [32, 30, 40, 50, 60, 78]
a_live = [1, 3, 2, 1, 2, 4]

a_alive_for = [a + b for a, b in zip(a_x_axis, a_live)]

b_x_axis = [22, 25, 45, 55, 60, 72]
b_live = [1, 3, 2, 1, 2, 4]
b_alive_for = [a + b for a, b in zip(b_x_axis, b_live)]

a_y_axis = []
b_y_axis = []

for i in range(0, len(a_x_axis)):
    a_y_axis.append('process-1')
    b_y_axis.append('process-2')


print(""size of a: %s"" % len(a_x_axis))
print(""size of a: %s"" % len(a_y_axis))
plt.xlabel('time (s)')
plt.scatter(a_x_axis, [1]*len(a_x_axis))
plt.scatter(a_alive_for, [1]*len(a_x_axis))

plt.scatter(b_x_axis, [2]*len(b_x_axis))
plt.scatter(b_alive_for, [2]*len(b_x_axis))

for i in range(0, len(a_x_axis)):
    plt.plot([a_x_axis[i],a_alive_for[i]], [1,1], 'green')

for i in range(0, len(b_x_axis)):
    plt.plot([b_x_axis[i],b_alive_for[i]], [2,2], 'green')

plt.show()
</code></pre>

<p>Output:</p>

<p><a href=""https://i.stack.imgur.com/SEHgR.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SEHgR.jpg"" alt=""enter image description here""></a></p>
","9609447","","","2","1105","harvpan","2018-04-06 20:08:42","7030","853","477","708","50645806","50646530","2018-06-01 14:35:13","3","1223","<p>I am trying to visualize some data regarding the time at which the process was running or alive and the time it was idle. For each process, I have <code>a_x_axis</code> the time at which process started running and <code>a_live_for</code> is the time it was alive after it woke up. I have two data points in for each process. I am trying to connect these two dots by a line by connecting 1st green dot with the first red dot and second green dot with the second red dot and so on, so I can see alive and idle time for each process in the large data set. I looked into scatter plot examples but could not find any way to solve this issue.</p>

<pre><code>import matplotlib.pyplot as plt

a_x_axis = [32, 30, 40, 50, 60, 78]
a_live = [1, 3, 2, 1, 2, 4]

a_alive_for = [a + b for a, b in zip(a_x_axis, a_live)]

b_x_axis = [22, 25, 45, 55, 60, 72]
b_live = [1, 3, 2, 1, 2, 4]
b_alive_for = [a + b for a, b in zip(b_x_axis, b_live)]

a_y_axis = []
b_y_axis = []

for i in range(0, len(a_x_axis)):
    a_y_axis.append('process-1')
    b_y_axis.append('process-2')


print(""size of a: %s"" % len(a_x_axis))
print(""size of a: %s"" % len(a_y_axis))
plt.xlabel('time (s)')
plt.scatter(a_x_axis, [1]*len(a_x_axis))
plt.scatter(a_alive_for, [1]*len(a_x_axis))

plt.scatter(b_x_axis, [2]*len(b_x_axis))
plt.scatter(b_alive_for, [2]*len(b_x_axis))

plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/dyWwk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dyWwk.png"" alt=""enter image description here""></a></p>
","9629245","9629245","2018-06-01 14:54:07","matplotlib connecting the dots in scatter plot","<python><matplotlib>","2","14","1529"
"50646584","2018-06-01 15:19:30","3","","<p>If I correct understood you, you need a <code>hover</code>over an element. To be able to do it, you can try this:</p>

<pre><code>hover = ActionChains(webDriver).move_to_element(element_to_hover_over)
hover.perform()
</code></pre>

<p>Also you can have a look to <a href=""http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.common.action_chains"" rel=""nofollow noreferrer"">Documentation</a></p>

<p>EDIT:</p>

<p>In your case I would use this two selectors to get the information you need:</p>

<pre><code>//span[@class='dl-ask']
//span[@class='dl-bid']
</code></pre>

<p>They are changing dynamically without <code>hover</code> need. </p>

<p><a href=""https://i.stack.imgur.com/6wAv2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6wAv2.png"" alt=""image""></a></p>

<p>Output will be like this:</p>

<pre><code>7420.06x2 // you have to split this string and extract 7420.06
7424.00×0.007977999746799469 // and here 7424.00
</code></pre>

<p>Since you want to get dynamically from elements you provided, there is a solution for it. I wouldn't use it for every problem, because it is not a ""clean"" one. But for our specific case I think it is a best one. </p>

<p>I have found that if you hover to particular <code>candle</code> in the graphic, every minute when graphic will be updated, all <code>candles</code> will be shifted to the left. Also if you <code>hover on (x,y)</code> position which is (y) position for one of the <code>candles</code>, you get the value of the <code>candle</code>. And that is exactly what you need. My propose is to <code>hover</code> to the <code>candle</code> you need and the values from your elements will be updated since the values this <code>candle</code> be updated(every 1 minute). If you want the information with minimum delay, you have to hover on the right <code>candle</code>.</p>

<p><a href=""https://i.stack.imgur.com/gQqzn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gQqzn.png"" alt=""Example""></a></p>

<p>Use this to move to the mouse to your candle:</p>

<pre><code>move_to_element_with_offset(to_element, xoffset, yoffset)
</code></pre>

<blockquote>
  <p>Move the mouse by an offset of the specified element. Offsets are
  relative to the top-left corner of the element.  Args:    </p>
  
  <ul>
  <li>to_element: The WebElement to move to.</li>
  <li>xoffset: X offset to move to.</li>
  <li>yoffset: Y offset to move to.</li>
  </ul>
</blockquote>

<p>Also you may want to <code>zoom in</code> the graphic. You can try the suggestions <a href=""https://stackoverflow.com/questions/47274852/mouse-scroll-wheel-with-selenium-webdriver-on-element-without-scrollbar?noredirect=1&amp;lq=1"">here</a>, or:</p>

<pre><code>webdriver.execute_script(""window.scrollBy(0, 150);"") // 0 - xPos, 150 - yPos (values are in px).
</code></pre>
","8625512","8625512","2018-06-02 07:58:00","8","2842","Andrei Suvorkov","2017-09-18 08:13:10","4441","927","1530","443","50646502","","2018-06-01 15:14:23","0","2919","<p>I am trying to scrape data from tradingviews charts. This is the code that I am using and so far is working</p>

<pre><code>from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import time

MAMAarray = []
FAMAarray = []

# example option: add 'incognito' command line arg to options
option = webdriver.ChromeOptions()
option.add_argument(""--incognito"")

# create new instance of chrome in incognito mode
browser = webdriver.Chrome(chrome_options=option)

# go to website of interest
browser.get(""https://www.tradingview.com/chart/vKzVQllW/#"")

# wait up to 10 seconds for page to load
timeout = 10
try:
    WebDriverWait(browser, timeout).until(EC.visibility_of_element_located((By.XPATH, ""/html/body/div[1]"")))
except TimeoutException:
    print(""Timed out waiting for page to load"")
    browser.quit()


time.sleep(5)

# get MAMA and FAMA values for BTCUSDT on binance
MAMA_element = browser.find_element_by_xpath(""/html/body/div[1]/div[1]/div/div[1]/div/table/tbody/tr[1]/td[2]/div/div[3]/div[3]/div/span[3]"")
FAMA_element = browser.find_element_by_xpath(""/html/body/div[1]/div[1]/div/div[1]/div/table/tbody/tr[1]/td[2]/div/div[3]/div[3]/div/span[4]"")

MAMAarray.append(float(MAMA_element.text))
FAMAarray.append(float(FAMA_element.text))

print(MAMAarray)
print(FAMAarray)

while True:
    MAMA_element = browser.find_element_by_xpath(
        ""/html/body/div[1]/div[1]/div/div[1]/div/table/tbody/tr[1]/td[2]/div/div[3]/div[3]/div/span[3]"")
    FAMA_element = browser.find_element_by_xpath(
        ""/html/body/div[1]/div[1]/div/div[1]/div/table/tbody/tr[1]/td[2]/div/div[3]/div[3]/div/span[4]"")
    if (float(MAMA_element.text)) != MAMAarray[-1]:
        MAMAarray.append(float(MAMA_element.text))
    if (float(FAMA_element.text)) != FAMAarray[-1]:
        FAMAarray.append(float(FAMA_element.text))

    print(MAMAarray)
    print(FAMAarray)
</code></pre>

<p>Here is the output: so you can see that the numbers append but ONLY when I go into the chart manually and move my cursor over new candles(bars)</p>

<pre><code>[7415.969]
[7417.39]
[7415.969, 7428.644]
[7417.39, 7435.585]
[7415.969, 7428.644, 7430.56]
[7417.39, 7435.585, 7431.722]
[7415.969, 7428.644, 7430.56, 7415.496]
[7417.39, 7435.585, 7431.722, 7417.39]
[7415.969, 7428.644, 7430.56, 7415.496, 7415.969]
[7417.39, 7435.585, 7431.722, 7417.39]
[7415.969, 7428.644, 7430.56, 7415.496, 7415.969]
[7417.39, 7435.585, 7431.722, 7417.39]
[7415.969, 7428.644, 7430.56, 7415.496, 7415.969]
[7417.39, 7435.585, 7431.722, 7417.39]
[7415.969, 7428.644, 7430.56, 7415.496, 7415.969]
[7417.39, 7435.585, 7431.722, 7417.39, 7424.887]
[7415.969, 7428.644, 7430.56, 7415.496, 7415.969, 7439.161]
[7417.39, 7435.585, 7431.722, 7417.39, 7424.887, 7424.409]
</code></pre>

<p>How do I go about automating this so that it automatically gets the newest value instead of me having to cursor over the new bar for it to get the new value</p>

<p>EDIT:</p>

<p>This is the value Im looking for Like I said I can get it to output through the script, but I cant get the value to update when a new candle is made</p>

<p><a href=""https://i.stack.imgur.com/hRhmG.png"" rel=""nofollow noreferrer"">this</a></p>
","9879211","8625512","2018-06-03 14:53:44","Web Scraping tradingview chart with python selenium","<python><selenium><canvas>","1","1","3354"
"50646605","2018-06-01 15:20:35","0","","<p>This will wait until it finds the image:</p>

<pre><code>icon_to_click = ""Recycle Bin""

r = None
while r is None:
    r = pyautogui.locateOnScreen('rb.png', grayscale = True)
print icon_to_click + ' now loaded'
</code></pre>
","5108710","","","0","228","feltersnach","2015-07-12 19:38:42","245","115","25","6","50643931","50646605","2018-06-01 12:55:49","0","991","<p>I have one application which I am able to automate using the pyautogui module. I am keeping the reference image in the script directory, and after some sleep time, I am able to complete it. In some builds, the specific image takes a while to appear. After which my script throws <code>attribute not found error</code>.</p>

<p>I need to know how to control pyautogui to wait until the image appears on-screen and, after that, proceed for the next execution.</p>

<p>I have searched many stack overflow questions where I found wait_until() in the pywinauto module where a script waits for specific window. In the same way, do we have anything for pyautogui to wait for a specific image to appear on-screen?</p>

<p>Python version used: 3.6.</p>
","3664681","3750257","2019-08-08 00:57:28","How to wait for the specific image to appear on screen using pyautogui?","<python><python-3.x><pywinauto><pyautogui>","2","0","747"
"50646607","2018-06-01 15:20:43","0","","<p>You can <a href=""https://cloud.google.com/vision/docs/auth#using_a_service_account"" rel=""nofollow noreferrer"">create a service account</a> and select JSON as key type for authenticating yo Cloud Vision API. The steps to setting up authentication is provided <a href=""https://cloud.google.com/vision/docs/libraries#setting_up_authentication"" rel=""nofollow noreferrer"">here</a>. Once you have the JSON key file, you can setup the environment variable using the ‘export GOOGLE_APPLICATION_CREDENTIALS’ command pointing to the JSON key file. You can use the <a href=""https://cloud.google.com/vision/docs/libraries#client-libraries-usage-php"" rel=""nofollow noreferrer"">PHP version</a> of the client library for the Cloud Vision API. Also here is the <a href=""https://googlecloudplatform.github.io/google-cloud-php/#/docs/google-cloud/v0.67.0/vision/visionclient"" rel=""nofollow noreferrer"">PHP API Reference documentation</a> for additional help.</p>
","9730810","","","0","948","Amruth Bahadursha","2018-05-02 14:19:53","54","31","4","0","50132852","","2018-05-02 10:51:01","-1","122","<p>I am using GCP Cloud Vision API with Python to retrieve some information from images. Specifically, I am sending a photo of a product to this API and I retrieve the web entities related to it; one of them is almost always the brand. </p>

<p>My basic Python script that does this job is the following:</p>

<pre><code>import  io
from google.cloud import vision
from google.cloud.vision import types
import os
import cv2
import numpy as np

os.environ[""GOOGLE_APPLICATION_CREDENTIALS""] = ""/Users/User/PycharmProjects/Project_name.json""

def detect_text(file):

    client = vision.ImageAnnotatorClient()

    with io.open(file, 'rb') as image_file:
        content = image_file.read()

    image = types.Image(content=content)
    web_detection = client.web_detection(image=image).web_detection
    print(web_detection)

file_name = ""/Users/User/Desktop/Image.jpg""
img = cv2.imread(file_name)
detect_text(file_name)
</code></pre>

<p>The <code>Project_name.json</code> file contains some authorisation credentials so that I can have access to the Google Cloud API Client.</p>

<p>I want to do the same thing with PHP. For this reason I have written the the following PHP script so far:</p>

<pre><code>&lt;?php

namespace Google\Cloud\Samples\Vision;
use Google\Cloud\Vision\V1\ImageAnnotatorClient;

function detect_web($path)
{
    $imageAnnotator = new ImageAnnotatorClient();

    $image = file_get_contents($path);
    $response = $imageAnnotator-&gt;webDetection($image);
    $web = $response-&gt;getWebDetection();

    print($web);
}

$path = '/Users/User/Desktop/Image.jpg';
detect_web($path);

?&gt;
</code></pre>

<p>But when I run this I get the following error: </p>

<pre><code>Fatal error: Uncaught Error: Class 'Google\Cloud\Vision\V1\ImageAnnotatorClient' not found in /opt/lampp/htdocs/index.php:12 Stack trace: #0 /opt/lampp/htdocs/index.php(26): Google\Cloud\Samples\Vision\detect_web('/Users/User...') #1 {main} thrown in /opt/lampp/htdocs/index.php on line 12
</code></pre>

<p>Obviously, this error appears because (first of all) I have not provided the Google Authorisation Credentials as I did in Python above (<code>os.environ[""GOOGLE_APPLICATION_CREDENTIALS""]=""/Users/User/PycharmProjects/Project_name.json""</code>).</p>

<p>What should I do to get the appropriate Google library and access it through my PHP script to get the exact same result as in my Python script?</p>
","9024698","9024698","2018-05-02 11:02:56","Convert Python to PHP - GCP Authorisation Credentials","<php><python><google-cloud-platform>","1","5","2402"
"50646677","2018-06-01 15:24:33","0","","<p>Try downloading the .tar.gz file from pypi.org and then in cmd go to the directory with the file and do</p>

<pre><code>pip install &lt;file name&gt;
</code></pre>
","9848775","","","0","167","hobonoobo","2018-05-25 20:23:57","8","1","0","0","50646462","","2018-06-01 15:11:55","1","343","<p>I tried to install <strong>kmeans</strong> package but failed! </p>

<p>I've used so many different ways to solve the issue but none of them works. Hope that someone have idea of how to deal with the issue! Thanks a lot!! </p>

<h1>The error message is as below</h1>

<pre><code>Collecting kmeans
  Using cached https://files.pythonhosted.org/packages/d1/7e/87d12a99d7ccfd2c85b
19899012177e1718c2d6c0148fad421523160e84b/kmeans-1.0.2.tar.gz
Installing collected packages: kmeans
  Running setup.py install for kmeans ... error
    Complete output from command c:\users\AAA\appdata\local\programs\python\pyt
hon36\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\AAA\\A
ppData\\Local\\Temp\\pip-install-hjr78jwg\\kmeans\\setup.py';f=getattr(tokenize,
 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(comp
ile(code, __file__, 'exec'))"" install --record C:\Users\AAA\AppData\Local\Temp\
pip-record-rek9bpzw\install-record.txt --single-version-externally-managed --com
pile:
    running install
    running build
    running build_py
    creating build
    creating build\lib.win-amd64-3.6
    creating build\lib.win-amd64-3.6\kmeans
    copying kmeans\performance.py -&gt; build\lib.win-amd64-3.6\kmeans
    copying kmeans\tests.py -&gt; build\lib.win-amd64-3.6\kmeans
    copying kmeans\__init__.py -&gt; build\lib.win-amd64-3.6\kmeans
    running egg_info
    writing kmeans.egg-info\PKG-INFO
    writing dependency_links to kmeans.egg-info\dependency_links.txt
    writing top-level names to kmeans.egg-info\top_level.txt
    reading manifest file 'kmeans.egg-info\SOURCES.txt'
    writing manifest file 'kmeans.egg-info\SOURCES.txt'
    copying kmeans\lib.c -&gt; build\lib.win-amd64-3.6\kmeans
    running build_ext
    building 'kmeans/lib' extension
    creating build\temp.win-amd64-3.6
    creating build\temp.win-amd64-3.6\Release
    creating build\temp.win-amd64-3.6\Release\kmeans
    C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\x86_amd64\cl.exe

/c /nologo /Ox /W3 /GL /DNDEBUG /MD -Ic:\users\AAA\appdata\local\programs\pytho
n\python36\include -Ic:\users\AAA\appdata\local\programs\python\python36\includ
e ""-IC:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE"" ""-IC:\Progr
am Files (x86)\Windows Kits\10\include\10.0.10240.0\ucrt"" ""-IC:\Program Files (x
86)\Windows Kits\8.1\include\shared"" ""-IC:\Program Files (x86)\Windows Kits\8.1\
include\um"" ""-IC:\Program Files (x86)\Windows Kits\8.1\include\winrt"" /Tckmeans/
lib.c /Fobuild\temp.win-amd64-3.6\Release\kmeans/lib.obj -Wno-error=declaration-
after-statement -O3 -std=c99
    cl : Command line error D8021 : invalid numeric argument '/Wno-error=declara
tion-after-statement'
    error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\B
IN\\x86_amd64\\cl.exe' failed with exit status 2

    ----------------------------------------
Command ""c:\users\AAA\appdata\local\programs\python\python36\python.exe -u -c ""
import setuptools, tokenize;__file__='C:\\Users\\AAA\\AppData\\Local\\Temp\\pip
-install-hjr78jwg\\kmeans\\setup.py';f=getattr(tokenize, 'open', open)(__file__)
;code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exe
c'))"" install --record C:\Users\AAA\AppData\Local\Temp\pip-record-rek9bpzw\inst
all-record.txt --single-version-externally-managed --compile"" failed with error
code 1 in C:\Users\AAA\AppData\Local\Temp\pip-install-hjr78jwg\kmeans\
</code></pre>

<p>====</p>
","9881882","","","Python kmeans package installation failure","<python><k-means>","1","1","3472"
"50646687","2018-06-01 15:25:27","0","","<p>Unfortunately there is no workaround here unless you provide a <code>tf.Variable()</code> (which is not possible in your case) to the <code>parameter</code> of <code>tf.nn.embedding_lookup()</code>/<code>tf.gather()</code>. 
This is happening because, When you declare them with a placeholder of shape <code>[None, None]</code>, from <code>tf.gather()</code> function <code>tf.IndexedSlices()</code> become a <a href=""https://stackoverflow.com/questions/44779036/what-is-the-sparse-representation-of-a-tensor""><code>sparse tensor</code></a>.</p>

<p>I have already done projects facing this <code>warning</code>. What I can tell you that if there is a <code>tf.nn.dynamic_rnn()</code> next to the <code>embedding_lookup</code> then make the parameter named <code>swap_memory</code> of <code>tf.nn.dynamic_rnn()</code> to <code>True</code>. Also to avoid <code>OOM</code> or <code>Resource Exhausted error</code> make the batch size smaller (test for different batch size).</p>

<p>There are already some good explanation on this. Please refer to the following Question of the Stackoverflow.</p>

<p><a href=""https://stackoverflow.com/questions/35892412/tensorflow-dense-gradient-explanation"">Tensorflow dense gradient explanation?</a> </p>
","2277037","2277037","2018-06-01 15:30:42","0","1243","Maruf","2013-04-13 08:45:35","472","78","154","2","50644567","","2018-06-01 13:28:38","1","413","<p>My dataset consists of sentences. Each sentence has a variable length and is initially encoded as a sequence of vocabulary indexes, ie. a tensor of shape [sentence_len]. The batch size is also variable.</p>

<p>I have grouped sentences of similar lengths into buckets and padded where necessary, to bring each sentence in a bucket to the same length.</p>

<p>How could I deal with having both an unknown sentence length AND batch size?</p>

<p>My data provider would tell me what the sentence length is at every batch, but I don't know how to feed that -> the graph is already built at that point. The input is represented with a placeholder <code>x = tf.placeholder(tf.int32, shape=[batch_size, sentence_length], name='x')</code>. I can turn <code>batch_size</code> or <code>sentence_length</code> to <code>None</code>, but not both.</p>

<p>UPDATE: in fact, interestingly, I can set both to <code>None</code>, but I get <code>Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.</code>Note: the next layer is an embedding_lookup.</p>

<p>I'm not sure what this means and how to avoid it. I assume it has something to do with using <code>tf.gather</code> later, which I need to use.
Alternatively is there any other way to achieve what I need?</p>

<p>Thank you.</p>
","4494842","4494842","2018-06-01 14:56:18","Tensorflow: variable sequence length AND batch size","<python><tensorflow><machine-learning>","1","2","1329"
"50646691","2018-06-01 15:25:44","3","","<p>Possibly you mean to use <a href=""https://matplotlib.org/gallery/subplots_axes_and_figures/axhspan_demo.html"" rel=""nofollow noreferrer""><code>axhspan</code></a>.</p>

<pre><code>p1 = 4700
p2 = 7700
ax1.axhspan(p1, p2, color=""limegreen"", alpha=0.5)
</code></pre>

<p><a href=""https://i.stack.imgur.com/cvabu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cvabu.png"" alt=""enter image description here""></a></p>
","4124317","4124317","2018-06-01 15:47:10","2","432","ImportanceOfBeingErnest","2014-10-09 07:50:43","173272","28712","2299","3162","50646523","50646691","2018-06-01 15:15:42","1","67","<p>I'm trying to create a shaded rectangle based on two vertical coordinates (the length of the rectangle should span the whole x axis).
I've tried to use <code>fill_between</code> with the following (p1 &amp; p2 are the data coordinates):</p>

<pre><code>f, ax1 = plt.subplots()   
x = np.linspace(200,300,2000)
y = x**(2)
y1 = x
p1 = 4700
p2 = 7700

ax1.plot(x, y, lw=2)
ax1.set_xlabel('Temperature $[K]$', fontweight='bold')
ax1.set_ylabel('Pressure $[mb]$', fontweight='bold')
ax1.fill_between(x, y1.fill(p1), y1.fill(p2))
</code></pre>

<p>But i'm getting the following error:</p>

<pre><code>TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
</code></pre>

<p>What is wrong? Any better way to do this?</p>

<p>Thank you!!</p>
","3952424","","","Shade area between 2 y coordinates in matplotlib","<python><matplotlib>","4","1","850"
"50646707","2018-06-01 15:26:41","0","","<p><code>json.loads(ciscodata)</code> returns an array of dictionaries.</p>

<p>The statement <code>o_node = (jsonarray.get('identifier'))</code> fails because it is trying to get a dictionary key when <code>jsonarray</code> is a list.</p>

<p>Try <code>o_node = jsonarray[0].get('identifier')</code></p>

<p>I am not sure why you have parens around jsonarray -- those shouldn't be there.</p>

<p>In general, you can do something like:</p>

<pre><code>for elem in jsonarray:
    identifier = elem.get('identifier')
    title = elem.get('title')
    ... etc
    do_something(identifier, title, ...)
</code></pre>

<p>This makes it easier to debug because you can test the values you get back from the api call.</p>

<p>If you are always getting the same keys in your dictionary, you can skip this assignment and just do </p>

<pre><code>for elem in jsonarray:
    do_something(elem.get('key1'), elem.get('key2'), ...)`
</code></pre>

<p>Hope that helps</p>
","3614855","","","1","956","fiacre","2014-05-08 04:55:12","645","70","99","13","50646321","50646707","2018-06-01 15:04:32","1","50","<p>I am working on a script that will get data from a Website (Cisco Patches site) and based on the data received, I need to post it to another site (ServiceNow Event Management). The POST needs to be REST/JSON with specific keys for this to work.</p>

<p>I have enough code to GET the data and I have the code to POST working.</p>

<p>I am having a hard time with converting the data I get from GET to map it into valid JSON key value pairs to POST.</p>

<p>I am using the following code to get a list of new patches from Cisco website. I am getting the correct data but the format if the data is not how I can use it to post to another tool in JSON format (using different keys but values from the returned information.</p>

<p>This works -</p>

<pre><code>def getjson(ciscourl):
    response = urllib.request.urlopen(ciscourl)
    ciscodata = response.read().decode(""utf-8"")
    return json.loads(ciscodata)
</code></pre>

<p>The data I get back looks like below (this query resulted in 2 patches):</p>

<p><code>[{""identifier"":""cisco-sa-20180521-cpusidechannel"",""title"":""CPU Side-Channel Information Disclosure Vulnerabilities: May 2018"",""version"":""1.5"",""firstPublished"":""2018-05-22T01:00:00.000+0000"",""lastPublished"":""2018-05-31T20:44:16.123+0000"",""workflowStatus"":null,""id"":1,""name"":""Cisco Security Advisory"",""url"":""https://tools.cisco.com/security/center/content/CiscoSecurityAdvisory/cisco-sa-20180521-cpusidechannel"",""severity"":""Medium"",""workarounds"":""No"",""cwe"":null,""cve"":""CVE-2018-3639,CVE-2018-3640"",""ciscoBugId"":"""",""status"":""Updated"",""summary"":""On May 21, 2018, researchers disclosed two vulnerabilities that take advantage of the implementation of speculative execution of instructions on many modern microprocessor architectures to perform side-channel information disclosure attacks. These vulnerabilities could allow an unprivileged, "",""totalCount"":6,""relatedResource"":[]},{""identifier"":""cisco-sa-20180516-firepwr-pb"",""title"":""Cisco Firepower Threat Defense Software Policy Bypass Vulnerability"",""version"":""1.0"",""firstPublished"":""2018-05-16T16:00:00.000+0000"",""lastPublished"":""2018-05-16T16:00:00.000+0000"",""workflowStatus"":null,""id"":1,""name"":""Cisco Security Advisory"",""url"":""https://tools.cisco.com/security/center/content/CiscoSecurityAdvisory/cisco-sa-20180516-firepwr-pb"",""severity"":""Medium"",""workarounds"":""No"",""cwe"":""CWE-693"",""cve"":""CVE-2018-0297"",""ciscoBugId"":""CSCvg09316"",""status"":""New"",""summary"":""A vulnerability in the detection engine of Cisco Firepower Threat Defense software could allow an unauthenticated, remote attacker to bypass a configured Secure Sockets Layer (SSL) Access Control (AC) policy to block SSL traffic.The vulnerability is due to the incorrect handling "",""totalCount"":6,""relatedResource"":[]}]</code></p>

<p>I can extract values from this, as such <code>print(jarray.get('identifier'))</code> but I am having a hard time being able to map these values into my own JSON map with keys I define. So the value from the key <code>identifier</code> I got back, needs to map to a key called <code>""node""</code> in my JSON map.</p>

<p>I have tried <code>json.loads</code>, <code>json.load</code>, <code>json.dump</code>, <code>json.dumps</code>. Each time the error is Attribute Type error.</p>

<p><strong>This is the code where I am confused:</strong></p>

<pre><code>def createJson(l):
#try:

    jsonarray = l
    o_source = ""CiscoUpdatePatchChecker""
    o_node = (jsonarray.get('identifier')) #this does not work
    o_metric_name = (""Critical"")
    o_type = (""test"")
    o_resource = (""test_resource"")
    o_description = jsonarray  #this works
    o_event_class = (""test event class"")
    o_additional_info = jsonarray
    print (""-"" * 50)
    print (o_source, o_node, o_metric_name, o_type, o_resource, o_description, o_event_class, o_additional_info)
    print (""-"" * 50)
    data = {""source"": o_source, ""node"": o_node, ""metric_name"": o_metric_name, ""type"": o_type, ""resource"": o_resource, ""event_class"": o_event_class, ""description"": o_description, ""additional_info"": o_additional_info}
    return json.dumps(data)
# except:
    #pass
</code></pre>

<p>Beyond this, the rest of the code just posts the data to ITSM which is working. -</p>

<pre><code>def postjson(data):
    # try:
    url = posturl
    auth = HTTPBasicAuth(username, password)
    head = {'Content-type': 'application/json',
            'Accept': 'application/json'}
    payld = data
    ret = requests.post(url, auth=auth , data=payld, headers=head)
    # sys.stdout.write(ret.text)
    returned_data = ret.json()
    print(returned_data)
</code></pre>

<p>So my issue is to map data I am getting back to my keys:value pairs in a JSON map, &amp; I will need to loop the code for as many times as the number of patches are retrieved. I am currently planning to loop in my main function for number of JSON maps that need to POST.</p>

<p>For now, I am just take all the data I get and mapping all the data I get to the <code>""description""</code> and <code>""additional_info""</code> field. This works and I can post the data fine.</p>

<p>It will help me tremendously if someone can point me to examples of how to manipulate the data I am getting from my GET request.</p>
","9085420","355230","2018-06-01 16:01:46","Need to get or convert data from a site to be valid JSON","<python><json><python-3.x><rest>","1","0","5189"
"50646720","2018-06-01 15:27:18","1","","<p>IIUC, the issue is with the use of the <code>.fill()</code> function. You can avoid that, using <code>fill_between</code> with your <code>x</code> array and just the <code>p1</code> and <code>p2</code> values, as <a href=""https://matplotlib.org/api/_as_gen/matplotlib.pyplot.fill_between.html"" rel=""nofollow noreferrer""><code>fill_between</code></a> can accept scalar values for <code>y1</code> and <code>y2</code>.</p>

<pre><code>ax1.fill_between(x, p1, p2)
</code></pre>

<p><a href=""https://i.stack.imgur.com/1xAGv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1xAGv.png"" alt=""enter image description here""></a></p>

<p>Note, you can change the color and transparancy easily by adding a <code>color</code> and <code>alpha</code> argument:</p>

<pre><code>ax1.fill_between(x, p1, p2, color='limegreen', alpha=0.5)
</code></pre>
","6671176","6671176","2018-06-01 16:02:38","0","855","sacuL","2016-08-03 07:43:41","32843","1871","2095","286","50646523","50646691","2018-06-01 15:15:42","1","67","<p>I'm trying to create a shaded rectangle based on two vertical coordinates (the length of the rectangle should span the whole x axis).
I've tried to use <code>fill_between</code> with the following (p1 &amp; p2 are the data coordinates):</p>

<pre><code>f, ax1 = plt.subplots()   
x = np.linspace(200,300,2000)
y = x**(2)
y1 = x
p1 = 4700
p2 = 7700

ax1.plot(x, y, lw=2)
ax1.set_xlabel('Temperature $[K]$', fontweight='bold')
ax1.set_ylabel('Pressure $[mb]$', fontweight='bold')
ax1.fill_between(x, y1.fill(p1), y1.fill(p2))
</code></pre>

<p>But i'm getting the following error:</p>

<pre><code>TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
</code></pre>

<p>What is wrong? Any better way to do this?</p>

<p>Thank you!!</p>
","3952424","","","Shade area between 2 y coordinates in matplotlib","<python><matplotlib>","4","1","850"
"50646735","2018-06-01 15:28:07","-2","","<p>You've misunderstood <code>fill_between</code>. When  you use it is advisable to express <code>y</code> as a function:</p>

<pre><code>def y(x):
    return x**2
section = np.arange(start,end,0.1)
plt.fill_between(section,y(section))
</code></pre>

<p>will do the job with suitable values for <code>start</code> and <code>end</code>.</p>
","2041983","","","6","340","Paula Thomas","2013-02-05 05:55:24","912","161","36","11","50646523","50646691","2018-06-01 15:15:42","1","67","<p>I'm trying to create a shaded rectangle based on two vertical coordinates (the length of the rectangle should span the whole x axis).
I've tried to use <code>fill_between</code> with the following (p1 &amp; p2 are the data coordinates):</p>

<pre><code>f, ax1 = plt.subplots()   
x = np.linspace(200,300,2000)
y = x**(2)
y1 = x
p1 = 4700
p2 = 7700

ax1.plot(x, y, lw=2)
ax1.set_xlabel('Temperature $[K]$', fontweight='bold')
ax1.set_ylabel('Pressure $[mb]$', fontweight='bold')
ax1.fill_between(x, y1.fill(p1), y1.fill(p2))
</code></pre>

<p>But i'm getting the following error:</p>

<pre><code>TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
</code></pre>

<p>What is wrong? Any better way to do this?</p>

<p>Thank you!!</p>
","3952424","","","Shade area between 2 y coordinates in matplotlib","<python><matplotlib>","4","1","850"
"50646739","2018-06-01 15:28:21","1","","<p>Convert matrices into regular matrix  <code>y_pred = y_pred.A</code> and <code>y_true = y_true.A</code>, then compute <code>accuracy_score(y_true, y_pred)</code></p>
","3731726","3374996","2018-06-02 05:21:14","0","169","jujuBee","2014-06-11 20:33:06","420","27","69","3","50646415","50646739","2018-06-01 15:09:19","0","436","<p>I am trying to run accuracy_score from sklearn.metrics in Python. My true y's and predicted y's are both in sparse matrix format --</p>

<pre><code>import scipy.sparse as sp
from sklearn.metrics import accuracy_score
y_true = sp.csr_matrix(y.values) # where y is a multi-label dataframe
y_pred = model.predict(X) # X is same format as y_true    
accuracy_score(y_true, y_pred)
</code></pre>

<p>I get the following error:</p>

<pre><code>TypeError: len() of unsized object
</code></pre>

<p>I checked the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html"" rel=""nofollow noreferrer"">documentation </a> and it should be able to accept sparse matrices. </p>

<p>Just for clarity, when I try to look at the contents, I get the following for both:</p>

<pre><code>[In]  y_true
[Out] &lt;9646x1248 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
                 with 36700 stored elements in Compressed Sparse Row format&gt;
[In]  y_pred
[Out] &lt;9646x1248 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
                 with 373603 stored elements in Compressed Sparse Row format&gt;
</code></pre>

<p>Why am I getting this error and how do I fix my input?</p>
","6331020","","","Type error with sklearn's accuracy_score function","<python><scipy><scikit-learn><typeerror><sparse-matrix>","1","3","1214"
"50646740","2018-06-01 15:28:23","1","","<p>When Gunicorn is called by Airflow, it uses <code>~\airflow\www\gunicorn_config.py</code> as its config file.</p>
","9881810","","","0","117","T3D","2018-06-01 14:33:23","26","0","0","0","50264760","50646740","2018-05-10 02:52:36","1","874","<p>I'm trying to run Apache Airflow's webserver from a virtualenv on a Redhat machine, with some configuration options from a Gunicorn config file. Gunicorn and Airflow are both installed in the virtualenv. The command <code>airflow webserver</code> starts Airflow's webserver and the Gunicorn server. The config file has options to make sure Gunicorn uses/accepts TLSv1.2 only, as well as a list of ciphers to use. </p>

<p>The Gunicorn config file is <code>gunicorn.py</code>. This file is referenced through an environment variable <code>GUNICORN_CMD_ARGS=""--config=/path/to/gunicorn.py ...""</code> in <code>.bashrc</code>. This variable also sets a couple of other variables in addition to <code>--config</code>. However, when I run the <code>airflow webserver</code> command, the options in <code>GUNICORN_CMD_ARGS</code> are never applied. </p>

<p>Seeing as how Gunicorn is not called from command line, but instead by Airflow, I'm assuming this is why the <code>GUNICORN_CMD_ARGS</code> environment variable is not read, but I'm not sure and I'm new to both technologies...</p>

<p>TL;DR:
Is there another way to set up Gunicorn to automatically reference a config file, without the <code>GUNICORN_CMD_ARGS</code> environment variable? </p>

<p>Here's what I'm using:</p>

<ul>
<li>gunicorn 19.8.1</li>
<li>apache-airflow 1.9.0</li>
<li>python 2.7.5</li>
</ul>
","1930818","","","Apache Airflow: Gunicorn Configuration File Not Being Read?","<python><virtualenv><config><gunicorn><airflow>","1","0","1369"
"50646741","2018-06-01 15:28:26","1","","<p>As the <a href=""https://docs.python.org/3/library/functions.html?highlight=isinstance#isinstance"" rel=""nofollow noreferrer"">docs</a> says, <code>.isinstance</code> takes <code>object</code> as the first argument and <code>classinfo</code> as the second argument. </p>

<p>The correct way is as follows:   </p>

<pre><code>import datetime 
df.loc[df['date'].apply(lambda x: isinstance(x, datetime.datetime))]
</code></pre>
","9609447","9609447","2018-06-01 16:17:38","5","425","harvpan","2018-04-06 20:08:42","7030","853","477","708","50646640","","2018-06-01 15:22:23","2","252","<p>Basically I am trying to delete rows of a pandas dataframe where values in a certain column are not instances of datetime. I have tried:</p>

<pre><code>df = df[df[‘date’] == isinstance(datetime)]
</code></pre>

<p>I know isinstance takes two arguments (I am missing the value to be checked) but I’m not sure what to put there. </p>
","7664441","9209546","2018-06-01 15:52:24","Deleting rows of pandas dataframe based on instance type","<python><pandas><datetime>","3","0","336"
"50646809","2018-06-01 15:33:14","0","","<p>Well i took a bit time, and i have a solution, its still in ""rough"" condition, but it works and it does not block the gui when i use it.
I put that logic in a new thread, and i call it from there, there needs to done a lot tweaking. 
I dont know if this is the most ""elegant"" approach, so if anyone else have better idea, please dont hesitate to say.</p>

<pre><code>class Threaddy(QThread):

def __init__(self):
    QThread.__init__(self)

def __del__(self):
    self.wait()

def run(self):

    song_list=[]
    r = requests.get('https://www.youtube.com/playlist?list=PLD6s0l-FZhjkc-TYwXO5GbwyxFqTd5Y9J')
    page = r.text
    soup=bs(page,'html.parser')
    res=soup.find_all('a',{'class':'pl-video-title-link'})
    for l in res:
        #print (l.get(""href""))
        #print(""https://www.youtube.com""+l.get(""href""))
        yt ='https://www.youtube.com'
        temp =l.get(""href"")
        url =yt+temp
        video = pafy.new(url)
        bestaudio = video.getbestaudio()
        song = bestaudio.url
        #print(video.getbestaudio().url)
        song_list.append(video)

    for song in song_list:

        media=instance.media_new(song.getbestaudio().url) #THIS WORKS NOW

        media.get_mrl()
        player.set_media(media)
        player.play()
        print(song.title) #SO DOES THIS
        playing = set([1,2,3,4])
        time.sleep(1)
        duration = player.get_length() / 1000
        mm, ss = divmod(duration, 60)

        while True:
            state = player.get_state()
            if state not in playing:
                break
            continue    
</code></pre>
","5536399","5536399","2018-06-01 17:06:35","2","1603","ImRaphael","2015-11-07 09:51:10","78","91","12","0","50626583","","2018-05-31 14:22:39","1","304","<p>I have this piece of code that works with no issues :</p>

<pre><code>Media_list = instance.media_list_new(song_list)
list_player = instance.media_list_player_new()
list_player.set_media_list(Media_list)
list_player.play() 
</code></pre>

<p>how ever i would like to itterate through a list, and use normal vlc player to play it.</p>

<pre><code>playing = set([1,2,3,4])
for i in song_list:
player.set_mrl(i)
player.play()
play=True
while play == True:
    time.sleep(1)
    play_state = player.get_state()
    if play_state in playing:
        continue
    else:
        play = False
</code></pre>

<p>This does almost the same thing, and it suits my needs better, however it freezes my GUi,(qml/pyside2). So now i am cofused, am i supposed to make a new thread for this, or is there some other way to do this in vlc.</p>

<p>Well i did try creating new thread and running the function above in it, however same issue, the moment player goes in to for loop and start play method, the gui freezes.(the vlc works normaly, and plays the playlist, but gui is unresponsive for duration)</p>

<p>so just to expand a bit , this is the part that i have, and it works ok, but i cant get data from my songs during their play time, since all i have is url, and not the metadata .</p>

<pre><code>song_list=[]
r = requests.get('https://www.youtube.com/playlist?list=PLD6s0l-FZhjkc-TYwXO5GbwyxFqTd5Y9J')
page = r.text
soup=bs(page,'html.parser')
res=soup.find_all('a',{'class':'pl-video-title-link'})
for l in res:
    #print (l.get(""href""))
    #print(""https://www.youtube.com""+l.get(""href""))
    yt ='https://www.youtube.com'
    temp =l.get(""href"")
    url =yt+temp
    video = pafy.new(url)
    bestaudio = video.getbestaudio()
    song = bestaudio.url
    #print(video.getbestaudio().url)
    song_list.append(song)

Media_list = instance.media_list_new(song_list)
list_player = instance.media_list_player_new()
list_player.set_media_list(Media_list)
list_player.play() 
</code></pre>

<p>what i would want is:</p>

<pre><code>@Slot()
def print_yt_playlist(self):
song_list=[]
r = requests.get('https://www.youtube.com/playlist?list=PLD6s0l-FZhjkc-TYwXO5GbwyxFqTd5Y9J')
page = r.text
soup=bs(page,'html.parser')
res=soup.find_all('a',{'class':'pl-video-title-link'})
for l in res:
    #print (l.get(""href""))
    #print(""https://www.youtube.com""+l.get(""href""))
    yt ='https://www.youtube.com'
    temp =l.get(""href"")
    url =yt+temp
    video = pafy.new(url)
    bestaudio = video.getbestaudio()
    song = bestaudio.url
    #print(video.getbestaudio().url)
    song_list.append(video)
 playing = set([1,2,3,4])
 for i in song_list:
     media = instance.media_new(i.getbestaudio().url)
     print(i.Artist) #THIS is what i want, i want to be able to acces that data for the song that is playing
     print(i.Duration) #and this and so on, that is why i want to loop through list, since i dont think i can do it with media_list
     player.set_media(media)
     player.play()
     play=True
 while play == True:
    time.sleep(1)
    play_state = player.get_state()
    if play_state in playing:
        continue
    else:
        play = False
</code></pre>

<p>Or more simple, is there a way that i paste ""video"" in to the media_list and then from there i could access data about current song, as well playing the song ?</p>

<p>I dont know what could help you from qml side, the only thing i do is trigger this function on button click.</p>
","5536399","5536399","2018-06-01 08:03:50","vlc player freezes GUI (python thread?)","<python><python-3.x><libvlc><pyside2>","1","11","3441"
"50646824","2018-06-01 15:33:49","0","","<p>I figured it out eventually! The least_squares function (or any of the optimization functions in scipy I believe) take only one argument of optimization. This does not mean that n parameters can't be added; it means that when you define a cost/objective function, you must define one parameter as a list. </p>

<pre><code>def function(params,data):
     theta, phi = params
     utility = p2*theta - (1-theta)*np.minimum((guilt + phi), (inequity - phi))
return utility
</code></pre>

<p>an alternative construction </p>

<pre><code>def function(params,data):
     utility = p2*params[0]- (1-params[0])*np.minimum((guilt + params[1]), (inequity - params[1]))
return utility
</code></pre>

<p>Hope this helps. Note that the function above might not actually minimize, it's a random function. </p>
","9847364","","","0","798","Filippo Ricci","2018-05-25 14:14:29","1","2","0","0","50531652","50646824","2018-05-25 14:36:09","0","78","<p>I am doing some utility modeling but I got stuck when trying to optimize my objective function and I don't understand the problem. </p>

<p>I have a utility function (1) that takes experimental data as arguments as well as two parameters (theta and phi) with which I want to optimize function (1). The function returns the optimal choice given aforementioned data and parameter combinations. </p>

<p>The objective function (2) is basically a sum of squared residuals (SSE) which takes function (1) as an input and determines how well the optimal choice from function (1) performs against actual data. </p>

<p>This is where things go wrong. I try to optimize function 2 in respects of parameters theta and phi. Here's the code:</p>

<pre><code>from scipy.optimize import minimize
    def costmixed(theta,phi,data):
    predictions = []
    for trialNr,trialDat in data.iterrows():
        predictions.append(mixed_model( 
        trialDat['inv'],trialDat['mult'],trialDat['belMult'],theta,phi))

    return np.sum(np.square(data['ret'] - np.array(predictions))) #returns sum of squared residuals.
#initial guesses
x0=[1000,0.5]
sol=minimize(costmixed,x0,args=[data],bounds=bnds)
</code></pre>

<p>And the subsequent errors I receive are: </p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-46-87cded55e0f8&gt; in &lt;module&gt;()
      7     return np.sum(np.square(data['ret'] - np.array(predictions))) #returns sum of squared residuals.
      8 
----&gt; 9 sol=minimize(costmixed,x0,args=[data],bounds=bnds)

C:\ProgramData\Anaconda3\lib\site-packages\scipy\optimize\_minimize.py in minimize(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)
    448     elif meth == 'l-bfgs-b':
    449         return _minimize_lbfgsb(fun, x0, args, jac, bounds,
--&gt; 450                                 callback=callback, **options)
    451     elif meth == 'tnc':
    452         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,

C:\ProgramData\Anaconda3\lib\site-packages\scipy\optimize\lbfgsb.py in _minimize_lbfgsb(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)
    326             # until the completion of the current minimization iteration.
    327             # Overwrite f and g:
--&gt; 328             f, g = func_and_grad(x)
    329         elif task_str.startswith(b'NEW_X'):
    330             # new iteration

C:\ProgramData\Anaconda3\lib\site-packages\scipy\optimize\lbfgsb.py in func_and_grad(x)
    271     if jac is None:
    272         def func_and_grad(x):
--&gt; 273             f = fun(x, *args)
    274             g = _approx_fprime_helper(x, fun, epsilon, args=args, f0=f)
    275             return f, g

C:\ProgramData\Anaconda3\lib\site-packages\scipy\optimize\optimize.py in function_wrapper(*wrapper_args)
    290     def function_wrapper(*wrapper_args):
    291         ncalls[0] += 1
--&gt; 292         return function(*(wrapper_args + args))
    293 
    294     return ncalls, function_wrapper

TypeError: costmixed() missing 1 required positional argument: 'data'
</code></pre>

<p>I hope you wizards are able to help me! I am new to python so I may be missing something quite simple. Thanks in advance. </p>
","9847364","","","multiparametric optimization with scipy and experimental data","<python><optimization><scipy>","1","1","3402"
"50646839","2018-06-01 15:34:31","1","","<p>This appears to be a bug in the latest version of pandas:</p>

<p><a href=""https://github.com/pandas-dev/pandas/issues/21158"" rel=""nofollow noreferrer"">https://github.com/pandas-dev/pandas/issues/21158</a></p>

<p>I'm running pandas '0.23.0' and I can reproduce the same error.
You can see in the github discussion thread that error arises due to condition case when <em>null</em> value occurs on the nesting level greater than 0. It seems to have been changed around two months ago that seems to have made it's way into 0.23.0 release two weeks ago:</p>

<p><a href=""https://github.com/pandas-dev/pandas/commit/01882ba5b4c21b0caf2e6b9279fb01967aa5d650#diff-9c654764f5f21c8e9d58d9ebf14de86d"" rel=""nofollow noreferrer"">https://github.com/pandas-dev/pandas/commit/01882ba5b4c21b0caf2e6b9279fb01967aa5d650#diff-9c654764f5f21c8e9d58d9ebf14de86d</a></p>

<p>Other than waiting for the new release or downgrading your production env (which is not a good idea, since it will quite likely break things), you could think of how to handle multiple package versions in your env. Pip is not capable of doing so unless you create different virtual environments, neither is conda I believe. What you could do, if you really need to load files like those, is to load the '0.22.0' package as a local module by cloning it from git as a temporary, hacky, solution - just to load your dict. But there might be some dataframe API inconsistencies when you load with 0.22.0 and try to use it with 0.23.0. </p>

<p>Your solution of converting strings might not be that bad after-all.</p>

<p>Happy hacking.</p>
","4357954","","","0","1591","user59271","2014-12-13 19:43:17","171","15","67","0","50645240","50646839","2018-06-01 14:05:34","2","1054","<p>i have below json which i get from external webservice :</p>

<pre><code>text=""""""
     [{
        ""id"":""1"",
         ""name"" : ""abc"",
         ""address"":{
                    ""flat"":""123"",
                    ""city"":""paris"",
                    ""street"":null
         },
         ""error"":null
     }]
</code></pre>

<p>Now i want to create dataframe from this json. When i try below :</p>

<pre><code>from pandas.io.json import json_normalize
import json
import pandas as pd

resp_json = json.loads(text)
response = json_normalize(resp_json)
</code></pre>

<p>But this gives me below error:</p>

<p><code>Error at response = json_normalize(resp_json)
 KeyError : 'street'</code></p>

<p>I believe its because street attribute has value as null thats why it is throwing this error. How can this be resolved?</p>

<p>If i do like below, I am able to resolve but ideally its not the right solution</p>

<pre><code>text = text.replace('""street"":null','""street"":""""')
</code></pre>

<p><strong>NOTE:</strong> -  When I use python verion 3.6.3 :: Anaconda Inc. and pandas version 0.20.3  I do not see this issue and json_normalize is able to work properly. This is my local machine setup.</p>

<p>On production machine we have - Python - 3.5.1 and pandas 0.23.0. There we encounter above issue.</p>
","2362960","","","Pandas json_normalize fails with null values in JSON","<python><json><python-3.x><pandas>","1","3","1294"
"50646863","2018-06-01 15:35:55","1","","<p>Whenever you call plt.show() it shows a plot. If you only want to show plots of groups of 24 subplots you simply call plt.show() once every 24 steps. I just plotted some lines in the example below.</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
for i in range(0,421):
    x = np.linspace(0,1,100)
    plt.plot(x,x+i)
    if(i%24 == 0):
        plt.show()
</code></pre>
","3369049","","","0","392","Tristhal","2014-03-01 19:00:53","100","12","11","0","50646682","50646863","2018-06-01 15:24:56","0","17","<p>Alright so i want to plot 421 in a 6x4 sub plot and i want this to create a new image after every 24th image. I've tried somethings before but it ends up giving normal 421 plots one below the other</p>

<pre><code>for i in range(0,421):
a = df.iloc[i:i+1]
x = np.concatenate([a['t11_arms_number_a31_1_weighted_fraction'],a['t11_arms_number_a32_2_weighted_fraction'],a['t11_arms_number_a33_3_weighted_fraction'],a['t11_arms_number_a34_4_weighted_fraction'],a['t11_arms_number_a36_more_than_4_weighted_fraction'],a['t11_arms_number_a37_cant_tell_weighted_fraction']])
y = np.linspace(1,6,6)
plt.plot(y,x)
plt.show()
</code></pre>
","9760503","","","multiple sub plots with for looping","<python><subplot>","1","0","631"
"50646872","2018-06-01 15:36:26","3","","<p>Instead of a series of <code>if/elif</code> statements, you can store the numeric values for the grades and the suffixes in dictionaries. Also, you do not have to use <code>startswith</code> or <code>endswith</code>, you could just use <code>s[0]</code> and <code>s[1]</code> to get the first or second character in the string, after checking the length. You can also use <code>s[1:]</code> to get everything starting at the <code>1</code>th position, even if it's empty.</p>

<pre><code>grades = {'A': 4, 'B': 3, 'C': 2, 'D': 1, 'F': 0}
signs = {'+': 0.3, '-': -0.3, '': 0}
def gp(s):
    return grades[s[0]] + signs[s[1:]]

&gt;&gt;&gt; gp(""A+"")
4.3
&gt;&gt;&gt; gp(""C-"")
1.7
&gt;&gt;&gt; gp(""F"")
0
</code></pre>
","1639625","","","2","718","tobias_k","2012-08-31 20:34:34","63214","4895","5994","655","50646663","","2018-06-01 15:23:59","-1","58","<p>I must modify the function gp so it will handle + and - grades by adding or subtracting 0.3 points. For example, a B + is worth 3.3 points, and a C- is 1.7 points. </p>

<p>Example. </p>

<pre><code>&gt;&gt;&gt; gp('A-')
3.7

&gt;&gt;&gt;gp('B+')
3.3
</code></pre>

<p>The suggestion is I could just add a bunch of elif clauses to test each grade separately, but a similar design is to use a call to s.startswith to figure out the value of the letter grade, then use s.endswith to see if you should add or subtract 0.3 points.</p>

<p>So far this is what I have.</p>

<pre><code>def gp(s):
       A = 4
       return A
       B = 3
       return B
       C = 2
       return C
       D = 1
       return D
       F = 0
       return f
</code></pre>
","9862492","1639625","2018-06-01 15:25:58","Modifying a function to return different values","<python><function><variables><return>","5","4","752"
"50646879","2018-06-01 15:36:46","0","","<p>You should use dictionnaries</p>

<pre><code>def function(grade)
  dict = {'A':4, 'B':3, 'C':2, 'D':1, 'F':0}
  if grade[1] == '+':
    return dict[grade[0]]+0.3
  elif grade[1] == '-':
    return dict[grade[0]] - 0.3
</code></pre>

<p>Then add or substract 0.3 depending on the sign</p>
","9882112","9882112","2018-06-01 15:45:05","3","291","user9882112","2018-06-01 15:36:28","1","0","0","0","50646663","","2018-06-01 15:23:59","-1","58","<p>I must modify the function gp so it will handle + and - grades by adding or subtracting 0.3 points. For example, a B + is worth 3.3 points, and a C- is 1.7 points. </p>

<p>Example. </p>

<pre><code>&gt;&gt;&gt; gp('A-')
3.7

&gt;&gt;&gt;gp('B+')
3.3
</code></pre>

<p>The suggestion is I could just add a bunch of elif clauses to test each grade separately, but a similar design is to use a call to s.startswith to figure out the value of the letter grade, then use s.endswith to see if you should add or subtract 0.3 points.</p>

<p>So far this is what I have.</p>

<pre><code>def gp(s):
       A = 4
       return A
       B = 3
       return B
       C = 2
       return C
       D = 1
       return D
       F = 0
       return f
</code></pre>
","9862492","1639625","2018-06-01 15:25:58","Modifying a function to return different values","<python><function><variables><return>","5","4","752"
"50646893","2018-06-01 15:37:21","2","","<p>There are several problems with the <code>gp</code> function.</p>

<p>While it is possible to enumerate them, I suggest you read the following resources:</p>

<ul>
<li><a href=""https://docs.python.org/3/tutorial/controlflow.html#defining-functions"" rel=""nofollow noreferrer"">How to declare a Python function</a></li>
<li><a href=""https://docs.python.org/3/tutorial/controlflow.html#more-on-defining-functions"" rel=""nofollow noreferrer"">How to use function arguments</a></li>
<li><a href=""https://stackoverflow.com/questions/7129285/what-is-the-purpose-of-the-return-statement"">What is the purpose of the return statement?</a></li>
</ul>

<p>When you're ready, I strongly recommend you use dictionaries to store your mappings. Here's an example:</p>

<pre><code>grade_dict = dict(zip('ABCDF', (4, 3, 2, 1, 0)))
sign_dict = dict(zip('-+', (-0.3, 0.3)))

def gp(s):

    grade, *sign = s

    if sign:
        return grade_dict[grade] + sign_dict[sign[0]]
    else:
        return grade_dict[grade]

res = gp('A-')  # 3.7
</code></pre>

<p><strong>Edit</strong>: My schooling didn't involve GPA, so I didnt't catch this. If you need to cap at 4.0, you can use this modification:</p>

<pre><code>def gp(s):

    grade, *sign = s

    if sign:
        res = grade_dict[grade] + sign_dict[sign[0]]
    else:
        res = grade_dict[grade]

    return max(res, 4.0)
</code></pre>
","9209546","9209546","2018-06-01 16:16:14","2","1377","jpp","2018-01-12 14:47:22","109049","18235","7890","3496","50646663","","2018-06-01 15:23:59","-1","58","<p>I must modify the function gp so it will handle + and - grades by adding or subtracting 0.3 points. For example, a B + is worth 3.3 points, and a C- is 1.7 points. </p>

<p>Example. </p>

<pre><code>&gt;&gt;&gt; gp('A-')
3.7

&gt;&gt;&gt;gp('B+')
3.3
</code></pre>

<p>The suggestion is I could just add a bunch of elif clauses to test each grade separately, but a similar design is to use a call to s.startswith to figure out the value of the letter grade, then use s.endswith to see if you should add or subtract 0.3 points.</p>

<p>So far this is what I have.</p>

<pre><code>def gp(s):
       A = 4
       return A
       B = 3
       return B
       C = 2
       return C
       D = 1
       return D
       F = 0
       return f
</code></pre>
","9862492","1639625","2018-06-01 15:25:58","Modifying a function to return different values","<python><function><variables><return>","5","4","752"
"50646896","2018-06-01 15:37:26","1","","<p><code>on_command_error</code> must be a coroutine, and you must register it as an event with your <code>bot</code></p>

<pre><code>@bot.event
async def on_command_error(error, ctx):
    if isinstance(error, commands.NoPrivateMessage):
        await bot.send_message(ctx.message.author, 'This command cannot be used in private messages.')
    elif isinstance(error, commands.DisabledCommand):
        await bot.send_message(ctx.message.author, 'Sorry. This command is disabled and cannot be used.')
    elif isinstance(error, commands.CheckFailure):
        await bot.send_message(ctx.message.author, 'Sorry. You dont have permission to use this command.')
    elif isinstance(error, commands.MissingRequiredArgument):
        command = ctx.message.content.split()[1]
        await bot.send_message(ctx.message.channel, ""Missing an argument: "" + command)
    elif isinstance(error, commands.CommandNotFound):
        await bot.send_message(ctx.message.channel, codify(""I don't know that command""))
</code></pre>
","6779307","6622817","2018-06-12 02:51:50","1","1014","Patrick Haugh","2016-08-31 14:38:46","36209","4654","2715","1256","50646101","50646896","2018-06-01 14:52:07","1","261","<p>This is my code and I added error handling, but it's not working. I'm getting an error in bash when the command is executed by a non-admin. Below command if I type <code>?hello</code> with admin role it works, but when a non-admin types that commands, they receive <code>You don't have permission to use this command.</code> </p>

<pre><code>token = ""xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx""
prefix = ""?""

import discord
from discord.ext import commands

bot = commands.Bot(command_prefix=prefix)
bot.remove_command(""help"")

@bot.event
async def on_ready():
    print('Logged in as')
    print(bot.user.name)
    print(bot.user.id)
    print('------')

@bot.command(pass_context=True)
async def on_command_error(self, exception, ctx):
    if isinstance(exception, CheckFailure):
        print(""{0} does not have permission to run `{1}`"".format(ctx.message.author, ctx.command.name))
    elif isinstance(exception, CommandNotFound):
        # This is handled in CustomCommands
        pass
    else:
        print(exception)
        await self.on_error(""on_command_error"", exception, ctx) 

@bot.command(pass_context=True)
@commands.has_any_role(""Admin"", ""Moderator"")
async def hello(ctx):
    msg = 'Hello... {0.author.mention}'.format(ctx.message)
    await bot.say(msg)

bot.run(token)
</code></pre>
","9783164","2773979","2018-06-01 15:34:46","Python Bot error message","<python><python-3.x><discord><discord.py>","1","5","1299"
"50646915","2018-06-01 15:38:15","1","","<p>My suggestion would be to use a dictionary for how many points each specific grade would get. Using a dictionary is somewhat like the Python equivalent for a <code>switch</code> statement and is generally consider more idiomatic than a long <code>if/elif/else</code> chain. (My answer is assuming you're using the standard 4.0 GPA calculation scale. If not, this can obviously be changed to suite your needs.)</p>

<pre><code>grade_to_points = {
    'A+':   4.0,
    'A':    4.0,
    'A-':   3.7,
    'B+':   3.3,
    'B':    3.0,
    ...
}

def gp(grade):
    points = grade_to_points.get(grade)
    if points is None:
        raise ValueError('{:r} is not a valid grade!'.format(grade))
</code></pre>
","6640099","6640099","2018-06-01 16:14:10","0","706","Christian Dean","2016-07-26 13:37:17","16589","2594","3508","1289","50646663","","2018-06-01 15:23:59","-1","58","<p>I must modify the function gp so it will handle + and - grades by adding or subtracting 0.3 points. For example, a B + is worth 3.3 points, and a C- is 1.7 points. </p>

<p>Example. </p>

<pre><code>&gt;&gt;&gt; gp('A-')
3.7

&gt;&gt;&gt;gp('B+')
3.3
</code></pre>

<p>The suggestion is I could just add a bunch of elif clauses to test each grade separately, but a similar design is to use a call to s.startswith to figure out the value of the letter grade, then use s.endswith to see if you should add or subtract 0.3 points.</p>

<p>So far this is what I have.</p>

<pre><code>def gp(s):
       A = 4
       return A
       B = 3
       return B
       C = 2
       return C
       D = 1
       return D
       F = 0
       return f
</code></pre>
","9862492","1639625","2018-06-01 15:25:58","Modifying a function to return different values","<python><function><variables><return>","5","4","752"
"50646951","2018-06-01 15:40:21","1","","<p>You can use <code>filter</code>:</p>

<pre><code>list1 = [{'numeric_id': 1, 'ref': 'link1'}, {'numeric_id': 2, 'ref': 'link2'}, {'numeric_id': 3, 'ref': 'link3'}, {'numeric_id': 4, 'ref': 'link4'}, {'numeric_id': 5, 'ref': 'link5'}]
list2 = [{'numeric_id': 1, 'ref': 'link1'}, {'numeric_id': 2, 'ref': 'link2'}, {'numeric_id': 4, 'ref': 'link4'}, {'numeric_id': 5, 'ref': 'link5'}]
new_list1 = list(filter(lambda x:any(c['numeric_id'] == x['numeric_id'] for c in list2), list1))
new_list2 = list(filter(lambda x:any(c['numeric_id'] == x['numeric_id'] for c in list1), list2))
</code></pre>

<p>Output:</p>

<pre><code>[{'numeric_id': 1, 'ref': 'link1'}, {'numeric_id': 2, 'ref': 'link2'}, {'numeric_id': 4, 'ref': 'link4'}, {'numeric_id': 5, 'ref': 'link5'}]
[{'numeric_id': 1, 'ref': 'link1'}, {'numeric_id': 2, 'ref': 'link2'}, {'numeric_id': 4, 'ref': 'link4'}, {'numeric_id': 5, 'ref': 'link5'}]
</code></pre>
","7326738","7326738","2018-06-01 15:47:02","3","917","Ajax1234","2016-12-21 16:39:57","49079","3709","2930","360","50646910","","2018-06-01 15:38:02","0","34","<p>After certain manipulations I get two lists of dictionaries sorted by numeric_id key.
lets'say I have </p>

<pre><code>list1 = [
        {'ref': 'link1', 'numeric_id': 1},
        {'ref': 'link2', 'numeric_id': 2},
        {'ref': 'link3', 'numeric_id': 3},
        {'ref': 'link4', 'numeric_id': 4},
        {'ref': 'link5', 'numeric_id': 5}
]

list2 = [
        {'ref': 'different_link1', 'numeric_id': 1},
        {'ref': 'different_link2', 'numeric_id': 2},
        {'ref': 'different_link4', 'numeric_id': 4},
        {'ref': 'different_link5', 'numeric_id': 5}
]
</code></pre>

<p>And in the second list the value 3 in ""numeric_id"" key is not present while the first list contains such key-value pair. Then I have to remove this dictionary from the list 1 as I need to have only matching pairs based on numeric_id in both lists.
Also can be the opposite case, when the value is not present in the first list, while it is in the second one. I cannot know what will be the case beforehand.</p>

<p>The result should be two list without any unpaired elements. Because the lists contain dictionaries with different links, the only connection between them is the value of numeric_id key</p>

<p>The task seemed to be quite easy but I'm already quite lost.
Could you please help?
Found a lot of seamingly similar questions but couldn't find the proper solution for my case.</p>

<p>Thanks in advance!</p>
","7143792","7143792","2018-06-01 15:43:21","Find and remove the dict from one of two lists of dicts if there is a key-value pair not present in at least one of the lists","<python><list><dictionary>","2","0","1408"
"50646968","2018-06-01 15:41:43","7","","<p>another way to check is type</p>

<p>type(object) </p>

<p>that return the type of the object like </p>

<p>pyspark.sql.dataframe.DataFrame</p>
","5682709","","","0","147","Kno X","2015-12-15 15:16:50","71","0","0","0","36731365","36732761","2016-04-19 23:44:54","4","7431","<p>I'm using python, and this is Spark Rdd/dataframes.</p>

<p>I tried <code>isinstance(thing, RDD)</code> but RDD wasn't recognized.</p>

<p>The reason I need to do this:</p>

<p>I'm writing a function where both RDD and dataframes could be passed in, so I'll need to do input.rdd to get the underlying rdd if a dataframe is passed in.</p>
","3808877","10239789","2019-01-06 13:02:09","Check Type: How to check if something is a RDD or a dataframe?","<python><apache-spark><dataframe><apache-spark-sql><rdd>","2","0","341"
"50646973","2018-06-01 15:42:07","0","","<p>The <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.fill.html"" rel=""nofollow noreferrer""><code>fill</code></a> method of an array doesn't return an array, it returns <code>None</code>. </p>
","1388292","1388292","2018-06-01 15:47:38","0","221","Jacques Gaudin","2012-05-10 23:01:38","8213","656","1988","169","50646523","50646691","2018-06-01 15:15:42","1","67","<p>I'm trying to create a shaded rectangle based on two vertical coordinates (the length of the rectangle should span the whole x axis).
I've tried to use <code>fill_between</code> with the following (p1 &amp; p2 are the data coordinates):</p>

<pre><code>f, ax1 = plt.subplots()   
x = np.linspace(200,300,2000)
y = x**(2)
y1 = x
p1 = 4700
p2 = 7700

ax1.plot(x, y, lw=2)
ax1.set_xlabel('Temperature $[K]$', fontweight='bold')
ax1.set_ylabel('Pressure $[mb]$', fontweight='bold')
ax1.fill_between(x, y1.fill(p1), y1.fill(p2))
</code></pre>

<p>But i'm getting the following error:</p>

<pre><code>TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
</code></pre>

<p>What is wrong? Any better way to do this?</p>

<p>Thank you!!</p>
","3952424","","","Shade area between 2 y coordinates in matplotlib","<python><matplotlib>","4","1","850"
"50647039","2018-06-01 15:45:53","1","","<p>From the documentation of <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html"" rel=""nofollow noreferrer""><code>np.where</code></a></p>

<blockquote>
  <p>If only condition is given, return the tuple condition.nonzero(), the indices where condition is True</p>
</blockquote>

<p>So we look into the documentation of <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html"" rel=""nofollow noreferrer"">'np.nonzero'</a> </p>

<blockquote>
  <p><strong>Returns a tuple of arrays, one for each dimension of a</strong>, containing the indices of the non-zero elements in that dimension. The values in a are always tested and returned in row-major, C-style order. The corresponding non-zero values can be obtained with:</p>
</blockquote>

<p>So how can this be <em>useful</em> for <code>np.where/np.nonzero</code> return a tuple of arrays? I think it is related to <a href=""https://docs.scipy.org/doc/numpy/user/basics.indexing.html#indexing-multi-dimensional-arrays"" rel=""nofollow noreferrer""><strong>indexing multi-dimensional arrays</strong></a>. </p>

<p>From the example of the <a href=""https://docs.scipy.org/doc/numpy/user/basics.indexing.html#indexing-multi-dimensional-arrays"" rel=""nofollow noreferrer"">documentation</a> if we have </p>

<pre><code>y = np.arange(35).reshape(5,7)
</code></pre>

<p>We can do </p>

<pre><code>y[np.array([0,2,4]), np.array([0,1,2])]
</code></pre>

<p>to select <code>y[0, 0]</code>, <code>y[2, 1]</code>, <code>y[4, 2]</code>.</p>

<blockquote>
  <p>In this case, if the index arrays have a matching shape, and there is an index array for each dimension of the array being indexed, the resultant array has the same shape as the index arrays, and the values correspond to the index set for each position in the index arrays. In this example, the first index value is 0 for both index arrays, and thus the first value of the resultant array is y[0,0]. The next value is y[2,1], and the last is y[4,2]. </p>
</blockquote>

<p>Hope that indexing multi-dimensional arrays would justify that <code>np.nonzero/np.where</code> return a tuple of arrays such that it can be used to select elements later on. </p>
","4829258","","","2","2190","Tai","2015-04-24 15:18:28","5199","574","766","17","50646102","50646154","2018-06-01 14:52:09","8","3257","<p>When I run this code:</p>

<pre><code>import numpy as np
a = np.array([1, 2, 3, 4, 5, 6])
print(np.where(a &gt; 2))
</code></pre>

<p>it would be natural to get an array of indices where <code>a &gt; 2</code>, i.e. <code>[2, 3, 4, 5]</code>, but instead we get:</p>

<pre><code>(array([2, 3, 4, 5], dtype=int64),)
</code></pre>

<p>i.e. a tuple with empty second member.</p>

<p>Then, to get the the ""natural"" answer of <code>numpy.where</code>, we have to do:</p>

<pre><code>np.where(a &gt; 2)[0]
</code></pre>

<p><strong>What's the point in this tuple? In which situation is it useful?</strong></p>

<p>Note: I'm speaking here only about the use case <code>numpy.where(cond)</code> and not <code>numpy.where(cond, x, y)</code> that also exists (see documentation).</p>
","1422096","9209546","2018-06-01 16:59:52","What is the purpose of numpy.where returning a tuple?","<python><arrays><numpy>","3","5","776"
"50647102","2018-06-01 15:49:48","6","","<p>The <code>crc32</code> function outputs an unsigned 32-bit number, and the code tests if the CRC value is lower than the test_ratio times the maximum 32-bit number.</p>

<p>The <code>&amp; 0xffffffff</code> mask is there only to <a href=""https://docs.python.org/3/library/zlib.html#zlib.crc32"" rel=""noreferrer"">ensure compatibility with Python 2 and 3</a>. In Python 2 the same function could return a <em>signed</em> integer, in a range from -(2^31) to (2^31) - 1, masking this with the <code>0xffffffff</code> mask normalises the value to a signed.</p>

<p>So basically, either version turns the identifier into an integer, and the hash is used to make that integer reasonably uniformly distributed in a range; for the MD5 hash that's the last byte making the value fall between 0 and 255, for the CRC32 checksum the value lies between 0 and (2^32)-1. This integer is then compared to the full range; if it falls below the <code>test_ratio * maximum</code> cut-off point it is considered selected.</p>

<p>You could also use a random function, but then you'd get a different subset of your input each time you picked a sample; by hashing the identifier you get to produce a <em>consistent</em> subset. The difference between the two methods is that they'll produce a different subset, so you could use both together to pick multiple, independent subsets from the same input.</p>

<p>Compare:</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from zlib import crc32
&gt;&gt;&gt; from hashlib import md5
&gt;&gt;&gt; import random
&gt;&gt;&gt; identifier = np.int64(random.randrange(2**63))
&gt;&gt;&gt; md5(identifier).digest()[-1]
243
&gt;&gt;&gt; md5(identifier).digest()[-1] / 256  # as a ratio of the full range
0.94921875
&gt;&gt;&gt; crc32(identifier)
4276259108
&gt;&gt;&gt; crc32(identifier) / (2 ** 32)   # ratio again
0.9956441605463624
&gt;&gt;&gt; identifier = np.int64(random.randrange(2**63))  # different id to compare
&gt;&gt;&gt; md5(identifier).digest()[-1] / 256  # as a ratio of the full range
0.83203125
&gt;&gt;&gt; crc32(identifier) / (2 ** 32)   # ratio again
0.10733163682743907
</code></pre>

<p>So the two different methods produce different outputs, but as long as the CRC32 and MD5 hashes produce reasonably <em>uniformly distributed</em> hash values, then either will give you a fair 20% sampling rate.</p>
","100297","100297","2018-06-01 16:07:20","0","2352","Martijn Pieters","2009-05-03 14:53:57","770256","252083","5762","19510","50646890","","2018-06-01 15:37:04","2","417","<p>I would like to ask you about explanation of the following short function in Python..</p>

<pre><code>from zlib import crc32

def test_set_check(identifier, test_ratio):
    return crc32(np.int64(identifier)) &amp; 0xffffffff &lt; test_ratio * 2**32
</code></pre>

<p>The above-mentioned function should be the same as the following function:</p>

<pre><code>import hashlib

def test_set_check(identifier, test_ratio, hash=hashlib.md5):
    return hash(np.int64(identifier)).digest()[-1] &lt; 256 * test_ratio
</code></pre>

<p>Both functions should be used for data sampling (select some rows in a table). For example, if <code>test_ratio</code> is 0.2 then it means that I want to sample 20% data, the value is lower or equal to 51 (~20% of 256). I understand how the second function works but I don't understand the first one. Could you please explain to me the first function? I don't understand the following part: <code>crc32(np.int64(identifier)) &amp; 0xffffffff &lt; test_ratio * 2**32</code></p>
","9516346","100297","2018-06-01 16:05:35","How does the CRC32 function work when using sampling data?","<python><hash><hex>","1","0","1009"
"50647107","2018-06-01 15:50:02","1","","<p>I am not very familiar reading Matlab files, but you already got an array. If you want to put each string in a variable what you can do is:</p>

<pre><code>string1 = featurenames[0][0]
string2 = featurenames[1][0]
</code></pre>

<p>Please, if this is not the answer you are looking, could you be more specific on your question? thanks!</p>
","2992789","","","0","343","daniel","2013-11-14 15:32:50","547","91","103","4","50646592","50647297","2018-06-01 15:19:53","2","230","<p>I have a <code>.mat</code> file with variables containing numbers and strings. When I load it and get the variable containing strings I don't understand how to actually get the string out of it:</p>

<pre><code>data = scipy.io.loadmat(pathName)
featurenames=data['featurenames']
print(featurenames[0:2,0])
</code></pre>

<p>As an output I get:</p>

<pre><code>[array(['Intensity_SubsBlue_Nuclei_1_IntegratedIntensity'], dtype='&lt;U47')
 array(['Intensity_SubsBlue_Nuclei_2_MeanIntensity'], dtype='&lt;U41')]
</code></pre>

<p>How do I get to this <code>array</code>? I want to have just strings.</p>

<p>Thank you!</p>
","7635884","2971485","2018-06-01 15:46:44","How to get string after loading .mat file to Python","<python><matlab><scipy>","3","0","623"
"50647126","2018-06-01 15:51:16","1","","<p>For efficiency, you should convert your series to <code>datetime</code> and then mask for non-null values:</p>

<pre><code>df['date'] = pd.to_datetime(df['date'], errors='coerce')

df = df[df['date'].notnull()]
</code></pre>
","9209546","","","0","228","jpp","2018-01-12 14:47:22","109049","18235","7890","3496","50646640","","2018-06-01 15:22:23","2","252","<p>Basically I am trying to delete rows of a pandas dataframe where values in a certain column are not instances of datetime. I have tried:</p>

<pre><code>df = df[df[‘date’] == isinstance(datetime)]
</code></pre>

<p>I know isinstance takes two arguments (I am missing the value to be checked) but I’m not sure what to put there. </p>
","7664441","9209546","2018-06-01 15:52:24","Deleting rows of pandas dataframe based on instance type","<python><pandas><datetime>","3","0","336"
"50647140","2018-06-01 15:52:16","0","","<p>What you appear to have is a list of scipy/numpy arrays. The problem you're experiencing is that while datatype doesn't strictly matter to Python, it definitely does to scipy and numpy, and I assume you're looking to get these to Python strings. The snippet below should do it (assuming your output is named <code>matFileOut</code>):</p>

<pre><code>## for the zeroth array in your list, convert the scipy/numpy array to a Python list, then take the zeroth (only) element
matFileOut[0].tolist()[0]
</code></pre>
","5438550","","","0","515","jshrimp29","2015-10-12 22:20:35","396","11","136","4","50646592","50647297","2018-06-01 15:19:53","2","230","<p>I have a <code>.mat</code> file with variables containing numbers and strings. When I load it and get the variable containing strings I don't understand how to actually get the string out of it:</p>

<pre><code>data = scipy.io.loadmat(pathName)
featurenames=data['featurenames']
print(featurenames[0:2,0])
</code></pre>

<p>As an output I get:</p>

<pre><code>[array(['Intensity_SubsBlue_Nuclei_1_IntegratedIntensity'], dtype='&lt;U47')
 array(['Intensity_SubsBlue_Nuclei_2_MeanIntensity'], dtype='&lt;U41')]
</code></pre>

<p>How do I get to this <code>array</code>? I want to have just strings.</p>

<p>Thank you!</p>
","7635884","2971485","2018-06-01 15:46:44","How to get string after loading .mat file to Python","<python><matlab><scipy>","3","0","623"
"50647143","2018-06-01 15:52:33","0","","<p>With small change to the <a href=""https://stackoverflow.com/a/46999157/5916727"">similar answer</a>, you can try following:</p>

<pre><code>df = df[df['date'].apply(isinstance, args=(datetime,))]
</code></pre>
","5916727","","","0","212","student","2016-02-12 04:04:50","9736","2093","384","54","50646640","","2018-06-01 15:22:23","2","252","<p>Basically I am trying to delete rows of a pandas dataframe where values in a certain column are not instances of datetime. I have tried:</p>

<pre><code>df = df[df[‘date’] == isinstance(datetime)]
</code></pre>

<p>I know isinstance takes two arguments (I am missing the value to be checked) but I’m not sure what to put there. </p>
","7664441","9209546","2018-06-01 15:52:24","Deleting rows of pandas dataframe based on instance type","<python><pandas><datetime>","3","0","336"
"50647158","2018-06-01 15:53:25","1","","<p>Instead of</p>

<pre><code>_h = hashlib.md5(_userPassword.encode())
</code></pre>

<p>you might want</p>

<pre><code>_h = hashlib.md5(_userPassword.encode()).hexdigest()
</code></pre>

<p>And if you're storing the MD5 hash of the password, the</p>

<pre><code>password = md5(%s)
</code></pre>

<p>part the query isn't going to match it if you bind <code>_h</code>.</p>
","62288","","","3","372","Dave W. Smith","2009-02-04 06:23:17","17838","1787","516","21","50647042","","2018-06-01 15:46:05","1","1415","<p>i have this code which i want to convert the password into md5</p>

<pre><code>class UserLogin(Resource):
    def post(self):

            # Parse the arguments

            parser = reqparse.RequestParser()
            parser.add_argument('username')
            parser.add_argument('password')
            args = parser.parse_args()

            _user = args['username']
            _userPassword = args['password']
            _h = hashlib.md5(_userPassword.encode())
            conn = mysql.connect()
            cursor = conn.cursor()
            cursor.execute('''select * from user where username = %s &amp;&amp; password = %s''', (_user, _h))
            data = cursor.fetchall()

            return jsonify(data)
</code></pre>

<p>but the error says: h = hashlib.md5(_userPassword.encode())
AttributeError: 'NoneType' object has no attribute 'encode'</p>

<p>and when i remove the encode() from hashlib, the error return is:_h = hashlib.md5(_userPassword)
TypeError: object supporting the buffer API required</p>

<p>please help me. im using python3.6</p>
","9876074","9876074","2018-06-01 16:07:47","TypeError: object supporting the buffer API required in flask api","<python><python-3.x><api><flask>","1","0","1069"
"50647177","2018-06-01 15:54:55","0","","<p>I'm not sure how much this will help, but for me, I was trying to issue a POST request and you need a new HTTP Connection in order to do it. You can't use the same connection for multiple requests. I keep on getting the same error: httplib.BadStatusLine: ''. I believe the documentation outlines this, I just overlooked it. </p>
","5451370","","","2","332","John Conrad Geenty","2015-10-15 20:27:24","11","10","0","0","27619258","27620850","2014-12-23 11:16:13","16","35431","<p>As always, I frequently have issues, and I have thoroughly searched for an answer to the current one but find myself at a loss. Here are some of the places I have searched:
- <a href=""https://stackoverflow.com/questions/10708828/how-to-fix-httplib-badstatusline-exception"">How to fix httplib.BadStatusLine exception?</a>
- <a href=""https://stackoverflow.com/questions/6757252/python-httplib2-handling-exceptions"">Python httplib2 Handling Exceptions</a>
- <a href=""https://stackoverflow.com/questions/15968031/python-http-status-code"">python http status code</a></p>

<p>My issue is the following. I have created a spider and want to crawl different urls. When I crawl each url independently everything works fine. However, when I try to crawl both I get the following error: <code>httplib.BadStatusLine: ''</code></p>

<p>I have followed some advice that I read (see links mentioned above) and can print the response.status for each request works, but the response.url does not print and the error is thrown. (I only print both statements to try to identify the source of the error). </p>

<p>I hope that this is clear. </p>

<p>I am using scrapy and selenium</p>

<pre><code>class PeoplePage(Spider):
    name = ""peopleProfile""
    allowed_domains = [""blah.com""]
    handle_httpstatus_list = [200, 404]
    start_urls = [
        ""url1"",
        ""url2""
    ]

    def __init__(self):
        self.driver = webdriver.Firefox()

    def parse(self, response):
        print response.status
        print '???????????????????????????????????'
        if response.status == 200:
            self.driver.implicitly_wait(5)
            self.driver.get(response.url)
            print response.url
            print '!!!!!!!!!!!!!!!!!!!!'

            # DO STUFF

        self.driver.close()
</code></pre>
","4287137","-1","2017-05-23 12:02:04","httplib.BadStatusLine: ''","<python><selenium><scrapy>","4","0","1803"
"50647188","2018-06-01 15:55:53","0","","<p>Maybe you can try this :<br>
1. extract <code>numeric_id</code> from <code>list2</code><br>
2. append each element of <code>list1</code> if its <code>numeric_id</code> is in <code>list2</code></p>

<pre><code>list2_numeric_id = [element[""numeric_id""] for element in list2]
[element for element in list1 if element[""numeric_id""] in list2_numeric_id]
&gt;&gt; [{'numeric_id': 1, 'ref': 'link1'}, {'numeric_id': 2, 'ref': 'link2'}, {'numeric_id': 4, 'ref': 'link4'}, {'numeric_id': 5, 'ref': 'link5'}]
</code></pre>
","8187340","","","0","516","J. Doe","2017-06-20 08:29:22","1545","144","476","4","50646910","","2018-06-01 15:38:02","0","34","<p>After certain manipulations I get two lists of dictionaries sorted by numeric_id key.
lets'say I have </p>

<pre><code>list1 = [
        {'ref': 'link1', 'numeric_id': 1},
        {'ref': 'link2', 'numeric_id': 2},
        {'ref': 'link3', 'numeric_id': 3},
        {'ref': 'link4', 'numeric_id': 4},
        {'ref': 'link5', 'numeric_id': 5}
]

list2 = [
        {'ref': 'different_link1', 'numeric_id': 1},
        {'ref': 'different_link2', 'numeric_id': 2},
        {'ref': 'different_link4', 'numeric_id': 4},
        {'ref': 'different_link5', 'numeric_id': 5}
]
</code></pre>

<p>And in the second list the value 3 in ""numeric_id"" key is not present while the first list contains such key-value pair. Then I have to remove this dictionary from the list 1 as I need to have only matching pairs based on numeric_id in both lists.
Also can be the opposite case, when the value is not present in the first list, while it is in the second one. I cannot know what will be the case beforehand.</p>

<p>The result should be two list without any unpaired elements. Because the lists contain dictionaries with different links, the only connection between them is the value of numeric_id key</p>

<p>The task seemed to be quite easy but I'm already quite lost.
Could you please help?
Found a lot of seamingly similar questions but couldn't find the proper solution for my case.</p>

<p>Thanks in advance!</p>
","7143792","7143792","2018-06-01 15:43:21","Find and remove the dict from one of two lists of dicts if there is a key-value pair not present in at least one of the lists","<python><list><dictionary>","2","0","1408"
"50647203","2018-06-01 15:56:50","6","","<p>No, that is not possible with slicing.  Slicing only supports start, stop, and step - there is no way to represent stepping with ""groups"" of size larger than 1.</p>
","674039","","","0","168","wim","2011-03-23 23:40:27","187587","12233","9064","5087","50647167","50647227","2018-06-01 15:54:16","26","1388","<p>I am trying to get <em>m</em> values while stepping through every <em>n</em> elements of an array. For example, for <em>m</em> = 2 and <em>n</em> = 5, and given</p>

<pre><code>a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
</code></pre>

<p>I want to retrieve</p>

<pre><code>b = [1, 2, 6, 7]
</code></pre>

<p>Is there a way to do this using slicing? I can do this using a nested list comprehension, but I was wondering if there was a way to do this using the indices only. For reference, the list comprehension way is:</p>

<pre><code> b = [k for j in [a[i:i+2] for i in range(0,len(a),5)] for k in j]
</code></pre>
","9877290","9209546","2018-06-08 18:00:57","Stepping with multiple values while slicing an array in Python","<python><arrays><list>","7","2","611"
"50647227","2018-06-01 15:58:53","24","","<p>I agree with wim that you can't do it with just slicing. But you can do it with just one list comprehension:</p>

<pre><code>&gt;&gt;&gt; [x for i,x in enumerate(a) if i%n &lt; m]
[1, 2, 6, 7]
</code></pre>
","953482","","","1","210","Kevin","2011-09-19 20:18:13","63460","6576","2598","748","50647167","50647227","2018-06-01 15:54:16","26","1388","<p>I am trying to get <em>m</em> values while stepping through every <em>n</em> elements of an array. For example, for <em>m</em> = 2 and <em>n</em> = 5, and given</p>

<pre><code>a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
</code></pre>

<p>I want to retrieve</p>

<pre><code>b = [1, 2, 6, 7]
</code></pre>

<p>Is there a way to do this using slicing? I can do this using a nested list comprehension, but I was wondering if there was a way to do this using the indices only. For reference, the list comprehension way is:</p>

<pre><code> b = [k for j in [a[i:i+2] for i in range(0,len(a),5)] for k in j]
</code></pre>
","9877290","9209546","2018-06-08 18:00:57","Stepping with multiple values while slicing an array in Python","<python><arrays><list>","7","2","611"
"50647228","2018-06-01 15:58:53","0","","<p>There are certain limitations when using AWS lambda.</p>

<p>1) The total size of your uncompressed code and dependencies should be less than 250MB.</p>

<p>2) The size of your zipped code and dependencies should be less than 75MB.</p>

<p>3) The total fixed size of all function packages in a region should not exceed 75GB.</p>

<p>If you are exceeding the limit, try finding smaller libraries with less dependencies or breaking your functionality in to multiple micro services rather than building code which does all the work for you. This way you don't have to include every library in each function. Hope this helps. </p>
","5940691","","","0","630","Keerthikanth Chowdary","2016-02-17 13:25:54","149","69","18","0","48241838","","2018-01-13 16:28:38","1","607","<p>I am doing an example of a Simple Linear Regression in Python and I want to make use of Lambda functions to make it work on AWS so that I could interface it with Alexa. The problem is my Python package is 114 MB. I have tried to separate the package and the code so that I have two lambda functions but to no avail. I have tried every possible way on the internet.  </p>

<p>Is there any way I could upload the packages on S3 and read it from there like how we read csv's from S3 using boto3 client?</p>
","2524115","472495","2018-01-13 21:56:09","Working around AWS Lambda space limitations","<python><amazon-web-services><amazon-s3><aws-lambda>","2","5","507"
"50647231","2018-06-01 15:59:04","0","","<p>This works for me:</p>

<pre><code>from ctypes import *
def MessageBox(title, text, style):
    sty = int(style) + 4096
    return windll.user32.MessageBoxW(0, text, title, sty) #MB_SYSTEMMODAL==4096
    ## Button Styles:
    ###  0:OK  --  1:OK|Cancel -- 2:Abort|Retry|Ignore -- 3:Yes|No|Cancel -- 4:Yes|No -- 5:Retry|No -- 6:Cancel|Try Again|Continue
    ## To also change icon, add these values to previous number
    ### 16 Stop-sign  ### 32 Question-mark  ### 48 Exclamation-point  ### 64 Information-sign ('i' in a circle)
</code></pre>

<h2>Usage:</h2>

<pre><code>MessageBox('Here is my Title', 'Message to be displayed', 64)
</code></pre>
","1447509","","","0","651","cssyphus","2012-06-10 15:41:15","25010","1825","4244","85","11765324","11765414","2012-08-01 18:28:42","3","5558","<p>I've recently started learning Python and wrote a little script that informs me when a certain website changes content. I then added it as a scheduled task to Windows so it can run every 10 minutes. I'd like to be informed of the website changing right away so I added a win32ui MessageBox that pops up if the script detects that the website has changed. Here's the little code snippet I'm using for the MessageBox (imaginative text, I know):</p>

<pre><code>win32ui.MessageBox(""The website has changed."", ""Website Change"", 0)
</code></pre>

<p>My issue is this, I spend most of my time using remote desktop so when the MessageBox does pop up it sits behind the remote desktop session, is there any way to force the MessageBox to appear on top of it?</p>

<p>On a similar note when the script runs the command line opens up very briefly over the remote desktop session which I don't want, is there any way of stopping this behaviour?</p>

<p>I'm happy with Windows specific solutions as I'm aware it might mean dealing with the windowing manager or possibly an alternative way to inform me rather than using a MessageBox.</p>
","902766","902766","2012-08-01 18:34:32","Have Win32 MessageBox appear over other programs","<python><windows><user-interface>","6","0","1129"
"50647247","2018-06-01 15:59:49","5","","<p>In short, no, you cannot. But you can use <code>itertools</code> to remove the need for intermediary lists:</p>

<pre><code>from itertools import chain, islice

res = list(chain.from_iterable(islice(a, i, i+2) for i in range(0, len(a), 5)))

print(res)

[1, 2, 6, 7]
</code></pre>

<p>Borrowing @Kevin's logic, if you want a vectorised solution to avoid a <code>for</code> loop, you can use 3rd party library <code>numpy</code>:</p>

<pre><code>import numpy as np

m, n = 2, 5
a = np.array(a)  # convert to numpy array
res = a[np.where(np.arange(a.shape[0]) % n &lt; m)]
</code></pre>
","9209546","9209546","2018-06-01 16:05:01","0","588","jpp","2018-01-12 14:47:22","109049","18235","7890","3496","50647167","50647227","2018-06-01 15:54:16","26","1388","<p>I am trying to get <em>m</em> values while stepping through every <em>n</em> elements of an array. For example, for <em>m</em> = 2 and <em>n</em> = 5, and given</p>

<pre><code>a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
</code></pre>

<p>I want to retrieve</p>

<pre><code>b = [1, 2, 6, 7]
</code></pre>

<p>Is there a way to do this using slicing? I can do this using a nested list comprehension, but I was wondering if there was a way to do this using the indices only. For reference, the list comprehension way is:</p>

<pre><code> b = [k for j in [a[i:i+2] for i in range(0,len(a),5)] for k in j]
</code></pre>
","9877290","9209546","2018-06-08 18:00:57","Stepping with multiple values while slicing an array in Python","<python><arrays><list>","7","2","611"
"50647263","2018-06-01 16:00:39","0","","<p>If you need to do this in python I suggest looking at this answer:
<a href=""https://stackoverflow.com/questions/51520/how-to-get-an-absolute-file-path-in-python"">How to get an absolute file path in Python</a></p>

<p>It is always better to find an absolute path, If using bash:</p>

<pre><code>readlink -f filename 
</code></pre>
","9722265","","","0","333","pypalms","2018-04-30 17:02:13","319","24","16","8","50647131","","2018-06-01 15:51:46","1","228","<p>I am using a find statement to execute python scripts which are in sub folders of all sub folders of the current directory</p>

<p>The find statement is <code>find . -name \process.py -type f -exec python3 {} \;</code> </p>

<p>The problem I am encountering is that the script is using relative paths e.g <code>..\data</code> for retrieving other resources. These relative paths are resolved as required when the script is executed individually by running it from its directory but when running the script from a parent directory two levels up using the <code>find</code> command the path resolves relative to that parent directory causing errors</p>
","2319414","","","Python - Resolve to an absolute path relative to a script file","<python>","2","1","654"
"50647274","2018-06-01 16:01:50","0","","<p>May be you need to create a string format of your JSON payload data.
Try this: </p>

<pre><code>payload=json.dumps({
    'ping': '1.967',
    'download': '570.01',
    'upload': '697.85',
    'timestamp': '1527845073'
})
</code></pre>
","6487676","","","1","238","Manoj Acharya","2016-06-20 06:21:13","547","66","35","6","50641843","50647274","2018-06-01 10:51:59","0","102","<p>I'm posting a simple JSON structure to Amazon Gateway API from Python. </p>

<p>Here's the JSON structure:</p>

<pre><code>payload = {
'ping': '1.967',
'download': '570.01',
'upload': '697.85',
'timestamp': '1527845073'
}
</code></pre>

<p>And here's the python code:</p>

<pre><code>headers = {'content-type':'application/json'}
r = requests.post(""https:xxx.execute-api.us-east-1.amazonaws.com/beta/device"", data = payload ,headers=headers)
print(r.status_code, r.reason)
print(r.text)
</code></pre>

<ul>
<li>r.status_code is returned as <em>'200'</em>  </li>
<li>r.reason is returned as <em>'Ok'</em></li>
<li>r.text is returned as <em>{""__type"":""com.amazon.coral.validate#ValidationException"",""message"":""One or more parameter values were invalid: An AttributeValue may not contain an empty string""}</em></li>
</ul>

<p>The parameters are clearly not empty so I'm not sure why I'm getting that error. I have a Body Mapping Template in the Integration Request that looks like this:</p>

<pre><code>{ 
""TableName"": ""device"",
""Item"": {
""id"": {
        ""S"": ""$context.requestId""
        },
""ping"": {
        ""S"": ""$input.path('$.ping')""
        },
""download"": {
        ""S"": ""$input.path('$.download')""
    },
""upload"": {
        ""S"": ""$input.path('$.upload')""
    },
""timestamp"": {
       ""S"": ""$input.path('$.timestamp')""
}
}
}
</code></pre>

<p>And when I run the test from within AWS Gateway API Console, the data inputs into the DynamoDB just fine. However, it won't work from Python. Any idea what I could be doing wrong? I'm betting it's a silly mistake. Thank you.</p>
","1385759","","","POST to AWS Gateway API from Python into Dynamodb","<python><amazon-web-services><amazon-dynamodb>","1","2","1579"
"50647297","2018-06-01 16:03:32","0","","<p>It looks like <code>featurenames</code> is a <code>cell</code> in MATLAB.  All matrices and cells are 2d in that language. Cells can contain a mix of elements, size and type.  That's more like a <code>python</code> list than a numpy numeric array.  <code>loadmat</code> returns that as a 2d object dtype array - containing arrays.</p>

<p>You selected <code>featurenames[0:2, 0]</code>, which returns 2 of those cell elements as a 1d array.</p>

<p>I can recreate your array with:</p>

<pre><code>In [9]: arr = np.empty(2, dtype=object)
In [11]: arr[:] = [np.array(['Intensity_SubsBlue_Nuclei_1_IntegratedIntensity'],
    ...:  dtype='&lt;U47'),
    ...:  np.array(['Intensity_SubsBlue_Nuclei_2_MeanIntensity'], dtype='&lt;U41')]
    ...: 
    ...:  
In [12]: arr
Out[12]: 
array([array(['Intensity_SubsBlue_Nuclei_1_IntegratedIntensity'], dtype='&lt;U47'),
       array(['Intensity_SubsBlue_Nuclei_2_MeanIntensity'], dtype='&lt;U41')],
      dtype=object)
In [13]: print(arr)
[array(['Intensity_SubsBlue_Nuclei_1_IntegratedIntensity'], dtype='&lt;U47')
 array(['Intensity_SubsBlue_Nuclei_2_MeanIntensity'], dtype='&lt;U41')]
</code></pre>

<p>So you have to access the elements, and then the element within each of those:</p>

<pre><code>In [14]: arr[0][0]
Out[14]: 'Intensity_SubsBlue_Nuclei_1_IntegratedIntensity'
In [15]: [a.item() for a in arr]
Out[15]: 
['Intensity_SubsBlue_Nuclei_1_IntegratedIntensity',
 'Intensity_SubsBlue_Nuclei_2_MeanIntensity']
</code></pre>

<p>For a single element array, <code>[0]</code> or <code>item()</code> work equally well.</p>

<p>Or the outer elements can be joined into one array with <code>concatenate</code>.  Note the change in <code>dtype</code>:</p>

<pre><code>In [16]: np.concatenate(arr)
Out[16]: 
array(['Intensity_SubsBlue_Nuclei_1_IntegratedIntensity',
       'Intensity_SubsBlue_Nuclei_2_MeanIntensity'], dtype='&lt;U47')
In [17]: _[0]
Out[17]: 'Intensity_SubsBlue_Nuclei_1_IntegratedIntensity'
</code></pre>
","901925","901925","2018-06-01 22:06:47","1","1966","hpaulj","2011-08-19 06:44:39","130801","8991","3044","37","50646592","50647297","2018-06-01 15:19:53","2","230","<p>I have a <code>.mat</code> file with variables containing numbers and strings. When I load it and get the variable containing strings I don't understand how to actually get the string out of it:</p>

<pre><code>data = scipy.io.loadmat(pathName)
featurenames=data['featurenames']
print(featurenames[0:2,0])
</code></pre>

<p>As an output I get:</p>

<pre><code>[array(['Intensity_SubsBlue_Nuclei_1_IntegratedIntensity'], dtype='&lt;U47')
 array(['Intensity_SubsBlue_Nuclei_2_MeanIntensity'], dtype='&lt;U41')]
</code></pre>

<p>How do I get to this <code>array</code>? I want to have just strings.</p>

<p>Thank you!</p>
","7635884","2971485","2018-06-01 15:46:44","How to get string after loading .mat file to Python","<python><matlab><scipy>","3","0","623"
"50647302","2018-06-01 16:03:56","1","","<p>If the <a href=""http://www.pyqtgraph.org/documentation/_modules/pyqtgraph/opengl/items/GLMeshItem.html#GLMeshItem"" rel=""nofollow noreferrer"">code</a> of the <code>GLMeshItem</code> <code>paint()</code> method is revised:</p>

<pre><code>if self.colors is None:
    color = self.opts['color']
    if isinstance(color, QtGui.QColor):
        glColor4f(*fn.glColor(color))
    else:
        glColor4f(*color)
</code></pre>

<p>so the <code>setColor()</code> function expects a <code>QColor</code> or a tuple of 4 elements so you can use the following methods:</p>

<pre><code>bg.setColor((0., 1., 0., 1))
</code></pre>

<p><a href=""https://i.stack.imgur.com/wXpy2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wXpy2.png"" alt=""enter image description here""></a></p>

<pre><code>color = QtGui.QColor(""pink"")
bg.setColor(color)
</code></pre>

<p><a href=""https://i.stack.imgur.com/LLnCz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LLnCz.png"" alt=""enter image description here""></a></p>

<pre><code>color = QtGui.QColor(120, 14, 12)
bg.setColor(color)
</code></pre>

<p><a href=""https://i.stack.imgur.com/EOYzR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EOYzR.png"" alt=""enter image description here""></a></p>
","6622587","","","0","1273","eyllanesc","2016-07-21 23:29:11","114264","27275","2584","21000","50645841","50647302","2018-06-01 14:37:00","0","215","<p>My goal is to create stacked 3d bar plot, for that, I am trying to change color of GlBarGraphItem from PYQTgraph library.</p>

<p>Here is my code:</p>

<pre><code>from pyqtgraph.Qt import QtCore, QtGui
import pyqtgraph.opengl as gl
import numpy as np 

app = QtGui.QApplication([])
w = gl.GLViewWidget()
w.opts['distance'] = 100
w.showMaximized()
w.setWindowTitle('pyqtgraph example: GLViewWidget')

ax = gl.GLAxisItem()
ax.setSize(20,20,20)
w.addItem(ax)

pos = np.mgrid[0:1,0:1,0:1].reshape(3,1,1).transpose(1,2,0)

size = np.empty((1,1,3)) 
size[...,0:2] = 1
size[...,2] = 5

bg = gl.GLBarGraphItem(pos, size)
##bg.setColor(1., 1., 1., 1.)
w.addItem(bg)

if __name__ == '__main__':
    import sys
    if (sys.flags.interactive != 1) or not hasattr(QtCore, 'PYQT_VERSION'):
        QtGui.QApplication.instance().exec_()
</code></pre>

<p>I tried to use .setColor() method with no success, the object (GlBarGraphItem) itself does not have a method for setting color I believe.</p>

<p>Any hint how to progress?</p>
","7422287","","","How to change default color of GlBarGraphItem in PYQTgraph","<python><python-3.x><pyqtgraph>","1","0","1019"
"50647313","2018-06-01 16:05:00","0","","<p>I'm running Apache Airflow on an Amazon EC2-Instance and I was getting <code>""ImportError: No module named 'ldap3'</code>. I used these two sites <a href=""https://www.python-ldap.org/en/latest/installing.html"" rel=""nofollow noreferrer"">https://www.python-ldap.org/en/latest/installing.html</a> and <a href=""http://ldap3.readthedocs.io/installation.html"" rel=""nofollow noreferrer"">http://ldap3.readthedocs.io/installation.html</a> to run the commands <code>sudo python -m pip install python-ldap</code> and <code>sudo pip install ldap3</code> but my pip wasn't working for the last command so after some investigation I found out in my <code>/usr/bin/</code> directory I had <code>pip, pip-2.7, pip-3.6, and pip-python</code> so I changed the command to <code>pip-3.6 install ldap3</code> and then everything worked! Hope this helps someone.</p>
","3299397","","","0","848","Kyle Bridenstine","2014-02-11 23:18:29","1877","710","1792","8","44372329","","2017-06-05 15:27:15","1","6890","<p>I'm running Python 3.5 (on Windows) and I have installed <code>python-ldap</code> from <a href=""https://pypi.python.org/pypi/python-ldap/"" rel=""nofollow noreferrer"">https://pypi.python.org/pypi/python-ldap/</a> </p>

<p>I also tried using ldap3 but I keep getting an error saying </p>

<p><code>""ImportError: No module named 'ldap'</code> </p>

<p>I looked around and saw some people saying there's no python-ldap for 3.5 so I installed 2.6 still getting the same error. </p>

<p>Is there a way to import ldap and make it work for Python 3.5? </p>
","1321468","1321468","2017-06-05 15:35:53","ImportError: No module named 'ldap' Python 3.5","<python><ldap>","5","5","551"
"50647322","2018-06-01 16:05:49","3","","<p>As a Docker newbie, I had a problem that manifested itself in this way when I was following the tutorial for Docker at:</p>

<p><a href=""https://docs.docker.com/get-started/part2"" rel=""nofollow noreferrer"">https://docs.docker.com/get-started/part2</a></p>

<p>I'm using Docker 17.03.1-ce on a corporate LAN.</p>

<p>I checked and double checked my DNS settings. I'd used various ways of configuring the DNS that I'd found in my searches across the Internet. Some caused errors on startup. The approach that I ultimately settled upon for configuring the DNS was the one in the Troubleshoot Linux section of the above link above where the DNS is configured via the daemon.json file in the /etc/docker directory.</p>

<p>However, I still had this same issue. What finally solved the problem for me was the <strong>configuration of the proxy via the http_proxy and https_proxy environment variables</strong>. I had them specified in my Dockerfile, but I neglected to do so before the RUN pip command.</p>

<p>Even though it appeared to be a DNS issue, moving these ENV commands ahead of the RUN command made the difference for me. In case that is helpful for anyone with this problem.</p>
","9361261","","","0","1188","Deon McClung","2018-02-14 17:05:17","46","3","23","0","28668180","","2015-02-23 06:41:46","61","50475","<p>I'm following the <a href=""http://www.fig.sh/index.html"">fig guide</a> to using docker with a python application, but when docker gets up to the command</p>

<pre><code>RUN pip install -r requirements.txt
</code></pre>

<p>I get the following error message:</p>

<pre><code>Step 3 : RUN pip install -r requirements.txt
 ---&gt; Running in fe0b84217ad1
Collecting blinker==1.3 (from -r requirements.txt (line 1))
  Retrying (Retry(total=4, connect=None, read=None, redirect=None)) after connection broken by 'ProtocolError('Connection aborted.', gaierror(-2, 'Name or service not known'))': /simple/blinker/
</code></pre>

<p>This repeats several times and then I get another message:</p>

<pre><code>Could not find any downloads that satisfy the requirement blinker==1.3 (from -r requirements.txt (line 1))
  No distributions at all found for blinker==1.3 (from -r requirements.txt (line 1))
</code></pre>

<p>So for some reason pip can't access any packages from inside a docker container. Is there anything I need to do to allow it internet access?</p>

<p>However pip works fine to install things outside of the docker container, and worked fine even with that exact package (<code>blinker==1.3</code>) so that's not the problem. Also this problem isn't specific to that package. I get the same issue with any <code>pip install</code> command for any package.</p>

<p>Does anyone have any idea what's going on here?</p>
","2148718","2148718","2015-02-23 06:49:08","Can't install pip packages inside a docker container with Ubuntu","<python><docker><pip><fig>","16","3","1426"
"50647324","2018-06-01 16:05:55","2","","<p>A better solution is:</p>

<pre><code>self.connect(self.completer, QtCore.SIGNAL(""activated(const QString&amp;)""), self.line_edit.clear, QtCore.Qt.QueuedConnection)
</code></pre>
","4934609","","","0","182","Liron Lavi","2015-05-24 17:44:08","192","47","511","4","49029338","50647324","2018-02-28 12:13:50","1","210","<p>I have a problem clearing text from QLineEdit after selecting item from QCompleter. I want to print the text of the item selected from QCompleter and then immediately clear QLineEdit, I only succeeded in printing the text, but I couldn't manage to clear the QLineEdit text afterwards.</p>

<p>This is my code:</p>

<pre><code>import sys
from PyQt4 import QtGui, QtCore

auto_completer_words = [""chair""]


def get_data(model):
    model.setStringList(auto_completer_words)


class MainWindow(QtGui.QMainWindow):
    def __init__(self):
        super(self.__class__, self).__init__()
        self.resize(300, 300)

        self.line_edit = QtGui.QLineEdit(self)
        self.line_edit.setGeometry(QtCore.QRect(100, 100, 100, 30))

        self.completer = QtGui.QCompleter()
        self.line_edit.setCompleter(self.completer)

        model = QtGui.QStringListModel()
        self.completer.setModel(model)
        get_data(model)

        self.completer.activated.connect(self.get_data_in_le)


def get_data_in_le(self):
    print(self.line_edit.text())
    self.line_edit.clear()


def main():
    app = QtGui.QApplication(sys.argv)
    w = MainWindow()
    w.show()
    sys.exit(app.exec_())


if __name__ == '__main__':
    main()
</code></pre>
","4934609","","","Fail to clear QLineEdit after selecting QCompleter item","<python><pyqt4><qlineedit><qcompleter>","2","0","1251"
"50647362","2018-06-01 16:08:38","0","","<p>Well this is what I came up with. Might not be the fastest method (interested to know what is, please!) but it does the job. I split the csv into a different file for each hurricane. I then loaded these files in again one by one, reformatting the header as additional columns, and concatenating all of the files into a single data-frame. Please let me know if I could have done this in a more efficient way :)</p>

<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import glob

# split hurricanes into separate files
partNum = 1
outHandle = None
for line in open(""data/atlantic_1851_2017_2.csv"",""r"").readlines():
    if line.startswith('AL'):
        if outHandle is not None:
            outHandle.close()
        outHandle = open(""data/part%d.csv"" % (partNum,), ""w"")
        partNum += 1
    outHandle.write(line)
outHandle.close()


# read in each file as data-frame
files = glob.glob('data/part*.csv')
frames = []
for csv in files:  
    with open(csv) as f:
        first_line = f.readline() 
    first_line = first_line.split(',')    
    df = pd.read_csv(csv, skiprows=[0], header=None)
    df['ID'] = first_line[0]
    df['Name'] = first_line[1]
    frames.append(df)


# concatenate into a single data-frame
df = pd.concat(frames)
df = df.drop(columns=[8,9,10,11,12,13,14,15,16,17,18,19,20])
df.columns = ['Date','Time','Record_ID','Strength','Lat','Long','Max_Wind_Knots','Max_Pressure_mb','ID','Name']
print(df.head(5))
</code></pre>
","1895093","","","0","1480","astro person","2012-12-11 15:29:47","81","36","9","0","50645280","50647362","2018-06-01 14:07:37","-1","518","<p>I want to use some data from the NOAA website. It is a csv file with data for all hurricanes since 1851, with a format like this: <a href=""https://storage.googleapis.com/kaggle-datasets/692/1307/atlantic.pdf?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&amp;Expires=1528119944&amp;Signature=N5nu8BId0S5GBN0btBrvAYXzuBrOdgCuJrwcfNGXcLVgzDvjFV3BCNNAW2Qj1Fhkl2m%2FTkkgJMwchR3DK5V39fKZANhAoe64BGpKcwGJuBLZ3UulZ6syq43vqAWNf29siHy3ghzsE%2FOdoZssHIwK%2B0MY1lMs1R41p%2FZzVdjZ7%2Bg4n0EtVir4is0zwRfQLi0Z1wT0dgJrNrLFzmjtTpZhuDN75x4Tl5u%2FEeA3o9Rx31GH8ts3emEIK2jXPNf567wvXt6%2FOAMJWJSigOrYYQ4CrIbbEy%2FUPkR3rIgc4oGG%2FFSFUvHL2TEkB5Y%2F6Ub4FWIu%2BzMjO%2FrhLx2GO7TjCZq%2Fng%3D%3D"" rel=""nofollow noreferrer"" title=""Format example / README"">Format example / README file</a></p>

<p>As you can see, although everything is contained within one csv file, each hurricane has it's own table, with a separate header. </p>

<p>How can I remove the headers and put the information in a ""Hurricane Name"" column instead? I want to combine everything into a single data frame, so it's easier to use. Thanks!</p>

<p>Example:</p>

<blockquote>
  <p>AL092011,              IRENE,     3, </p>
  
  <p>20110821, 0000,  , TS, 15.0N,  59.0W,  45, 1006,  105,    0,    0,<br>
  45,    0,    0,    0,    0,    0,    0,    0,    0, </p>
  
  <p>20110821, 0600,  , TS, 16.0N,  60.6W,  45, 1006,  130,    0,    0,<br>
  80,    0,    0,    0,    0,    0,    0,    0,    0, </p>
  
  <p>20110821, 1200,  , TS, 16.8N,  62.2W,  45, 1005,  130,    0,    0,<br>
  70,    0,    0,    0,    0,    0,    0,    0,    0, </p>
  
  <p>AL092012,              ANOTHER_NAME,     2, </p>
  
  <p>20110821, 1800,  , TS, 17.5N,  63.7W,  50,  999,  130,   20,    0,<br>
  70,   30,    0,    0,    0,    0,    0,    0,    0, </p>
  
  <p>20110822, 0000,  , TS, 17.9N,  65.0W,  60,  993,  130,   30,   30,<br>
  90,   30,    0,    0,   30,    0,    0,    0,    0,</p>
</blockquote>

<p>I would like the header information into columns, like so:</p>

<blockquote>
  <p>AL092011, IRENE, 20110821, 0000,  , TS, 15.0N,  59.0W,  45, 1006,  105,    0,    0,<br>
  45,    0,    0,    0,    0,    0,    0,    0,    0, </p>
  
  <p>AL092011, IRENE, 20110821, 0600,  , TS, 16.0N,  60.6W,  45, 1006,  130,    0,    0,<br>
  80,    0,    0,    0,    0,    0,    0,    0,    0, </p>
  
  <p>AL092011, IRENE, 20110821, 1200,  , TS, 16.8N,  62.2W,  45, 1005,  130,    0,    0,<br>
  70,    0,    0,    0,    0,    0,    0,    0,    0, </p>
  
  <p>AL092012, ANOTHER_NAME, 20110821, 1800,  , TS, 17.5N,  63.7W,  50,  999,  130,   20,    0,<br>
  70,   30,    0,    0,    0,    0,    0,    0,    0, </p>
  
  <p>AL092012, ANOTHER_NAME, 20110822, 0000,  , TS, 17.9N,  65.0W,  60,  993,  130,   30,   30,<br>
  90,   30,    0,    0,   30,    0,    0,    0,    0,</p>
</blockquote>
","1895093","1895093","2018-06-01 15:12:14","Python: CSV file with multiple headers - combine into one data frame?","<python><pandas><csv><dataframe><data-cleaning>","1","8","2822"
"50647380","2018-06-01 16:10:15","0","","<pre><code>from itertools import combinations
search = list(combinations(range(len(solutions)), 2))

for i in search:
    res = list(set(solutions[i[0]]) &amp; set(solutions[i[1]]))
    if len(res) != 0:
        save = [i, res[0]]

idxList1 = solutions[save[0][0]].index(save[1])
idxList2 = solutions[save[0][1]].index(save[1])
</code></pre>

<p>This does what it is supposed to, but it seems like a poor solution.</p>
","1972356","","","1","419","NicolaiF","2013-01-12 12:44:13","421","77","77","2","50647260","50647386","2018-06-01 16:00:33","-3","81","<p>I have a list of lists:</p>

<pre><code>[[10, 9, 8], [8, 7], [1, 2, 3]]
</code></pre>

<p>The sublists are not necessarily of the same size.
I need to find a number that occurs in two seperate lists, and return the list index and the numbers index in those lists.</p>

<p>in this case it would be 8, list 0, list 1, list 0 idx 2, list 1 idx 0.</p>

<p>Right now I am doing it with a bunch of for loops, but this is ridiculously slow... are there are faster more pythonic way of achieving this?</p>
","1972356","","","Finding an element that occurs in two lists, in a list of lists","<python><algorithm><list><set>","2","7","501"
"50647386","2018-06-01 16:10:33","3","","<p>You can <code>enumerate</code> the lists and items in the list and store the index-tuples for each element in a <code>dict</code>, then filter those entries that have more than two occurrences.</p>

<pre><code>lst = [[10, 9, 8], [8, 7], [1, 2, 3]]
in_list = collections.defaultdict(list)
for i, x in enumerate(lst):
    for k, y in enumerate(x):
        in_list[y].append((i, k))

res = {k: v for k, v in in_list.items() if len(v) &gt; 1}
# {8: [(0, 2), (1, 0)]}
</code></pre>

<p>(This assumes that no element appears more than once in <em>the same</em> sublist.)</p>

<p>While this also uses ""a bunch of for loop"" (depending on your definition of ""a bunch""), it will visit every element in the nested list only exactly once, giving it O(n) (n = comb. size of lists).</p>
","1639625","","","0","776","tobias_k","2012-08-31 20:34:34","63214","4895","5994","655","50647260","50647386","2018-06-01 16:00:33","-3","81","<p>I have a list of lists:</p>

<pre><code>[[10, 9, 8], [8, 7], [1, 2, 3]]
</code></pre>

<p>The sublists are not necessarily of the same size.
I need to find a number that occurs in two seperate lists, and return the list index and the numbers index in those lists.</p>

<p>in this case it would be 8, list 0, list 1, list 0 idx 2, list 1 idx 0.</p>

<p>Right now I am doing it with a bunch of for loops, but this is ridiculously slow... are there are faster more pythonic way of achieving this?</p>
","1972356","","","Finding an element that occurs in two lists, in a list of lists","<python><algorithm><list><set>","2","7","501"
"50647437","2018-06-01 16:13:52","0","","<p>Make sure you are in the correct VirtualEnvironment. I updated PyCharm and for some reason had to point my project at my VE again. Opening the terminal, I was not in my VE when attempting zappa update (and got this error). Restarting PyCharm, all back to normal.</p>
","960471","","","0","270","andyw","2011-09-23 05:31:42","929","125","411","3","34134879","40012454","2015-12-07 13:35:31","60","52780","<p>Every time I want to config something with AWS I get the following error : </p>

<pre><code>""The config profile (myname) could not be found""
</code></pre>

<p>like : aws configure</p>

<p>I'm using Python 3.4 and I want to use AWS CLI Keyring to encrypt my credentials.. </p>
","5650183","","","AWS : The config profile (MyName) could not be found","<python><amazon-web-services><credentials><aws-cli>","9","1","279"
"50647440","2018-06-01 16:13:59","3","","<p>There are other ways to do it, which all have advantages for some cases, but none are ""just slicing"".</p>

<hr>

<p>The most general solution is probably to group your input, slice the groups, then flatten the slices back out. One advantage of this solution is that you can do it lazily, without building big intermediate lists, and you can do it to any iterable, including a lazy iterator, not just a list.</p>

<pre><code># from itertools recipes in the docs
def grouper(iterable, n, fillvalue=None):
    ""Collect data into fixed-length chunks or blocks""
    # grouper('ABCDEFG', 3, 'x') --&gt; ABC DEF Gxx""
    args = [iter(iterable)] * n
    return itertools.zip_longest(*args, fillvalue=fillvalue)
groups = grouper(a, 5)
truncated = (group[:2] for group in groups)
b = [elem for group in truncated for elem in group]
</code></pre>

<p>And you can convert that into a pretty simple one-liner, although you still need the <code>grouper</code> function:</p>

<pre><code>b = [elem for group in grouper(a, 5) for elem in group[:2]]
</code></pre>

<hr>

<p>Another option is to build a list of indices, and use <code>itemgetter</code> to grab all the values. This might be more readable for a more complicated function than just ""the first 2 of every 5"", but it's probably less readable for something as simple as your use:</p>

<pre><code>indices = [i for i in range(len(a)) if i%5 &lt; 2]
b = operator.itemgetter(*indices)(a)
</code></pre>

<p>… which can be turned into a one-liner:</p>

<pre><code>b = operator.itemgetter(*[i for i in range(len(a)) if i%5 &lt; 2])(a)
</code></pre>

<hr>

<p>And you can combine the advantages of the two approaches by writing your own version of <code>itemgetter</code> that takes a lazy index iterator—which I won't show, because you can go even better by writing one that takes an index filter function instead:</p>

<pre><code>def indexfilter(pred, a):
    return [elem for i, elem in enumerate(a) if pred(i)]
b = indexfilter((lambda i: i%5&lt;2), a)
</code></pre>

<p>(To make <code>indexfilter</code> lazy, just replace the brackets with parens.)</p>

<p>… or, as a one-liner:</p>

<pre><code>b = [elem for i, elem in enumerate(a) if i%5&lt;2]
</code></pre>

<p>I think this last one might be the most readable. And it works with any iterable rather than just lists, and it can be made lazy (again, just replace the brackets with parens). But I still don't think it's <em>simpler</em> than your original comprehension, and it's not just slicing.</p>
","908494","","","0","2495","abarnert","2011-08-23 20:55:30","272076","19605","5576","4509","50647167","50647227","2018-06-01 15:54:16","26","1388","<p>I am trying to get <em>m</em> values while stepping through every <em>n</em> elements of an array. For example, for <em>m</em> = 2 and <em>n</em> = 5, and given</p>

<pre><code>a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
</code></pre>

<p>I want to retrieve</p>

<pre><code>b = [1, 2, 6, 7]
</code></pre>

<p>Is there a way to do this using slicing? I can do this using a nested list comprehension, but I was wondering if there was a way to do this using the indices only. For reference, the list comprehension way is:</p>

<pre><code> b = [k for j in [a[i:i+2] for i in range(0,len(a),5)] for k in j]
</code></pre>
","9877290","9209546","2018-06-08 18:00:57","Stepping with multiple values while slicing an array in Python","<python><arrays><list>","7","2","611"
"50647446","2018-06-01 16:14:13","0","","<p>Figured it out. As usual, it was much less complicated than I assumed:</p>

<pre><code>    from pandas.io import packers
    class DataFrameWrap(pandas.DataFrame):
        pass
    packers.DataFrameWrap = DataFrameWrap

    df = DataFrameWrap()
    packed_df = df.to_msgpack()
    pandas.read_msgpack(packed_df)
</code></pre>
","3757614","","","0","329","user3757614","2014-06-19 18:00:22","1606","83","9","0","50634405","","2018-06-01 00:05:11","0","53","<p>What I have is a class that inherits from DataFrame, but overrides some behavior for business logic reasons. All is well and good, but I need the ability to import and export them. msgpack appears to be a good choice, but doesn't actually work. (Using the standard msgpack library doesn't even work on regular Dataframes, and the advice there is to use the dataframe msgpack functions.)</p>

<pre><code>    class DataFrameWrap(pandas.DataFrame):
        pass
    df = DataFrameWrap()
    packed_df = df.to_msgpack()
    pandas.read_msgpack(packed_df)
</code></pre>

<p>This results in the error</p>

<pre><code>File ""C:\Users\REDACTED\PROJECT_NAME\lib\site-packages\pandas\io\packers.py"", line 627, in decode
return globals()[obj[u'klass']](BlockManager(blocks, axes))
KeyError: u'DataFrameWrap'
</code></pre>

<p>when it reaches the read_msgpack() line. This works if I replace the DataFrameWrap() with a regular DataFrame().</p>

<p>Is there a way to tell pandas where to find the DataFrameWrap class? From reading the code, it looks like if I could inject {""DataFrameWrap"": DataFrameWrap} into the globals as seen from this file, it would work, but I'm not sure how to actually do that. There also might be a proper way to do this, but it's not obvious.</p>
","3757614","","","How to use read_msgpack with a Dataframe child class","<python><pandas><dataframe>","1","0","1264"
"50647448","2018-06-01 16:14:21","1","","<p>You should install msgpack and then install pandas again.</p>
","9121655","","","2","65","Ashok Kumar","2017-12-20 08:16:15","81","21","15","0","50647391","","2018-06-01 16:10:59","3","15914","<p>I tried to install pandas on my cmd and this is the output</p>

<pre><code>Requirement already satisfied: pandas in c:\users\name\anaconda3\lib\site-packages (0.23.0)
Requirement already satisfied: python-dateutil&gt;=2.5.0 in c:\users\name\anaconda3\lib\site-packages (from pandas) (2.7.3)
Requirement already satisfied: pytz&gt;=2011k in c:\users\name\anaconda3\lib\site-packages (from pandas) (2018.4)
Requirement already satisfied: numpy&gt;=1.9.0 in c:\users\name\anaconda3\lib\site-packages (from pandas) (1.14.3)
Requirement already satisfied: six&gt;=1.5 in c:\users\name\anaconda3\lib\site-packages (from python-dateutil&gt;=2.5.0-&gt;pandas) (1.11.0)

**distributed 1.21.8 requires msgpack, which is not installed.**
</code></pre>

<p>This last line is in red.</p>

<p>Im on windows 10, I installed anaconda</p>
","2414528","7976758","2018-06-01 22:15:35","Error on installing packages on python","<python><pip><package>","6","0","825"
"50647478","2018-06-01 16:16:51","2","","<p>TF can automatically create a tensor from a data frame as long as it has only one data type, in this case it seems to have different data types. </p>

<p>Without <code>literal_eval</code> the code seems to work, as each of the features are string and not of mixed type:</p>

<pre><code>train = pd.read_csv(""train.csv"", names=CSV_COLUMN_NAMES, header=0, delimiter="","")

Features,labels = train,train.pop('type')

dataset = tf.data.Dataset.from_tensor_slices((dict(Features), labels))
iterator = dataset.make_initializable_iterator()
next_element = iterator.get_next()

with tf.Session() as sess:
  sess.run(iterator.initializer)
  print(sess.run(next_element))
  print(sess.run(next_element))
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>({'y': b'[2, 3, 4]', 'x': b'[1, 2, 3]'}, b'A')
({'y': b'[0, 1, 2]', 'x': b'[2, 7, 9]'}, b'B')
</code></pre>

<p>Based on this solution: (<a href=""https://stackoverflow.com/questions/19459017/how-to-convert-a-numpy-2d-array-with-object-dtype-to-a-regular-2d-array-of-float"">How to convert a Numpy 2D array with object dtype to a regular 2D array of floats</a> ) if we convert the mixed object type to same (with np.vstack), it works.</p>

<pre><code>train['x'] = train['x'].apply(literal_eval)
train['y'] = train['y'].apply(literal_eval)

Features,labels = train,train.pop('type')
dataset = tf.data.Dataset.from_tensor_slices(((np.vstack(Features['x']),    np.vstack(Features['y'])), labels))

iterator = dataset.make_initializable_iterator()
next_element = iterator.get_next()

with tf.Session() as sess:
   sess.run(iterator.initializer)
   print(sess.run(next_element))
   print(sess.run(next_element))
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>((array([1, 2, 3]), array([2, 3, 4])), b'A')
((array([2, 7, 9]), array([0, 1, 2])), b'B')
</code></pre>
","8143158","","","1","1822","vijay m","2017-06-11 01:43:37","11660","849","61","3","50641588","50647478","2018-06-01 10:39:08","2","2405","<p>So I have some train data in a csv file <code>train.csv</code> with the following format:</p>

<pre><code>x;y;type
[1,2,3];[2,3,4];A
[2,7,9];[0,1,2];B
</code></pre>

<p>This file is parsed as a <code>pd.DataFrame</code> with the following:
</p>

<pre><code>CSV_COLUMN_NAMES = ['x', 'y', 'type']
train = pd.read_csv(""train.csv"", names=CSV_COLUMN_NAMES, header=0, delimiter="";"")
train['x'] = train['x'].apply(literal_eval)
train['y'] = train['y'].apply(literal_eval)
</code></pre>

<p>So far so good. The <code>literal_eval</code> function is applied so <code>x</code> and <code>y</code> are treated as array. The next step is to create a <code>DataSet</code> with the following:</p>

<pre><code>features, labels = train, train.pop('type')
dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))
</code></pre>

<p>And here is where it breaks :( It spills the following errors:
</p>

<pre><code>TypeError: Expected binary or unicode string, got [1, 2, 3]
</code></pre>

<p>Why is binary or unicode string expected? Are vector feature columns not allowed? Or am I doing something wrong? Please shed me some light</p>
","3694078","","","How to create a tensorflow dataset from a DataFrame with vector columns?","<python><pandas><dataframe><tensorflow>","2","0","1131"
"50647564","2018-06-01 16:22:42","1","","<p>Try printing the data with different types, so as <code>int</code>, <code>char</code> or even as a <code>list</code>. Your problem is that the presented data is not of the type, as you print it. This makes the strange symbols.</p>
","8620128","","","1","234","itzFlubby","2017-09-16 19:32:42","190","47","17","1","50646571","","2018-06-01 15:18:53","0","692","<p>I'm trying to read an USB RFID device with python.
The divce works in HID mode and I can find it as /dev/hidraw0
I have a tag which code is '210054232F' (I can see the code printed on the shell scanning it with the RFID)</p>

<p>So I try to open the device with a python script and capture the code read but I'm stuck...</p>

<p>This is the python code: </p>

<pre><code>import sys

fp = open('/dev/hidraw0', 'rb')

while True:
   buffer = fp.read(16)
   for c in buffer:
       if ord(c) &gt; 0:
           print c
   print ""\n""
</code></pre>

<p>This is the output (with a lot of square and unorintable characters in the middle):
<a href=""https://i.stack.imgur.com/ZkeSd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZkeSd.png"" alt=""""></a>
If I print the code with:</p>

<pre><code>for c in buffer:
       if ord(c) &gt; 0:
           print ord(c)
</code></pre>

<p>This is the output:</p>

<p>1
31
1</p>

<p>1
30
1</p>

<p>1
39
1</p>

<p>1
39
1</p>

<p>1
34
1</p>

<p>1
33
1</p>

<p>1
31
1</p>

<p>1
32
1</p>

<p>1
31
1</p>

<p>1
2
9
1</p>

<p>1
40
1</p>

<p>I can't find any kind of pattern to decode the data.</p>

<p>Have you any suggestions or other way to solve the issue?</p>

<p>Thanks,
Federico</p>
","7046886","7046886","2018-06-01 16:07:33","Read USB RFID with a Raspberry & python","<python><linux><raspberry-pi><rfid><hid>","1","1","1234"
"50647577","2018-06-01 16:23:25","0","","<p>With itertools you could get an iterator with:</p>

<pre><code>from itertools import compress, cycle

a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
n = 5
m = 2

it = compress(a, cycle([1, 1, 0, 0, 0]))
res = list(it)
</code></pre>
","1388292","1388292","2018-06-02 17:40:21","2","224","Jacques Gaudin","2012-05-10 23:01:38","8213","656","1988","169","50647167","50647227","2018-06-01 15:54:16","26","1388","<p>I am trying to get <em>m</em> values while stepping through every <em>n</em> elements of an array. For example, for <em>m</em> = 2 and <em>n</em> = 5, and given</p>

<pre><code>a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
</code></pre>

<p>I want to retrieve</p>

<pre><code>b = [1, 2, 6, 7]
</code></pre>

<p>Is there a way to do this using slicing? I can do this using a nested list comprehension, but I was wondering if there was a way to do this using the indices only. For reference, the list comprehension way is:</p>

<pre><code> b = [k for j in [a[i:i+2] for i in range(0,len(a),5)] for k in j]
</code></pre>
","9877290","9209546","2018-06-08 18:00:57","Stepping with multiple values while slicing an array in Python","<python><arrays><list>","7","2","611"
"50647594","2018-06-01 16:24:22","0","","<p>That's what viewmodels are well suited for. They are proxy models: they adapt the source model for a certain organization of display. You'd be usually deriving from <code>QAbstractProxyModel</code>. A simple example is provided e.g. <a href=""https://stackoverflow.com/a/21569029/1329652"">here</a>.</p>
","1329652","","","0","305","Kuba Ober","2012-04-12 16:24:40","74972","8871","6744","776","50607790","","2018-05-30 14:55:29","2","70","<p>I just learned about the model/view concept in Qt and arrived at a rather general question.</p>

<p>I understood how Models can be used to associate certain <strong>data types</strong> to appropriate Widgets, for example <code>QAbstractListModel</code> can associate a list to a <code>QListView</code> or a <code>QComboBox</code>.</p>

<p>Special benefit arises, when multiple widgets are associated to the same data via a common model, such as a <code>QComboBox</code> and several <code>QListView</code>s communicating to the same, single <code>QAbstractListModel</code>.</p>

<p>I noticed, that in the textbook demonstrations this situation is always chosen such that the widgets display the data in the <strong>same structure</strong>, e.g. the entries of the Combo and the List at position <code>i</code> are both derived from the <code>i</code>eth list item, rather than from the total content of the data.</p>

<p>My question is, generally, how would one approach the situation where <strong>the same data is to be displayed by different widgets in completely different structures</strong>, i.e. the entries (and their number) is a general function of the whole dataset?</p>

<p>To give a (rather complex) example, say that my <em>data</em> is a list of lists of integers, </p>

<pre><code>my_list = [[1,2,3], [2,3,4,5]]
</code></pre>

<p>and I would like to display it using two widgets as follows:</p>

<pre><code>QTableView

List      | Length | First Item
----------+--------+-----------
[1,2,3]   | 3      | 1
[2,3,4,5] | 4      | 2


QTreeView

+ [1,2,3]
   + Length
      + 3
   + First item
      + 1
   + Left-truncated Sublists
      + [2,3]
         + Length
            + 2
         + First item
            + 2
         + Left-truncated Sublists
            + [3]
               + This list very nice because it contains no 2
               + Length
                  + 1
               + First item
                  + 3
      + [3]
         + This list very nice because it contains no 2
         + Length
            + 1
         + First item
            + 3
+ [2,3,4,5]
   + Length
      + 4
   + First item
      + 2
   + Left-truncated Sublists
      + [3,4,5]
         + This list very nice because it contains no 2
         + Length
            + 3
         + First item
            + 3
         + Left-truncated Sublists
            + [4,5]
               + This list very nice because it contains no 2
               + Length
                  + 2
               + First item
                  + 4
               + Left-truncated Sublists
                  [5]
                   + This list very nice because it contains no 2
                   + Length
                      + 1
                   + First item
                      + 5
            + [5]
               + This list very nice because it contains no 2
               + Length
                  + 1
               + First item
                  + 5
      + [4,5]
         + This list very nice because it contains no 2
         + Length
            + 2
         + First item
            + 4
         + Left-truncated Sublists
            + [5]
               + This list very nice because it contains no 2
               + Length
                  + 1
               + First item
                  + 5
      + [5]
         + This list very nice because it contains no 2
         + Length
            + 1
         + First item
            + 5
+ there are 2 lists in total
</code></pre>

<p>Of course I do not expect a fully coded detailled answer, but some hints or resources would be nice, how to approach this using the MVC in <code>Qt</code>/<code>PyQt</code>.</p>
","1954677","1954677","2018-05-30 19:31:32","What is the programmatic approach in Qt's MVC to get widgets with a structurally different view on the same data?","<python><qt><model-view-controller><pyqt>","1","3","3665"
"50647595","2018-06-01 16:24:32","0","","<p>YAML was designed to include human readable forms of data such as yours, (but there are multiple ways of representing data in YAML that are not so readable) </p>

<p>XML, I've seen described as having the readability of binary combined with the inefficiency of ASCII.</p>

<p>JSON has a bit too many double quotes to make its actual data stand out. And if you want to edit the data by hand, you really have to take care with trailing comma's in arrays and objects.</p>

<hr>

<p>There is of course no viewer or browser that directly supports your format, but if you start with JSON it is possible to write a JavaScript program that displays each dataset properly with hyperlinks. You can do the same when you start with XML via de DOMparser that is built into the browser. There are also YAML parsers in javascript that can do the same for YAML based data, but these would have to be installed and loaded into the browser.</p>

<hr>

<p>If you don't want to program in javascript, I would go for putting the data in YAML and have a Python program that (recursively) looks at all the individual YAML files and generates HTML from these, including correct hyperlinks (to the HTML ""version"" of the dependencies) and either links to the images, or in place display of the images. Make the program smart enough to only (re-)generate the HTML if the corresponding file containng the YAML document has a newer timestamp.</p>

<p>This is similar to how some blog systems work that generate static views from markup. And since you want to process the data using Python anyway, you should be able to re-use some of the code you write.</p>

<p>You should make your <code>/publish/path/metadata/poster.yaml</code>:</p>

<pre><code>created_by: John
creation_date: 2018-11-12
version: 005
creator_comments: Updated to latest published images for Poppy
path_to_file: /publish/path/images/poster.png
dependencies:
- /publish/path/metadata/poppy.yaml
- /publish/path/metadata/dwarf.yaml
- /publish/path/metadata/giant.yaml
</code></pre>

<p>As you can see you don't have to write dates as strings, YAML directly supports the YYYY-MM-DD format (where it is unclear if your creation_date is the MMDDYYYY as used in the USA or the DDMMYYYY as is more wideley used in other English speaking countries). How you display dates in your HTML is of course your preference.</p>

<p>With your YAML you should adhere to the latest spec (1.2 from 2009) and use <code>ruamel.yaml</code> (disclaimer: I am the author of that package). If you go for YAML 1.1 (in which case you can use PyYAML), you'll have to quote and define your versions as scalar strings as PyYAML otherwise interprets <code>version: 015</code> as the number 13. <code>ruamel.yaml</code> also correctly round-trips and writes such integers again with leading zeros. If your version would contain non-numerical data, then YAML automatically loads this as a string (no need to quote).</p>

<p>For dumping the HTML there are many options, using some library where you create a tree structure and then dumping that has the advantage that you cannot generate invalid HTML. But even if you generate HTML ""by hand"", you should have your output relatively quickly debugged.</p>

<p>The conversion program can of course also check that all references exist and warn you if they don't.</p>

<hr>

<p>A simple program that does the above (with not so good looking HTML as output):</p>

<pre><code>from datetime import date
from pathlib import Path
from ruamel.yaml import YAML
from ruamel.yaml.scalarint import ScalarInt

yaml = YAML()

def convert_data(d, fp, level=0):
    """"""recursively write a loaded YAML document as HTML""""""
    if isinstance(d, dict):
        print('&lt;table&gt;', file=fp)
        for k in d:
            print('&lt;tr&gt;&lt;td&gt;', file=fp)
            convert_data(k, fp, level=level+1)
            print('&lt;/td&gt;&lt;td&gt;', file=fp)
            v = d[k]
            convert_data(v, fp, level=level+1)
            print('&lt;/td&gt;&lt;/tr&gt;', file=fp)
        print('&lt;/table&gt;', file=fp)
        return
    if isinstance(d, list):
        print('&lt;ul&gt;', file=fp)
        for elem in d:
            print('&lt;li&gt;', file=fp)
            convert_data(elem, fp, level=level+1)
            print('&lt;/li&gt;', file=fp)
        print('&lt;/ul&gt;', file=fp)
        return
    if isinstance(d, str) and d and d[0] == '/':
        if d.endswith('.yaml'):
            h = Path(d).with_suffix('.html')
            print('&lt;a href=""{}""&gt;{}&lt;/a&gt;'.format(h, d), file=fp)
            return
        if d.endswith('.png'):
            print('&lt;img src=""{}""&gt;'.format(d), file=fp)
            return
    if isinstance(d, ScalarInt):
        if d._width is not None:
            # integer with leading zeros
            print('{:0{}d}'.format(d, d._width), file=fp)
        return
    if isinstance(d, date):
        # print the date in DDMMYYYY format
        print('{:%d%m%Y}'.format(d), file=fp)
        return
    print(d, file=fp)

def convert_file(yaml_file, html_file):
    data = yaml.load(yaml_file)
    with html_file.open('w') as fp:
        print('&lt;html&gt;\n&lt;body&gt;', file=fp)
        convert_data(data, fp)
        print('&lt;/body&gt;\n&lt;/html&gt;', file=fp)

def main():
    for yaml_file in Path('.').glob('*.yaml'):
        html_file = yaml_file.with_suffix('.html')
        if True or not html_file.exists() or \
           html_file.stat().st_mtime &lt; yaml_file.stat().st_mtime:
            convert_file(yaml_file, html_file)


if __name__ == '__main__':
    main()
</code></pre>

<p>You could of course make the links and images explicit by using tags ( <code>!link /publish/path/metadata/poppy.yaml</code> and <code>!img /publish/path/images/poster.png</code> and have classes with a constructor for these tags that then dump appropriate HTML. This does however not necessarily give you better readable YAML.</p>
","1307905","1307905","2018-06-01 19:13:14","2","5926","Anthon","2012-04-02 11:30:58","37849","5547","1679","2001","50641988","","2018-06-01 11:01:37","1","88","<p>I need to store some meta information and dependencies between my assets, in files that I can use to do some validations down the line. </p>

<p>Taking JSON as an example, my metadata file would look like this (<code>/publish/path/metadata/poster.json</code>):</p>

<pre><code>{
    'created_by': 'John',
    'creation_date': '12112018',
    'version': '005',
    'creator_comments': 'Updated to latest published images for Poppy',
    'path_to_file': '/publish/path/images/poster.png',
    'dependencies': [
                     '/publish/path/metadata/poppy.json',
                     '/publish/path/metadata/dwarf.json',
                     '/publish/path/metadata/giant.json'
                     ]
}
</code></pre>

<p>and (<code>/publish/path/metadata/poppy.json</code>):</p>

<pre><code>  {
        'created_by': 'Daug',
        'creation_date': '12102018',
        'version': '003',
        'creator_comments': 'Poppy is more red on top',
        'path_to_file': '/publish/path/images/poppy.png',
        'dependencies': [
                         '/publish/path/metadata/poppy_drawing.json',
                         '/publish/path/metadata/poppy_effect.json'
                         ]
    }
</code></pre>

<p>I am looking for a file format would be most appropriate fit to do the following</p>

<ol>
<li>be able to store references to other files</li>
<li>is supported by python libraries that can process the references</li>
<li>can be read easily by humans</li>
<li>viewer or browser support that allows me to traverse the referenced files</li>
</ol>

<p>What do you think fits best to my use case?</p>
","9876238","1307905","2018-06-01 19:00:52","Most appropriate file format for storing cross references","<python><json><xml><reference><yaml>","2","3","1620"
"50647600","2018-06-01 16:25:02","0","","<p>Here's my <a href=""https://stackoverflow.com/questions/50600074/is-there-any-quick-way-to-check-whether-scrapy-login-a-website-successfully/50633599#50633599"">example</a> how to make HTTPS or HTTP login. First you need to collect formdata from page. Usually it need to take hidden inputs from page. Then you need to send formdata dict using FormRequest.</p>
","5909920","","","1","361","Oleg T.","2016-02-10 18:13:06","150","28","27","0","17308011","","2013-06-25 21:36:15","0","866","<p>I am trying to make a web crawler that will login to an https website using my credentials and then crawl certain parts of the site. I am using the Scrapty in python but i am not 100% sure if it is possible since in the website i do not see anything about https only the following : </p>

<pre><code>*cookies and session handling
*HTTP compression
*HTTP authentication 
*HTTP cache
</code></pre>

<p>If, so any ideas as how to start?</p>
","2475251","","","Scrapy - using scrapy is it possible to login to an https website","<python><login><https><scrapy><screen-scraping>","2","0","441"
"50647626","2018-06-01 16:27:12","0","","<p>I'm not sure whether you're trying to find the peak CPU usage during execution of that <code>#kalman code</code>, or the average, or the total, or something else? </p>

<p>But you can't get any of those by calling <code>cpu_percent()</code> after it's done. You're just measuring the CPU usage of calling <code>cpu_percent()</code>.</p>

<hr>

<p>Some of these, you can get by calling <a href=""http://psutil.readthedocs.io/en/latest/#psutil.cpu_times"" rel=""nofollow noreferrer""><code>cpu_times()</code></a>. This will tell you how many seconds were spent doing various things, including total ""user CPU time"" and ""system CPU time"". The docs indicate exactly which values you get on each platform, and what they mean.</p>

<p>In fact, if this is all you want (including only needing those two values), you don't even need <code>psutils</code>, you can just use <a href=""https://docs.python.org/3/library/os.html#os.times"" rel=""nofollow noreferrer""><code>os.times()</code></a> in the standard library.</p>

<p>Or you can just use the <a href=""http://pubs.opengroup.org/onlinepubs/009696899/utilities/time.html"" rel=""nofollow noreferrer""><code>time</code></a> command from outside your program, by running <code>time python myscript.py</code> instead of <code>python myscript.py</code>.</p>

<hr>

<p>For others, you need to call <code>cpu_percent</code> <em>while your code is running</em>. One simple solution is to run a background process with <code>multiprocessing</code> or <code>concurrent.futures</code> that just calls <code>cpu_percent</code> on your main process. To get anything useful, the child process may need to call it periodically, aggregating the results (e.g., to find the maximum), until it's told to stop, at which point it can return the aggregate.</p>

<p>Since this is not quite trivial to write, and definitely not easy to explain without knowing how much familiarity you have with multiprocessing, and there's a good chance <code>cpu_times()</code> is actually what you want here, I won't go into details unless asked.</p>
","908494","","","6","2051","abarnert","2011-08-23 20:55:30","272076","19605","5576","4509","50647469","","2018-06-01 16:16:26","0","588","<p>I have implemented kalman filter. I want to find out how much of cpu energy is being consumed by my script. I have checked other posts on Stackoverflow and following them I downloaded psutil library. Now, I am unaware of where to put the statements to get the correct answer. Here is my code:</p>

<pre><code>if __name__ == ""__main__"":
   #kalman code
    pid = os.getpid()
    py = psutil.Process(pid)
    current_process = psutil.Process();
    memoryUse = py.memory_info()[0]/2.**30  # memory use in GB...I think
    print('memory use:', memoryUse)
    print(current_process.cpu_percent())
    print(psutil.virtual_memory()) #  physical memory usage
</code></pre>

<p>Please inform whether I am headed in the right direction or not.
The above code generated following results. </p>

<pre><code>('memory use:', 0.1001129150390625)
0.0
svmem(total=6123679744, available=4229349376, percent=30.9, used=1334358016, free=3152703488, active=1790803968, inactive=956125184, buffers=82894848, cached=1553723392, shared=289931264, slab=132927488)
</code></pre>

<p>Edit: Goal: find out energy consumed by CPU while running this script
    ​</p>
","9432695","9432695","2018-06-01 22:49:43","How to check cpu consumption by a python script","<python><psutil>","1","3","1142"
"50647646","2018-06-01 16:28:49","0","","<p>This is not an issue with TensorFlow but rather an issue with your version of OpenSSL. You can check your current version on OSX with:  </p>

<pre><code>$ python3 -c ""import ssl; print(ssl.OPENSSL_VERSION)"" 
</code></pre>

<p>Then upgrade your version with:</p>

<pre><code>$ brew update
$ brew install openssl
</code></pre>
","7582820","","","0","328","andrew","2017-02-17 20:27:09","2330","2573","429","0","50612518","50647646","2018-05-30 19:56:44","1","77","<p>I ran into a problem while upgrading the <code>tensorflow</code>. I currently use 0.12.1 version. Here is the message I got.</p>

<pre><code>Exception:
Traceback (most recent call last):
   File ""/Users/peymanghahremani/tensorflow/lib/python2.7/site-packages/pip/basecommand.py"", line 215, in main
status = self.run(options, args)
  File ""/Users/peymanghahremani/tensorflow/lib/python2.7/site-packages/pip/commands/install.py"", line 335, in run
wb.build(autobuilding=True)
  File ""/Users/peymanghahremani/tensorflow/lib/python2.7/site-packages/pip/wheel.py"", line 749, in build
self.requirement_set.prepare_files(self.finder)
  File ""/Users/peymanghahremani/tensorflow/lib/python2.7/site-packages/pip/req/req_set.py"", line 380, in prepare_files
ignore_dependencies=self.ignore_dependencies))
  File ""/Users/peymanghahremani/tensorflow/lib/python2.7/site-packages/pip/req/req_set.py"", line 620, in _prepare_file
session=self.session, hashes=hashes)
  File ""/Users/peymanghahremani/tensorflow/lib/python2.7/site-packages/pip/download.py"", line 821, in unpack_url
hashes=hashes
  File ""/Users/peymanghahremani/tensorflow/lib/python2.7/site-packages/pip/download.py"", line 659, in unpack_http_url
hashes)
  File ""/Users/peymanghahremani/tensorflow/lib/python2.7/site-packages/pip/download.py"", line 853, in _download_http_url
stream=True,
  File ""/Users/peymanghahremani/tensorflow/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py"", line 488, in get
return self.request('GET', url, **kwargs)
  File ""/Users/peymanghahremani/tensorflow/lib/python2.7/site-packages/pip/download.py"", line 386, in request
return super(PipSession, self).request(method, url, *args, **kwargs)
  File ""/Users/peymanghahremani/tensorflow/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py"", line 475, in request
resp = self.send(prep, **send_kwargs)
  File ""/Users/peymanghahremani/tensorflow/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py"", line 596, in send
r = adapter.send(request, **kwargs)
  File ""/Users/peymanghahremani/tensorflow/lib/python2.7/site-packages/pip/_vendor/cachecontrol/adapter.py"", line 47, in send
resp = super(CacheControlAdapter, self).send(request, **kw)
  File ""/Users/peymanghahremani/tensorflow/lib/python2.7/site-packages/pip/_vendor/requests/adapters.py"", line 497, in send
raise SSLError(e, request=request)
SSLError: [SSL: TLSV1_ALERT_PROTOCOL_VERSION] tlsv1 alert protocol version (_ssl.c:590)
</code></pre>
","7929261","1485877","2018-05-30 20:12:10","Error in upgrading tensorflow","<python><python-2.7><tensorflow>","1","2","2454"
"50647676","2018-06-01 16:31:49","0","","<p>To read the .csv, I use pandas. </p>

<pre><code>import pandas as pd
fruit_df = pd.read('directory_where_csv_is_saved/file_name.csv')
</code></pre>

<p>I would widen the dataframe to hold more columns by unpacking the dictionary column ""Fruit"" by first getting the keys from the dictionaries within the 'Fruit' column and converting that set of keys to an iterable list.</p>

<pre><code>key_set = set()
for i in range(len(fruit_df)):
    for key in fruit_df['Fruit'][i].keys():
        if key not in key_set:
            key_set.add(key)
        else:
            pass
key_set_list = list(key_set)
</code></pre>

<p>Then unpack the dictionary:</p>

<pre><code>for i in range(len(key_set_list)):
    fruit_df[key_set_list[i]] = [d.get(key_set_list[i]) for d in fruit_df['Fruit']]
</code></pre>

<p>Your dataframe should be wider (more columns) with each new column being the dictionary key, and the respective values in the applicable rows.</p>
","8544512","5708620","2018-06-01 17:31:53","0","947","Sylvia Tran","2017-08-31 16:53:46","1","8","0","0","50647213","","2018-06-01 15:57:49","0","225","<p>I am new to python.
I need to read csv file which has various columns.
In csv file One column contains data like key and value pairs.
Using pandas how to extract the keys and values of that column from csv.</p>

<p>Ex: column name : <code>fruit</code></p>

<p>Data in that column :</p>

<pre><code>{ """"apple"": ""1,2,3,4"", ""orange"":""5,6,7,8""}
</code></pre>

<p>How to get keys and its values of fruit column from csv file?
Any suggestions?</p>
","804343","5708620","2018-06-01 17:30:56","Python: how read csv data if column having data in the format of key value pairs","<python><pandas><csv>","1","3","445"
"50647687","2018-06-01 16:32:33","5","","<p>You need to split by "", "". Note also that <code>readlines</code> returns a <code>list</code>, one element for each line. It appears your input file only consists of one line, so you will need to extract the first element of your list.</p>

<p>Finally, you need to ensure you compare strings with strings or integers with integers. In the below example, I assume the leading <code>0</code>s are important and compare strings throughout.</p>

<pre><code>from io import StringIO

text_file = StringIO('00501, 00544, 00601, 00602, 00603, 00604')

## create set of zip codes
zip_codes = set(text_file.readlines()[0].split(', '))

## input series of strings
s = pd.Series(['00501', '00544', '00601', '00605'])
s_res = s.isin(zip_codes).tolist()

# [True, True, True, False]

## input series of integers
t = pd.Series([501, 544, 601, 605])
t_res = t.astype(str).str.zfill(5).isin(zip_codes).tolist()

# [True, True, True, False]
</code></pre>
","9209546","","","0","939","jpp","2018-01-12 14:47:22","109049","18235","7890","3496","50647587","50647687","2018-06-01 16:23:56","2","92","<p>I have a dataframe of addresses, including zip codes. I am trying to see if those zip codes are valid by importing a list from a text file of valid zip codes. I can't seem to get the correct datatype either from the list or the dataframe. </p>

<p>My list looks like this in the text file</p>

<pre><code>00501, 00544, 00601, 00602, 00603, 00604, 00605, 00606, 00610, 00611, 00612, 00613, 00614, 00616, 00617, 00622, 00623, 00624, 00627, 00631, 00636, 00637, 00638, 00641, 00646, 00647, 00650, 00652, 00653, 00656, 00659, 00660, 00662, 00664, 00667, 00669, 00670, 00674, 00676, 00677, 00678, 00680, 00681, 00682, 00683, 00685, 00687, 00688, 00690, 00692, 00693, 00694, 00698, 00703, 00704, 00705, 00707, 00714, 00715, 00716, 00717, 00718, 00719, 00720, 00721, 00723, 00725, 00726..... 
</code></pre>

<p>And I am importing the list like so...</p>

<pre><code>text_file = open(""/C:/valid_zipcodes.txt"", ""r"")
zip_codes = text_file.readlines()
</code></pre>

<p>I'm looking to return a bool with the is in function. This works if I create a very simple list in the code    </p>

<pre><code>zip_codes2 = [12401,12603]
df['valid_zip'] = df['Zip Code'].isin(zip_codes)
&gt;&gt;valid_zip True

print(df.dtypes)

&gt;&gt;Zip Code int64
</code></pre>

<p>I've tried to the datatype to object with the following, it changed to object, but couldn't find it's counterpart in the list from the text file. Even when I adjusted the textfile to read '12345','12346'.</p>

<pre><code>df['Zip Code'] =  df['Zip Code'].astype(str)
</code></pre>

<p>Anyone got any ideas? </p>
","5370313","9209546","2018-06-03 17:55:13","Importing a list from text file to compare to dataframe with isin()","<python><string><pandas><series>","1","1","1560"
"50647695","2018-06-01 16:33:00","8","","<p>You can convert to tflite directly in python directly. You have to <a href=""https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/framework/graph_util_impl.py#L206"" rel=""noreferrer"">freeze the graph</a> and use <a href=""https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/contrib/lite/python/lite.py#L141"" rel=""noreferrer"">toco_convert</a>. It needs the input and output names and shapes to be determined ahead of calling the API just like in the commandline case. </p>

<h3>An example code snippet</h3>

<p>Copied from <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md"" rel=""noreferrer"">documentation</a>, where a ""frozen"" (no variables) graph is defined as part of your code:</p>

<pre><code>import tensorflow as tf

img = tf.placeholder(name=""img"", dtype=tf.float32, shape=(1, 64, 64, 3))
val = img + tf.constant([1., 2., 3.]) + tf.constant([1., 4., 4.])
out = tf.identity(val, name=""out"")
with tf.Session() as sess:
  tflite_model = tf.contrib.lite.toco_convert(sess.graph_def, [img], [out])
  open(""test.tflite"", ""wb"").write(tflite_model)
</code></pre>

<p>In the example above, there is no freeze graph step since there are no variables. If you have variables and run toco without freezing graph, i.e. converting those variables to constants first, then toco will complain!</p>

<h3>If you have frozen graphdef and know the inputs and outputs</h3>

<p>Then you don't need the session. You can directly call toco API:</p>

<pre><code>path_to_frozen_graphdef_pb = '...'
input_tensors = [...]
output_tensors = [...]
frozen_graph_def = tf.GraphDef()
with open(path_to_frozen_graphdef_pb, 'rb') as f:
  frozen_graph_def.ParseFromString(f.read())
tflite_model = tf.contrib.lite.toco_convert(frozen_graph_def, input_tensors, output_tensors)
</code></pre>

<h3>If you have non-frozen graphdef and know the inputs and outputs</h3>

<p>Then you have to load the session and freeze the graph first before calling toco:</p>

<pre><code>path_to_graphdef_pb = '...'
g = tf.GraphDef()
with open(path_to_graphdef_pb, 'rb') as f:
  g.ParseFromString(f.read())
output_node_names = [""...""]
input_tensors = [..]
output_tensors = [...]

with tf.Session(graph=g) as sess:
  frozen_graph_def = tf.graph_util.convert_variables_to_constants(
      sess, sess.graph_def, output_node_names)
# Note here we are passing frozen_graph_def obtained in the previous step to toco.
tflite_model = tf.contrib.lite.toco_convert(frozen_graph_def, input_tensors, output_tensors)
</code></pre>

<h3>If you don't know inputs / outputs of the graph</h3>

<p>This can happen if you did not define the graph, ex. you downloaded the graph from somewhere or used a high level API like the tf.estimators that hide the graph from you. In this case, you need to load the graph and poke around to figure out the inputs and outputs before calling toco. See my answer to <a href=""https://stackoverflow.com/questions/50581883/how-do-i-export-a-tensorflow-model-as-a-tflite-file/50635091#50635091"">this SO question</a>.</p>
","9789871","9789871","2018-06-05 21:47:20","5","3065","Pannag Sanketi","2018-05-14 17:27:15","1021","220","10","1","50632152","53500093","2018-05-31 20:17:50","4","10609","<p>I'm new with working on Tensorflow.
I have a model saved after training as pb file, I want to use tensorflow mobile and it's important to work with TFLITE file.
The problem is most of the examples I found after googling for converters are command on terminal or cmd. 
Can you please share me an example of converting to tflite files using python code? </p>

<p>Thanks</p>
","4024038","","","Tensorflow Convert pb file to TFLITE using python","<python><tensorflow><tensorflow-lite>","4","0","375"
"50647803","2018-06-01 16:42:51","0","","<p>Turns out to be a bug in PyPy3.</p>

<p>Here is the fixed ticket:
<a href=""https://bitbucket.org/pypy/pypy/issues/2841/remote-multprocessing-issue#comment-45861347"" rel=""nofollow noreferrer"">https://bitbucket.org/pypy/pypy/issues/2841/remote-multprocessing-issue#comment-45861347</a></p>
","2864877","","","0","291","Techniquab","2013-10-09 22:33:55","147","27","33","0","50353517","","2018-05-15 15:03:16","0","53","<p>I am trying to run the <a href=""https://docs.python.org/3/library/multiprocessing.html#using-a-remote-manager"" rel=""nofollow noreferrer"">remote manager example code</a> from the multiprocessing documentation in pypy3 but I get an error connecting the client.</p>

<pre><code>Traceback (most recent call last):  
  File ""C:/temp/testpypy/mp_client.py"", line 7, in &lt;module&gt;
  m.connect()
  File ""C:\Python\pypy3-v6.0.0-win32\lib-python\3\multiprocessing\managers.py"", line 455, in connect
    conn = Client(self._address, authkey=self._authkey)
  File ""C:\Python\pypy3-v6.0.0-win32\lib-python\3\multiprocessing\connection.py"", line 493, in Client
    answer_challenge(c, authkey)
  File ""C:\Python\pypy3-v6.0.0-win32\lib-python\3\multiprocessing\connection.py"", line 732, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File ""C:\Python\pypy3-v6.0.0-win32\lib-python\3\multiprocessing\connection.py"", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File ""C:\Python\pypy3-v6.0.0-win32\lib-python\3\multiprocessing\connection.py"", line 407, in _recv_bytes
    buf = self._recv(4)
  File ""C:\Python\pypy3-v6.0.0-win32\lib-python\3\multiprocessing\connection.py"", line 386, in _recv
    buf.write(chunk)
TypeError: 'str' does not support the buffer interface
</code></pre>

<p>if I try to connect to it from a CPython interpreter (which is my ultimate goal) I get the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""c:\Python\3.5.4.2\WinPython\python-3.5.4.amd64\lib\multiprocessing\managers.py"", line 455, in connect
     conn = Client(self._address, authkey=self._authkey)
  File ""c:\Python\3.5.4.2\WinPython\python-3.5.4.amd64\lib\multiprocessing\connection.py"", line 493, in Client
     answer_challenge(c, authkey)
  File ""c:\Python\3.5.4.2\WinPython\python-3.5.4.amd64\lib\multiprocessing\connection.py"", line 737, in answer_challenge
     response = connection.recv_bytes(256)        # reject large message
  File ""c:\Python\3.5.4.2\WinPython\python-3.5.4.amd64\lib\multiprocessing\connection.py"", line 218, in recv_bytes
     self._bad_message_length()
  File ""c:\Python\3.5.4.2\WinPython\python-3.5.4.amd64\lib\multiprocessing\connection.py"", line 151, in _bad_message_length
     raise OSError(""bad message length"")
OSError: bad message length
</code></pre>
","2864877","","","Error connecting to PyPy3 multiprocessing remote manager","<python><multiprocessing><python-3.5><pypy>","1","0","2410"
"50647904","2018-06-01 16:49:47","0","","<p>How about</p>

<pre><code>df.loc[:, [f'column_{i}' for i in range(1, 61)] + ['column_81']]
</code></pre>

<p>or</p>

<pre><code>df.reindex([f'column_{i}' for i in range(1, 61)] + ['column_81'], axis=1)
</code></pre>

<p>if you want to fill missing columns, if there are, with default <code>NaN</code> values.</p>
","5665690","5665690","2018-06-01 18:39:25","2","316","tarashypka","2015-12-10 18:24:56","3870","309","4278","169","50647832","","2018-06-01 16:44:20","6","682","<p>Suppose I want to select a range of columns from a dataframe: Call them 'column_1' through 'column_60'. I know I could use loc like this:
<code>df.loc[:, 'column_1':'column_60']</code>
That will give me all rows in columns 1-60. </p>

<p>But what if I wanted that range of columns plus 'column_81'. This <i>doesn't</i> work:
<code>df.loc[:, 'column_1':'column_60', 'column_81']</code></p>

<p>It throws a ""Too many indexers"" error. 
Is there another way to state this using loc? Or is loc even the best function to use in this case? </p>

<p>Many thanks.</p>
","9782611","9209546","2018-06-01 20:47:52","Can you use loc to select a range of columns plus a column outside of the range?","<python><python-3.x><pandas><dataframe>","3","1","562"
"50647997","2018-06-01 16:56:16","0","","<p>You can do this efficiently with <code>pandas</code>.</p>

<p>This example uses <code>str.split</code> and regex to apply the necessary formatting.</p>

<pre><code>import pandas as pd
import re

def formatter(t):
    return re.sub('[0-9]','', t[0]), int(t[1])

def converter(x):
    return dict(formatter(i.split(':')) for i in x.split('+++'))

# read file
df = pd.read_csv('file.csv', sep='|')

# apply manipulations
res = df[['Day']].join(df['Asset Allocation'].apply(converter).apply(pd.Series))\
                 .fillna(0).astype(int)

print(res)

   Day  DAX  FTSE  NYSE  STOXX
0    0   94   143   100      0
1    1   95   143   103      0
2    2   97     0   102      0
3    3   97     0   102    102
</code></pre>
","9209546","","","2","725","jpp","2018-01-12 14:47:22","109049","18235","7890","3496","50646878","50647997","2018-06-01 15:36:45","2","40","<p>I have a CSV file in (roughly) the following format:</p>

<pre><code>Day   |  Asset Allocation     
0     |  NYSE:100+++FTSE100:143+++DAX30:94
1     |  NYSE:103+++FTSE97:143+++DAX30:95
2     |  NYSE:102+++DAX30:97
3     |  NYSE:102+++DAX30:97+++STOXX:102
</code></pre>

<p>So all assets are summarized in one column and separated by ""+++"". The order of the assets within the column can change as the CSV file progresses. I would like to rearrange the data to the following format:</p>

<pre><code>Day  |  NYSE  | FTSE  |  DAX  |  STOXX
0    |  100   | 143   | 94    |  0
1    |  103   | 143   | 95    |  0
2    |  102   | 0     | 97    |  0
3    |  102   | 0     | 97    |  102
</code></pre>

<p>Preferably, I would like to be able to do it in Python, just because I do have some prior experience and might be able to find my way around more quickly. However, as the final dataset is likely to contain several million rows, I would of course be open to employ any other way that can handle the data more efficiently. </p>

<p>I would greatly appreciate if someone could point me in the right direction. Thanks!</p>
","3397807","","","Extract column headers from cell and rearrange columns accordingly","<python><pandas><csv><data-manipulation>","1","0","1118"
"50648055","2018-06-01 17:01:07","1","","<p>In Python a <code>dict</code> is not ordered. You can sort the items by the order of their key with:</p>

<pre><code>for key, value in sorted(request.form.items()): 
    ...
</code></pre>

<p>If you need a specific order, you can specify a key function to <a href=""https://docs.python.org/3.3/library/functions.html#sorted"" rel=""nofollow noreferrer"">sorted</a></p>
","1388292","","","0","368","Jacques Gaudin","2012-05-10 23:01:38","8213","656","1988","169","50647914","50648055","2018-06-01 16:50:28","0","26","<p>I want to print user-entered values to a file but every time I run it again, even with the same data (or no data at all), they are in a different order.  This is the part of my .py file that does this:</p>

<pre><code>for key, value in request.form.items():
    data = ""%s=%s\n"" % (key, shlex.quote(value))
    configfile.write(bytes(data, 'UTF-8'))
</code></pre>

<p>Here is an example of one output: (the first 3 lines of 40)</p>

<pre><code> IP=''
 cloud_radio=NO
 TO=''
</code></pre>

<p>Here is another example where I entered the exact same data: (again the first 3 lines out of 41)</p>

<pre><code>key=''
port2=''
IP=''
</code></pre>

<p>Is it possible to set the order that they output in? OR make sure that the order is the same each time?</p>
","7275070","","","How is the order of request.form.items() determinded?","<python><html><forms><request.form>","1","1","756"
"50648074","2018-06-01 17:02:09","0","","<p>After handling the default case, which submits a null query, I was able to fix my issue. Below is the corrected code.</p>

<pre><code>def search_bar(request):
term = request.GET.get('term')

if term:
    results = objCW.search(term)
    context = {'results': results}
    return render(request, 'uccx/search.html', context)
else:
    context = {'': ''}
    return render(request, 'uccx/search.html', context)
</code></pre>
","7292978","","","0","426","t0bi","2016-12-13 20:01:12","41","9","4","0","50630464","50648074","2018-05-31 18:16:01","2","48","<p>I am having some difficulty retrieving the user submitted data from the form. I am able to hard code a 'term' and successfully search the database, but with the current code I have now, I receive a MultiValueDictKeyError when the results are dynamically populated. So, I am wondering what approach I should use to handle the line: ""term = request.GET['term']"". </p>

<h1>views.py</h1>

<pre><code>    def search(self, request):
    try:
        r = requests.get(
            self.URL + 'company/contacts?childConditions=communicationItems/value=' + request,
            headers=self.Header)
        r.raise_for_status()
    except:
        print(r.text)
        raise
    return r.json()

def search_bar(request):
    term = request.GET['term']
    results = objCW.search(term)
    context = {'results': results}
    return render(request, 'uccx/search.html', context)
</code></pre>

<h1>urls.py</h1>

<pre><code>urlpatterns = [
path('', views.table_content, name='table_content'),
path('search_form', views.search_bar, name='search_bar')]
</code></pre>

<h1>search.html</h1>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;html&gt;
  &lt;head&gt;
    &lt;meta charset=""utf-8""&gt;
    &lt;title&gt;SEARCH&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
  &lt;form action="""" method=""get""&gt;
      &lt;input type=""text"" name=""term""&gt;
      &lt;input type=""submit"" value='Search'&gt;
  &lt;/form&gt;
  {% if results %}
  Found the following items:
  &lt;table class=""table""&gt;
    &lt;thead&gt;
    &lt;tr&gt;
        &lt;th&gt;First Name&lt;/th&gt;
        &lt;th&gt;Last Name&lt;/th&gt;
        &lt;th&gt;Company&lt;/th&gt;
        &lt;th&gt;E-Mail&lt;/th&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    {% for result in results %}
    &lt;tr&gt;
        &lt;td&gt; {{ result.id }}&lt;/td&gt;
        &lt;td&gt;{{ result.lastName }}&lt;/td&gt;
        &lt;td&gt; {{ result.company.name }}&lt;/td&gt;
        &lt;td&gt; {{ result.communicationItems.0.value }}&lt;/td&gt;
    &lt;/tr&gt;
    {% endfor %}
    {% endif %}
    &lt;/tbody&gt;
  &lt;/table&gt;
  &lt;/body&gt;
&lt;/html&gt;</code></pre>
</div>
</div>
</p>
","7292978","","","Basic Django Search Implementation","<python><html><django>","1","3","2321"
"50648093","2018-06-01 17:03:17","0","","<p>I use <strong>os</strong> to do it right now with this script:</p>

<pre><code>#!venv/bin/python
import os

pybabel = 'venv/bin/pybabel'
os.system(pybabel + ' extract -F babel.cfg -k lazy_gettext -o messages.pot app')
os.system(pybabel + ' update -i messages.pot -d app/translations')
os.unlink('messages.pot')
</code></pre>

<p>Hope it give you an idea</p>
","7080573","","","1","361","Jonathan Arias","2016-10-27 13:19:55","125","13","7","1","45485057","56350520","2017-08-03 13:03:46","1","216","<p>Right now, I’m extracting messages using</p>

<pre><code>pybabel extract -F babel.cfg -o messages.pot .
</code></pre>

<p>This walks through all my Python files and extracts messages correctly.  However, I call this through <code>subprocess.call()</code>, which is pretty ugly, given PyBbel is also written in Python.</p>

<p>I took a look into PyBabel, and it uses setuptools comands to do its work. I could copy the <code>extract_messages.run()</code> method to my Python script, but it doesn’t feel too elegant.  Is there a better way to do it?  There are tons of articles on how to create new setuptools commands, but no one writes about invoking them…</p>
","1305139","","","Extract messages programmatically with PyBabel","<python><setuptools><babel>","2","0","664"
"50648095","2018-06-01 17:03:19","0","","<p>I figured it out. You can just modify the test to expect an error as follows:</p>

<pre><code>Compare different numbers
   Run Keyword And Expect Error  *  I compare ${num_a} with ${num_b}
</code></pre>

<p>Alternatively to look for a specific error (rather than wildcard):</p>

<pre><code>Compare different numbers
       Run Keyword And Expect Error  ValueError: Numbers are not the same.  I compare ${num_a} with ${num_b}
</code></pre>

<p>Note the double space between the command, error and keyword.</p>
","6191539","","","0","512","Catsunami","2016-04-12 05:42:10","316","43","16","9","50647581","50648095","2018-06-01 16:23:45","0","1235","<p>We use robot framework at work to do some automated testing, and I need to add a few tests. This format is already in the repo, I cannot change anything drastically.</p>

<p>I am using a keywords file that looks like this:</p>

<pre><code>#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from robot.api.deco import keyword

class testkeywords(object):
   """"""
   """"""

   def __init__(self):
      pass

   @keyword('I compare ${first_number} with ${second_number}')
   def compare(self, first_number, second_number):
      if (int(first_number) != int(second_number)):
         raise ValueError('Numbers are not the same.')
</code></pre>

<p>The .robot file has two tests, one that passes and one that fails:</p>

<pre><code>*** Settings ***
Library         testkeywords.py

*** Variables ***
${num_a}       5
${num_b}       6

*** Test Cases ***
Compare same numbers
   I compare ${num_a} with ${num_a}

Compare different numbers
   I compare ${num_a} with ${num_b}
</code></pre>

<p>The <code>Compare different numbers</code> test fails as expected, but it's still a FAIL. How can I set it to expect a failure and hence pass the keyword?</p>
","6191539","","","How to expect a failure in robot framework in Python?","<python><robotframework>","1","3","1145"
"50648124","2018-06-01 17:05:39","1","","<p>How about sorting the output using a <a href=""https://blog.codinghorror.com/sorting-for-humans-natural-sort-order/"" rel=""nofollow noreferrer"">natural sort</a>:</p>

<pre><code>import re
_nsre = re.compile(r'(\d+)')
def natural_sort_key(s):
    return [int(text) if text.isdigit() else text.lower()
            for text in re.split(_nsre, s)]

s = ""1 10 2 100 101 102 103 104 105 106 107 108 109 11 110 111 112 113 114 115 116 117 118 119 12 120 121 122 123 124 125 126 127 128 129 13 130 131 132 133 134 135 136 137 138 139 14 140 141 142 143 144 145 146 147 148 149 15 150 151 152 153 154 155 156 157 158 159 16 17 18 19 20 200c 21 22 23 24 25 26 260 261 262 263 264 265fs 266fs 267c 268c 269c 27 273c 274c 275c 276c 28 29 2c 30 302 31 32c 33c 34 35c 36 37 38 3c 4 5 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 524 6 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 7 701 702 703 704 705 706 707 708 709 710 8 801 802 803 804 805 806 807 808 809 810 9 901 902 S1 S10 S11 S12 S13 S14 S15 S16 S17 S18 S19 S2 S20 S3 S4 S5 S6 S7 S8 S9""
list1 = s.split(' ')
list1.sort(key=natural_sort_key)
</code></pre>

<p>Output <code>list1</code>:</p>

<pre><code>['1',  '2',  '2c',  '3c',  '4',  '5',  '6',  '7',  '8',  '9',  '10',  '11',  '12',  '13',  '14',  '15',  '16',  '17',  '18',  '19',  '20',  '21',  '22',  '23',  '24',  '25',  '26',  '27',  '28',  '29',  '30',  '31',  '32c',  '33c',  '34',  '35c',  '36',  '37',  '38',  '100',  '101',  '102',  '103',  '104',  '105',  '106',  '107',  '108',  '109',  '110',  '111',  '112',  '113',  '114',  '115',  '116',  '117',  '118',  '119',  '120',  '121',  '122',  '123',  '124',  '125',  '126',  '127',  '128',  '129',  '130',  '131',  '132',  '133',  '134',  '135',  '136',  '137',  '138',  '139',  '140',  '141',  '142',  '143',  '144',  '145',  '146',  '147',  '148',  '149',  '150',  '151',  '152',  '153',  '154',  '155',  '156',  '157',  '158',  '159',  '200c',  '260',  '261',  '262',  '263',  '264',  '265fs',  '266fs',  '267c',  '268c',  '269c',  '273c',  '274c',  '275c',  '276c',  '302',  '501',  '502',  '503',  '504',  '505',  '506',  '507',  '508',  '509',  '510',  '511',  '512',  '513',  '514',  '515',  '516',  '517',  '518',  '519',  '520',  '521',  '522',  '524',  '601',  '602',  '603',  '604',  '605',  '606',  '607',  '608',  '609',  '610',  '611',  '612',  '613',  '614',  '615',  '616',  '617',  '618',  '619',  '620',  '621',  '622',  '623',  '701',  '702',  '703',  '704',  '705',  '706',  '707',  '708',  '709',  '710',  '801',  '802',  '803',  '804',  '805',  '806',  '807',  '808',  '809',  '810',  '901',  '902',  'S1',  'S2',  'S3',  'S4',  'S5',  'S6',  'S7',  'S8',  'S9',  'S10',  'S11',  'S12',  'S13',  'S14',  'S15',  'S16',  'S17',  'S18',  'S19',  'S20'] 
</code></pre>
","8291949","","","1","2836","wp78de","2017-07-11 20:24:46","11473","1220","983","586","50645204","50645573","2018-06-01 14:03:59","3","118","<p>I have the following in Manager:</p>

<pre><code>class NullIf(Func):
template = ""NULLIF(%(expressions)s, '')""

class MySiteManager(models.Manager):

def get_queryset(self):
    qws = MySiteQuerySet(self.model, using=self._db).filter(
        some_id=settings.BASE_SOME_ID).annotate(
            # This is made for sorting by short labels as by numeric values
            short_label_numeric=Cast(
                NullIf(Func(
                    F('short_label'),
                    Value('^(\D+)|(\w+)'),
                    Value(''),
                    Value('g'),
                    function='regexp_replace')),
                models.BigIntegerField())
            ).order_by('short_label_numeric', 'short_label')

    for q in qws:
        print(q.short_label, end='\n')

    return qws
</code></pre>

<p>Output of print values looks like:</p>

<p>1
10
100
101
102
103
104
105
106
107
108
109
11
110
111
112
113
114
115
116
117
118
119
12
120
121
122
123
124
125
126
127
128
129
13
130
131
132
133
134
135
136
137
138
139
14
140
141
142
143
144
145
146
147
148
149
15
150
151
152
153
154
155
156
157
158
159
16
17
18
19
20
200c
21
22
23
24
25
26
260
261
262
263
264
265fs
266fs
267c
268c
269c
27
273c
274c
275c
276c
28
29
2c
30
302
31
32c
33c
34
35c
36
37
38
3c
4
5
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
524
6
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
7
701
702
703
704
705
706
707
708
709
710
8
801
802
803
804
805
806
807
808
809
810
9
901
902
S1
S10
S11
S12
S13
S14
S15
S16
S17
S18
S19
S2
S20
S3
S4
S5
S6
S7
S8
S9</p>

<p>And my question:
How to build queryset with output looks like e.g. 1 2 3 3c 4 5 6 6c ... 264 265fs 266fs 267c 268c 269c ... S1 S2 S3 S4 ??? Does someone have any assumptions?</p>
","6360146","","","Get sorted queryset by specified field with regex in django","<python><regex><django><django-models><django-queryset>","2","0","1816"
"50648153","2018-06-01 17:07:55","4","","<p>Fix the datetime string - it must match exactly. Read <a href=""https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior"" rel=""nofollow noreferrer"">the documentation</a>.</p>

<pre><code>""%Y-%m-%d-%H-%M-%S""
</code></pre>

<p>Your string <code>'%d-%b-%Y-%H-%M-%S'</code> is parsing day, month name abreviated (locale aware), year, H-M-S.</p>

<p>If you need to validate to 0 before the months/days etc, combine your parsing with a regex check:</p>

<pre><code>import re
import datetime

def validate(date_text):
    """"""Validates the overall structure with regex and parses the datetime using 
    strptime to test for ""existing"" months and times. """"""
    try:
        dt = datetime.datetime.strptime(date_text, '%Y-%m-%d-%H-%M-%S')
        if re.match(r""\d{4}-\d{2}-\d{2}-\d{2}-\d{2}-\d{2}"", date_text) is None:
            raise ValueError()
    except ValueError:
        raise ValueError(""Incorrect data format, should be YYYY-MM-DD-HH-MI-SS"")    

v = [""2018-06-01-10-20-30"", ""2018-6-01-10-20-30"", ""2018-21-01-10-20-30""]
for k in v:
    try:
        print(""Validating: "", k)
        validate(k)
        print(""ok"")
    except ValueError as e:
        print(e)
</code></pre>

<p>Output:</p>

<pre><code>Validating:  2018-06-01-10-20-30
ok
Validating:  2018-6-01-10-20-30       # missing 0
Incorrect data format, should be YYYY-MM-DD-HH-MI-SS
Validating:  2018-21-01-10-20-30      # no 21 month possible
Incorrect data format, should be YYYY-MM-DD-HH-MI-SS
</code></pre>
","7505395","7505395","2018-06-01 18:16:19","1","1492","Patrick Artner","2017-02-02 10:46:51","30736","5120","3506","4713","50648096","50648153","2018-06-01 17:03:21","-1","63","<p>How can I validate this date time in python 3 - <code>2018-05-30-16-54-00</code> ?</p>

<p>When I pass this date text to below method , an error is returned.</p>

<pre><code>def validate(date_text):
    try:
        datetime.datetime.strptime(date_text, '%d-%b-%Y-%H-%M-%S')
    except ValueError:
        raise ValueError(""Incorrect data format, should be YYYY-MM-DD-HH-MI-SS"")     
</code></pre>
","1042646","","","Python3 Date Validation","<python><python-3.x>","1","8","401"
"50648201","2018-06-01 17:11:00","-1","","<p>Your problems is with the colons at the end </p>

<blockquote>
  <p>:</p>
</blockquote>
","9010507","9010507","2018-06-01 17:14:55","2","91","GILO","2017-11-26 13:41:18","34","20","37","0","50648111","50648214","2018-06-01 17:04:35","0","48","<p>I am running an elif loop but whenever i run this line of code it gives me the error message I can't find the syntax error here</p>

<pre><code>elif (rand_num-10)&lt;=guess_num&lt;=rand_num or rand_num&lt;=guess_num&lt;=(rand_num+10)
</code></pre>

<p>the error message is </p>

<blockquote>
  <p>elif (rand_num-10)&lt;=guess_num&lt;=rand_num or rand_num&lt;=guess_num&lt;=(rand_num+10)
                                                                                  SyntaxError: invalid syntax</p>
</blockquote>
","9799632","789671","2018-06-01 17:08:15","python syntax error comparision operator","<python>","2","3","518"
"50648214","2018-06-01 17:12:19","2","","<p>You missed the <code>:</code> after elif and condition.</p>

<pre><code>rand_num = 2
guess_num = 3
rand_num = 4


if rand_num &lt; 3:
    pass
elif (rand_num - 10) &lt;= guess_num &lt;= rand_num or rand_num &lt;= guess_num &lt;= (rand_num + 10):
    print('working')
else:
    print('not working')

# prints working
</code></pre>
","9731310","","","0","333","Ninja Warrior 11","2018-05-02 16:08:14","212","53","21","7","50648111","50648214","2018-06-01 17:04:35","0","48","<p>I am running an elif loop but whenever i run this line of code it gives me the error message I can't find the syntax error here</p>

<pre><code>elif (rand_num-10)&lt;=guess_num&lt;=rand_num or rand_num&lt;=guess_num&lt;=(rand_num+10)
</code></pre>

<p>the error message is </p>

<blockquote>
  <p>elif (rand_num-10)&lt;=guess_num&lt;=rand_num or rand_num&lt;=guess_num&lt;=(rand_num+10)
                                                                                  SyntaxError: invalid syntax</p>
</blockquote>
","9799632","789671","2018-06-01 17:08:15","python syntax error comparision operator","<python>","2","3","518"
"50648222","2018-06-01 17:12:35","0","","<p>Cross origin is enabled by default on Flask-SocketIO. My guess is that the way you are testing this is flawed. While running your example application, I can send a request to the main Socket.IO endpoint and I do get the <code>Access-Control-Allow-Origin</code> header in the response:</p>

<pre><code>~ $ http http://localhost:5000/socket.io/
HTTP/1.1 200 OK
Access-Control-Allow-Credentials: true
Access-Control-Allow-Origin: *
Connection: keep-alive
Content-Length: 119
Content-Type: application/octet-stream
Date: Fri, 01 Jun 2018 17:10:01 GMT
Set-Cookie: io=dd8d67788df54510830fea64bc82b1fd



+-----------------------------------------+
| NOTE: binary data not shown in terminal |
+-----------------------------------------+
</code></pre>
","904393","","","8","747","Miguel","2011-08-21 07:21:52","46780","6860","362","5","50632723","50648222","2018-05-31 21:01:06","0","1068","<p>I wrote this simple flask-socketio code:</p>

<pre><code>from flask import Flask
from flask_socketio import SocketIO, send

app = Flask(__name__)
app.config['SECRET_KEY'] = 'mysecret'

socketio = SocketIO(app)


@socketio.on('message')
def handle_message(msg):
    print 'Message:' + msg
    send(msg, broadcast=True)


if __name__ == '__main__':
    socketio.run(app)
</code></pre>

<p>When I see chrome network analyzing, I can see the ""Access-Control-Allow-Origin"" value as <code>null</code>.</p>

<p>According to <code>Flask-socketio</code> documentation: (See <code>API Reference</code> @ <a href=""http://flask-socketio.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">http://flask-socketio.readthedocs.io/en/latest/</a>)</p>

<pre><code>Parameters:
...
cors_allowed_origins – List of origins that are allowed to connect to this server. All origins are allowed by default.
</code></pre>

<p>Another suggestion I found on searching is using <code>flask-CORS</code>:</p>

<pre><code>app.config['SECRET_KEY'] = 'mysecret'
cors = CORS(app)

socketio = SocketIO(app)
</code></pre>

<p>I get the same result.</p>

<p>What is a way to allow Cross-Origin requests with flask-socketio?</p>

<p>Thanks in advance.</p>
","1798362","","","Flask-socketio - failed to set ""Access-Control-Allow-Origin"" response header","<python><flask><cross-domain><flask-socketio>","2","0","1220"
"50648266","2018-06-01 17:16:50","6","","<p>Here is another version (Python-side mostly) which works well with JupyterLab:</p>

<pre><code>from time import time
from IPython import get_ipython
from IPython.display import Audio, display


class Beeper:

    def __init__(self, threshold, **audio_kwargs):
        self.threshold = threshold
        self.start_time = None    # time in sec, or None
        self.audio = audio_kwargs

    def pre_execute(self):
        if not self.start_time:
            self.start_time = time()

    def post_execute(self):
        end_time = time()
        if self.start_time and end_time - self.start_time &gt; self.threshold:
            audio = Audio(**self.audio, autoplay=True)
            display(audio)
        self.start_time = None


beeper = Beeper(5, url='http://www.soundjay.com/button/beep-07.wav')

ipython = get_ipython()
ipython.events.register('pre_execute', beeper.pre_execute)
ipython.events.register('post_execute', beeper.post_execute)
</code></pre>

<p>The beep will automatically be emitted after each code execution which took more than 5 seconds, but the consecutive executions are not counted together.</p>

<p>For example:</p>

<pre><code># cell 0:
from time import sleep
# cell 1:
sleep(6)    # will ring
</code></pre>

<p>If you then add another cell</p>

<pre><code># cell 3:
sleep(3)    # it won't ring
</code></pre>

<p>Tested with JupyterLab 0.32.1 and Jupyter notebook 5.5.0. </p>

<p><strong>Edit:</strong> to reduce the clutter of the shown audio players I use following snippet (for Python older than 3.6 you need to use <code>.format()</code> instead of f-strings):</p>

<pre><code>from IPython.display import Audio, display


class InvisibleAudio(Audio):
    def _repr_html_(self):
        audio = super()._repr_html_()
        audio = audio.replace('&lt;audio', f'&lt;audio onended=""this.parentNode.removeChild(this)""')
        return f'&lt;div style=""display:none""&gt;{audio}&lt;/div&gt;'
</code></pre>

<p>and then use <code>InvisibleAudio</code> instead of <code>Audio</code> in <code>post_execute</code>.</p>
","6646912","6646912","2018-09-18 09:53:02","2","2045","krassowski","2016-07-27 21:37:06","1920","171","595","5","17323336","20806969","2013-06-26 14:45:22","31","12014","<p>I often run long-running cells in my IPython notebook. I'd like the notebook to automatically beep or play a sound when the cell is finished executing. Is there some way to do this in iPython notebook, or maybe some command I can put at the end of a cell that will automatically play a sound? </p>

<p>I'm using Chrome if that makes any difference.</p>
","939259","247482","2016-05-04 11:48:41","Automatically play sound in IPython notebook","<python><ipython><ipython-notebook>","7","3","356"
"50648309","2018-06-01 17:20:16","0","","<p>It's hard to debug that without the the full source but I'm going to make a guess: On line 158 did you mean to use ""Potential_Rejections"" rather than ""Reasons_For_rejection""?</p>
","6506855","","","0","182","jemiah","2016-06-24 01:47:39","32","10","0","0","50646292","","2018-06-01 15:02:24","0","25","<p><a href=""https://i.stack.imgur.com/HF3mk.png"" rel=""nofollow noreferrer"">My code</a></p>

<p>I am trying to add the attributes to this text file on a new line, without it erasing previous data, so I am able to print the entire list of sets of data at the end</p>

<p><a href=""https://i.stack.imgur.com/uEBBx.png"" rel=""nofollow noreferrer"">Issue with my code</a></p>

<p>as you can see, my code is not working due to the way in which I am trying to add and I have tried 5 different ways around this including adding the array to another array, to a dictionary and the raw array to the text file but none worked.</p>
","9881852","","","Attempting to add properties from an array into a text file in python?","<python><arrays><text-files>","2","1","617"
"50648389","2018-06-01 17:27:21","0","","<p>If someone is lookin for a python3 answer here you go:</p>

<pre><code>import urllib.request
    req = urllib.request.urlopen(""http://en.wikipedia.org/w/api.php?action=parse&amp;page=China&amp;format=json&amp;prop=text"")
    print(req.read())
</code></pre>

<p>I'm using python version 3.7.0b4.</p>
","4799446","","","0","302","Gothburz","2015-04-17 04:31:08","2352","452","123","37","10570969","10571067","2012-05-13 10:22:22","1","2171","<p>I've been trying to parse a wikipedia page in Python and have been quite successful using the API. </p>

<p>But, somehow the API documentation seems a bit too skeletal for me to get all the data. 
As of now, I'm doing a requests.get() call to</p>

<pre><code>http://en.wikipedia.org/w/api.php?action=query&amp;prop=extracts&amp;titles=China&amp;format=json&amp;exintro=1
</code></pre>

<p>But, this only returns me the first paragraph. Not the entire page. I've tried to use allpages and search but to no avail. A better explanation of how to get the data from a wiki page would be of real help. All the data and not just the introduction as returned by the previous query. </p>
","105167","","","How to parse a wikipedia page in Python?","<python>","3","0","682"
"50648391","2018-06-01 17:27:36","0","","<p>Had this same question and follow the <code>lambda</code> example from @Ron Norris's comment. However, the documentation isn't clear about how you can find the <code>child_element</code> relative to the <code>ancestor_element</code>, even when using <code>lambda</code>.</p>

<p>You can actually replace <code>driver</code> in the <code>WebDriverWait</code> call with your <code>ancestor_element</code>, to explicitly only search things ""under"" the <code>ancestor_element</code> (even though the docstring states that it has to be an instance of <code>WebDriver</code>, I found that a <code>WebElement</code> also works).</p>

<p>So I wound up with something like:</p>

<pre><code>child_element =  WebDriverWait(ancestor_element,10).until(
    EC.presence_of_element_located((By.XPATH, child_xpath))
)
</code></pre>

<p>If you want to use a <code>lambda</code>, you could then do:</p>

<pre><code>child_element =  WebDriverWait(ancestor_element,10).until(
    lambda x: x.find_element_by_xpath(child_xpath)
)
</code></pre>
","1370384","","","0","1026","user","2012-05-02 15:08:14","2687","187","1136","14","46963104","","2017-10-26 20:18:39","0","702","<p>Using Selenium WebDriver with Python 3.4. </p>

<p>I'm writing a scraper, and locating elements using XPaths relative to some non-root ancestor element, such as below:</p>

<pre><code>ancestor_element = driver.find_element_by_xpath(ancestor_xpath)
child_element = ancestor_element.find_element_by_xpath(child_xpath)
</code></pre>

<p>This works as expected. However, I am unsure how to do this relative location with an explicit wait call, as the examples I have seen use this syntax: </p>

<pre><code>child_element =  WebDriverWait(driver,10).until(
    EC.presence_of_element_located((By.XPATH, child_xpath))
)
</code></pre>

<p>which appears to evaluate the XPath against the page root, and throws an error complaining about the "".//"" beginning of the XPath string.</p>

<p>Any advice on this?</p>
","","","","Selenium explicit wait with relative XPath locator","<python><python-3.x><selenium><xpath><selenium-webdriver>","1","5","804"
"50648406","2018-06-01 17:28:42","0","","<p>As the name suggests, <strong>UnboundLocalErrors</strong> are only raised when improperly referencing an unassigned local variable.</p>

<p>In most cases this will occur when trying to modify a local variable before it is actually assigned within the local scope.</p>

<p>I am sure that you didn't assign 'Reason for rejection'....</p>

<p>Pls check the code </p>
","8766071","","","0","367","Michael Yadidya","2017-10-12 14:55:33","553","39","10","0","50646292","","2018-06-01 15:02:24","0","25","<p><a href=""https://i.stack.imgur.com/HF3mk.png"" rel=""nofollow noreferrer"">My code</a></p>

<p>I am trying to add the attributes to this text file on a new line, without it erasing previous data, so I am able to print the entire list of sets of data at the end</p>

<p><a href=""https://i.stack.imgur.com/uEBBx.png"" rel=""nofollow noreferrer"">Issue with my code</a></p>

<p>as you can see, my code is not working due to the way in which I am trying to add and I have tried 5 different ways around this including adding the array to another array, to a dictionary and the raw array to the text file but none worked.</p>
","9881852","","","Attempting to add properties from an array into a text file in python?","<python><arrays><text-files>","2","1","617"
"50648424","2018-06-01 17:29:37","5","","<p>My answer is based on the <a href=""https://github.com/keras-team/keras/issues/5794#issuecomment-303683985"" rel=""nofollow noreferrer"">comment of Keras GH issue</a>. It calculates validation precision and recall at every epoch for a onehot-encoded classification task. Also please look at this <a href=""https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras"">SO answer</a> to see how it can be done with <code>keras.backend</code> functionality.</p>

<pre class=""lang-py prettyprint-override""><code>import keras as keras
import numpy as np
from keras.optimizers import SGD
from sklearn.metrics import precision_score, recall_score

model = keras.models.Sequential()
# ...
sgd = SGD(lr=0.001, momentum=0.9)
model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])


class Metrics(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self._data = []

    def on_epoch_end(self, batch, logs={}):
        X_val, y_val = self.validation_data[0], self.validation_data[1]
        y_predict = np.asarray(model.predict(X_val))

        y_val = np.argmax(y_val, axis=1)
        y_predict = np.argmax(y_predict, axis=1)

        self._data.append({
            'val_recall': recall_score(y_val, y_predict),
            'val_precision': precision_score(y_val, y_predict),
        })
        return

    def get_data(self):
        return self._data


metrics = Metrics()
history = model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), callbacks=[metrics])
metrics.get_data()
</code></pre>
","495656","495656","2019-02-14 07:29:51","1","1569","vogdb","2010-11-03 08:18:13","3262","119","419","0","43076609","50544931","2017-03-28 17:53:43","38","38434","<p>I am building a multi-class classifier with Keras 2.02 (with Tensorflow backend)，and I do not know how to calculate precision and recall in Keras. Please help me.</p>
","4787786","","","How to calculate precision and recall in Keras","<python><precision><keras><precision-recall>","5","0","170"
"50648425","2018-06-01 17:29:42","0","","<pre><code>a = pd.DataFrame({'description': ['foo', 'bar', 'bas', 'foo bar', 
'foobar', 'bar baz', 'bazbar']})
a['tag'] = a.description.apply(lambda x: ', '.join(x.split()))
print(a)

description    tag
foo            foo
bar            bar
bas            bas
foo bar        foo, bar
foobar         foobar
bar baz        bar, baz
bazbar         bazbar
</code></pre>

<p>A <code>.apply(lambda...)</code> is a nice paradigm in pandas that will go over every row and perform a function.  Here I'm making a list out of space-delimitated words in the description column via <code>.split()</code> then converting it back into a string with a <code>,</code> separation via <code>.join()</code></p>
","3281513","","","1","691","dylanjf","2014-02-06 21:31:10","116","12","239","0","50647756","","2018-06-01 16:38:12","0","59","<p>So I have a very large pandas dataframe consisting of transaction data:</p>

<pre><code>description   amount
foo           10
bar           5
baz           9
foo bar       12
foobar        15
bar baz       20
bazbar        19
</code></pre>

<p>Expected output is as follows:</p>

<pre><code>description   amount    tag
foo           10        foo 
bar           5         bar
baz           9         baz
foo bar       12        foo, bar
foobar        15        foobar 
bar baz       20        bar, baz
bazbar        19        bazbar
</code></pre>

<p>My thought process is as follows:</p>

<ul>
<li>Create a huge list of unique <code>words</code> from all rows in <code>df['description']</code></li>
<li>Then for the new column <code>df['tag']</code>, for each item in list, do a <code>isin</code> and assign the relevant tag.</li>
</ul>

<p>I'm not sure how to create the list of unique <code>words</code>. Plus, I'm not sure this is the correct approach to solve this problem.</p>

<p>Appreciate any help!</p>
","5236124","5236124","2018-06-01 16:53:51","Text classification in pandas (python)","<python><pandas><text><classification>","1","13","1015"
"50648437","2018-06-01 17:31:01","0","","<p>I would like to explain why there is no <code>ACCOUNT_USERNAME_MAX_LENGTH</code>. If you open source code you will see that <code>max_length</code> validator comes from <code>username</code> model field <a href=""https://github.com/pennersr/django-allauth/blob/330bf899dd77046fd0510221f3c12e69eb2bc64d/allauth/account/forms.py#L277"" rel=""nofollow noreferrer"">https://github.com/pennersr/django-allauth/blob/330bf899dd77046fd0510221f3c12e69eb2bc64d/allauth/account/forms.py#L277</a></p>

<pre><code>username_field.max_length = get_username_max_length()
</code></pre>

<p>Where <code>get_username_max_length</code> is function that actually pulls <code>max_length</code> value from <code>User</code> model <a href=""https://github.com/pennersr/django-allauth/blob/8fbbf8c1d32832d72de5ed1c7fd77600af57ea6f/allauth/utils.py#L64"" rel=""nofollow noreferrer"">https://github.com/pennersr/django-allauth/blob/8fbbf8c1d32832d72de5ed1c7fd77600af57ea6f/allauth/utils.py#L64</a></p>

<pre><code>def get_username_max_length():
    from .account.app_settings import USER_MODEL_USERNAME_FIELD
    if USER_MODEL_USERNAME_FIELD is not None:
        User = get_user_model()
        max_length = User._meta.get_field(USER_MODEL_USERNAME_FIELD).max_length
    else:
        max_length = 0
    return max_length
</code></pre>

<p><strong>First approach:</strong> So you could change <code>max_length</code> value directly on your <code>User</code>'s model <code>username</code> field if you have it swapped. </p>

<p>I don't think overriding form fields or <code>__init__</code> method will actually work the it suggested by other answers, because assign of <code>max_length</code> happens in subclass of <code>ACCOUNT_SIGNUP_FORM_CLASS</code> <a href=""https://github.com/pennersr/django-allauth/blob/330bf899dd77046fd0510221f3c12e69eb2bc64d/allauth/account/forms.py#L259"" rel=""nofollow noreferrer"">https://github.com/pennersr/django-allauth/blob/330bf899dd77046fd0510221f3c12e69eb2bc64d/allauth/account/forms.py#L259</a> </p>

<pre><code>class BaseSignupForm(_base_signup_form_class()):
</code></pre>

<p>where <code>_base_signup_form_class</code> is function that gets your <code>ACCOUNT_SIGNUP_FORM_CLASS</code></p>

<p><strong>Second approach:</strong> is to subclass <code>SignupView</code> and override it's <code>SignupForm</code> read <a href=""https://stackoverflow.com/questions/20923776/override-signup-view-django-allauth"">Override signup view django-allauth</a> and <a href=""https://stackoverflow.com/questions/12303478/how-to-customize-user-profile-when-using-django-allauth"">How to customize user profile when using django-allauth</a></p>

<p>In that <code>SignupForm</code> you could actually do what @MehdiB or @PeterSobhi suggested. </p>

<p><code>ImproperlyConfigured</code> issue occurs because of <a href=""https://github.com/pennersr/django-allauth/issues/1792"" rel=""nofollow noreferrer"">https://github.com/pennersr/django-allauth/issues/1792</a></p>

<p>So you be sure that these forms are defined in different python modules as per <a href=""https://github.com/pennersr/django-allauth/issues/1749#issuecomment-304628013"" rel=""nofollow noreferrer"">https://github.com/pennersr/django-allauth/issues/1749#issuecomment-304628013</a></p>

<pre><code># base/forms.py 
# this is form that your ACCOUNT_SIGNUP_FORM_CLASS is points to
class BaseSignupForm(forms.Form):

    captcha = ReCaptchaField(
        public_key=config(""RECAPTCHA_PUBLIC_KEY""),
        private_key=config(""RECAPTCHA_PRIVATE_KEY""),
    )

    class Meta:
        model = User

    def signup(self, request, user):
        """""" Required, or else it throws deprecation warnings """"""
        pass

# data1/forms.py
# this is your signup form
from django.core.validators import MaxLengthValidator
from allauth.account.forms import SignupForm

class MySignupForm(SignupForm):
    def __init__(self, *args, **kwargs):
        super(MySignupForm, self).__init__(*args, **kwargs)
        self.fields['username']['validators'] += MaxLengthValidator(150, ""Username should be less than 150 character long"")

# views.py

from allauth.account.views import SignupView
class MySignupView(SignupView):
    form_class = MySignupForm

# urls.py

url(r""^signup/$"", MySignupView.as_view(), name=""account_signup""),
</code></pre>
","3627387","3627387","2018-06-02 05:32:06","8","4268","Sardorbek Imomaliev","2014-05-12 06:58:14","9993","744","72","1","50548685","50941366","2018-05-27 03:06:23","1","472","<p>I'm using Django allauth as my user account framework for my django site. The docs show there is an <a href=""http://django-allauth.readthedocs.io/en/latest/configuration.html"" rel=""nofollow noreferrer"">ACCOUNT_USERNAME_MIN_LENGTH</a> however there is no <code>ACCOUNT_USERNAME_MAX_LENGTH</code> for some reason. </p>

<p>Is there any way to create a max length for username?</p>

<p>Here's my custom allauth signup form - maybe I can do something here?:</p>

<pre><code>class AllauthSignupForm(forms.Form):

    captcha = ReCaptchaField(
        public_key=config(""RECAPTCHA_PUBLIC_KEY""),
        private_key=config(""RECAPTCHA_PRIVATE_KEY""),
    )

    class Meta:
        model = User

    def signup(self, request, user):
        """""" Required, or else it throws deprecation warnings """"""
        pass
</code></pre>

<p><strong>Edit:</strong> Trying to subclass SignupView</p>

<p><strong>draft1/forms.py</strong></p>

<pre><code>class AllauthSignupForm(SignupForm):
    def __init__(self, *args, **kwargs):
        super(AllauthSignupForm, self).__init__(*args, **kwargs)
        self.fields['username']['validators'] += MaxLengthValidator(150,
                                                                    ""Username should be less than 150 character long"")

    captcha = ReCaptchaField(
        public_key=config(""RECAPTCHA_PUBLIC_KEY""),
        private_key=config(""RECAPTCHA_PRIVATE_KEY""),
    )

    class Meta:
        model = User

    def signup(self, request, user):
        """""" Required, or else it throws deprecation warnings """"""
        pass
</code></pre>

<p><strong>draft1/views.py</strong></p>

<pre><code>from allauth.account.views import SignupView

class MySignupView(SignupView):
    form_class = AllauthSignupForm
</code></pre>

<p><strong>allauth/account/urls.py</strong></p>

<pre><code>url(r""^signup/$"", MySignupView.as_view(), name=""account_signup""),
</code></pre>

<p><strong>draft1/settings.py</strong></p>

<pre><code>ACCOUNT_SIGNUP_FORM_CLASS = 'draft1.forms.AllauthSignupForm'
</code></pre>

<p>The above code returns this error:</p>

<pre><code>Traceback (most recent call last):
  File ""/Users/zorgan/Desktop/postr1/lib/python3.5/site-packages/django/utils/autoreload.py"", line 228, in wrapper
    fn(*args, **kwargs)
  File ""/Users/zorgan/Desktop/postr1/lib/python3.5/site-packages/django/core/management/commands/runserver.py"", line 125, in inner_run
    self.check(display_num_errors=True)
  File ""/Users/zorgan/Desktop/postr1/lib/python3.5/site-packages/django/core/management/base.py"", line 359, in check
    include_deployment_checks=include_deployment_checks,
  File ""/Users/zorgan/Desktop/postr1/lib/python3.5/site-packages/django/core/management/base.py"", line 346, in _run_checks
    return checks.run_checks(**kwargs)
  File ""/Users/zorgan/Desktop/postr1/lib/python3.5/site-packages/django/core/checks/registry.py"", line 81, in run_checks
    new_errors = check(app_configs=app_configs)
  File ""/Users/zorgan/Desktop/postr1/lib/python3.5/site-packages/django/core/checks/urls.py"", line 16, in check_url_config
    return check_resolver(resolver)
  File ""/Users/zorgan/Desktop/postr1/lib/python3.5/site-packages/django/core/checks/urls.py"", line 26, in check_resolver
    return check_method()
  File ""/Users/zorgan/Desktop/postr1/lib/python3.5/site-packages/django/urls/resolvers.py"", line 254, in check
    for pattern in self.url_patterns:
  File ""/Users/zorgan/Desktop/postr1/lib/python3.5/site-packages/django/utils/functional.py"", line 35, in __get__
    res = instance.__dict__[self.name] = self.func(instance)
  File ""/Users/zorgan/Desktop/postr1/lib/python3.5/site-packages/django/urls/resolvers.py"", line 405, in url_patterns
    patterns = getattr(self.urlconf_module, ""urlpatterns"", self.urlconf_module)
  File ""/Users/zorgan/Desktop/postr1/lib/python3.5/site-packages/django/utils/functional.py"", line 35, in __get__
    res = instance.__dict__[self.name] = self.func(instance)
  File ""/Users/zorgan/Desktop/postr1/lib/python3.5/site-packages/django/urls/resolvers.py"", line 398, in urlconf_module
    return import_module(self.urlconf_name)
  File ""/Users/zorgan/Desktop/postr1/lib/python3.5/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""&lt;frozen importlib._bootstrap&gt;"", line 986, in _gcd_import
  File ""&lt;frozen importlib._bootstrap&gt;"", line 969, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 958, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 673, in _load_unlocked
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 665, in exec_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 222, in _call_with_frames_removed
  File ""/Users/zorgan/Desktop/vorsso/venvor/draft1/urls.py"", line 6, in &lt;module&gt;
    from . import views
  File ""/Users/zorgan/Desktop/vorsso/venvor/draft1/views.py"", line 11, in &lt;module&gt;
    from .forms import UserSettingsForm
  File ""/Users/zorgan/Desktop/vorsso/venvor/draft1/forms.py"", line 8, in &lt;module&gt;
    from allauth.account.forms import SignupForm
  File ""/Users/zorgan/Desktop/postr1/lib/python3.5/site-packages/allauth/account/forms.py"", line 228, in &lt;module&gt;
    class BaseSignupForm(_base_signup_form_class()):
  File ""/Users/zorgan/Desktop/postr1/lib/python3.5/site-packages/allauth/account/forms.py"", line 216, in _base_signup_form_class
    fc_classname))
django.core.exceptions.ImproperlyConfigured: Module ""draft1.forms"" does not define a ""AllauthSignupForm"" class
</code></pre>
","6733153","6733153","2018-06-02 02:01:24","How to add max_length to allauth username","<python><django><django-allauth>","5","1","5561"
"50648449","2018-06-01 17:31:48","9","","<p>The problem was that I didn't specify a value input option when I updated my cells. In my case, the solution looks like this: <code>worksheet.update_cells(cell_list, value_input_option='USER_ENTERED')</code> Note the value_input_option flag, it must be set to <code>'USER_ENTERED'</code> so that cells are updated just as if they were entered in the Google Sheets UI.</p>
","8404760","","","4","375","lvkv","2017-08-02 09:58:02","101","3","0","0","50648249","50648449","2018-06-01 17:15:01","2","1278","<p>Right now, I'm using gspread and the Google Sheets API to update cell values, setting <code>cell.value</code> equal to a string of a specific formula.</p>

<p>Example code:</p>

<pre><code># Calculates sum of cells in current row from column B to H
G_SHEETS_ROW_SUM_COMMAND = '''=SUM(INDIRECT(CONCATENATE(""B"",ROW(),"":H"",ROW())))'''

for cell in cell_list:
    cell.value = G_SHEETS_ROW_SUM_COMMAND
</code></pre>

<p>When my spreadsheet is populated, however, my command is prefixed with an apostrophe (presumably to keep the cell from being interpreted as a formula, though that's exactly what I'd like it to do). </p>

<p>Here's an example from my spreadsheet:</p>

<p><a href=""https://i.stack.imgur.com/w886h.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w886h.png"" alt=""enter image description here""></a></p>

<p>Is there a way to remove this apostrophe automatically?</p>

<p>I've looked into value rendering options and input_value, though these options seem to be unavailable for writing to sheets.</p>
","8404760","9337071","2018-06-06 22:51:53","Updating cell values with formulas results in apostrophe prefixes with Sheets API","<python><google-spreadsheet-api><google-sheets-api><gspread>","1","0","1033"
"50648491","2018-06-01 17:34:20","0","","<p>Now sure what you meant by sub arrays, but sounds like you want all the permutations of the contents in a given array. Which can be found here:</p>

<p><a href=""https://stackoverflow.com/questions/104420/how-to-generate-all-permutations-of-a-list-in-python"">How to generate all permutations of a list in Python</a></p>
","9154512","","","0","322","Shen Huang","2017-12-29 21:39:55","6","9","0","0","50648443","50648495","2018-06-01 17:31:24","-2","911","<p>How to find all the subarrays of a given array in the fastest possible way?
for eg:a=[1,2,3,4,5]
The purpose of question is to large array input and find all the possible saubarrays</p>
","8369288","4244136","2018-06-01 23:38:59","find all subarrays of a given array in python","<python><arrays><python-3.x><arraylist>","2","1","189"
"50648495","2018-06-01 17:34:32","0","","<pre><code>def sub_lists(my_list):
    subs = [[]]
    for i in range(len(my_list)):
        n = i+1
        while n &lt;= len(my_list):
            sub = my_list[i:n]
            subs.append(sub)
            n += 1

    return subs
</code></pre>
","7975242","","","0","247","Nandish Patel","2017-05-07 05:48:21","91","15","4","0","50648443","50648495","2018-06-01 17:31:24","-2","911","<p>How to find all the subarrays of a given array in the fastest possible way?
for eg:a=[1,2,3,4,5]
The purpose of question is to large array input and find all the possible saubarrays</p>
","8369288","4244136","2018-06-01 23:38:59","find all subarrays of a given array in python","<python><arrays><python-3.x><arraylist>","2","1","189"
"50648540","2018-06-01 17:38:30","0","","<p><code>to_csv</code> returns <code>None</code> that's why you got that error
to maintain formatted_file, you could try this,</p>

<pre><code>final_file=formatted_file.copy()
</code></pre>

<p>or</p>

<pre><code>final_file=pd.read_csv('out.csv')
</code></pre>
","4684861","","","4","261","Mohamed Thasin ah","2015-03-18 10:49:38","4803","833","1351","258","50648516","","2018-06-01 17:36:43","1","625","<p>I am attempting to create an upload tool that takes an .xls file and then converts it to a pandas dataframe before finally saving it as a csv file to be processed and analyzed. After the file comes out of this code: </p>

<pre><code>    def xls_to_csv(data):
        #Formats into pandas dataframe. Index removes first column of .xls file.
        formatted_file = pd.read_excel(data, index_col=0)
        #Converts the formatted file into a csv file and saves it.
        final_file = formatted_file.to_csv('out.csv')
</code></pre>

<p>It saves properly and in the right location, however when I attempt to plug the resulted file into other functions that contain loops, I raise a TypeError: 'NoneType' object is not iterable.</p>

<p>The file is saved as 'out.csv' and I am able to open it manually, however the open command won't even work without this error being raised.</p>

<p>Using Python 3.6!</p>

<p>Thanks in advanced! </p>
","9543107","","","Python/Pandas to_csv saving as NoneType and raising TypeError","<python><django><python-3.x><pandas><data-management>","3","1","938"
"50648562","2018-06-01 17:39:55","1","","<p>I had some issues with the tabbing which I had to fix but I'm going to assume those were from copying it over or from Stack Overflow.</p>

<p>Other than that, everything seems to be working. </p>

<p><strong>Just run the program from the command line with the command:</strong><br>
<code>python YourFileName.py 12345</code>  </p>

<p>where 12345 is the zip code you want to query.</p>

<p>Make sure you are using Python3 not ipython as mentioned in the comments.</p>
","1830793","1830793","2018-06-01 17:48:39","4","470","Zev","2012-11-16 20:51:03","2334","215","463","115","50648393","","2018-06-01 17:27:48","-2","113","<p>Full disclosure: Python rookie. Trying to use code shared by other people to scrape the internet page (real estate listing) by tweaking the arguments. My questions are probably very rudimentary, so if you can provide some links for further reading and study, that will be great too. I just can't seem to figure out how to proceed from this step. Thanks for your time in advance.</p>

<pre><code>from lxml import html
import requests
import unicodecsv as csv
import argparse

def parse(zipcode,filter=None):

if filter==""newest"":
    url = ""https://www.zillow.com/homes/for_sale/{0}/0_singlestory/days_sort"".format(zipcode)
elif filter == ""cheapest"":
    url = ""https://www.zillow.com/homes/for_sale/{0}/0_singlestory/pricea_sort/"".format(zipcode)
else:
    url = ""https://www.zillow.com/homes/for_sale/{0}_rb/?fromHomePage=true&amp;shouldFireSellPageImplicitClaimGA=false&amp;fromHomePageTab=buy"".format(zipcode)

for i in range(5):
    # try:
    headers= {
                'accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'accept-encoding':'gzip, deflate, sdch, br',
                'accept-language':'en-GB,en;q=0.8,en-US;q=0.6,ml;q=0.4',
                'cache-control':'max-age=0',
                'upgrade-insecure-requests':'1',
                'user-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'
    }
    response = requests.get(url,headers=headers)
    print(response.status_code)
    parser = html.fromstring(response.text)
    search_results = parser.xpath(""//div[@id='search-results']//article"")
    properties_list = []

    for properties in search_results:
        raw_address = properties.xpath("".//span[@itemprop='address']//span[@itemprop='streetAddress']//text()"")
        raw_city = properties.xpath("".//span[@itemprop='address']//span[@itemprop='addressLocality']//text()"")
        raw_state= properties.xpath("".//span[@itemprop='address']//span[@itemprop='addressRegion']//text()"")
        raw_postal_code= properties.xpath("".//span[@itemprop='address']//span[@itemprop='postalCode']//text()"")
        raw_price = properties.xpath("".//span[@class='zsg-photo-card-price']//text()"")
        raw_info = properties.xpath("".//span[@class='zsg-photo-card-info']//text()"")
        raw_broker_name = properties.xpath("".//span[@class='zsg-photo-card-broker-name']//text()"")
        url = properties.xpath("".//a[contains(@class,'overlay-link')]/@href"")
        raw_title = properties.xpath("".//h4//text()"")

        address = ' '.join(' '.join(raw_address).split()) if raw_address else None
        city = ''.join(raw_city).strip() if raw_city else None
        state = ''.join(raw_state).strip() if raw_state else None
        postal_code = ''.join(raw_postal_code).strip() if raw_postal_code else None
        price = ''.join(raw_price).strip() if raw_price else None
        info = ' '.join(' '.join(raw_info).split()).replace(u""\xb7"",',')
        broker = ''.join(raw_broker_name).strip() if raw_broker_name else None
        title = ''.join(raw_title) if raw_title else None
        property_url = ""https://www.zillow.com""+url[0] if url else None 
        is_forsale = properties.xpath('.//span[@class=""zsg-icon-for-sale""]')
        properties = {
                        'address':address,
                        'city':city,
                        'state':state,
                        'postal_code':postal_code,
                        'price':price,
                        'facts and features':info,
                        'real estate provider':broker,
                        'url':property_url,
                        'title':title
        }
        if is_forsale:
            properties_list.append(properties)
    return properties_list
    # except:
    #   print (""Failed to process the page"",url)

if __name__==""__main__"":
    argparser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)
    argparser.add_argument('zipcode',help = '')
    sortorder_help = """"""
    available sort orders are :
    newest : Latest property details,
    cheapest : Properties with cheapest price
    """"""
argparser.add_argument('sort',nargs='?',help = sortorder_help,default ='Homes For You')
    args = argparser.parse_args()
    zipcode = args.zipcode
    sort = args.sort
    print (""Fetching data for %s""%(zipcode))
    scraped_data = parse(zipcode,sort)
    print (""Writing data to output file"")
    with open(""properties-%s.csv""%(zipcode),'wb')as csvfile:
        fieldnames = ['title','address','city','state','postal_code','price','facts and features','real estate provider','url']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for row in  scraped_data:
            writer.writerow(row)
</code></pre>

<p>I usually don't expect the code to work from the get go and was ready to quick. After I ran it, the error appears:
<a href=""https://i.stack.imgur.com/t2vHC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t2vHC.png"" alt=""System Exit""></a></p>

<p>I know the first one was to make me run ""%tb"" but I don't know how to deal with the second one, where shall I run 'exit' or 'quit'?</p>

<p>After I ran ""%tb"", following messages appear:</p>

<pre><code>SystemExit                                Traceback (most recent call last)
&lt;ipython-input-29-dcd1916da548&gt; in &lt;module&gt;()
     76     """"""
     77         argparser.add_argument('sort',nargs='?',help = sortorder_help,default ='Homes For You')
---&gt; 78         args = argparser.parse_args()
     79         zipcode = args.zipcode
     80         sort = args.sort

C:\Users\AppData\Local\Continuum\Anaconda3\lib\argparse.py in parse_args(self, args, namespace)
   1731         if argv:
   1732             msg = _('unrecognized arguments: %s')
-&gt; 1733             self.error(msg % ' '.join(argv))
   1734         return args
   1735 

C:\Users\AppData\Local\Continuum\Anaconda3\lib\argparse.py in error(self, message)
   2387         self.print_usage(_sys.stderr)
   2388         args = {'prog': self.prog, 'message': message}
-&gt; 2389         self.exit(2, _('%(prog)s: error: %(message)s\n') % args)

C:\Users\AppData\Local\Continuum\Anaconda3\lib\argparse.py in exit(self, status, message)
   2374         if message:
   2375             self._print_message(message, _sys.stderr)
-&gt; 2376         _sys.exit(status)
   2377 
   2378     def error(self, message):

SystemExit: 2
</code></pre>

<p>What shall I do with this? Is there something that I shall do in the command line to fix it?</p>

<p>Thanks again</p>
","5336013","4799172","2018-06-01 17:31:17","System Exit appeared when scraping webpage (zillow) using existing Python code. How to cope with them?","<python>","1","4","6631"
"50648574","2018-06-01 17:40:58","4","","<p>What about this:</p>

<pre><code>image = [[[1, 2, 3], [4,5,6]], [[7,8,9], [10, 11, 12]], [[13,14,15], [16,17,18]]]
result = [x for z in zip(*image) for x in zip(*z)]
print(result)
</code></pre>

<p>Output:</p>

<pre class=""lang-none prettyprint-override""><code>[(1, 7, 13), (2, 8, 14), (3, 9, 15), (4, 10, 16), (5, 11, 17), (6, 12, 18)]
</code></pre>
","1782792","1782792","2018-06-01 17:50:55","4","354","jdehesa","2012-10-29 11:43:40","37080","2442","2562","26","50648436","50648574","2018-06-01 17:30:57","2","24","<p>I have an image coded as follows: <code>image[RGBchannel][y][x]</code></p>

<p>I would like to access the <code>RGBchannel</code> triple for each index. This is the method I'm currently using. Is there a more efficient way to write it?</p>

<pre><code>image = [[[1, 2, 3], [4,5,6]], [[7,8,9], [10, 11, 12]], [[13,14,15], [16,17,18]]]

for y in range(len(image[0]):
    for x in range(len(image[0][0]):
        rgb = []
        for channel in range(len(image)):
            rgb.append(image[channel][y][x])
        print rgb
</code></pre>

<p>And output is:</p>

<pre><code>[1, 7, 13]
[2, 8, 14]
[3, 9, 15]
[4, 10, 16]
[5, 11, 17]
[6, 12, 18]
</code></pre>

<p>Without importing any 3rd party libraries or importing any internal modules, is there a better way to do this?</p>
","823859","","","Accessing same index of embedded lists","<python>","2","1","778"
"50648582","2018-06-01 17:41:28","4","","<p>No, with <code>typing</code> (PEP 484 and PEP 526) you can't declare that a specific key is to be present in a container. All that you can declare are the <em>types</em>, not the runtime contents. <code>'x'</code> is a <em>specific string value</em>, not a type.</p>

<p>I know you specifically don't want to have to do this, but the perhaps the answer <em>is</em> to consider a different data structure, such as a named tuple or a dataclass, where you can specify the attribute a type has.</p>

<p>This is what the typescript declaration does, really:</p>

<blockquote>
  <p>The <code>printLabel</code> function has a single parameter that requires that the object passed in has a property called <code>label</code> of type string.</p>
</blockquote>

<p>Python attributes are the moral equivalent of Typescript object properties. That Typescript object notation and Python dictionaries have a lot in common perhaps confuses matters, but you should not look upon Typescript object declarations as anything but classes, when trying to map concepts to Python.</p>

<p><code>mypy</code> does include extensions to <code>typing</code> which fall outside of the Python type hinting standard. This includes the <a href=""https://mypy.readthedocs.io/en/latest/more_types.html#typeddict"" rel=""nofollow noreferrer""><code>mypy_extensions.TypeDict</code> object</a>, which works a lot like a <a href=""https://docs.python.org/3/library/typing.html#typing.TypeVar"" rel=""nofollow noreferrer""><code>typing.TypeVar</code> declaration</a>:</p>

<pre><code>from mypy_extensions import TypedDict

SomeDict = TypeDict('SomeDict', {'x': str})

def f(d: SomeDict) -&gt; None:
    x = d['x']
    print(x)
</code></pre>

<p>or using a class-based syntax:</p>

<pre><code>class SomeDict(TypeDict)
    x: str
</code></pre>

<p>Keys in a <code>TypeDict</code> declaration are either all required, or all optional (when you set <code>total=False</code> on the declaration); you'd have to use inheritance to produce a type with some keys optional, see the documentation linked. </p>

<p>But be aware that <code>TypeDict</code> is <em>experimental</em> and could be dropped again or altered drastically in a future mypy release, and is not available in other Python type-hint checkers.</p>
","100297","100297","2018-09-20 11:49:51","2","2262","Martijn Pieters","2009-05-03 14:53:57","770256","252083","5762","19510","50648538","50648582","2018-06-01 17:38:19","3","588","<p>Suppose I have a function which takes a dictionary as a parameter:</p>

<pre><code>def f(d: dict) -&gt; None:
    x = d[""x""]
    print(x)
</code></pre>

<p>Can I specify that this dictionary <em>must</em> have the key <code>""x""</code> to mypy? I'm looking for something similar to <a href=""https://www.typescriptlang.org/docs/handbook/interfaces.html"" rel=""nofollow noreferrer"">interface from typescript</a>, without changing <code>d</code> to a class.</p>

<p>The reason I don't want to change <code>d</code> to a class, is because I am modifying a large existing codebase to add <code>mypy</code> type checking and this dictionary is used in many places. I would have to modify a lot of code if I had to change all instances of <code>d[""x""]</code> to <code>d.x</code>.</p>
","755934","100297","2018-06-01 17:40:08","mypy set dictionary keys / interface","<python><python-3.x><type-hinting><mypy>","2","0","778"
"50648613","2018-06-01 17:44:46","0","","<p>PatternFills use colours in add odd way: the colours refer to the colours in a pattern; when the fill is solid only the foreground colour, <code>fgColor</code>, is used.</p>

<p>From the specification:</p>

<blockquote>
  <p>This element is used to specify cell fill information for pattern and
  solid color cell fills. For solid cell fills (no pattern), fgColor is
  used. For cell fills with patterns specified, then the cell fill color
  is specified by the bgColor element.</p>
</blockquote>
","2385133","","","0","500","Charlie Clark","2013-05-15 09:17:31","11508","2698","229","1497","50648065","","2018-06-01 17:01:33","0","162","<p>I'm trying to fill range with RGB #00B0F0 but everytime I'm getting all black row like this.<a href=""https://i.stack.imgur.com/L0R5L.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L0R5L.png"" alt=""enter image description here""></a>
Here is the code.</p>

<pre><code>maxrow = 5
    maxcol = 17
    # #my_blue = openpyxl.styles.colors.Color(rgb='00FF0000')
    for colNo in range(1,maxcol+1):
        for rowNo in range(1,maxrow+1):
            worksheet.cell(row=rowNo, column=colNo).fill = PatternFill(bgColor='50FFC7CE',fill_type='solid')
            worksheet.cell(row = rowNo, column=colNo).font = Font(bold =True)
</code></pre>

<p>Please help as I want to fill the range with the same RGB mentioned</p>
","9466677","1033581","2019-02-10 14:47:26","Openpyxl Color formatting bg_colour","<python><openpyxl>","1","0","730"
"50648615","2018-06-01 17:44:48","1","","<p><a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.isclose.html"" rel=""nofollow noreferrer""><code>numpy.isclose</code></a> is close (pun intended) to what you want. I evaluates
the formula:</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>absolute(a - b) &lt;= (atol + rtol * absolute(b))</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>So you could use that to filter your data, with <code>atol</code> the smallest absolute difference you want to still consider and <code>rtol</code> set at 3%.</p>
","7207392","","","0","537","Paul Panzer","2016-11-24 23:39:00","37645","7185","734","1","50648329","","2018-06-01 17:21:49","-2","124","<p>I am comparing numbers from 2 dictionaries (total comparisons ~ 1M). Here is a code snippet:</p>

<pre><code>for i in dict1:
    val1 = dict[i]
    val2 = dict2[i]

    if (val1 != 0.000):
        perctg_diff = (val1 - val2)/val1 * 100
        if perctg_diff &gt; 3.0:
            dict3.update({i:(val1,val2,perctg_diff)})
    if (val2 !=0.000):
        perctg_diff = (val2 - val1)/val2 * 100
        if perctg_diff &gt; 3.0:
            dict3.update({i:(val1,val2,perctg_diff)})
</code></pre>

<p>I am finding percentage difference and writing the difference when more than 3% in <code>dict3</code>. After execution of script, I found some of the numbers in <code>dict3</code> are </p>

<pre><code>(1052712, (2.88541545330242e-33, 2.3194405728563e-27, 99.9998755986471))
(1052713, (8.1367737331018e-34, 7.83224080670401e-31, 99.8961118033279))
(1052715, (1.79168848952333e-33, 6.71766997709614e-31, 99.733287211841))
(1052717, (1.03397638198887e-25, 4.49948480152819e-26, 56.4836791255002))
(1400879, (0.0, 1.39114642689358e-36, 100.0))
(1290291, (0.0, 1.89369462623834e-20, 100.0))
</code></pre>

<p>What is effective/efficient way I can get rid of the numerical roundoff and ignore the comparison when numbers are these small?</p>

<p>(Using python 2.7 with numpy)</p>
","9329547","","","Python how to handle very small numbers","<python><python-2.7><numpy>","1","5","1275"
"50648639","2018-06-01 17:46:29","2","","<p>Here is a similar way with zipping twice.</p>

<pre><code>image = [
    [[1, 2, 3], [4, 5, 6]],
    [[7, 8, 9], [10, 11, 12]],
    [[13, 14, 15], [16, 17, 18]],
]

image_TT = [[*x] for z in zip(*map(zip, *image)) for x in z]
# [[1, 7, 13], [4, 10, 16], [2, 8, 14], [5, 11, 17], [3, 9, 15], [6, 12, 18]]
in_order = image_TT[::2] + image_TT[1::2]
# [[1, 7, 13], [2, 8, 14], [3, 9, 15], [4, 10, 16], [5, 11, 17], [6, 12, 18]]
</code></pre>
","4585963","4585963","2018-06-01 17:51:37","0","440","hilberts_drinking_problem","2015-02-19 22:15:40","7219","620","1491","135","50648436","50648574","2018-06-01 17:30:57","2","24","<p>I have an image coded as follows: <code>image[RGBchannel][y][x]</code></p>

<p>I would like to access the <code>RGBchannel</code> triple for each index. This is the method I'm currently using. Is there a more efficient way to write it?</p>

<pre><code>image = [[[1, 2, 3], [4,5,6]], [[7,8,9], [10, 11, 12]], [[13,14,15], [16,17,18]]]

for y in range(len(image[0]):
    for x in range(len(image[0][0]):
        rgb = []
        for channel in range(len(image)):
            rgb.append(image[channel][y][x])
        print rgb
</code></pre>

<p>And output is:</p>

<pre><code>[1, 7, 13]
[2, 8, 14]
[3, 9, 15]
[4, 10, 16]
[5, 11, 17]
[6, 12, 18]
</code></pre>

<p>Without importing any 3rd party libraries or importing any internal modules, is there a better way to do this?</p>
","823859","","","Accessing same index of embedded lists","<python>","2","1","778"
"50648666","2018-06-01 17:48:45","0","","<p>Pandas <code>to_csv</code> function saves the file but does not return anything. To loop through the csv file later you'll have to change the code to look like this.</p>

<pre><code>formatted_file.to_csv('out.csv')
final_file = open('out.csv', 'r')
</code></pre>
","5434007","","","0","266","Augie Doebling","2015-10-11 17:38:28","21","4","32","0","50648516","","2018-06-01 17:36:43","1","625","<p>I am attempting to create an upload tool that takes an .xls file and then converts it to a pandas dataframe before finally saving it as a csv file to be processed and analyzed. After the file comes out of this code: </p>

<pre><code>    def xls_to_csv(data):
        #Formats into pandas dataframe. Index removes first column of .xls file.
        formatted_file = pd.read_excel(data, index_col=0)
        #Converts the formatted file into a csv file and saves it.
        final_file = formatted_file.to_csv('out.csv')
</code></pre>

<p>It saves properly and in the right location, however when I attempt to plug the resulted file into other functions that contain loops, I raise a TypeError: 'NoneType' object is not iterable.</p>

<p>The file is saved as 'out.csv' and I am able to open it manually, however the open command won't even work without this error being raised.</p>

<p>Using Python 3.6!</p>

<p>Thanks in advanced! </p>
","9543107","","","Python/Pandas to_csv saving as NoneType and raising TypeError","<python><django><python-3.x><pandas><data-management>","3","1","938"
"50648700","2018-06-01 17:52:11","0","","<p>An alternative to <code>import jinja2.ext</code> in the source file is to specifically include <code>jinja2.ext</code> in the setup.py:</p>

<pre><code>from cx_Freeze import setup,Executable

includefiles = [ 'templates\index.html']
includes = ['jinja2.ext']  # add jinja2.ext here
excludes = ['Tkinter']

setup(
name = 'index',
version = '0.1',
description = 'membership app',
author = 'Me',
author_email = 'me@me.com',
# Add includes to the options
options = {'build_exe':   {'excludes':excludes,'include_files':includefiles, 'includes':includes}},   
executables = [Executable('index.py')]
)
</code></pre>
","3520697","3520697","2018-06-01 18:05:59","0","612","auzn","2014-04-10 18:00:16","45","9","2","0","14041450","14069859","2012-12-26 13:44:31","8","2999","<p>I am using Flask to develop a python app. At the moment, I want this app to be run locally. It runs locally fine through python, but when I use cx_freeze to turn it into an exe for Windows, I can no longer use the Flask.render_template() method. The moment I try to execute a render_template,  I get an http 500 error, exactly as if the html template I'm trying to render does not exist.</p>

<p>The main python file is called index.py. At first I tried to run: <code>cxfreeze index.py</code>. This did not include the ""templates"" directory from the Flask project in the cxfreeze ""dist"" directory. So then I tried using this setup.py script and running <code>python setup.py build</code>. This now includes the templates folder and the index.html template, but I still get the http: 500 error when it tries to render the template.</p>

<pre><code>from cx_Freeze import setup,Executable

includefiles = [ 'templates\index.html']
includes = []
excludes = ['Tkinter']

setup(
name = 'index',
version = '0.1',
description = 'membership app',
author = 'Me',
author_email = 'me@me.com',
options = {'build_exe': {'excludes':excludes,'include_files':includefiles}}, 
executables = [Executable('index.py')]
)
</code></pre>

<p>Here is an example method from the script:</p>

<pre><code>@app.route('/index', methods=['GET'])
def index():
    print ""rendering index""
    return render_template(""index.html"")
</code></pre>

<p>If I run <code>index.py</code> then in the console I get:</p>

<pre><code> * Running on http://0.0.0.0:5000/
 rendering index
 127.0.0.1 - - [26/Dec/2012 15:26:41] ""GET / HTTP/1.1"" 200 -
 127.0.0.1 - - [26/Dec/2012 15:26:42] ""GET /favicon.ico HTTP/1.1"" 404 -
</code></pre>

<p>and the page is displayed correctly in my browser, but if I run <code>index.exe</code>, I get</p>

<pre><code> * Running on http://0.0.0.0:5000/
rendering index
127.0.0.1 - - [26/Dec/2012 15:30:57] ""GET / HTTP/1.1"" 500 -
127.0.0.1 - - [26/Dec/2012 15:30:57] ""GET /favicon.ico HTTP/1.1"" 404 -
</code></pre>

<p>and </p>

<pre><code>Internal Server Error

The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.
</code></pre>

<p>in my browser.</p>

<p>If I return raw html, e.g.</p>

<pre><code>@app.route('/index', methods=['GET'])
def index():
    print ""rendering index""
    return ""This works""
</code></pre>

<p>then it works fine. So a possible work around is to stop using Flask's templates and hardcode all the html logic into the main python file. This gets very messy though, so I'd like to avoid it if possible.</p>

<p>I'm using Python 2.7 32-bit, Cx_freeze for Python 2.7 32-bit, and Flask 0.9</p>

<p>Thanks for any help and ideas!</p>
","1081231","","","using cx_freeze on flask app","<python><flask><cx-freeze>","2","0","2742"
"50648721","2018-06-01 17:53:55","0","","<p>You can use pandas.concat():</p>

<p><code>pd.concat([df.loc[:,'column_1':'columns_60'],df.loc[:,'column_81']],axis=1)</code></p>
","6311483","","","1","133","Boergler","2016-05-09 16:44:01","121","5","31","0","50647832","","2018-06-01 16:44:20","6","682","<p>Suppose I want to select a range of columns from a dataframe: Call them 'column_1' through 'column_60'. I know I could use loc like this:
<code>df.loc[:, 'column_1':'column_60']</code>
That will give me all rows in columns 1-60. </p>

<p>But what if I wanted that range of columns plus 'column_81'. This <i>doesn't</i> work:
<code>df.loc[:, 'column_1':'column_60', 'column_81']</code></p>

<p>It throws a ""Too many indexers"" error. 
Is there another way to state this using loc? Or is loc even the best function to use in this case? </p>

<p>Many thanks.</p>
","9782611","9209546","2018-06-01 20:47:52","Can you use loc to select a range of columns plus a column outside of the range?","<python><python-3.x><pandas><dataframe>","3","1","562"
"50648726","2018-06-01 17:54:06","1","","<p>With Tensorflow for backend (I don't know much about Theano), using <a href=""https://www.tensorflow.org/api_docs/python/tf/gather_nd"" rel=""nofollow noreferrer""><code>tf.gather_nd()</code></a>:</p>

<pre class=""lang-python prettyprint-override""><code>import keras.backend as K
import tensorflow as tf

# `a` and `b` the numpy arrays defined in the question
A = tf.constant(a)
B = tf.constant(b)

# Obtaining your max indices over axis 1, which will be used as indices for axis 1 of A:
col_ind = K.argmax(B, axis=1)

# Creating row range, which will be used as indices for axis 0 of A:
row_ind = K.arange(col_ind.shape[0], dtype='int64')

# Stacking the indices together:
ind = K.stack((row_ind, col_ind), axis=-1)

# Gathering the results:
c = tf.gather_nd(A, ind) # no equivalent I know in K, and no idea about theano...

with tf.Session() as sess:
    print(c.eval())
    # [[2 7]
    #  [8 1]
    #  [5 7]
    #  [9 9]
    #  [3 6]
    #  [6 4]
    #  [6 5]]
</code></pre>
","624547","624547","2018-06-01 19:30:12","3","978","benjaminplanche","2011-02-19 17:06:24","8823","606","381","76","50648271","","2018-06-01 17:17:31","1","341","<p>I have 2 tensors <code>a</code> and <code>b</code> which have the following shapes</p>

<pre><code>&gt;&gt;K.int_shape(a)
(None, 5 , 2)
&gt;&gt;K.int_shape(b)
(None, 5)
</code></pre>

<p>What I want to get is a tensor <code>c</code> </p>

<pre><code>&gt;&gt;K.int_shape(c)
(None, 2)
</code></pre>

<p>such that along axis 0, you pick the index of largest element in <code>b</code> and use that to index <code>a</code> along axis 1.</p>

<p>Example - say I have </p>

<pre><code>a = np.array([[[2, 7],
    [6, 5],
    [9, 9],
    [4, 2],
    [5, 9]],

   [[8, 1],
    [8, 8],
    [3, 9],
    [9, 2],
    [9, 1]],

   [[3, 9],
    [6, 4],
    [5, 7],
    [5, 2],
    [5, 6]],

   [[7, 5],
    [9, 9],
    [9, 5],
    [9, 8],
    [5, 7]],

   [[6, 3],
    [1, 7],
    [3, 6],
    [8, 2],
    [3, 2]],

   [[6, 4],
    [5, 9],
    [8, 6],
    [5, 2],
    [5, 2]],

   [[2, 6],
    [6, 5],
    [3, 1],
    [6, 2],
    [6, 4]]])
</code></pre>

<p>and I have</p>

<pre><code>b = np.array([[ 0.27,  0.25,  0.23,  0.06,  0.19],
[ 0.3 ,  0.13,  0.17,  0.2 ,  0.2 ],
[ 0.08,  0.04,  0.40,  0.36,  0.12],
[ 0.3 ,  0.33,  0.11,  0.07,  0.19],
[ 0.15,  0.21,  0.30,  0.12,  0.22],
[ 0.3 ,  0.13,  0.23,  0.1 ,  0.23],
[ 0.26,  0.35 ,  0.25 ,  0.07,  0.07]])
</code></pre>

<p>What I expect <code>c</code> to be</p>

<pre><code>c = np.zeros((7,2))
for i in range(7):
    ind = np.argmax(b[i, :])
    c[i, :] = a[i, ind, :]
c
array([[ 2.,  7.],
   [ 8.,  1.],
   [ 5.,  7.],
   [ 9.,  9.],
   [ 3.,  6.],
   [ 6.,  4.],
   [ 6.,  5.]])
</code></pre>
","8453556","","","How do I index based on another array in Keras","<python><indexing><keras><theano>","2","3","1537"
"50648751","2018-06-01 17:56:07","0","","<p>I've been working on a similar project this month that uses shipyards, containers, and packages like yours. A solution that I've been using is just using self._id = id(self) to use pythons built in 'id' function. Also I see this is from last year so it may not be relevant for your project anymore</p>
","9882619","","","0","305","Skeet","2018-06-01 17:39:43","1","9","0","0","46924955","","2017-10-25 05:58:13","-1","457","<p>I'm trying to build a Shipyard function by using python linked list and class.
This is what I have right now:</p>

<pre><code>class Package:

    def __init__(self,owner,destination,weight):
    self.weight = weight # sets the package weight to the weight that is input
    self.destination = destination # sets the package destination to the destination that was input
    self.owner = owner # sets the package owner to the owner that was input
    self.ID = self.ID() # calls the ID function to make a unique ID
    self.nPack = None # used to singuly link list in a container

def ID(self):
    global packnum
    package == 100000 + packnum # adds the package number to 100000 to make the ID
    packnum += 1 # increases pack num by 1
    return package # returns the ID

def readIn(self,Id):
    self.ID = Id # sets the package.id to the sent id

class Container:

def __init__(self,P):
    self.destination = P.destination # makes the container desintaion to that of the first package
    self.weight = P.weight # makes the container weigh as much as the only package in the list
    self.fpackage = P # a pointer that points to the first package in the container
    self.ID = self.ID() # calls ID() to make a custom conainer ID
    self.nt = None # pointer that points to the next container in the yard
    self.lt = None # pointer that points to the previous container in the yard

def readIn(self,Id):
    self.ID = Id # sets the container ID to that of the file
    self.fpackage = None # makes it so there is no package in the container
    self.weight = 0 # sets teh weight to 0

def ID(self):
    global conNum # holds the number of container that exist + 1
    contain = 300000 + conNum # adds conNum to 300000 to make an ID
    conNum += 1 # increments conNum
    return contain #returns the ID

def maxweight(self,P):
    if (self.weight + P.weight) &gt; 2000: # if the package will put the container over the limit
        return 1 # returns true
    else:
        return 0 # if the package will fit returns false

def add(self,P,N):
    if self.fpackage == None: # if there are no packages in the container
        self.fpackage = P # makes the new package equal the head
        self.weight += P.weight # adds the weigh into the container
    elif P.weight &lt; self.fpackage.weight: # if the package is lighter than the current first package
        P.nPack = self.fpackage # makes the P.nPack point to the current head
        self.fpackage = P # makes P the new head
        self.weight += P.weight # adds the weight of P to the container
    elif N.nPack == None: # if the current package being looked at is the end of the list
        N.nPack = P # makes the end point to P making P the new end
        self.weight += P.weight # adds the weight to the container
    elif N.nPack.weight &gt;= P.weight: # if the next package is larger than the new package
        P.nPack = N.nPack # makes p point the the next package
        N.nPack = P # makes the current package point to the new package
        self.weight += P.weight # adds teh weight of P into the container
    else: # if none of the above are true
        self.add(P, N.nPack) # recurses forward in the list

def search(self,Id,pack):
    found = 0 # a variable used to indicate if the ID is found in the container
    noin = 0 # a variable used to indicate if the ID was not found in the container
    while found == 0 and noin == 0: # runs till one of teh two variable are make true
        if pack.ID == Id: # if the current package is the one being looked for
            found = 1 # makes found = 1
        elif pack.nPack == None: # if the current package being looked at is the last in the list
            noin = 1 # sets noin to 1
        else: # if neather of the above is true
            pack = pack.nPack # moves to the next package in the list
    if found == 0 and noin == 1: # if the package was not found
        return 2 # returns 2
    elif found == 1: # if the package was found in the container
        return 1 # returns 1

def remove(self,Id,pack):
    found = 0 # a variable used to indicate if the package was removed
    noin = 0 # a variable used to indicate if the package was not found in the current container
    while found == 0 and noin == 0: # while both variable are false
        if self.fpackage.ID == Id: # if the first package in the list is the target
            self.weight -= pack.weight # removes the weight of the package from the container
            self.fpackage = pack.nPack # makes the head point to the second package in the container
            found = 1 # sets found to 1
        elif pack.nPack == None: # if the current package is the end of the list
            noin = 1 # sets noin to 1
        elif pack.nPack.ID == Id: # if the next package is the target
            self.weight -= pack.nPack.weight # removes the targest weight from the container
            pack.nPack = pack.nPack.nPack #  makes the current package point to the package after the target
            found = 1 # sets found to 1
        else: # if none of the above are true
            pack = pack.nPack # moves to the next package in the list
    if found == 0 and noin == 1: # if the package was not found
        return 2 # returns 2
    elif found == 1: # if the package was found
        return 1 # returns 1

def writeall(self,pack,file):
    if pack != None: # if its not the last package
        file.write(str(pack.ID) + ', ' + pack.owner +', ' + str(pack.weight) +'\n') # creates a string of the ID, Ownerm weight of the package
        self.writeall(pack.nPack,file) # recurses to the next package in the list

def printall(self,pack):
    if pack != None: # if its not the last package in the container
        print ('  ' + str(pack.ID) + ' , ' + pack.owner + ' , ' + pack.destination + ' , ' + str(pack.weight)) # prints out the ID, owner and weight of the package
        self.printall(pack.nPack) # recureses to the next package in the list

class Shipyard:

def __init__(self):
    self.cCount = 0 # sets teh number of container to 0
    self.FContainer = None # sets the first container refrence to None

def readIn(self,file):
    n = file.readline()
    while n != '': # while not at the end of the file
        if n[0] == '%': # if the read in information is for a container
            line = [] # a blank list to put the good character into
            for x in n: # goes through all the letters
                if x.isalpha() == 1 or x.isdigit() or x == ' ': # checks if its a character to keep
                    line.append(x) # puts the character into the list
            l = ''.join(line) # reforms all the words from the individual charcters
            l = l.split(' ') # makes a list fo the values
            l.pop(0) # removes the blank value in the 0 spot of teh list
            dest = l[1] # makes the destination equal the input value(used when adding packages)
            tem = Package('temp',l[1].capitalize(),2000) # makes a temporary package that will make a new container
            temp = Container(tem) # makes a container with the temp package
            temp.readIn(int(l[0])) # makes the container empty but set to the destination
            self.place(temp,self.FContainer) # puts the container in its appropriate place in teh yard(alphabeticly by destination)
            n = file.readline() # reads the next line of the file
        else: # if the line was a package
            line = [] # a blank list to put the good character into
            for x in n: # goes through all the letters
                if x.isalpha() == 1 or x.isdigit() or x == ' ' or x == '_': # checks if its a character to keep
                    line.append(x) # puts the character into the list
            l = ''.join(line) # reforms all the words from the individual charcters
            l = l.split(' ') # makes a list of the read values
            if len(l) == 4: # if the file input is like the exapmle with the name seperated
                a = l[0] # sets a to the ID
                b = str(l[1] + '_' + l[2]) # combins the first and last name seperated by a _
                c = l[3] # makes c = weight
                l = [a,b,c] # remakes the list of values
            P = Package(l[1], dest.capitalize(), int(l[2])) # makes a package using the values in the list
            P.readIn(int(l[0])) # sets the package Id to that from the file
            self.fillContainer(P,self.FContainer) # adds the package into the appropriate container
            n = file.readline() # eads teh next line in the file

def place(self,P,F):
    if self.FContainer == None: # if there are no containers in the yard
        self.FContainer = P # sets the first container to the empty container
    elif P.destination &lt; self.FContainer.destination: # if the destaintion of the empty container comes before the first container
        self.FContainer.lt = P # make the current first container previous point to the empty container
        P.nt = self.FContainer # sets the empty conainers next pointer to point at the current first Container
        self.FContainer = P # sets the empty container to the yards first containers
    elif F.nt != None and P.destination &gt;= F.nt.destination: # if the destination of the empty container comes after the next container in the list
        self.place(P,F.nt) # recurse so with the next container in the list being the current
    elif F.nt != None and P.destination &lt; F.nt.destination: # if the next containers destination comes after the new containers destination
        F.nt.lt = P # sets the next containers previous pointer to point to the new container
        P.nt = F.nt # sets the new container's next pointer to point to the next container
        P.lt = F # sets the new container's previous pointer to point to the current container
        F.nt = P  # sets the current container's next pointer to point to the new container
    elif F.nt == None: # if the current container is the end of the list
        F.nt = P # makes the current container's next pointer point to the new container
        P.lt = F # makes the new container's previous pointer point to the current container

def Search(self,Id,dest,cont):
    found = 0 # a variable that is used to keep track if the package is found
    while cont.destination == dest and found == 0: # if the current container's destination is the same as the input one and the package is not found
        found = cont.search(Id,cont.fpackage) # searches throught the packages of the current container looking for the matching package ID
    if found == 1: # if the package was found
        return cont # it returns the container class of the current container
    elif cont.nt != None and cont.nt.destination &lt;= dest and found != 1: # if the package was not found and the next container's destination is less than or matches the target
        return self.Search(Id,dest,cont.nt) # recurses with the next container being the current
    elif cont.nt == None or cont.nt.destination &gt; dest: # if there is no next container or the next container's destination is larger than the target
        return 0   # return 0

def fillContainer(self,New,cont):
    if self.FContainer == None: # if there are no containers in the yard yet
        self.FContainer = Container(New) # makes the first container point to the new container
    elif New.destination == cont.destination: # if the current container has the same destination as the package
        # the next if statment checks if there is no room in the container and if the next container is to a diffrent destination
        if cont.maxweight(New) == 1 and cont.nt != None and cont.nt.destination != New.destination:
            self.newcontainer(cont,cont.nt,New) # if it passes it inserts a new container between the two
        # the next if statment passes if there is no room in the container and the next container is to the same destination
        elif cont.maxweight(New) == 1 and cont.nt != None and cont.nt.destination == New.destination: 
            self.fillContainer(New,cont.nt) # it recurses with with the next container being the current
        elif cont.maxweight(New) == 1 and cont.nt == None: # this checks if this container is the last in line and there is no room left in the container
            c = Container(New) # makes a new container set to c
            c.lt = cont # makes c's previous pointer point to the current container
            cont.nt = c # makes the current container's next point point to c
        else: # if it fails all the above if statments
            cont.add(New,cont.fpackage) # it adds the package to the current container
    elif cont.nt != None and cont.nt.destination &lt;= New.destination: # if the next container's destination is less than or equal to the package destination
        self.fillContainer(New,cont.nt) # puts the packahe into the next container
    elif cont.nt != None and cont.nt.destination &gt; New.destination and cont.destination &lt; New.destination:
        self.newcontainer(cont,cont.nt,New) # makes a new container with the package in it that is placed between the current and next container
    else: # if the current container is the first or last container
        if New.destination &lt; cont.destination: # if the package destination comes before the first containers destination
            c = Container(New) # makes a new container with the package
            c.nt = cont # makes the new container point to current container
            cont.lt = c # makes the current container point back to the new container
            self.FContainer = c # makes the new container the first container in the yard
        else: # if the current container is the last in the list
            c = Container(New) # makes a new container with the package
            c.lt = cont # makes the new container point back to the last in the list
            cont.nt = c # makes the last container in the list point to the new container

def newcontainer(self,cont,contn,New):
    new = Container(New) # makes the new container
    contn.lt = new # makes the later container point back to new
    cont.nt = new  # makes the first container point to new
    new.nt = contn # makes new point forward to the later container
    new.lt = cont  # makes new point back to the first container

def locate(self,dest,cont):
    if cont.destination == dest: # if the containers destination is the same as the peramiter
        return cont # returns the container
    elif cont.nt != None: # if the next container is not none 
        return self.locate(dest,cont.nt) # recurses to the next container
    else:
        return 0 # returns 0 if no matching container is found

def remove(self,Id,dest,cont):
    found = 0 # if package has been found
    while cont.destination == dest and found == 0: # while the container is for the same destination and the package has not been found
        found = cont.remove(Id,cont.fpackage) # it goes through the contianer to try and find the package
    if found == 1: # if the package was found in the container
        if cont.fpackage == None: # if there is no more packages in the container
            if cont.lt == None and cont.nt == None: # if there are no more packages in the yard
                self.FContainer = None # sets the first container in the yard to None
            elif cont.lt == None and cont.nt != None: # if the container is the first container
                self.FContainer = cont.nt # makes the first container pointer point to the next container
            elif cont.lt != None and cont.nt == None:
                cont.lt.nt = None
            else: # if there is a before and after the current container
                cont.lt.nt = cont.nt # makes the previous container point to the next container
                cont.nt.lt = cont.lt # makes the next container point back to the previous container
    elif cont.nt != None and cont.nt.destination &lt;= dest and found != 1: # if the next container has a lower or equal destination as the target
        self.remove(Id,dest,cont.nt) # recurses with the next container being set as the current

def printMD(self,dest,cont):
    if cont.destination == dest: # if the container is going to the target destination
        print('% ' + str(cont.ID) + ', ' + cont.destination + ', ' + str(cont.weight)) # prints out the information for the destination
        cont.printall(cont.fpackage) # calls print all that prints the packages in the container
    if cont.nt != None: # if not at the end of the list of containers
        self.printMD(dest,cont.nt) # moves to the next container

def fileWrite(self,cont,file):
    if cont == None: # if there is no more containers
        file.close() # closes the file
    else: # if there is still containers to be writen
        file.write('% ' + str(cont.ID) + ', ' + cont.destination + '\n') # writes the container information to the file
        cont.writeall(cont.fpackage,file) # writes the packages in the container to the file
        self.fileWrite(cont.nt,file) # uses recursion to go to the next container

def printMA(self,cont):
    if cont == None: # if there are no more containers
        None # terminates the function
    else:
        print('% ' + str(cont.ID) + ', ' + cont.destination + ', ' + str(cont.weight)) # prints out the container information
        cont.printall(cont.fpackage) # prints out all the packages in the container
        self.printMA(cont.nt) # goes to the next container in the list

def printCC(self,cont):
    if cont == None: # if there are no more containers
        None # terminates teh function
    else:
        print('% ' + str(cont.ID) + ', ' + cont.destination + ', Has ' + str(2000 - cont.weight) + ' Pounds of space left') # prints the information for the current container
        self.printCC(cont.nt) # goes to the next container

def shipout(self):
    weight = 0 # a variable that keeps track of the weight of the containers shiped out
    count = 0 # keeps track of the number of containers shipped out
    dest = input(""What destinations do you want to send containers to: \n"") # gets the target destination from the user
    dest = dest.capitalize()
    first = self.locate(dest,self.FContainer) # finds the first container with a matching destination
    if first == 0: # if no containers have a matching destination
        print(""There are no container to go to that destination \n"") # prints a message stating there are no containers
    else: # else there were containers
        st = first # keeps track of the first container
        while first.nt != None and first.destination == dest: # while the container equals the target
            count += 1 # increases the container count by 1
            weight += first.weight # adds the weight of the container to the weight
            first = first.nt # moves first to the next container
        if st.lt == None and first.destination != dest: # if the first container was the start
            first.lt = None # makes the last container looked at point back to None
            self.FContainer = first # makes thefirst container in the yard equal first
            print(""There were "" + str(count) + "" Containers shipped out weighing "" + str(weight) + ""\n"") # prints out the final statment
        elif st.lt != None and first.nt == None and first.destination == dest: # if the final container is the end of the list
            weight += first.weight # adds the weight of the last container
            count += 1 # adds 1 to the count of containers
            st.lt.nt = None # makes the container before st point to None
            print(""There were "" + str(count) + "" Containers shipped out weighing "" + str(weight) + ""\n"") # prints out the end statment
        elif st.lt == None and first.nt == None and first.destination == dest: # if the all of the containers were shipped out
            weight += first.weight # adds the weight of the last container to the count
            count += 1 # increases the number of container by one
            self.FContainer = None # sets the first container in the yard to none
            print(""There were "" + str(count) + "" Containers shipped out weighing "" + str(weight) + ""\n"") # prints out the end statment
        else: # if the containers sent out were in the middle of the list
            st.lt.nt = first # sets the next pointer of the preceding container to point to the last container
            first.lt = st.lt # sets the last container to point to the container preceding the ones being shipped out
            print(""There were "" + str(count) + "" Containers shipped out weighing "" + str(weight) + ""\n"") # print the end statement
</code></pre>

<p>And this is the code for display menu for user:</p>

<pre><code>from ShippingYardClass import * # imports the 3 classes Shipyard, Container and Package

def Main():
    print(""The commands that this program takes are:"") 
    print(""a = add package"")
    print(""s = search for a package"") 
    print(""p = print"")
    print(""r = remove package"")
    print(""sh = ship containers"")
    print(""q = quit"")

    order = input(""Make a decision by entering letter that you would like to do:"") # gets what the user wants to do
    while order != 'q': 
        if order == 'a':
            values = input(""Enter the owner, destination and weight(seperate them with a space, if there is a space in the owner or destination use a _ to represent it): \n"") 
            i = values.split(' ') 
            while len(i) != 3: 
                values = input(""Error: enter the owner, destination and weight(seperate them with a space, if ther is a space in it use a _ to represent it): \n "")
                i = values.split(' ') 
            while i[2].isdigit() == 0 or int(i[2]) &gt; 2000:
                i[2] = input(""You did not enter a valid weight, Please enter one below 2000: \n"") 
            New = Package(i[0].capitalize(),i[1].capitalize(),int(i[2]))
        Shipyard.fillContainer(New,Shipyard.FContainer)

    if order == 's':
        values = input(""Enter the ID number then the Destination of the package(seperate them with a space, Use _ for spaces in destination): \n "")
        i = values.split(' ')
        while len(i) != 2 and i[0].isdigit != 1:
            values = input(""Enter the ID number then the Destination of the package(seperate them with a space): \n "") 
            i = values.split(' ') 
        r = Shipyard.Search(int(i[0]),i[1].capitalize(),Shipyard.FContainer) 
        if r != 0: 
            print(""Package "" + i[0] + "" going to "" + i[1].capitalize() + "" is in Container "" + str(r.ID) + ""\n"") 
        else: 
            print(""There is no Package with the ID "" + i[0] + "" going to "" + i[1].capitalize() + "" is not in the yard \n"") 

    if order == 'p':
        t = input(""what would you like to print(md = manifest for a destination, ma = manifest for the yard, cc = list of containors): \n"") 
        if t == 'md': 
            dest = input(""what is the destination you want a manifest for: \n "") 
            Shipyard.printMD(dest,Shipyard.FContainer) 
        if t == 'ma':
            Shipyard.printMA(Shipyard.FContainer) 
        if t == 'cc':
            Shipyard.printContainers(Shipyard.FContainer) 

    if order == 'r':
        values = input(""enter the ID and Destination of the package you want removed(seperate them with a space): \n "")
        i = values.split(' ') 
        while len(i) != 2 and i[0].isdigit() == 0: 
            values = input(""enter the ID and Destination of the package you want removed(seperate them with a space): \n "")
            i = values.split(' ') 
        Shipyard.remove(int(i[0]),i[1].capitalize(),Shipyard.FContainer) 

        if order == 'sh': 
            Shipyard.shipout()
        if order != 'a' and order != 'p'and order != 'r'and order != 'sh'and order != 'q':
            raise TypeError(""Wrong input letter"")
        return Main()

Main()
</code></pre>

<p>However, when I run the meun function and choose ""a"" in the menu and try to enter a owner, destination and weight. For example: Jack Japan 80. It comes out an error ""NameError: name 'package' is not defined"" back to the ID function of Package class. 
What's wrong for my code and how do it fix it? Thank you!</p>
","8829479","","","Shipyard system linked list python","<python>","3","3","24320"
"50648758","2018-06-01 17:56:34","0","","<p>There is no any issue with the python version regard to you question.The problem is that, loop will run infinitely because there is no condition to exit from the loop.So to exit from the loop after print statement insert <code>break</code>  keyword as follows.</p>

<pre><code>people = int(input(""Will you be travelling by yourself (1), or as a group of two (2)?: ""))
while people: 
    if people == 1:
        print(""\nAh, a holiday for one! How adventurous."")
        break
    elif people == 2:
        print(""\nOoh, bringing a friend! Sounds like fun!"")
        break
    else:
        print(""\nPlease enter either 1 or 2 to determine the number of travellers."")
        people = int(input(""Will you be travelling by yourself (1), or as a group of two (2)?: ""))
</code></pre>
","7866263","","","0","783","Mash962","2017-04-14 08:05:30","19","8","10","0","50647383","","2018-06-01 16:10:24","0","62","<p>I am creating  a travel agent game in Python 3.1. I have reached an error with my while loop. It will constantly repeat the print() response. I know this is because it is true as long as here is a response for people, but I have no clue how to fix it.    </p>

<pre><code>people = int(input(""Will you be travelling by yourself (1), or as a group of 
two (2)?: ""))
while people: 
    if people == 1:
        print(""\nAh, a holiday for one! How adventurous."")
    elif people == 2:
        print(""\nOoh, bringing a friend! Sounds like fun!"")
    else:
        print(""\nPlease enter either 1 or 2 to determine the number of 
        travellers."")
        people = int(input(""Will you be travelling by yourself (1), or as a 
        group of two (2)?: ""))
</code></pre>
","9637024","","","Issue With Infinite While Loop in Python 3.1","<python><loops><infinite>","1","7","769"
"50648761","2018-06-01 17:56:42","2","","<p>If i understand your requirements correctly:</p>

<ul>
<li>A user must be able to configure a filter which specifies which <code>Thing</code>s should be selected - without any programming knowledge, just through a GUI</li>
<li>A user must be able to set up timed tasks and specify which web service is targeted and which filter should be used to send the appropriate data</li>
</ul>

<p>Based on this premises, i would:</p>

<ul>
<li>Use <a href=""https://github.com/modlinltd/django-advanced-filters"" rel=""nofollow noreferrer"">https://github.com/modlinltd/django-advanced-filters</a></li>
<li>Create a Model/Form/View for <code>TimedTasks</code> which contain the relation to <code>AdvancedFilter</code> and the webservice specs</li>
<li>Retrieve the data through <code>AdvancedFilter#query</code> in the task runner, set up and call the webservice</li>
</ul>

<p>This way the user can use the Overview page of <code>Thing</code> to create his data filters, and can link them in the creation of <code>TimedTask</code> along with the webservice config.</p>
","3820185","","","3","1059","wiesion","2014-07-09 11:21:21","2008","165","89","27","50648312","50648761","2018-06-01 17:20:36","1","75","<p>I am working on an application where end users need to be able to define queryset filters through an interface. These filters are used to select instances of a model to ship off to another web service at timed intervals. Contrived example:</p>

<pre><code>class Thing(models.Model):
    stuff = models.CharField()
</code></pre>

<p>I need users to be able to configure a timed task where only <code>Thing</code>s with a value of <code>test</code> for the <code>stuff</code> field will be selected.</p>

<p>I currently have a working POC for this functionality, but it involves a lot of hand coded logic.</p>

<p>Given that django has such a rich community and ecosystem, I was wondering if I am missing an opportunity to do this in a simpler way.</p>

<p>Looking forward to your feedback!</p>
","1366989","","","Django - User Defined Queryset Filtering","<python><django><django-queryset>","1","1","796"
"50648783","2018-06-01 17:58:15","3","","<p>If it were me, I'd parse the file into sections using <a href=""https://docs.python.org/3/library/stdtypes.html#str.startswith"" rel=""nofollow noreferrer""><code>str.startswith('[')</code></a>, and then use a <a href=""https://docs.python.org/3/tutorial/classes.html#generators"" rel=""nofollow noreferrer"">generator function</a> to pass the resulting lines to <a href=""https://docs.python.org/3/library/csv.html#csv.reader"" rel=""nofollow noreferrer""><code>csv.reader()</code></a> and <a href=""https://docs.python.org/3/library/csv.html#csv.DictReader"" rel=""nofollow noreferrer""><code>csv.DictReader()</code></a> for the two sections, respectively.</p>

<p>Here is an example:</p>

<pre><code>from csv import reader, DictReader
from pprint import pprint 

def lines_until_section_mark(f):
    for line in f:
        if line.startswith('['):
            break
        if line.strip():
            yield line

with open('cfg.txt') as f:
    # Eat until first section mark
    for line in lines_until_section_mark(f):
        pass

    # Construct first dictionary from first sectoin
    d = dict(reader(lines_until_section_mark(f)))

    # Construct second dictionary from second section
    d['data'] = list(DictReader(lines_until_section_mark(f)))

pprint(d)
</code></pre>

<p>Input file:</p>

<pre><code>[section-1] # basically a setup info with &lt;key-value&gt; pair
date,2/16/2018
label,test3
size,25

[section-2] # contains test parameters and data
NO,parameter1,parameter2
1,50,30
2,-20,32
</code></pre>

<p>Output:</p>

<pre><code>{'data': [{'NO': '1', 'parameter1': '50', 'parameter2': '30'},
          {'NO': '2', 'parameter1': '-20', 'parameter2': '32'}],
 'date': '2/16/2018',
 'label': 'test3',
 'size': '25'}
</code></pre>
","8747","8747","2018-06-01 18:08:26","2","1733","Robᵩ","2008-09-15 16:47:33","124214","7724","5618","743","50648481","50648783","2018-06-01 17:33:36","2","138","<p>I have a csv-like file that wants to transfer into dict in Python. Heres the sample file:</p>

<pre><code>file start:
...
...
[section-1] # basically a setup info with &lt;key-value&gt; pair
date,2/16/2018
label,test3
size,25
...
[section-2] # contains test parameters and data
NO,parameter1,parameter2
1,50,30
2,-20,32
...
...
file end
</code></pre>

<p>I roughly have an idea of how to handle this file, to goal is to easily access each record. Something like dict maybe:</p>

<pre><code>{'date':2/16/2018,
'label':test3,
'size':25,
'data':[{'NO':1,'parameter1':50,'parameter2':30}
        {'NO':2,'parameter2':-20,'parameter2',32}]
}
</code></pre>

<p>the usecase is I want to extract these data and load it into database. The [section-1] data will go to top-level table. And [section-2] data will be loaded into child table based on [section-1] info.</p>

<p>I'm very new to Python. Do you think this is the right direction to convert this file? Can someone tell me how to do this? </p>

<p>Thanks</p>
","7491202","7491202","2018-06-01 17:50:19","CSV to dict in Python","<python><csv><dictionary>","3","12","1009"
"50648793","2018-06-01 17:58:57","1","","<p>For parsing csv file you might want to use <a href=""https://github.com/hay/dataknead"" rel=""nofollow noreferrer"">dataknead</a> library. It works with Python 3 only.</p>

<p>You can install it like this: <code>pip install dataknead</code> (pip3 if you have different Python versions)</p>

<p>According to its documentation, let's assume you have a cities.csv like this:</p>

<pre><code>city,country,population
Amsterdam,nl,850000
Rotterdam,nl,635000
Venice,it,265000
</code></pre>

<p>You can read it like this:</p>

<pre><code>from dataknead import Knead
data = Knead(""cities.csv"").data()
</code></pre>

<p>The output of print(data) will be:</p>

<pre><code>[{'city': 'Amsterdam', 'population': '850000', 'country': 'nl'}, {'city': 'Rotterdam', 'population': '635000', 'country': 'nl'}, {'city': 'Venice', 'population': '265000', 'country': 'it'}]
</code></pre>

<p>I suggest you to parse one section firstly. Then you can decide how to divide file's section. Hope this helps.</p>
","9863586","","","0","983","alexeyjaga","2018-05-29 09:59:42","21","3","0","0","50648481","50648783","2018-06-01 17:33:36","2","138","<p>I have a csv-like file that wants to transfer into dict in Python. Heres the sample file:</p>

<pre><code>file start:
...
...
[section-1] # basically a setup info with &lt;key-value&gt; pair
date,2/16/2018
label,test3
size,25
...
[section-2] # contains test parameters and data
NO,parameter1,parameter2
1,50,30
2,-20,32
...
...
file end
</code></pre>

<p>I roughly have an idea of how to handle this file, to goal is to easily access each record. Something like dict maybe:</p>

<pre><code>{'date':2/16/2018,
'label':test3,
'size':25,
'data':[{'NO':1,'parameter1':50,'parameter2':30}
        {'NO':2,'parameter2':-20,'parameter2',32}]
}
</code></pre>

<p>the usecase is I want to extract these data and load it into database. The [section-1] data will go to top-level table. And [section-2] data will be loaded into child table based on [section-1] info.</p>

<p>I'm very new to Python. Do you think this is the right direction to convert this file? Can someone tell me how to do this? </p>

<p>Thanks</p>
","7491202","7491202","2018-06-01 17:50:19","CSV to dict in Python","<python><csv><dictionary>","3","12","1009"
"50648809","2018-06-01 18:00:35","0","","<p>If you are willing to use pandas you could do something like this. I am making a few assumptions about the data here. I am assuming size value equals the number of rows in your csv file. I am assuming you know the date and what you want to use as a name for the label.  </p>

<p>This code will get you the rows in the csv file and create a dictionary for each row and put it in a list. </p>

<pre><code>import pandas as pd
my_dict = {""date"": '06/01/2018', ""label"":""test3"",""size"":0}
df = pd.read_csv('your_csv_file.csv')

row_list = []

for i, row in enumerate(df.values):
    my_dict['size']+=1
    row_dict = {}
    for i in range(0, len(row)):
        row_dict['col'+str(i)] = row[i]
    row_list.append(row_dict)

my_dict[""data""] = row_list
print(my_dict)
</code></pre>
","6121991","","","0","776","MicahB","2016-03-27 22:25:28","119","11","10","0","50648481","50648783","2018-06-01 17:33:36","2","138","<p>I have a csv-like file that wants to transfer into dict in Python. Heres the sample file:</p>

<pre><code>file start:
...
...
[section-1] # basically a setup info with &lt;key-value&gt; pair
date,2/16/2018
label,test3
size,25
...
[section-2] # contains test parameters and data
NO,parameter1,parameter2
1,50,30
2,-20,32
...
...
file end
</code></pre>

<p>I roughly have an idea of how to handle this file, to goal is to easily access each record. Something like dict maybe:</p>

<pre><code>{'date':2/16/2018,
'label':test3,
'size':25,
'data':[{'NO':1,'parameter1':50,'parameter2':30}
        {'NO':2,'parameter2':-20,'parameter2',32}]
}
</code></pre>

<p>the usecase is I want to extract these data and load it into database. The [section-1] data will go to top-level table. And [section-2] data will be loaded into child table based on [section-1] info.</p>

<p>I'm very new to Python. Do you think this is the right direction to convert this file? Can someone tell me how to do this? </p>

<p>Thanks</p>
","7491202","7491202","2018-06-01 17:50:19","CSV to dict in Python","<python><csv><dictionary>","3","12","1009"
"50648810","2018-06-01 18:00:38","1","","<p>You can do this in O(n) with the existing data structure (iterating the set), but for O(1) you'll have to change data structure. You will need to make a lookup:</p>

<pre><code>from collections import defaultdict

positions = defaultdict(list)
for position, name in my_set:
    positions[name].append(position)
</code></pre>

<p>Now this is an O(1) operation:</p>

<pre><code>name in positions
</code></pre>

<p>Retrieving all per name:</p>

<pre><code>for pos in positions[name]:
    ...
</code></pre>

<p>If you want this to  keep in synch with <code>my_set</code> mutations, then you will need to add in hooks for updating <code>positions</code> at the same time as adds/deletes to <code>my_set</code>.  It might be better to rethink the underlying data structure entirely, for example, using a dict instead of a set in the first place.</p>
","674039","674039","2018-06-01 18:47:38","1","847","wim","2011-03-23 23:40:27","187587","12233","9064","5087","50648736","","2018-06-01 17:54:53","1","41","<p>I'm working with a set that contains tuples of the form <code>(position, name)</code> and need to check if a value already exists in the set for the name while ignoring the position.</p>

<p>Is there a way that I can use the <code>in</code> operator similar to <code>value in my_set</code>, ignoring the position variable in the tuple during comparison, but still retrieving it? Something similar to <code>(_, value) in my_set</code> or <code>(*, value) in my_set)</code>, but those don't work, first one returning an incorrect value, and the second raising a SyntaxError.</p>

<p>Obviously I can use a loop or a generator comprehension like <code>value in (tup[1] for tup in my_set)</code>, but that doesn't retrieve the position variable from that tuple, and I was curious if there was some form of one-liner comprehension that would do this.</p>
","2770044","2770044","2018-06-01 17:57:17","Ignoring a value in tuple comparison but still retrieving it","<python><set><tuples>","1","3","852"
"50648812","2018-06-01 18:00:40","0","","<p>Those distributions are for debian linux only. You may have better luck with the <a href=""https://github.com/squeaky-pl/portable-pypy#portable-pypy-distribution-for-linux"" rel=""nofollow noreferrer"">portable binaries</a> if the ones provided by OpenSuse are too old for you.</p>

<p>Edit: note that you used a link to an old version of the documents, you should have looked <a href=""http://doc.pypy.org/en/latest/install.html"" rel=""nofollow noreferrer"">here</a> which states ""These builds depend on dynamically linked libraries that may not be available on your OS""</p>
","2802665","","","0","572","mattip","2013-09-21 18:09:24","828","81","6","1","50629415","","2018-05-31 17:04:52","0","697","<p>I have been attempting to install and run pypy3 on a linux machine but am running into troubles. I have been using pypy on a mac but installed it using homebrew so didn't encounter any of these troubles.</p>

<p>I downloaded the most recent build and unpacked the tar file as described by <a href=""http://doc.pypy.org/en/release-2.5.x/install.html"" rel=""nofollow noreferrer"">their documentation</a>. I followed the steps exactly (except replacing pypy with pypy3 and using the appropriate file name). However, when typing</p>

<pre><code>./pypy3-v6.0.0-linux64/bin/pypy3
</code></pre>

<p>I get the following error:</p>

<pre><code>./pypy3-v6.0.0-linux64/bin/pypy3: error while loading shared libraries: libbz2.so.1.0: 
cannot open shared object file: No such file or directory
</code></pre>

<p>I'm not sure how to interpret this error. Despite the pypy documentation saying that it should run in place, <code>pypy3 filename.py</code> still returns the error:</p>

<pre><code>If 'pypy3' is not a typo you can use command-not-found to lookup the package that 
contains it, like this:
cnf pypy3
</code></pre>

<p>But <code>cnf pypy3</code> only confirms that the pypy3 is not found.</p>

<p>Any help on what I'm doing wrong would be appreciated.</p>
","8228600","","","Installing and running pypy on linux","<python><linux><installation><pypy>","1","4","1252"
"50648816","2018-06-01 18:01:06","20","","<p>Here I'm answering to OP's topic question rather than his exact problem. I'm doing this as the question shows up in the top when I google the topic problem.</p>

<p>You can implement a custom metric in two ways.</p>

<ol>
<li><p>As mentioned in <a href=""https://keras.io/metrics#custom-metrics"" rel=""noreferrer"">Keras docu</a>.
</p>

<pre><code>import keras.backend as K

def mean_pred(y_true, y_pred):
    return K.mean(y_pred)

model.compile(optimizer='sgd',
          loss='binary_crossentropy',
          metrics=['accuracy', mean_pred])
</code></pre>

<p>But here you have to remember as mentioned in Marcin Możejko's answer that <code>y_true</code> and <code>y_pred</code> are tensors. So in order to correctly calculate the metric you need to use <code>keras.backend</code> functionality. Please look at this SO question for details <a href=""https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras"">How to calculate F1 Macro in Keras?</a></p></li>
<li><p>Or you can implement it in a hacky way as mentioned in <a href=""https://github.com/keras-team/keras/issues/5794#issuecomment-303683985"" rel=""noreferrer"">Keras GH issue</a>. For that you need to use <code>callbacks</code> argument of <code>model.fit</code>.
</p>

<pre><code>import keras as keras
import numpy as np
from keras.optimizers import SGD
from sklearn.metrics import roc_auc_score

model = keras.models.Sequential()
# ...
sgd = SGD(lr=0.001, momentum=0.9)
model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])


class Metrics(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self._data = []

    def on_epoch_end(self, batch, logs={}):
        X_val, y_val = self.validation_data[0], self.validation_data[1]
        y_predict = np.asarray(model.predict(X_val))

        y_val = np.argmax(y_val, axis=1)
        y_predict = np.argmax(y_predict, axis=1)

        self._data.append({
            'val_rocauc': roc_auc_score(y_val, y_predict),
        })
        return

    def get_data(self):
        return self._data

metrics = Metrics()
history = model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), callbacks=[metrics])
metrics.get_data()
</code></pre></li>
</ol>
","495656","495656","2018-12-10 18:19:09","2","2238","vogdb","2010-11-03 08:18:13","3262","119","419","0","37657260","37663327","2016-06-06 12:17:34","18","24899","<p>I get this error : </p>

<blockquote>
  <p>sum() got an unexpected keyword argument 'out'</p>
</blockquote>

<p>when I run this code:</p>

<pre><code>import pandas as pd, numpy as np
import keras
from keras.layers.core import Dense, Activation
from keras.models import Sequential

def AUC(y_true,y_pred):
    not_y_pred=np.logical_not(y_pred)
    y_int1=y_true*y_pred
    y_int0=np.logical_not(y_true)*not_y_pred
    TP=np.sum(y_pred*y_int1)
    FP=np.sum(y_pred)-TP
    TN=np.sum(not_y_pred*y_int0)
    FN=np.sum(not_y_pred)-TN
    TPR=np.float(TP)/(TP+FN)
    FPR=np.float(FP)/(FP+TN)
    return((1+TPR-FPR)/2)

# Input datasets

train_df = pd.DataFrame(np.random.rand(91,1000))
train_df.iloc[:,-2]=(train_df.iloc[:,-2]&gt;0.8)*1


model = Sequential()
model.add(Dense(output_dim=60, input_dim=91, init=""glorot_uniform""))
model.add(Activation(""sigmoid""))
model.add(Dense(output_dim=1, input_dim=60, init=""glorot_uniform""))
model.add(Activation(""sigmoid""))

model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=[AUC])


train_df.iloc[:,-1]=np.ones(train_df.shape[0]) #bias
X=train_df.iloc[:,:-1].values
Y=train_df.iloc[:,-1].values
print X.shape,Y.shape

model.fit(X, Y, batch_size=50,show_accuracy = False, verbose = 1)
</code></pre>

<p>Is it possible to implement a custom metric aside from doing a loop on batches and editing the source code?</p>
","5190014","5974433","2017-07-10 08:59:38","how to implement custom metric in keras?","<python><neural-network><deep-learning><keras><metrics>","3","0","1371"
"50648822","2018-06-01 18:01:58","1","","<p>If I'm reading your question right I believe you are attempting to do two things.</p>

<ol>
<li>Find the median value of a column</li>
<li>Create a new column which is 0 if the value is less than the median or 1 if greater.</li>
</ol>

<p>Let's tackle #1 first:</p>

<p><code>median = df['originalcolumn'].median()</code></p>

<p>That easy! There's many great pandas functions for things like this.</p>

<p>Ok so number two:</p>

<p><code>df['newcolumn'] = df[df['originalcolumn'] &gt; median].astype(int)</code></p>

<p>What we're doing here is creating a new bool series, false if the value at that location is less than the median, true otherwise. Then we can cast that to an int which gives us 0s and 1s.</p>
","5434007","","","3","716","Augie Doebling","2015-10-11 17:38:28","21","4","32","0","50648413","","2018-06-01 17:29:03","0","47","<p>I have a csv datafile that I've split by a column value into 5 datasets for each person using:</p>

<pre><code>for i in range(1,6):
    PersonData = df[df['Person'] == i].values
    P[i] = PersonData
</code></pre>

<p>I want to sort the data into ascending order according to one column, then split the data half way at that column to find the median.</p>

<p>So I sorted the data with the following:</p>

<pre><code>dataP = {}

for i in range(1,6):
    sortData = P[i][P[i][:,9].argsort()]
    P[i] = sortData
    P[i] = pd.DataFrame(P[i])
dataP[1]
</code></pre>

<p>Using that I get a dataframe for each of my datasets 1-6 sorted by the relevant column (9), depending on which number I put into dataP[i].</p>

<p>Then I calculate half the length:</p>

<pre><code>for i in range(1,6):
    middle = len(dataP[i])/2
    print(middle)
</code></pre>

<p>Here is where I'm stuck!</p>

<p>I need to create a new column in each dataP[i] dataframe that splits the length in 2 and gives the value 0 if it's in the first half and 1 if it's in the second.</p>

<p>This is what I've tried but I don't understand why it doesn't produce a new list of values 0 and 1 that I can later append to dataP[i]:</p>

<pre><code>for n in range(1, (len(dataP[i]))):
    for n, line in enumerate(dataP[i]):
        if middle &gt; n:
            confval = 0
        elif middle &lt; n:
            confval = 1
for i in range(1,6): 
    Confval[i] = confval
Confval[1]
</code></pre>

<p>Sorry if this is basic, I'm quite new to this so a lot of what I've written might not be the best way to do it/necessary, and sorry also for the long post.</p>

<p>Any help would be massively appreciated. Thanks in advance!</p>
","9882450","389289","2018-06-01 17:30:37","Split a list by half the length and add new column with dependent values","<python><pandas>","1","0","1691"
"50648899","2018-06-01 18:07:17","0","","<p>I was able to solve it doing something that I thought was improper as it would crash my code.</p>

<p>I changed file 2 <code>candle.pyx</code> to:</p>

<pre><code>cdef class Candle:
    cdef:
        int ts

    def __init__(self, int ts):
        self.ts = ts
</code></pre>
","501054","501054","2018-06-01 18:43:54","4","278","joaoavf","2010-11-08 19:12:08","729","106","1960","3","50647917","","2018-06-01 16:50:36","1","444","<p>I have 3 cython files:</p>

<p>File 1 - <code>candle.pxd</code>:</p>

<pre><code>cdef class Candle:
    cdef:
        int ts
</code></pre>

<p>File 2 - <code>candle.pyx</code>:</p>

<pre><code>cdef class Candle:
    def __init__(self, int ts):
        self.ts = ts
</code></pre>

<p>File 3 - <code>feeder.pyx</code>:</p>

<pre><code>from src.cython.candle cimport Candle

cdef class Feeder:
    cdef instantiate_first_candle(self):
        cdef int a = 1

        # Instantiates Candle
        cdef Candle candle = Candle(a)
</code></pre>

<p>The exact error it is throwing is:</p>

<pre><code>from src.cython.feeder import Feeder
File ""src/cython/candle.pxd"", line 3, in init feeder
ValueError: src.cython.candle.Candle has the wrong size, try recompiling. Expected 16, got 24
</code></pre>

<p>I have not much of a clue of what is going on and how to solve this. I have tried quite a few different things without success.</p>

<p><strong>Update:</strong></p>

<p>I was able to import <code>Candle</code> on IPython, when I try to instantiate an object I get this error:</p>

<p><code>AttributeError: 'candle.Candle' object has no attribute 'ts'</code></p>
","501054","501054","2018-06-01 20:20:45","Cython: ValueError: '...'.Candle has the wrong size, try recompiling. Expected 16, got 24","<python><compilation><cython>","1","3","1161"
"50648938","2018-06-01 18:10:07","2","","<p>You can download 64 bit Python from this link <a href=""https://www.python.org/downloads/windows/"" rel=""nofollow noreferrer"">here</a>. Select the option you want but make sure it has x86-64 in the name, meaning it is the 32 bit instruction set for a 64 bit machine. </p>
","5624170","","","0","273","Muadh Ghuneim","2015-12-01 05:37:19","491","90","54","2","50648891","50648938","2018-06-01 18:06:46","0","45","<p>I downloaded the default python 3 from python.org, and I have been happy with it, but now I need to let python be able to use all of my RAM. I have read that a 64 bit installation will work. How can I do this?</p>
","8866053","","","Download 64 Bit Python?","<python><python-3.x><64-bit>","1","0","217"
"50648940","2018-06-01 18:10:11","0","","<p>It's hard to test without the data, but this may help</p>

<pre><code>from matplotlib.patches import Rectangle
axes[0,0].add_patch(Rectangle((-0.8, -1), 1.6, 1, color='white'))
</code></pre>
","3063243","","","0","194","phi","2013-12-03 20:57:15","2771","83","135","20","50590726","","2018-05-29 18:27:16","0","352","<p>My question: help needed with configuring the y-axis in seaborn so that it will display a range from -1.0 to -0.8, then a gap, then from 0.8 to 1.0 (for displaying correlation coefficients in a lmplot)</p>

<pre><code>ax_neg_1 = sns.lmplot('Aantal Sterkst Negatief', # Horizontal axis
       'Corr Sterkst Negatief', # Vertical axis
       data=df_PAR_metingen_half_1, # Data source
       fit_reg=False, # Don't fix a regression line
       hue=""Sterkst Negatief"", # Set color
       scatter_kws={""marker"": ""D"", # Set marker style
                    ""s"": 100, 'alpha':0.3}, size=12) # S marker size, transparency &amp; size

# Plot horizontal line
y=0
plt.axhline(y=y, c='red',linestyle='dashed',zorder=-1)

# Set title
plt.title('Correlations by chemical', fontweight='bold', fontsize=12)

# Set x-axis label
plt.xlabel('Count')

# Set y-axis label
plt.ylabel('Correlation Coefficient')

axes = ax_neg_1.axes
axes[0,0].set_ylim(-1, 1)
</code></pre>
","9866049","","","Seaborn lmplot: limits y-axis","<python><seaborn><correlation>","1","2","955"
"50648943","2018-06-01 18:10:35","1","","<p>I agree with @dparolin about processing the words file to see if words conform to the letters, not generating possible words and seeing if they are in the file.  This allows us not to read the file into memory, as we only need to examine one word at a time.  And it can be done with a recursive test:</p>

<pre><code>letters = 'catbt'

def is_match(letters, word):

    if not word:
        return True

    if not letters:
        return False

    letter = letters.pop()

    if letter in word:
        word.remove(letter)

    return is_match(letters, word)

with open('words.txt') as words:
    for word in words:
        word = word.strip()

        if is_match(list(letters), list(word)):
            print(word)
</code></pre>

<p><strong>EXAMPLE USAGE</strong></p>

<pre><code>% python3 test.py
act
at
bat
cab
cat
tab
tact
%
</code></pre>

<p>And we should be able to work with lots of letters without issue.</p>
","5771269","","","3","923","cdlane","2016-01-10 23:40:41","23572","1435","378","265","50633762","50648943","2018-05-31 22:39:31","2","154","<p>given letters: example of letters</p>

<pre><code>letters = 'hutfb' 
</code></pre>

<p>I am given a file with a list of words.</p>

<p>I need to write a recursive function that allows me to check all possibilities the letters can make. If the possibility is in the list of words from the file, I need to print that specific word. </p>

<p>so for letters given</p>

<p>they can create the words:</p>

<ul>
<li>a</li>
<li>cat</li>
<li>ac</li>
<li>act</li>
<li>cab</li>
</ul>

<p>and so on and on </p>

<p>each combination the letters make I need to check the file to see if its a valid word. if it is I need to print them. </p>

<p>I don't know how start to write this function.</p>
","9849592","9849592","2018-06-02 20:12:48","recursive function for wordsearch","<python><python-3.x><file><recursion>","4","4","684"
"50649036","2018-06-01 18:18:13","3","","<p>I think you're looking for <a href=""https://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.spatial.distance.cdist.html"" rel=""nofollow noreferrer""><code>cdist</code></a>:</p>

<pre><code>import pandas as pd
import numpy as np
from scipy.spatial.distance import cdist
from Levenshtein import ratio

arr1 = np.array(['faucet', 'faucets', 'bath', 'parts', 'bathroom'])
arr2 = np.array(['faucett', 'faucetd', 'bth', 'kichen'])

matrix = cdist(arr2.reshape(-1, 1), arr1.reshape(-1, 1), lambda x, y: ratio(x[0], y[0]))
df = pd.DataFrame(data=matrix, index=arr2, columns=arr1)
</code></pre>

<p>Result:</p>

<pre><code>           faucet   faucets      bath     parts  bathroom
faucett  0.923077  0.857143  0.363636  0.333333  0.266667
faucetd  0.923077  0.857143  0.363636  0.333333  0.266667
bth      0.222222  0.200000  0.857143  0.250000  0.545455
kichen   0.333333  0.307692  0.200000  0.000000  0.142857
</code></pre>
","6866811","6866811","2018-06-01 18:27:02","1","928","thesilkworm","2016-09-22 20:17:18","3845","151","1210","248","50648860","50649036","2018-06-01 18:04:21","2","301","<p>Say I have two arrays:</p>

<pre><code>import numpy as np
arr1 = np.array(['faucet', 'faucets', 'bath', 'parts', 'bathroom'])
arr2 = np.array(['faucett', 'faucetd', 'bth', 'kichen'])
</code></pre>

<p>and I want to compute the similarity of the strings in <code>arr2</code> to the strings in <code>arr1</code>.</p>

<p><code>arr1</code> is an array of correctly spelled words.</p>

<p><code>arr2</code> is an array of words not recognized in a dictionary of words.</p>

<p>I want to return a matrix which will then be turned into a pandas DataFrame.</p>

<p>My current solution (<a href=""https://stackoverflow.com/questions/46452724/string-distance-matrix-in-python-using-pdist"">credit</a>):</p>

<pre><code>from scipy.spatial.distance import pdist, squareform
from Levenshtein import ratio
arr3 = np.concatenate((arr1, arr2)).reshape(-1,1)
matrix = squareform(pdist(arr3, lambda x,y: ratio(x[0], y[0])))
df = pd.DataFrame(matrix, index=arr3.ravel(), columns=arr3.ravel())
</code></pre>

<p>Output:</p>

<pre><code>            faucet   faucets      bath     parts  bathroom   faucett  \
faucet    0.000000  0.923077  0.400000  0.363636  0.285714  0.923077   
faucets   0.923077  0.000000  0.363636  0.500000  0.266667  0.857143   
bath      0.400000  0.363636  0.000000  0.444444  0.666667  0.363636   
parts     0.363636  0.500000  0.444444  0.000000  0.307692  0.333333   
bathroom  0.285714  0.266667  0.666667  0.307692  0.000000  0.266667   
faucett   0.923077  0.857143  0.363636  0.333333  0.266667  0.000000   
faucetd   0.923077  0.857143  0.363636  0.333333  0.266667  0.857143   
bth       0.222222  0.200000  0.857143  0.250000  0.545455  0.200000   
kichen    0.333333  0.307692  0.200000  0.000000  0.142857  0.307692   

           faucetd       bth    kichen  
faucet    0.923077  0.222222  0.333333  
faucets   0.857143  0.200000  0.307692  
bath      0.363636  0.857143  0.200000  
parts     0.333333  0.250000  0.000000  
bathroom  0.266667  0.545455  0.142857  
faucett   0.857143  0.200000  0.307692  
faucetd   0.000000  0.200000  0.307692  
bth       0.200000  0.000000  0.222222  
kichen    0.307692  0.222222  0.000000
</code></pre>

<p><strong>The problem with this solution</strong>:
I waste time computing pairwise distance ratios on words I already know are correctly spelled.</p>

<p>What I'd like is to hand a function <code>arr1</code> and <code>arr2</code> (which can be different lengths!) and output a matrix (not necessarily square) with the ratios.</p>

<p>The result would look like this (without the computational overhead):</p>

<pre><code>&gt;&gt;&gt; df.drop(index=arr1, columns=arr2)

           faucet   faucets      bath     parts  bathroom
faucett  0.923077  0.857143  0.363636  0.333333  0.266667
faucetd  0.923077  0.857143  0.363636  0.333333  0.266667
bth      0.222222  0.200000  0.857143  0.250000  0.545455
kichen   0.333333  0.307692  0.200000  0.000000  0.142857
</code></pre>
","1577947","1577947","2018-06-01 18:14:12","Return Similarity Matrix From Two Variable-length Arrays of Strings (scipy option?)","<python><matrix><scipy><distance><levenshtein-distance>","1","1","2935"
"50649042","2018-06-01 18:18:31","1","","<p>JSON, YAML, and XML are all popular file formats with some pros and cons. There are other file formats as well such as <a href=""https://github.com/toml-lang/toml"" rel=""nofollow noreferrer"">TOML</a>, <a href=""https://en.wikipedia.org/wiki/INI_file"" rel=""nofollow noreferrer"">INI files</a> and others. Here is a overview of JSON, YAML and XML:</p>

<p><strong>JSON</strong></p>

<ul>
<li>Pros

<ul>
<li>very popular with good, native support in many language core libraries</li>
<li>supports schemas via JSON Schema (<a href=""http://json-schema.org/"" rel=""nofollow noreferrer"">http://json-schema.org/</a>) with filepath support</li>
<li>validation tools can validate filepaths and is discussed, e.g. <a href=""https://github.com/Julian/jsonschema/issues/98"" rel=""nofollow noreferrer"">https://github.com/Julian/jsonschema/issues/98</a></li>
<li>can reference external files as shown in Swagger spec JSON implementation</li>
</ul></li>
<li>Cons

<ul>
<li>hard to read multiple lines since line feeds are represented as <code>\n</code></li>
<li>less ideal for creating by hand</li>
</ul></li>
<li>Implementations

<ul>
<li>Google app credentials</li>
<li>NPM <code>package.json</code></li>
<li>Swagger / OpenAPI</li>
</ul></li>
</ul>

<p><strong>YAML</strong></p>

<ul>
<li>Pros

<ul>
<li>popular and easy to read multi-line values if you have them</li>
<li>specification is available: <a href=""http://yaml.org/spec/1.2/spec.html"" rel=""nofollow noreferrer"">http://yaml.org/spec/1.2/spec.html</a></li>
<li>can reference external files as shown in Swagger spec YAML implementation</li>
</ul></li>
<li>Cons

<ul>
<li>does not appear in as many core language libraries as JSON</li>
</ul></li>
<li>Some Implementations

<ul>
<li>Ruby on Rails configuration file</li>
<li>Swagger / OpenAPI</li>
</ul></li>
</ul>

<p><strong>XML</strong></p>

<ul>
<li>Pros

<ul>
<li>very large files can be handled by SAX parsers, think GBs</li>
<li>standardized schema: <a href=""https://www.w3.org/standards/xml/schema"" rel=""nofollow noreferrer"">https://www.w3.org/standards/xml/schema</a></li>
</ul></li>
<li>Cons

<ul>
<li>very verbose and more difficult to read</li>
<li>XML libraries are used to parse and create, e.g. libxml2.</li>
</ul></li>
<li>Some Implementations

<ul>
<li>RSS/Atom</li>
</ul></li>
</ul>

<p><strong>Summary</strong></p>

<ul>
<li>JSON if you have limited amount of data and either no data with multiple line values, or multi-line values that humans are not expected to read. This is good for configuration files because an external dependency is often not needed</li>
<li>YAML if you have more data that needs to be human created/edited, including multi-line values</li>
<li>XML if you have a lot of data, in the GB range</li>
</ul>

<p>For your viewer requirement, you could use a schema to identify a file link and then modify an existing viewer to add a link when present. Of course, you can always create your own from scratch as well.</p>

<p>For your requirements as you've stated, it seems JSON and YAML would be the most appropriate and popular. A benefit is that there are many generic tools to convert JSON and YAML back and forth. Automatic conversion is not as prevalent for other file formats.</p>
","1908967","1908967","2018-06-04 00:08:31","0","3213","Grokify","2012-12-10 02:53:02","8308","2916","962","7","50641988","","2018-06-01 11:01:37","1","88","<p>I need to store some meta information and dependencies between my assets, in files that I can use to do some validations down the line. </p>

<p>Taking JSON as an example, my metadata file would look like this (<code>/publish/path/metadata/poster.json</code>):</p>

<pre><code>{
    'created_by': 'John',
    'creation_date': '12112018',
    'version': '005',
    'creator_comments': 'Updated to latest published images for Poppy',
    'path_to_file': '/publish/path/images/poster.png',
    'dependencies': [
                     '/publish/path/metadata/poppy.json',
                     '/publish/path/metadata/dwarf.json',
                     '/publish/path/metadata/giant.json'
                     ]
}
</code></pre>

<p>and (<code>/publish/path/metadata/poppy.json</code>):</p>

<pre><code>  {
        'created_by': 'Daug',
        'creation_date': '12102018',
        'version': '003',
        'creator_comments': 'Poppy is more red on top',
        'path_to_file': '/publish/path/images/poppy.png',
        'dependencies': [
                         '/publish/path/metadata/poppy_drawing.json',
                         '/publish/path/metadata/poppy_effect.json'
                         ]
    }
</code></pre>

<p>I am looking for a file format would be most appropriate fit to do the following</p>

<ol>
<li>be able to store references to other files</li>
<li>is supported by python libraries that can process the references</li>
<li>can be read easily by humans</li>
<li>viewer or browser support that allows me to traverse the referenced files</li>
</ol>

<p>What do you think fits best to my use case?</p>
","9876238","1307905","2018-06-01 19:00:52","Most appropriate file format for storing cross references","<python><json><xml><reference><yaml>","2","3","1620"
"50649046","2018-06-01 18:18:47","3","","<blockquote>
  <p>I need to run code in ""detached"" (not interactive). And when some
  error is detected I would like to run debugger. That's why I've been
  thinking about remote debugger/jupyter notebook or whatever. So - by
  default there is no debugging session - so I think that PyCharm remote
  debugger is not a case.</p>
</blockquote>

<p>Contrary to what you might seem to think here, you do not really need to run the code in a ""debugging session"" to use remote debugging.</p>

<p>Try the following:</p>

<ul>
<li><p>Install <code>pydevd</code> in the Python environment for your ""detached"" code:</p>

<pre><code>pip install pydevd
</code></pre></li>
<li><p>Within the places in that code, where you would have otherwise used <code>pdb.set_trace</code>, write</p>

<pre><code>import pydevd; pydevd.settrace('your-debugger-hostname-or-ip')
</code></pre></li>
</ul>

<p>Now whenever your code hits the <code>pydevd.settrace</code> instruction, it will attempt to connect to your debugger server.</p>

<p>You may then launch the debugger server from within Eclipse PyDev or Pycharm, and have the ""traced"" process connect to you ready for debugging. Read <a href=""http://www.pydev.org/manual_adv_remote_debugger.html"" rel=""nofollow noreferrer"">here</a> for more details.</p>

<p>It is, of course, up to you to decide what to do in case of a connection timeout - you can either have your process wait for the debugger forever in a loop, or give up at some point. Here is an example which seems to work for me (ran the service on a remote Linux machine, connected to it via SSH with remote port forwarding, launched the local debug server via Eclipse PyDev under Windows)</p>

<pre><code>import pydevd
import socket
from socket import error

def wait_for_debugger(ex, retries=10):
    print(""Bam. Connecting to debugger now..."")
    while True:
        try:
            pydevd.settrace()
            break
        except SystemExit:
            # pydevd raises a SystemExit on connection failure somewhy
            retries -= 1
            if not retries: raise ex
            print("".. waiting .."")

def main():
    print(""Hello"")
    world = 1
    try:
        raise Exception
    except Exception as ex:
        wait_for_debugger(ex)

main()
</code></pre>

<p>It seems you should start the local debug server before enabling port forwarding, though. Otherwise <code>settrace</code> hangs infinitely, apparently believing it has ""connected"" when it really hasn't.</p>

<p>There also seems to be a small project named <a href=""https://github.com/codedstructure/rpcpdb"" rel=""nofollow noreferrer"">rpcpdb</a> with a similar purpose, however I couldn't get it to work right out of the box so can't comment much (I am convinced that stepping through code in an IDE is way more convenient anyway).</p>
","318964","318964","2018-06-02 11:03:11","0","2802","KT.","2010-04-16 23:43:31","6196","381","451","23","50509148","50649046","2018-05-24 12:12:39","5","192","<p>I would like to set some debugging command (like <code>import ipdb; ipdb.set_trace()</code>) that would run debugger in jupyter (I would have to run a HTTP server).
Does anybody know about something like this?</p>

<p>Context: I have a long running tasks that are processed by a scheduler (not interactive mode). I would like to be able to debug such a task while running it the same way.</p>
","2146414","","","Python HTTP debuger","<python><debugging><jupyter-notebook>","1","7","396"
"50649049","2018-06-01 18:19:03","0","","<p>When you are getting ""Max retries exceeded with url"" errors, what is happening is the host is refusing the connection because you're quickly sending a lot of requests from the same IP address. </p>

<p>I'd need to see more code to know more, but I am guessing that you are calling requests.get too frequently, and need to put some time sleeps between your calls as to not spam the server.</p>

<p>Edit:
I see that the handshake is failing. This means you're having problems connecting and doing it frequently so you're being refused by the host.</p>
","5624170","","","1","553","Muadh Ghuneim","2015-12-01 05:37:19","491","90","54","2","50649003","50649049","2018-06-01 18:15:48","0","32","<p>I get this error</p>

<pre><code>HTTPSConnectionPool(host='api.fixer.io', port=443): Max retries exceeded with url: /latest?base=INR (Caused by SSLError(SSLError(1, '_ssl.c:510: error:14077410:SSL routines:SSL23_GET_SERVER_HELLO:sslv3 alert handshake failure')))
</code></pre>

<p>upon running this code</p>

<pre><code>response=requests.get(""https://api.fixer.io/latest?base=""+cfrm)
</code></pre>
","9882720","9839478","2018-06-01 19:21:05","Unable establish connection.Showing handshake error","<python><pythonanywhere>","1","2","401"
"50649052","2018-06-01 18:19:08","-1","","<p>Work backwards from the end, since the rest of the record seems to be fixed format, i.e., working backwards,</p>

<p>string representing units (this has no spaces) :
number : 
dash : 
number : 
number :
the text you want</p>

<pre><code>Haemoglobin 13.5 14-16 g/dl
Field 5 (all characters backwards from end until space reached) = g/gl
Field 4 (jump over space, all characters backwards until space or dash reached) = 16
Field 3 (jump over space if present, pick up dash) = -
Field 2 (jump over space if present, all characters backwards until space reached) = 14
Field 1 (jump over space, all characters backwards until space reached) = 13.5
Field 0 (jump over space and take the rest) = Haemoglobin

Total Cholesterol] 146 110 - 160 mg/dl
Field 5 (all characters backwards from end until space reached) = mg/dl
Field 4 (jump over space, all characters backwards until space or dash reached) = 160
Field 3 (jump over space if present, pick up dash) = -
Field 2 (jump over space if present, all characters backwards until space reached) = 110
Field 1 (jump over space, all characters backwards until space reached) = 146
Field 0 (jump over space and take the rest) = Total Cholesterol]
</code></pre>
","5841406","5841406","2018-06-01 19:49:23","4","1203","MandyShaw","2016-01-26 11:24:03","1000","453","465","251","50647561","","2018-06-01 16:22:28","0","153","<p>I have extracted text from scanned PDF using Tesseract. I've got output string as something like this..</p>

<pre><code>Haemoglobin 13.5 14-16 g/dl
Random Blood Sugar 186 60 - 160 mg/dl
Random Urine Sugar Nil
¢ Blood Urea 43 14-40 mg/dl
4 — Serum Creatinine 2.13 0.4-1.5 mg/dl
Serum Uric Acid 4.9 3.4-7.0 mg/dl
Serum Sodium 142 135 - 150 meq/L
/ Serum Potassium 2.6 3.5-5.0 meq/L
Total Cholesterol] 146 110 - 160 mg/dl
Triglycerides 162 60 - 180 mg/d]
</code></pre>

<p>Now i have to feed this to a dataframe or a csv with all the text in one column and values in other i.e..</p>

<pre><code>**Haemoglobin**            13.5   14-16     g/dl
**Random Blood Sugar**     186    60 - 160  mg/dl
</code></pre>

<p>so far, the best i could get through this is something like this...</p>

<pre><code>  text = text.split('\n')
  text = [x.split(' ') for x in text]
df = pd.DataFrame(text, columns['Header','Detail','a','e,','b','c','d','f'])
df

    Header    Detail   a      e     b      c      d  f
0 Haemoglobin 13.5    14-16   g/dl  None   None  None  None
1 Random      Blood   Sugar   186   60      -     160  mg/dl
2 Random      Urine   Sugar   Nil   None   None  None  None
</code></pre>

<p>Please help!!</p>
","9870073","9870073","2018-06-01 18:46:21","Importing Scanned PDF Extracted text to CSV","<python><regex><csv><dataframe><python-tesseract>","3","2","1213"
"50649107","2018-06-01 18:22:58","0","","<p>You code has several issues to address.</p>

<p>The first problem I see right away is your use of tkinters <code>Tk()</code> more than once. You should only ever create one instance of <code>Tk()</code> and then from there us a combination of <code>Frame</code> and <code>Toplevel</code> to create the rest of your interface.</p>

<p>Next do not use <code>root.wm_withdraw()</code> here to mask the problem you are seeing by using <code>Tk()</code> more than once. Keep in mind anything past the <code>mainloop()</code> wont run anyway until you close your program so that line is useless here.</p>

<p>For the most part this code can be reduced a lot and a cleaner version would look something like this.</p>

<pre><code>import tkinter as tk


class tkMethod(tk.Tk):
    def __init__(self):
        tk.Tk.__init__(self)
        self.title('GETFTP')
        self.ftpentry = tk.Entry(self)
        self.submit = tk.Button(self, text='Submit', command=self.do_something)
        self.submit.pack()
        self.ftpentry.pack()

    def do_something(self):
        x = self.ftpentry.get()
        # do something with x.

if __name__ == '__main__':
    run = tkMethod()
    run.mainloop()
</code></pre>
","7475225","","","1","1202","Mike - SMT","2017-01-26 17:53:11","11286","1306","856","369","50648208","","2018-06-01 17:11:32","0","35","<p>Having an issue with this and thought I would seek out some advice. I have a program that i'm creating which is a simple ftp login client. The following is the code that i'm stuck with :</p>

<p>code form the main module:</p>

<pre><code>from ftplib import FTP
import os
from TKPractice import tk_method



def main():

    P = tk_method()

    print('Welcome to the FTP Directory Transfer Tool.')
    print('You can use this program to move Directories from one platform to 
        another')
    Connect(P)

def Connect(P):

     pingstatus =  P.GETFTP()
</code></pre>

<p>and code from the class i've created:</p>

<p>from tkinter import *</p>

<pre><code>class tk_method(Tk):

  def __init__(self):


      Tk.__init__(self)


 def FTPSUBMIT(self):
     self.ftpentry = self.ftpentry.get()


 def GETFTP(self):
     root = Tk()
     root.title('GETFTP')
     root.wm_withdraw()
     self.ftpentry = Entry(self)
     self.submit = Button(self, text='Submit', command = self.FTPSUBMIT)

     self.submit.pack()
     self.ftpentry.pack()
     root.mainloop()





if __name__ == '__main__':
   root = Tk()
   run = tk_method()
   root.mainloop()
   root.wm_withdraw()
</code></pre>

<p>and the error i'm getting is  self.ftpentry = self.ftpentry.get()
AttributeError: 'str' object has no attribute 'get'. </p>

<p>Please advise! </p>
","9646421","9646421","2018-06-01 17:37:09","Trying to Get FTP Address and use the input to pass to the next function in python","<python><oop><tkinter><attributes>","1","11","1338"
"50649114","2018-06-01 18:23:20","2","","<p>Use <code>conda install tensorflow</code> instead of <code>pip</code> </p>

<p>For installations within <code>anaconda</code> virtual environments its preferable to use <code>conda install package_name</code> </p>

<p>or switch back to <strong>version &lt;1.6</strong> i.e <code>pip install tensorflow==1.5</code></p>

<p>Solution reference : <a href=""https://github.com/tensorflow/tensorflow/issues/17393"" rel=""nofollow noreferrer"">GitHub-TensorFlow</a></p>
","7907591","7907591","2018-06-01 18:28:39","0","462","Prateek","2017-04-23 00:10:35","3392","829","1563","251","50648558","50649114","2018-06-01 17:39:44","1","1210","<p>I've installed CPU version of Tensorflow on the Anaconda3-5.2.0-Windows-x64 on the Windows 10 with the following sequence of commands:</p>

<pre><code>conda create -n tensorflow pip python=3.6
activate tensorflow
pip install --ignore-installed --upgrade tensorflow
</code></pre>

<p>and no errors occured. But then I've tried to import Tensorflow and the following text appeared</p>

<pre><code>Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import tensorflow as tf
Traceback (most recent call last):
File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
return importlib.import_module(mname)
File ""C:\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
return _bootstrap._gcd_import(name[level:], package, level)
File ""&lt;frozen importlib._bootstrap&gt;"", line 994, in _gcd_import
File ""&lt;frozen importlib._bootstrap&gt;"", line 971, in _find_and_load
File ""&lt;frozen importlib._bootstrap&gt;"", line 955, in _find_and_load_unlocked
File ""&lt;frozen importlib._bootstrap&gt;"", line 658, in _load_unlocked
File ""&lt;frozen importlib._bootstrap&gt;"", line 571, in module_from_spec
File ""&lt;frozen importlib._bootstrap_external&gt;"", line 922, in create_module
File ""&lt;frozen importlib._bootstrap&gt;"", line 219, in _call_with_frames_removed
ImportError: DLL load failed:
During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in &lt;module&gt;
from tensorflow.python.pywrap_tensorflow_internal import *
File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in &lt;module&gt;
_pywrap_tensorflow_internal = swig_import_helper()
File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
return importlib.import_module('_pywrap_tensorflow_internal')
File ""C:\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
File ""C:\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 24, in &lt;module&gt;
from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
File ""C:\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in &lt;module&gt;
from tensorflow.python import pywrap_tensorflow
File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in &lt;module&gt;
raise ImportError(msg)
ImportError: Traceback (most recent call last):
File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 14, in swig_import_helper
return importlib.import_module(mname)
File ""C:\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
return _bootstrap._gcd_import(name[level:], package, level)
File ""&lt;frozen importlib._bootstrap&gt;"", line 994, in _gcd_import
File ""&lt;frozen importlib._bootstrap&gt;"", line 971, in _find_and_load
File ""&lt;frozen importlib._bootstrap&gt;"", line 955, in _find_and_load_unlocked
File ""&lt;frozen importlib._bootstrap&gt;"", line 658, in _load_unlocked
File ""&lt;frozen importlib._bootstrap&gt;"", line 571, in module_from_spec
File ""&lt;frozen importlib._bootstrap_external&gt;"", line 922, in create_module
File ""&lt;frozen importlib._bootstrap&gt;"", line 219, in _call_with_frames_removed
ImportError: DLL load failed:

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in &lt;module&gt;
from tensorflow.python.pywrap_tensorflow_internal import *
File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 17, in &lt;module&gt;
_pywrap_tensorflow_internal = swig_import_helper()
File ""C:\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 16, in swig_import_helper
return importlib.import_module('_pywrap_tensorflow_internal')
File ""C:\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.
</code></pre>

<p>I've checked out some similar questions here and I have installed Microsoft Visual C++ 2015 Redistributable.</p>
","9882293","","","Failed to import Tensorflow in Anaconda","<python><tensorflow>","1","0","4794"
"50649144","2018-06-01 18:25:29","-1","","<p>This link was useful to me, to install pytz in visual studio.</p>

<p><a href=""https://stackoverflow.com/questions/15185827/can-pip-be-used-with-python-tools-in-visual-studio?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa"">Can pip be used with Python Tools in Visual Studio?</a></p>

<p>Basically, go to Tools->Python -> Python environments.
Once you select the right version of Python (for me is 3.6 64 bits), you will see a menu with options: Overview, Packages, Intellisense
Click on Packages and you'll see a search input box. Put pytz there. Once the system found it, double click to install it.</p>

<p>Afterwards, you'll need to refresh DB on intellisense option under Python environments to make pytz available. On this way, pytz will be recognized.</p>
","8759469","8759469","2018-06-01 18:37:15","0","799","Danny Stark","2017-10-11 13:56:25","1","2","0","0","47437158","","2017-11-22 14:29:10","1","294","<p>I'm taking an online class to learn python, and I skipped a section that was giving me trouble because I had to install something that Visual Studio 2017 doesn't seem to recognize. Now that I've pretty far into the course, if not almost done, I wanted to figure out what's going on. I'm pretty sure Visual Studio 2017 is where the problem is.</p>

<p>What I'm trying to install is pytz. As far as my computer is concerned, it is downloaded. However, when I run a program in Visual Studio and import pytz, it says pytz is not a module it recognizes.</p>

<p>Is there a way to give Visual Studio a path to what I had downloaded? I looked in the Visual Studio 2017 installer and never saw any mention of pytz in any of the options. Any help is appreciated.</p>
","4855564","4855564","2017-11-22 18:30:08","Installing pytz for Python for Visual Studio 2017","<python><installation><visual-studio-2017><pytz>","1","5","761"
"50649150","2018-06-01 18:25:56","0","","<pre><code>    Date = []
    for i in range(1,32):
      date = '05/'+str(i)+'/2018'
      Date.append(date)
</code></pre>
","9839478","","","0","123","Vizag","2018-05-24 08:27:20","477","78","85","15","50648820","50649150","2018-06-01 18:01:41","-1","376","<p>am trying to create a simple graph for US yields but am stuck with this error: <code>ValueError: x and y must have same first dimension, but have shapes (1,) and (22,)</code></p>

<pre><code>import matplotlib.pyplot as plt
import pandas as pd
import xml.etree.ElementTree as ET


 for i in range(1,32):
  Date= ['05/'+str(i)+'/2018']
    Yield = 

 [1.68,1.69,1.68,1.67,1.69,1.69,
 1.68,1.69,1.68,1.70,1.69,1.69,1.70,
 1.68,1.71,1.73,1.76,1.74,1.70,1.77,1.77,1.76]

plt.plot(Date,Yield)
plt.xlabel('Date')
plt.ylabel('Percentage')
plt.show()
</code></pre>
","5325945","9609447","2018-06-01 18:21:45","ValueError: x and y must have same first dimension , what can I do here?","<python><matplotlib>","1","5","559"
"50649151","2018-06-01 18:25:58","4","","<p>Decorators are applied <em>when the function is declared</em>, so just before the class is created to which the functions are registered as methods.</p>

<p>So <code>func</code> passed into your decorator is <em>not bound to an instance</em>, and you can't just call it without explicitly passing in an instance. Moreover, you don't have access to the instance stored in <code>t</code>, that's created entirely outside of the decorator.</p>

<p>You'd have to explicitly pass along the instance to call the method on:</p>

<pre><code>t = Test()
d.call(t, 123)
</code></pre>

<p>or register methods <em>after</em> creating an instance, in the <code>__init__</code> method. Methods are bound when you look them up as an attribute on an instance, via the <a href=""https://docs.python.org/3/howto/descriptor.html"" rel=""nofollow noreferrer"">decriptor protocol</a>. Only bound methods have a reference to the instance that is to be bound to <code>self</code>:</p>

<pre><code>&gt;&gt;&gt; class Foo:
...     def bar(self):
...         return self
...
&gt;&gt;&gt; Foo.bar  # unbound
&lt;function Foo.bar at 0x108d38f28&gt;
&gt;&gt;&gt; Foo.bar()  # no self to bind to
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
TypeError: bar() missing 1 required positional argument: 'self'
&gt;&gt;&gt; instance = Foo()
&gt;&gt;&gt; instance.bar  # bound method
&lt;bound method Foo.bar of &lt;__main__.Foo object at 0x109a916d8&gt;&gt;
&gt;&gt;&gt; instance.bar.__self__ is instance  # to the instance
True
&gt;&gt;&gt; instance.bar()  # so the instance is passed in for self
&lt;__main__.Foo object at 0x109a916d8&gt;
&gt;&gt;&gt; Foo.bar(instance)  # or you can do it manually
&lt;__main__.Foo object at 0x109a916d8&gt;
</code></pre>

<p>If you <em>do</em> fix your registration to store bound methods, you'll need to take into account that the registration is now an extra reference to the instance, keeping it alive in memory even if all other references to the instance are removed. If this is an issue, you'd need to use <em>weak references to the unbound function and the instance</em>, not to the method, as methods are created <em>dynamically and generally have no other references to them</em>. See <a href=""https://stackoverflow.com/questions/21826700/using-python-weakset-to-enable-a-callback-functionality"">using python WeakSet to enable a callback functionality</a></p>
","100297","100297","2018-06-01 18:38:36","4","2408","Martijn Pieters","2009-05-03 14:53:57","770256","252083","5762","19510","50649020","","2018-06-01 18:16:59","1","92","<p>Finally I fix this ask use code like this:(2018-06-03)</p>

<pre><code>class Main:
    @app.route('^/$')
    def default(self):
        return 'hello from class ccc'

module=sys.modules[func.__module__]
cls=getattr(module,func.__qualname__.replace('.'+func.__name__,''))      
ins=cls()
m=getattr(cls,func.__name__)    
resp.body=m(cls) #func.__module__+'.'+func.__qualname__+' func:'+func.__name__
</code></pre>

<p>That's not pyhonic right?I'm new guy to python</p>

<p>//////old</p>

<pre><code>class D:
    def __init__(self):
        self.handlers={}

    def check(self,func):
        self.handlers['h']=func

        def decorator(*args,**kwargs):
            return func(*args,**kwargs)
        return decorator

    def call(self,p):
        return self.handlers['h'](p)

d=D()

class Test:
    @d.check
    def prt(self,v):
        print(v)

t=Test()
d.call(123)
</code></pre>

<p>There is error info：prt() missing 1 required positional argument: 'v'</p>

<p>It seems need a parameter named 'self' but how can i pass it?</p>

<p>//edit (2018-06-01)</p>

<p>Thanks to all.I ask this cause I try write a python web framework.And I want route to a method of class like below</p>

<pre><code>app=MyFrm()

class Controller:
    @app.route('/')
    def hello():
        return 'hello world'
</code></pre>

<p>But existing do like below.It's no need or nobody to do this in python?</p>

<pre><code>app = Flask(__name__)

@app.route(""/"")
def hello():
    return ""Hello World!""
</code></pre>

<p>Now I fixed this problem use @staticmethod</p>
","9882639","9882639","2018-06-03 09:28:38","Python Is there a way to use decorator wrap a function of class?","<python>","1","5","1547"
"50649164","2018-06-01 18:27:10","0","","<p>Turns out that the order of the images I was using was not the same for right and left camera... I was using</p>

<pre><code>images_left = glob.glob('Calibration/images/set1/left*' + images_format)
images_right = glob.glob('Calibration/images/set1/right*' + images_format)
</code></pre>

<p>When I should have been using something more like:</p>

<pre><code>images_left = sorted(glob.glob('Calibration/images/set1/left*' + images_format))
images_right = sorted(glob.glob('Calibration/images/set1/right*' + images_format))
</code></pre>

<p>This is because glob gets the images in an apparently random order so I was trying to match the wrong images. Now I finally get a 0.4 retval, which is not that bad.</p>
","4769750","","","0","712","Mikel Díez Buil","2015-04-09 14:55:56","151","8","5","0","50630769","50649164","2018-05-31 18:39:24","0","976","<p>Hi everyone I've been digging a bit into computer vision using Python and OpenCV and was trying to calibrate two cameras I've bought in order to do some 3D stereo reconstruction but I'm having some problems with it.</p>

<p>I've followed mostly this <a href=""https://docs.opencv.org/3.3.0/dc/dbb/tutorial_py_calibration.html"" rel=""nofollow noreferrer"">tutorial</a> in order to calibrate the cameras separately (I apply it to both of them) and then I intend to use the cv2.stereoCalibrate to get the relative calibration.</p>

<p>With the single camera calibration everything seems to be working correctly, I get a very low re-proyect error and as far as my knowledge goes the matrices seems to look OK. Here I leave the results of the single camera calibration.</p>

<p>cameraMatrix1 and distCoeffs1:</p>

<pre><code>[[ 951.3607329     0.          298.74117671]
 [   0.          954.23088299  219.20548594]
 [   0.            0.            1.        ]]

[[ -1.07320015e-01  -5.56147908e-01  -1.13339913e-03   1.85969704e-03
    2.24131322e+00]]
</code></pre>

<p>cameraMatrix2 and distCoeffs2:</p>

<pre><code>[[ 963.41078117    0.          362.85971342]
 [   0.          965.66793023  175.63216871]
 [   0.            0.            1.        ]]

[[ -3.31491728e-01   2.26020466e+00   3.86190151e-03  -2.32988011e-03
   -9.82275646e+00]]
</code></pre>

<p>So after having those I do the following (I fix the intrinsics as I already know them from the previous calibrations):</p>

<pre><code>stereocalibration_criteria = (cv2.TERM_CRITERIA_MAX_ITER + cv2.TERM_CRITERIA_EPS, 100, 1e-5)
stereocalibration_flags = cv2.CALIB_FIX_INTRINSIC
stereocalibration_retval, cameraMatrix1, distCoeffs1, cameraMatrix2, distCoeffs2, R, T, E, F = cv2.stereoCalibrate(objpoints,imgpoints_left,imgpoints_right,cameraMatrix1,distCoeffs1,cameraMatrix2,distCoeffs2,gray_left.shape[::-1],criteria = stereocalibration_criteria, flags = stereocalibration_flags)
</code></pre>

<p>I've tried several times to change the flags of the stereoCalibrate and switch the matrices to see if I was mistaken in the order and that mattered but I'm still blocked with this and get a retval of around 30 (and after that I try to rectify the images and of course the result is a disaster).</p>

<p>I've also tried using some calibration images from the internet and I do get the same result so I assume that the problem is not with the images I've taken. If anyone can point me in the right direction or knows what could be it will be very very welcome.</p>
","4769750","","","How to get a good cv2.stereoCalibrate after successful cv2.calibrateCamera","<python><opencv><computer-vision><camera-calibration>","1","0","2520"
"50649188","2018-06-01 18:29:17","0","","<pre><code>def xls_to_csv(data):

    # Read the excel file as a dataframe object
    formatted_dataframe = pd.read_excel(data, index_col=0)

    # Save the dataframe to a csv file in disk. The method returns None.
    formatted_file.to_csv('out.csv')

    # The dataframe object is still here
    final_dataframe = formatted_dataframe

    # The final file NAME
    final_filename = 'out.csv'
</code></pre>

<p>Your variable names are misleading</p>

<ul>
<li>Your <code>formatted_file</code> is in fact a data frame object</li>
<li>Your <code>final_file</code>: it is unclear for me whether you want the <code>filename</code> or <code>the dataframe</code>.</li>
</ul>
","3063243","","","0","670","phi","2013-12-03 20:57:15","2771","83","135","20","50648516","","2018-06-01 17:36:43","1","625","<p>I am attempting to create an upload tool that takes an .xls file and then converts it to a pandas dataframe before finally saving it as a csv file to be processed and analyzed. After the file comes out of this code: </p>

<pre><code>    def xls_to_csv(data):
        #Formats into pandas dataframe. Index removes first column of .xls file.
        formatted_file = pd.read_excel(data, index_col=0)
        #Converts the formatted file into a csv file and saves it.
        final_file = formatted_file.to_csv('out.csv')
</code></pre>

<p>It saves properly and in the right location, however when I attempt to plug the resulted file into other functions that contain loops, I raise a TypeError: 'NoneType' object is not iterable.</p>

<p>The file is saved as 'out.csv' and I am able to open it manually, however the open command won't even work without this error being raised.</p>

<p>Using Python 3.6!</p>

<p>Thanks in advanced! </p>
","9543107","","","Python/Pandas to_csv saving as NoneType and raising TypeError","<python><django><python-3.x><pandas><data-management>","3","1","938"
"50649226","2018-06-01 18:32:05","0","","<p>There should be values. If you use an IDE (i.g. Spyder) you may see it in the Variables Manager.</p>
","4658703","","","0","104","Fabi","2015-03-11 13:46:58","150","24","139","0","39687295","39689711","2016-09-25 13:22:24","2","1242","<p>I was trying to get the pixelvalues of a dicom file in python using the dicom library.</p>

<p>But it returns only an array with zeros. </p>

<p>My code was like this:</p>

<pre><code>import dicom

import numpy 

ds=pydicom.read_file(""sample.dcm"")

print(ds.pixel_array)
</code></pre>

<p>and the results is</p>

<pre><code> [[0 0 0 ..., 0 0 0]
 [0 0 0 ..., 0 0 0]
 [0 0 0 ..., 0 0 0]
 ..., 
 [0 0 0 ..., 0 0 0]
 [0 0 0 ..., 0 0 0]
 [0 0 0 ..., 0 0 0]]
</code></pre>

<p>Do you have any idea how to get the values of the pixels?</p>

<p>Thanks a lot in advance.</p>

<p>Andras</p>
","2955708","","","python dicom -- getting pixel value from dicom file","<python><numpy><pixel><dicom>","3","0","584"
"50649292","2018-06-01 18:37:02","0","","<p>After much trial and error, I found out that it's actually due to the PHP web server's upload_max_filesize being set too low.</p>

<p>My assumption is that the file was being truncated, so it was no longer a valid GIF.  However, I'm not going to research the real reason since it's no longer an issue.</p>
","4034910","","","0","309","matteorr","2014-09-12 13:02:46","53","5","0","0","50648324","","2018-06-01 17:21:36","0","168","<p>I am trying to replicate the following curl call in python3 requests.  However, when it sends the file, for some reason its mimeType is being sent as application/octet-stream with size 0.  I've tried every variation of the requests call as I can think of, and I've even tried hard-coding the mimeType and size, but it doesn't change the request my web server is seeing.</p>

<p>Here's the curl call I want to replicate:</p>

<pre><code>curl -X POST \
  http://myurl/api/v1/content/createFile \
  -H 'cache-control: no-cache' \
  -H 'content-type: multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW' \
  -H 'postman-token: c35cc623-2cd8-1143-ba64-5870156f7498' \
  -F apiKey=ABCD \
  -F file=@bear.gif
</code></pre>

<p>Here is the python code I'm trying:</p>

<pre><code>import requests
files = {
    'apiKey': (None, 'ABCD'),
    'file': ('bear.gif', open('bear.gif', 'rb')),
}
url = 'http://myurl/api/v1/content/createFile'
response = requests.post(url, files=files)
</code></pre>

<p>Here's what my web server sees from the curl call:</p>

<pre><code>""apiKey"" =&gt; ""ABCD""
""file"" =&gt; UploadedFile {#323 ▼
  -test: false
  -originalName: ""bear.gif""
  -mimeType: ""image/gif""
  -size: 1283057
  -error: 0
  #hashName: null
  path: ""/tmp""
  filename: ""phpizaC5J""
  basename: ""phpizaC5J""
  pathname: ""/tmp/phpizaC5J""
  extension: """"
  realPath: ""/tmp/phpizaC5J""
  aTime: 2018-06-01 16:20:30
  mTime: 2018-06-01 16:20:32
  cTime: 2018-06-01 16:20:32
  inode: 1441804
  size: 1283057
  perms: 0100600
  owner: 48
  group: 48
  type: ""file""
  writable: true
  readable: true
  executable: false
  file: true
  dir: false
  link: false
}
</code></pre>

<p>And here's what my web server sees when I post from python-requests:</p>

<pre><code>""apiKey"" =&gt; ""ABCD""
""file"" =&gt; UploadedFile {#323 ▼
  -test: false
  -originalName: ""bear.gif""
  -mimeType: ""application/octet-stream""
  -size: 0
  -error: 1
  #hashName: null
  path: """"
  filename: """"
  basename: """"
  pathname: """"
  extension: """"
  realPath: ""/var/www/html/myurl""
  aTime: 1970-01-01 00:00:00
  mTime: 1970-01-01 00:00:00
  cTime: 1970-01-01 00:00:00
  inode: false
  size: false
  perms: 00
  owner: false
  group: false
  type: false
  writable: false
  readable: false
  executable: false
  file: false
  dir: false
  link: false
}
</code></pre>

<p>What am I doing wrong?</p>
","4034910","","","python requests posting gifs as octet-stream instead of correct mime type","<python><python-3.x><python-requests>","1","0","2359"
"50649364","2018-06-01 18:43:04","2","","<p>You are applying the <code>format</code> function only on the ""test"" string.</p>

<p>Try:</p>

<pre><code>command = ""for i in \`python {0}_getSyslogs.py {1} {2} {3}\`\ndo\ngunzip -c {3}/\$i | egrep -i '{4}' &gt;&gt; test"" .format(a,b,c,d,e)
</code></pre>
","532312","","","0","258","Rakesh","2010-12-06 13:07:54","56694","5302","758","1508","50649316","50649402","2018-06-01 18:38:57","2","58","<p>This makes no sense to me. I define 5 variables:</p>

<pre><code>a='a'
b='b'
c='c'
d='d'
e='e'
</code></pre>

<p>Then I try to build a command using those variables:</p>

<pre><code>command = ""for i in \`python {0}_getSyslogs.py {1} {2} {3}\`\ndo\ngunzip -c {3}/\$i | egrep -i '{4}' &gt;&gt; "" .format(a,b,c,d,e)
</code></pre>

<p>This works as expected, with the resulting command:</p>

<pre><code>""for i in \\`python a_getSyslogs.py b c d\\`\ndo\ngunzip -c d/\\$i | egrep -i 'e' &gt;&gt; ""
</code></pre>

<p>What gets me is if I add one more component to the string (ie ""test""), the whole thing falls apart in that there is no more substitution going on:</p>

<pre><code>command = ""for i in \`python {0}_getSyslogs.py {1} {2} {3}\`\ndo\ngunzip -c {3}/\$i | egrep -i '{4}' &gt;&gt; "" + ""test"" .format(a,b,c,d,e)
</code></pre>

<p>The resulting command is as follow:</p>

<pre><code>""for i in \\`python {0}_getSyslogs.py {1} {2} {3}\\`\ndo\ngunzip -c {3}/\\$i | egrep -i '{4}' &gt;&gt; test""
</code></pre>

<p>This is probably a ""can't see the forest through the trees"" problem, but I've been trying all kinds of different combinations and nothing works. </p>

<p>I'm running python 2.7.10 on CentOS:</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>sys.version
      '2.7.10 (default, Oct  6 2017, 22:29:07) \n[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.31)]'</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>What am I doing wrong?</p>
","4403891","","","python .format not working as expected","<python><format>","4","1","1465"
"50649365","2018-06-01 18:43:07","0","","<p>Why not just,</p>

<pre><code>""... &gt;&gt; test"".format(a,b,c,d,e)
</code></pre>

<p>instead of</p>

<pre><code>""... &gt;&gt; "" + ""test"".format(a,b,c,d,e)
</code></pre>

<hr>

<p><code>.format</code> is only applicable to one <code>string</code> object and you are applying it to <code>""test""</code>.</p>

<p>If you <em>really</em> want to concat two strings, first store them in a variable and then do what you need to do.</p>

<pre><code>str1 = ""example{} "" + ""test""
str1.format('3')
# example3 test
</code></pre>
","4237254","4237254","2018-06-01 18:49:14","0","520","BcK","2014-11-10 21:01:26","1519","137","63","23","50649316","50649402","2018-06-01 18:38:57","2","58","<p>This makes no sense to me. I define 5 variables:</p>

<pre><code>a='a'
b='b'
c='c'
d='d'
e='e'
</code></pre>

<p>Then I try to build a command using those variables:</p>

<pre><code>command = ""for i in \`python {0}_getSyslogs.py {1} {2} {3}\`\ndo\ngunzip -c {3}/\$i | egrep -i '{4}' &gt;&gt; "" .format(a,b,c,d,e)
</code></pre>

<p>This works as expected, with the resulting command:</p>

<pre><code>""for i in \\`python a_getSyslogs.py b c d\\`\ndo\ngunzip -c d/\\$i | egrep -i 'e' &gt;&gt; ""
</code></pre>

<p>What gets me is if I add one more component to the string (ie ""test""), the whole thing falls apart in that there is no more substitution going on:</p>

<pre><code>command = ""for i in \`python {0}_getSyslogs.py {1} {2} {3}\`\ndo\ngunzip -c {3}/\$i | egrep -i '{4}' &gt;&gt; "" + ""test"" .format(a,b,c,d,e)
</code></pre>

<p>The resulting command is as follow:</p>

<pre><code>""for i in \\`python {0}_getSyslogs.py {1} {2} {3}\\`\ndo\ngunzip -c {3}/\\$i | egrep -i '{4}' &gt;&gt; test""
</code></pre>

<p>This is probably a ""can't see the forest through the trees"" problem, but I've been trying all kinds of different combinations and nothing works. </p>

<p>I'm running python 2.7.10 on CentOS:</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>sys.version
      '2.7.10 (default, Oct  6 2017, 22:29:07) \n[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.31)]'</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>What am I doing wrong?</p>
","4403891","","","python .format not working as expected","<python><format>","4","1","1465"
"50649383","2018-06-01 18:44:33","1","","<p><code>self.l</code> is only created when <code>self.submit</code> is called (which itself is only called by the callback of your button), so if <code>retranslateUi</code> is called before <code>submit</code> is, <code>self.l</code> won't exist yet. You should include anything that is expected to exist from the creation of the object in it's <code>__init__</code> method even if the value is just a placeholder such as <code>[]</code>.</p>
","3220135","","","2","444","Aaron","2014-01-21 16:33:44","4899","644","726","73","50649147","","2018-06-01 18:25:38","-1","35","<p>I have a list:</p>

<pre><code>def submit(self):
    self.l = []
</code></pre>

<p>I need to have whatever is in the list display in my GUI</p>

<pre><code>    def retranslateUi(self, Form):
    Form.setWindowTitle(_translate(""Form"", ""Math Game"", None))
    self.lineEdit.setStatusTip(_translate(""Form"", ""Enter answer here"", None))
    self.pushButton.setText(_translate(""Form"", ""SUBMIT"", None))
    self.label.setText(_translate(""Form"", self.l, None))
</code></pre>

<p>I get the Error:</p>

<pre><code>        self.label.setText(_translate(""Form"", self.l, None))
AttributeError: 'Ui_Form' object has no attribute 'l'
</code></pre>

<p>I'm trying to create a kids math game (a college assignment) and whatever is in the list has to display in the GUI, what am I doing wrong? I believe it may be something to do with the brackets in <code>(""Form, self.l, None"")</code> but I'm not too sure.</p>

<p>This is the entire code:</p>

<pre><code>    from PyQt4 import QtCore, QtGui
import random

try:
    _fromUtf8 = QtCore.QString.fromUtf8
except AttributeError:
    def _fromUtf8(s):
        return s

try:
    _encoding = QtGui.QApplication.UnicodeUTF8
    def _translate(context, text, disambig):
        return QtGui.QApplication.translate(context, text, disambig, _encoding)
except AttributeError:
    def _translate(context, text, disambig):
        return QtGui.QApplication.translate(context, text, disambig)

class Ui_Form(object):
    #this is backend code
    def submit(self):
        self.l = []
        if self.lineEdit.text == self.l():
            print(""Correct!"")
        else:
            print(""Incorrect!"")
        #this is GUI code
    def setupUi(self, Form):
        Form.setObjectName(_fromUtf8(""Form""))
        Form.setEnabled(True)
        Form.resize(311, 192)
        Form.setLayoutDirection(QtCore.Qt.LeftToRight)
        Form.setAutoFillBackground(True)
        self.lineEdit = QtGui.QLineEdit(Form)
        self.lineEdit.setGeometry(QtCore.QRect(10, 140, 191, 41))
        self.lineEdit.setAlignment(QtCore.Qt.AlignCenter)
        font = QtGui.QFont()
        font.setFamily(_fromUtf8(""Calibri""))
        font.setPointSize(20)
        font.setFamily(_fromUtf8(""Calibri""))
        self.lineEdit.setFont(font)
        self.lineEdit.setObjectName(_fromUtf8(""lineEdit""))
        self.pushButton = QtGui.QPushButton(Form)
        self.pushButton.setGeometry(QtCore.QRect(210, 140, 91, 41))
        self.pushButton.clicked.connect(self.submit)
        font = QtGui.QFont()
        font.setFamily(_fromUtf8(""Calibri""))
        font.setPointSize(12)
        self.pushButton.setFont(font)
        self.pushButton.setObjectName(_fromUtf8(""pushButton""))
        self.widget = QtGui.QWidget(Form)
        self.widget.setGeometry(QtCore.QRect(10, 10, 291, 121))
        self.widget.setAutoFillBackground(True)
        p = self.widget.palette()
        p.setColor(self.widget.backgroundRole(), QtCore.Qt.white)
        self.widget.setPalette(p)
        self.widget.setObjectName(_fromUtf8(""widget""))
        self.label = QtGui.QLabel(self.widget)
        self.label.setGeometry(QtCore.QRect(60, 40, 341, 41))
        font = QtGui.QFont()
        font.setPointSize(24)
        self.label.setFont(font)
        self.label.setLayoutDirection(QtCore.Qt.LeftToRight)
        self.label.setObjectName(_fromUtf8(""label""))
        self.widget.raise_()
        self.lineEdit.raise_()
        self.pushButton.raise_()
        self.retranslateUi(Form)
        QtCore.QMetaObject.connectSlotsByName(Form)

        #translation of names from output name to edited name
    def retranslateUi(self, Form):
        Form.setWindowTitle(_translate(""Form"", ""Math Game"", None))
        self.lineEdit.setStatusTip(_translate(""Form"", ""Enter answer here"", None))
        self.pushButton.setText(_translate(""Form"", ""SUBMIT"", None))
        self.label.setText(_translate(""Form"", self.l, None))
</code></pre>
","","","","How do I get something from my list to display in the GUI?","<python><list><user-interface><pyqt4><python-3.4>","1","3","3900"
"50649402","2018-06-01 18:46:04","1","","<p>Try this: <code>(command+""test"").format(a,b,c,d,e)</code> i.e. putting it in parentheses. Your current approach only uses format on <code>""test""</code></p>
","8518433","","","1","159","unholy_me","2017-08-25 20:10:02","330","61","25","6","50649316","50649402","2018-06-01 18:38:57","2","58","<p>This makes no sense to me. I define 5 variables:</p>

<pre><code>a='a'
b='b'
c='c'
d='d'
e='e'
</code></pre>

<p>Then I try to build a command using those variables:</p>

<pre><code>command = ""for i in \`python {0}_getSyslogs.py {1} {2} {3}\`\ndo\ngunzip -c {3}/\$i | egrep -i '{4}' &gt;&gt; "" .format(a,b,c,d,e)
</code></pre>

<p>This works as expected, with the resulting command:</p>

<pre><code>""for i in \\`python a_getSyslogs.py b c d\\`\ndo\ngunzip -c d/\\$i | egrep -i 'e' &gt;&gt; ""
</code></pre>

<p>What gets me is if I add one more component to the string (ie ""test""), the whole thing falls apart in that there is no more substitution going on:</p>

<pre><code>command = ""for i in \`python {0}_getSyslogs.py {1} {2} {3}\`\ndo\ngunzip -c {3}/\$i | egrep -i '{4}' &gt;&gt; "" + ""test"" .format(a,b,c,d,e)
</code></pre>

<p>The resulting command is as follow:</p>

<pre><code>""for i in \\`python {0}_getSyslogs.py {1} {2} {3}\\`\ndo\ngunzip -c {3}/\\$i | egrep -i '{4}' &gt;&gt; test""
</code></pre>

<p>This is probably a ""can't see the forest through the trees"" problem, but I've been trying all kinds of different combinations and nothing works. </p>

<p>I'm running python 2.7.10 on CentOS:</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>sys.version
      '2.7.10 (default, Oct  6 2017, 22:29:07) \n[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.31)]'</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>What am I doing wrong?</p>
","4403891","","","python .format not working as expected","<python><format>","4","1","1465"
"50649410","2018-06-01 18:46:29","0","","<p>I solved my problem by not taking a part of the original dataframe, but by getting a set of indices from it, for the mini example:</p>

<pre><code>d = {'A': [1,3,5,7], 'B': [2,4,6,8]}
df = pd.DataFrame(data=d)
temp = (df.loc[:,'A'] &gt;= 3) &amp; (df.loc[:,'A'] &lt;= 5)
df.loc[temp,'A'] = 0
print df
</code></pre>

<p>gives the desired result, I could imagine there are better solutions but it's not completely horrible anymore.</p>
","8670914","","","0","437","Sebastian","2017-09-25 15:04:58","1","4","0","0","50645098","","2018-06-01 13:58:54","0","48","<p>I'm trying to perform some manipulations on a specific part of a pandas dataframe. For that purpose I would like to get a view of my dataframe, save that in a variable and work with the variable (so as to not have to recompute the view every time). My code so far:</p>

<pre><code>spikes.loc[(stims.at[i, 'StimOnset'] &lt;= spikes['SpikeTimes']) &amp; (spikes['SpikeTimes'] &lt;= stims.at[i, 'StimOffset']), 'StimPeriod'] = True
temp = spikes.loc[(stims.at[i, 'StimOnset'] &lt;= spikes['SpikeTimes']) &amp; (spikes['SpikeTimes'] &lt;= stims.at[i, 'StimOffset'])]
temp['StimPeriod'] = True
</code></pre>

<p>(Sorry that it's so long, I don't see why it shouldn't work so I made sure ot include everything). So the first line performs the necessary computation, however the two following do not, and they also throw a warning. Now I could use the style of the first line for multiple computations, but that doesn't seem efficient. Is there any way around this so as to save the desired view in a variable?</p>

<p>A minimal code example to produce this effect:</p>

<pre><code>d = {'A': [1,3,5,7], 'B': [2,4,6,8]}
df = pd.DataFrame(data=d)
df.loc[(3 &lt;= df['A']) &amp; (df['A'] &lt;= 5), 'A'] = 0
print df
df = pd.DataFrame(data=d)
temp = df.loc[(3 &lt;= df['A']) &amp; (df['A'] &lt;= 5)]
temp['A'] = 0
print df
</code></pre>

<p>the first part produces the desired effect, the second does not.
The warning goes as follows: </p>

<blockquote>
  <p>/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:8:
  SettingWithCopyWarning:  A value is trying to be set on a copy of a
  slice from a DataFrame. Try using .loc[row_indexer,col_indexer] =
  value instead</p>
  
  <p>See the caveats in the documentation:
  <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy"" rel=""nofollow noreferrer"">http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy</a></p>
</blockquote>
","8670914","8670914","2018-06-01 18:14:36","Pandas, how to save a complex view in a variable","<python><pandas>","1","1","1947"
"50649411","2018-06-01 18:46:45","3","","<p>The problem with your code is a matter of buffering; the tldr is that you can fix it like this:</p>

<pre><code>sort_data.write(my_cols + '\n')
sort_data.flush()
subprocess.run(my_cmd, stdout=sort_data)
</code></pre>

<p>If you want to understand why it happens, and how the fix solves it:</p>

<p>When you open a file in text mode, you're opening a buffered file. Writes go into the buffer, and the file object doesn't necessarily flush them to disk immediately. (There's also stream-encoding from Unicode to bytes going on, but that doesn't really add a new problem, it just adds two layers where the same thing can happen, so let's ignore that.)</p>

<p>As long as all of your writes are to the buffered file object, that's fine—they get sequenced properly in the buffer, so they get sequenced properly on the disk.</p>

<p>But if you write to the underlying <code>sort_data.buffer.raw</code> disk file, or to the <code>sort_data.fileno()</code> OS file descriptor, those writes may get ahead of the ones that went to <code>sort_data</code>.</p>

<p>And that's exactly what happens when you use the file as a pipe in <code>subprocess</code>. This doesn't seem to be explained directly, but can be inferred from <a href=""https://docs.python.org/3/library/subprocess.html#frequently-used-arguments"" rel=""nofollow noreferrer"">Frequently Used Arguments</a>:</p>

<blockquote>
  <p><em>stdin</em>, <em>stdout</em> and <em>stderr</em> specify the executed program’s standard input, standard output and standard error file handles, respectively. Valid values are <code>PIPE</code>, <code>DEVNULL</code>, an existing file descriptor (a positive integer), an existing file object, and <code>None</code>.</p>
</blockquote>

<p>This implies pretty strongly—if you know enough about the way piping works on *nix and Windows—that it's passing the actual file descriptor/handle to the underlying OS functionality. But it doesn't actually say that. To really be sure, you have to check <a href=""https://github.com/python/cpython/blob/3.6/Lib/subprocess.py#L1154"" rel=""nofollow noreferrer"">the Unix source</a> and <a href=""https://github.com/python/cpython/blob/3.6/Lib/subprocess.py#L878"" rel=""nofollow noreferrer"">Windows source</a>, where you can see that it is calling <code>fileno</code> or <code>msvcrt.get_osfhandle</code> on the file objects.</p>
","908494","908494","2018-06-01 18:58:34","0","2345","abarnert","2011-08-23 20:55:30","272076","19605","5576","4509","50649160","50649411","2018-06-01 18:26:36","1","78","<p>I am merging several dataframes into one and sorting them using <code>unix sort</code>. Before I write the final sorted data I would like to add a prefix/header to that output.</p>

<p>So, my code is something like: </p>

<pre><code>my_cols =  '\t'.join(['CHROM', 'POS', ""REF"" ....])

my_cmd = [""sort"", ""-k1,2"", ""-V"", ""final_merged.txt""]

with open(output + 'mergedAndSorted.txt', 'w') as sort_data:
    sort_data.write(my_cols + '\n')  
    subprocess.run(my_cmd, stdout=sort_data)
</code></pre>

<p>But, this above doe puts <code>my_cols</code> at the end of the final output file (i.e <strong>mergedAndSorted.txt</strong>)</p>

<p>I also tried substituting:</p>

<pre><code>sort_data=io.StringIO(my_cols)  
</code></pre>

<p>but this gives me an error as I had expected.</p>

<p><br></p>

<p>How can I add that header to the begining of the subprocess output. I believe this can be achieved by a simple code change.</p>
","6346698","6346698","2018-06-01 18:37:23","add header to stdout of a subprocess in python","<python><subprocess><stdout><prepend>","1","8","926"
"50649431","2018-06-01 18:48:07","1","","<p>Map fields in python protobuf generated code operate pretty similarly to python dicts, so you can use .update() to copy over:</p>

<pre><code>dst.properties.update(src.properties)
</code></pre>
","6958629","","","0","197","fwph","2016-10-11 08:31:56","11","1","0","0","48672377","","2018-02-07 20:05:41","2","1003","<p><a href=""https://developers.google.com/protocol-buffers/docs/reference/python-generated#map-fields"" rel=""nofollow noreferrer"">Python Generated Code</a> explains most use-cases of protobuf <a href=""https://developers.google.com/protocol-buffers/docs/reference/python-generated#map-fields"" rel=""nofollow noreferrer"">map fields</a> in Python but not how to copy one map to another.</p>

<p>Given simple map</p>

<pre><code>message Src {
    map&lt;string, string&gt; properties = 1;
    ...
}

message Dst {
    map&lt;string, string&gt; properties = 1;
    ...
}
</code></pre>

<p>You cannot assign a value to an embedded message field, so there's no doing:</p>

<pre><code># Will not work.
dst = Dst()
dst.properties = src.properties
</code></pre>

<p>Nor is there an implementation of <a href=""https://developers.google.com/protocol-buffers/docs/reference/python/google.protobuf.message.Message-class#CopyFrom"" rel=""nofollow noreferrer"">CopyFrom</a> since map is not itself a message, it's a field within a message.</p>

<pre><code># Will not work.
dst = Dst()
dst.properties.CopyFrom(src.properties)
</code></pre>

<p>I also can't copy the entire message since I only want the map.</p>

<pre><code># Copies unwanted fields!
dst = Dst()
dst.CopyFrom(src)
</code></pre>

<p>I hope I don't have to iterate over all keys and assign one-by-one!</p>

<pre><code># Iterate over map keys
for key in src.properties:
    dst.properties[key] = src.properties[key]
</code></pre>
","828547","","","Is there a copy constructor for Map Fields in Python Protocol Buffers?","<python><protocol-buffers>","1","1","1471"
"50649465","2018-06-01 18:50:10","1","","<p>I should point that this needs a lot of work and honestly you haven't tried anything yet. But to give you a head start here's a code that cleans up some of the obvious problems in your input:</p>

<pre><code>import re
def isnum(x):
    try:
        float(x)
        return True
    except:
        return False

def clean_line(lnin):
    # clean the leading garbage
    ln=re.sub('^[^A-Za-z]+','',lnin).split()
    for i in range(len(ln)):
        if isnum(ln[i]):
            ind=i
            break
    Header=' '.join(ln[:ind])
    ln=[Header]+ln[ind:]
    if '-' in ln:
        ind=ln.index('-')
        ln[ind-1]=ln[ind-1]+'-'+ln[ind+1]
        del ln[ind:ind+2]
    return ln
</code></pre>

<p>Use the <code>clean_line</code> function to clean each of your lines. Then you can feed it to a dataframe.</p>
","1245694","1245694","2018-06-01 20:03:18","5","814","anishtain4","2012-03-02 18:03:48","1309","173","254","34","50647561","","2018-06-01 16:22:28","0","153","<p>I have extracted text from scanned PDF using Tesseract. I've got output string as something like this..</p>

<pre><code>Haemoglobin 13.5 14-16 g/dl
Random Blood Sugar 186 60 - 160 mg/dl
Random Urine Sugar Nil
¢ Blood Urea 43 14-40 mg/dl
4 — Serum Creatinine 2.13 0.4-1.5 mg/dl
Serum Uric Acid 4.9 3.4-7.0 mg/dl
Serum Sodium 142 135 - 150 meq/L
/ Serum Potassium 2.6 3.5-5.0 meq/L
Total Cholesterol] 146 110 - 160 mg/dl
Triglycerides 162 60 - 180 mg/d]
</code></pre>

<p>Now i have to feed this to a dataframe or a csv with all the text in one column and values in other i.e..</p>

<pre><code>**Haemoglobin**            13.5   14-16     g/dl
**Random Blood Sugar**     186    60 - 160  mg/dl
</code></pre>

<p>so far, the best i could get through this is something like this...</p>

<pre><code>  text = text.split('\n')
  text = [x.split(' ') for x in text]
df = pd.DataFrame(text, columns['Header','Detail','a','e,','b','c','d','f'])
df

    Header    Detail   a      e     b      c      d  f
0 Haemoglobin 13.5    14-16   g/dl  None   None  None  None
1 Random      Blood   Sugar   186   60      -     160  mg/dl
2 Random      Urine   Sugar   Nil   None   None  None  None
</code></pre>

<p>Please help!!</p>
","9870073","9870073","2018-06-01 18:46:21","Importing Scanned PDF Extracted text to CSV","<python><regex><csv><dataframe><python-tesseract>","3","2","1213"
"50649490","2018-06-01 18:51:32","2","","<p>The question states array, and by that if we are talking about NumPy arrays, we can surely use few obvious NumPy tricks and few not-so obvious ones. We can surely use <code>slicing</code> to get a 2D view into the input under certain conditions.</p>

<p>Now, based on the array length, let's call it <code>l</code> and <code>m</code>, we would have three scenarios :</p>

<h3>Scenario #1 :<code>l</code> is divisible by <code>n</code></h3>

<p>We can use slicing and reshaping to get a view into the input array and hence get constant runtime.</p>

<p>Verify the view concept :</p>

<pre><code>In [108]: a = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

In [109]: m = 2; n = 5

In [110]: a.reshape(-1,n)[:,:m]
Out[110]: 
array([[1, 2],
       [6, 7]])

In [111]: np.shares_memory(a, a.reshape(-1,n)[:,:m])
Out[111]: True
</code></pre>

<p>Check timings on a very large array and hence constant runtime claim :</p>

<pre><code>In [118]: a = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

In [119]: %timeit a.reshape(-1,n)[:,:m]
1000000 loops, best of 3: 563 ns per loop

In [120]: a = np.arange(10000000)

In [121]: %timeit a.reshape(-1,n)[:,:m]
1000000 loops, best of 3: 564 ns per loop
</code></pre>

<p><strong>To get flattened version :</strong></p>

<p>If we <strong>have</strong> to get a flattened array as output, we just need to use a flattening operation with <code>.ravel()</code>, like so -</p>

<pre><code>In [127]: a = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

In [128]: m = 2; n = 5

In [129]: a.reshape(-1,n)[:,:m].ravel()
Out[129]: array([1, 2, 6, 7])
</code></pre>

<p>Timings show that it's not too bad when compared with the other looping and vectorized numpy.where versions from other posts -</p>

<pre><code>In [143]: a = np.arange(10000000)

# @Kevin's soln
In [145]: %timeit [x for i,x in enumerate(a) if i%n &lt; m]
1 loop, best of 3: 1.23 s per loop

# @jpp's soln
In [147]: %timeit a[np.where(np.arange(a.shape[0]) % n &lt; m)]
10 loops, best of 3: 145 ms per loop

In [144]: %timeit a.reshape(-1,n)[:,:m].ravel()
100 loops, best of 3: 16.4 ms per loop
</code></pre>

<h3>Scenario #2 :<code>l</code> is not divisible by <code>n</code>, but the groups end with a complete one at the end</h3>

<p>We go to the non-obvious NumPy methods with <a href=""http://www.scipy-lectures.org/advanced/advanced_numpy/#example-fake-dimensions-with-strides"" rel=""nofollow noreferrer""><strong><code>np.lib.stride_tricks.as_strided</code></strong></a> that allows to go beyoond the memory block bounds (hence we need to be careful here to not write into those) to facilitate a solution using <code>slicing</code>. The implementation would look something like this -</p>

<pre><code>def select_groups(a, m, n):
    a = np.asarray(a)
    strided = np.lib.stride_tricks.as_strided

    # Get params defining the lengths for slicing and output array shape    
    nrows = len(a)//n
    add0 = len(a)%n
    s = a.strides[0]
    out_shape = nrows+int(add0!=0),m

    # Finally stride, flatten with reshape and slice
    return strided(a, shape=out_shape, strides=(s*n,s))
</code></pre>

<p>A sample run to verify that the output is a <code>view</code> -</p>

<pre><code>In [151]: a = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13])

In [152]: m = 2; n = 5

In [153]: select_groups(a, m, n)
Out[153]: 
array([[ 1,  2],
       [ 6,  7],
       [11, 12]])

In [154]: np.shares_memory(a, select_groups(a, m, n))
Out[154]: True
</code></pre>

<p>To get flattened version, append with <code>.ravel()</code>.</p>

<p>Let's get some timings comparison -</p>

<pre><code>In [158]: a = np.arange(10000003)

In [159]: m = 2; n = 5

# @Kevin's soln
In [161]: %timeit [x for i,x in enumerate(a) if i%n &lt; m]
1 loop, best of 3: 1.24 s per loop

# @jpp's soln
In [162]: %timeit a[np.where(np.arange(a.shape[0]) % n &lt; m)]
10 loops, best of 3: 148 ms per loop

In [160]: %timeit select_groups(a, m=m, n=n)
100000 loops, best of 3: 5.8 µs per loop
</code></pre>

<p>If we need a flattened version, it's still not too bad -</p>

<pre><code>In [163]: %timeit select_groups(a, m=m, n=n).ravel()
100 loops, best of 3: 16.5 ms per loop
</code></pre>

<h3>Scenario #3 :<code>l</code> is not divisible by <code>n</code>,and the groups end with a <em>incomplete one</em> at the end</h3>

<p>For this case, we would need an extra slicing at the end on top of what we had in the previous method, like so -</p>

<pre><code>def select_groups_generic(a, m, n):
    a = np.asarray(a)
    strided = np.lib.stride_tricks.as_strided

    # Get params defining the lengths for slicing and output array shape    
    nrows = len(a)//n
    add0 = len(a)%n
    lim = m*(nrows) + add0
    s = a.strides[0]
    out_shape = nrows+int(add0!=0),m

    # Finally stride, flatten with reshape and slice
    return strided(a, shape=out_shape, strides=(s*n,s)).reshape(-1)[:lim]
</code></pre>

<p>Sample run -</p>

<pre><code>In [166]: a = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])

In [167]: m = 2; n = 5

In [168]: select_groups_generic(a, m, n)
Out[168]: array([ 1,  2,  6,  7, 11])
</code></pre>

<p>Timings -</p>

<pre><code>In [170]: a = np.arange(10000001)

In [171]: m = 2; n = 5

# @Kevin's soln
In [172]: %timeit [x for i,x in enumerate(a) if i%n &lt; m]
1 loop, best of 3: 1.23 s per loop

# @jpp's soln
In [173]: %timeit a[np.where(np.arange(a.shape[0]) % n &lt; m)]
10 loops, best of 3: 145 ms per loop

In [174]: %timeit select_groups_generic(a, m, n)
100 loops, best of 3: 12.2 ms per loop
</code></pre>
","3293881","","","0","5484","Divakar","2014-02-10 17:11:35","173564","13312","6892","97","50647167","50647227","2018-06-01 15:54:16","26","1388","<p>I am trying to get <em>m</em> values while stepping through every <em>n</em> elements of an array. For example, for <em>m</em> = 2 and <em>n</em> = 5, and given</p>

<pre><code>a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
</code></pre>

<p>I want to retrieve</p>

<pre><code>b = [1, 2, 6, 7]
</code></pre>

<p>Is there a way to do this using slicing? I can do this using a nested list comprehension, but I was wondering if there was a way to do this using the indices only. For reference, the list comprehension way is:</p>

<pre><code> b = [k for j in [a[i:i+2] for i in range(0,len(a),5)] for k in j]
</code></pre>
","9877290","9209546","2018-06-08 18:00:57","Stepping with multiple values while slicing an array in Python","<python><arrays><list>","7","2","611"
"50649495","2018-06-01 18:52:00","0","","<p>Another dictionary (hash) approach.</p>

<pre><code>def gp(s):
    gradhash = {
        ""A+"": 4.0 + 0.3
        ,""A"": 4.0
        ,""A-"": 4.0 - 0.3
        ,""B+"": 3.0 + 0.3
        ,""B"": 3.0
        ,""B-"": 3.0 - 0.3
        ,""C+"": 2.0 + 0.3
        ,""C"": 2.0
        ,""C-"": 2.0 - 0.3
        ,""D+"": 1.0 + 0.3
        ,""D"": 1.0
        ,""D-"": 1.0 - 0.3
    }

    return gradhash[s]

print(gp('A-'))
</code></pre>
","447901","","","0","415","lit","2010-09-15 00:39:13","7163","1181","713","75","50646663","","2018-06-01 15:23:59","-1","58","<p>I must modify the function gp so it will handle + and - grades by adding or subtracting 0.3 points. For example, a B + is worth 3.3 points, and a C- is 1.7 points. </p>

<p>Example. </p>

<pre><code>&gt;&gt;&gt; gp('A-')
3.7

&gt;&gt;&gt;gp('B+')
3.3
</code></pre>

<p>The suggestion is I could just add a bunch of elif clauses to test each grade separately, but a similar design is to use a call to s.startswith to figure out the value of the letter grade, then use s.endswith to see if you should add or subtract 0.3 points.</p>

<p>So far this is what I have.</p>

<pre><code>def gp(s):
       A = 4
       return A
       B = 3
       return B
       C = 2
       return C
       D = 1
       return D
       F = 0
       return f
</code></pre>
","9862492","1639625","2018-06-01 15:25:58","Modifying a function to return different values","<python><function><variables><return>","5","4","752"
"50649505","2018-06-01 18:52:29","-1","","<p>Using regular expressions the below sample code will parse the text to a CSV string with the tokens: description, result, normal_value, unit.</p>

<p>Note the list test_results is usually read from a file using:</p>

<p>with open('name_test_file') as test_file:
    test_results = test_file.read().splitlines()</p>

<pre><code>import re
tests = 'Haemoglobin 13.5 14-16 g/dl\nRandom Blood Sugar 186 60 - 160 mg/dl\n'\
    'Random Urine Sugar Nil\n¢ Blood Urea 43 14-40 mg/dl\n'\
    '4 — Serum Creatinine 2.13 0.4-1.5 mg/dl\n'\
    'Serum Uric Acid 4.9 3.4-7.0 mg/dl\nSerum Sodium 142 135 - 150 meq/L\n'\
    '/ Serum Potassium 2.6 3.5-5.0 meq/L\n'\
    'Total Cholesterol] 146 110 - 160 mg/dl\n'\
    'Triglycerides 162 60 - 180 mg/d]\n'

test_results = tests.splitlines()

for test_result in test_results:
    print('input :', test_result)

    m = re.search(r'.*?(?=[a-zA-Z][a-zA-Z])(?P&lt;description&gt;.*?)(?=[ ][0-9])'
              r'[ ](?P&lt;result&gt;[0-9.]*?)(?=[ ][0-9])'
              r'[ ](?P&lt;normal_value&gt;[ 0-9.\-]*?)(?=[ ][a-zA-Z])'
              r'[ ](?P&lt;unit&gt;.[ a-zA-Z/]*)',
              test_result)

    if m is not None:
        normal_value = m.group('normal_value')
        unit = m.group('unit')

    else:
        m = re.search(r'.*?(?=[a-zA-Z][a-zA-Z])(?P&lt;description&gt;.*?)(?=[ ]Nil)'
                  r'[ ](?P&lt;result&gt;Nil).*',
                  test_result)
        normal_value = ''
        unit = ''

    if m is not None:
        description = m.group('description')
        result = m.group('result')

    else:
        description = test_result
        result = ''

    write_string = description + ',' + result + ',' + normal_value + ',' + unit
    print(write_string)
</code></pre>
","9874393","9874393","2018-06-02 22:55:00","2","1743","Bruno Vermeulen","2018-05-31 06:50:27","849","92","106","2","50647561","","2018-06-01 16:22:28","0","153","<p>I have extracted text from scanned PDF using Tesseract. I've got output string as something like this..</p>

<pre><code>Haemoglobin 13.5 14-16 g/dl
Random Blood Sugar 186 60 - 160 mg/dl
Random Urine Sugar Nil
¢ Blood Urea 43 14-40 mg/dl
4 — Serum Creatinine 2.13 0.4-1.5 mg/dl
Serum Uric Acid 4.9 3.4-7.0 mg/dl
Serum Sodium 142 135 - 150 meq/L
/ Serum Potassium 2.6 3.5-5.0 meq/L
Total Cholesterol] 146 110 - 160 mg/dl
Triglycerides 162 60 - 180 mg/d]
</code></pre>

<p>Now i have to feed this to a dataframe or a csv with all the text in one column and values in other i.e..</p>

<pre><code>**Haemoglobin**            13.5   14-16     g/dl
**Random Blood Sugar**     186    60 - 160  mg/dl
</code></pre>

<p>so far, the best i could get through this is something like this...</p>

<pre><code>  text = text.split('\n')
  text = [x.split(' ') for x in text]
df = pd.DataFrame(text, columns['Header','Detail','a','e,','b','c','d','f'])
df

    Header    Detail   a      e     b      c      d  f
0 Haemoglobin 13.5    14-16   g/dl  None   None  None  None
1 Random      Blood   Sugar   186   60      -     160  mg/dl
2 Random      Urine   Sugar   Nil   None   None  None  None
</code></pre>

<p>Please help!!</p>
","9870073","9870073","2018-06-01 18:46:21","Importing Scanned PDF Extracted text to CSV","<python><regex><csv><dataframe><python-tesseract>","3","2","1213"
"50649528","2018-06-01 18:54:31","0","","<p>I cannot test without the data, but this should work.</p>

<p>First, transform your data to <a href=""http://www.jeannicholashould.com/tidy-data-in-python.html"" rel=""nofollow noreferrer"">tidy form</a> </p>

<pre><code>df = df.melt(id_vars=['method'])
# method | variable | value
#   A    | Feature1 |  ...  
#   A    | Feature2 |  ...   
</code></pre>

<p>Then, use the standard seaborn API</p>

<pre><code>sns.violinplot(x='variable', y='value', hue='method', data=df)
</code></pre>
","3063243","","","0","486","phi","2013-12-03 20:57:15","2771","83","135","20","50423426","","2018-05-19 08:43:06","0","221","<p>My data format is like this:</p>

<pre><code>+--------+----------+----------+----------+----------+----------+----------+
| method | Feature1 | Feature2 | Feature3 | Feature4 | Feature5 | Feature6 |
+--------+----------+----------+----------+----------+----------+----------+
| A      | value    | value    | value    | value    | value    | value    |
+--------+----------+----------+----------+----------+----------+----------+
| B      | value    | value    | value    | value    | value    | value    |
+--------+----------+----------+----------+----------+----------+----------+
| A      | value    | value    | value    | value    | value    | value    |
+--------+----------+----------+----------+----------+----------+----------+
</code></pre>

<p>I want to plot violinplot like this:</p>

<p><a href=""https://i.stack.imgur.com/DWRrK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DWRrK.png"" alt=""enter image description here""></a></p>

<p>Where the X-axis is the features and Y-axis is the whole column value, and hue to method. So how to plot with seaborn?
I do read the example code, which seems like I have to reconstruct my data?</p>
","9815037","6296561","2018-05-19 08:53:43","How to plot split violinplot or group boxplot with seaborn?","<python><statistics><seaborn>","1","0","1170"
"50649548","2018-06-01 18:55:26","4","","<p>You can launch the correct version of Spyder by launching from <a href=""https://i.stack.imgur.com/BHxhI.png"" rel=""nofollow noreferrer"">Ananconda's Navigator</a>. From the dropdown, switch to your desired environment and then press the launch Spyder button. You should be able to check the results <a href=""https://i.stack.imgur.com/TnkOQ.png"" rel=""nofollow noreferrer"">right away.</a> </p>
","9205210","","","0","393","Francisco Camargo","2018-01-11 16:59:36","46","7","8","0","43592879","","2017-04-24 16:04:50","22","87078","<p>I am using 3.6 Python version in anaconda spyder on my mac. But I want to change it to Python 2.7. </p>

<p>Can any one tell me how to do that?</p>
","7695282","1181911","2018-06-12 18:39:17","How to change python version in anaconda spyder","<python><anaconda><spyder>","7","1","151"
"50649584","2018-06-01 18:58:25","0","","<h1>Problem 1:</h1>

<p>set <code>xlim</code> and <code>ylim</code>:</p>

<pre><code>plt.xlim((0,max(X)))
plt.ylim((0,max(X**2+A*X+2)))
</code></pre>

<h1>Problem 2:</h1>

<p>use <code>\n</code> for newlines, and use <code>verticalalignment='top'</code>:</p>

<pre><code>plt.text(min(X), max(X**2+A*X+2),'a=12\nb=2.09\n$S_{xy}$= 0.71\nr= 0.9\n$R^2$= 0.85',fontsize=12,fontweight='bold', verticalalignment='top')
</code></pre>

<p>Code:</p>

<pre><code>import matplotlib.pyplot as plt
X = np.arange(0,10,0.1)
A = 2
plt.plot(X,X**2+A*X+2,'b-')
plt.text(min(X), max(X**2+A*X+2),'a=12\nb=2.09\n$S_{xy}$= 0.71\nr= 0.9\n$R^2$= 0.85',fontsize=12,fontweight='bold', verticalalignment='top')
plt.xlabel('X_value')
plt.ylabel('Y_value')
plt.xlim((0,max(X)))
plt.ylim((0,max(X**2+A*X+2)))


plt.grid(True)
</code></pre>

<p><a href=""https://i.stack.imgur.com/o1mg0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/o1mg0.png"" alt=""enter image description here""></a></p>
","6671176","","","0","976","sacuL","2016-08-03 07:43:41","32843","1871","2095","286","50649461","50649584","2018-06-01 18:49:42","0","26","<p>I have the following plot which has variable <code>X</code> and <code>Y</code> values. In order to explain the issue I have assigend constant values for <code>X</code> and <code>Y</code> in the code below.</p>

<p>I want that the figure coordinates will start from <code>(0,0)</code> and also how can I put the text values<code>(a,b,Sxy,r,R^2)</code> under each other and not next to each other as showen below. </p>

<p>Note that I can't use specific values of <code>X,Y</code> because they are variable. So thats why I have to use <code>min()</code> and <code>max()</code> functions as coordinates to show the text values on upper left corner. </p>

<p>code:</p>

<pre><code>import numpy as np
from matplotlib.pyplot import *
X = np.arange(0,10,0.1)
A = 2
plot(X,X**2+A*X+2,'b-')
text(min(X), max(X**2+A*X+2),r'a=12, b=2.09 $S_{xy}$= 0.71 r= 0.9 $R^2$= 0.85',fontsize=12,fontweight='bold')
xlabel('X_value')
ylabel('Y_value')

grid(True)
show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/p8NdG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p8NdG.png"" alt=""enter image description here""></a></p>
","9836518","9836518","2018-06-01 18:54:31","Adjust Figure in Matplotlib","<python><python-3.x><matplotlib>","2","3","1131"
"50649591","2018-06-01 18:58:44","1","","<p>For the text just remove the raw character and set the vertical alignment to top.
for the limit it's better to just change the minimum rather than a range.</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
X = np.arange(0,10,0.1)
A = 2
plt.plot(X,X**2+A*X+2,'b-')
plt.text(min(X), max(X**2+A*X+2),'a=12,\nb=2.09\n $S_{xy}$= 0.71\n r= 0.9\n $R^2$= 0.85',fontsize=12,fontweight='bold',va='top')
plt.xlim(xmin=0)
plt.ylim(ymin=0)
plt.xlabel('X_value')
plt.ylabel('Y_value')
</code></pre>
","1245694","","","0","505","anishtain4","2012-03-02 18:03:48","1309","173","254","34","50649461","50649584","2018-06-01 18:49:42","0","26","<p>I have the following plot which has variable <code>X</code> and <code>Y</code> values. In order to explain the issue I have assigend constant values for <code>X</code> and <code>Y</code> in the code below.</p>

<p>I want that the figure coordinates will start from <code>(0,0)</code> and also how can I put the text values<code>(a,b,Sxy,r,R^2)</code> under each other and not next to each other as showen below. </p>

<p>Note that I can't use specific values of <code>X,Y</code> because they are variable. So thats why I have to use <code>min()</code> and <code>max()</code> functions as coordinates to show the text values on upper left corner. </p>

<p>code:</p>

<pre><code>import numpy as np
from matplotlib.pyplot import *
X = np.arange(0,10,0.1)
A = 2
plot(X,X**2+A*X+2,'b-')
text(min(X), max(X**2+A*X+2),r'a=12, b=2.09 $S_{xy}$= 0.71 r= 0.9 $R^2$= 0.85',fontsize=12,fontweight='bold')
xlabel('X_value')
ylabel('Y_value')

grid(True)
show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/p8NdG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p8NdG.png"" alt=""enter image description here""></a></p>
","9836518","9836518","2018-06-01 18:54:31","Adjust Figure in Matplotlib","<python><python-3.x><matplotlib>","2","3","1131"
"50649604","2018-06-01 18:59:36","0","","<p>There's a fundamental misunderstanding here about the function of the script tag.  ""value"" is not a valid attribute of the HTML script tag, which is used to import the text contents of a JavaScript file into your HTML file.  </p>

<p>To achieve what you want, you can simply remove:</p>

<pre><code>&lt;script src=""js/index2.js"" value = {{GraphData}}&gt;&lt;/script&gt;
</code></pre>

<p>and replace it with what you want imported:</p>

<pre><code>var chart = AmCharts.makeChart(""chartdiv"", {
  ""graphs"": [{
  ""id"": ""g3"",
  ""valueAxis"": ""v1"",
  ...
  }],
  ""Data"" = {{GraphData}}  
})
</code></pre>

<p>More info on the HTML script tag <a href=""https://www.w3schools.com/tags/tag_script.asp"" rel=""nofollow noreferrer"">here</a> and <a href=""https://www.w3schools.com/tags/att_script_src.asp"" rel=""nofollow noreferrer"">here</a>.</p>

<p>If you want to keep this script external, then you can use it to define a function that takes {{GraphData}} as an argument like this:</p>

<pre><code>function graphData(gd) {
  var chart = AmCharts.makeChart(""chartdiv"", {
    ""graphs"": [{
    ""id"": ""g3"",
    ""valueAxis"": ""v1"",
    ...
    }],
    ""Data"" = gd  
  });
}
</code></pre>

<p>and then call that function in your HTML file like this:</p>

<pre><code>&lt;script src=""js/index2.js""&gt;&lt;/script&gt;
&lt;script&gt;
  graphData({{GraphData}})
&lt;/script&gt;
</code></pre>
","7649466","7649466","2018-06-01 19:10:35","0","1370","Logan Bertram","2017-03-02 19:09:45","1203","133","646","125","50649510","","2018-06-01 18:52:45","1","84","<p>I have a js file that graphs a function with all the necessary parameters. I wish to do something like this in html:</p>

<pre><code>&lt;script  src=""js/index2.js"" value = {{GraphData}}&gt;&lt;/script&gt;
</code></pre>

<p>the js file looks something like this</p>

<pre><code>var chart = AmCharts.makeChart(""chartdiv"", {
  ""graphs"": [{
  ""id"": ""g3"",
  ""valueAxis"": ""v1"",
   ...
   }],
  ""Data"" = {{GraphData}} 
})
</code></pre>

<p>Is there a way to transfer the graph data to the js file?</p>
","9882833","","","Sending Jinja2 data to another js file -- flask","<javascript><python><html><flask><jinja2>","1","0","498"
"50649611","2018-06-01 18:59:59","1","","<p>I have written the following code for calculating crc8: </p>

<pre><code>acklist = [] # a list of your byte string data
x = 0xff
    crc = 0
    for i in range(len(acklist)):
        crc += int(acklist[i], 16) &amp; x

print(crc)

crc = ~crc
crc += 1

crc1 = crc &gt;&gt; 8 &amp; x
crc2 = crc &amp; x
</code></pre>
","2079325","","","0","318","Shakib","2013-02-16 22:01:42","11","17","0","0","34337284","34337678","2015-12-17 14:42:25","3","359","<p>Note: <a href=""https://stackoverflow.com/questions/5292290/crc16-in-python"">CRC16 in Python</a> topic does  not solve my problem. Deploying python package into server is restricted. Also my need is not a library to implement, I already wrote a function for this. My need is just correcting my function.</p>

<p>I need to implement a CRC calculator into our project for checking incoming data consistency. There is an example <strong>C snippet</strong> in device documents. I try to rewrite it in <strong>Python</strong> but I can't get right results in no way. For instance:</p>

<p>This is the data to be CRC calculated: <code>1f120f0c110e2103cc041f8ab002ea38040015440000000000000000083f</code></p>

<p>The calculated CRC value should be <code>9911</code>.</p>

<p>The original C snippet:</p>

<pre><code>static const U16 crctab16 [] =
{
0X0000, 0X1189, 0X2312, 0X329B, 0X4624, 0X57AD, 0X6536, 0X74BF,
0X8C48, 0X9DC1, 0XAF5A, 0XBED3, 0XCA6C, 0XDBE5, 0XE97E, 0XF8F7,
0X1081, 0X0108, 0X3393, 0X221A, 0X56A5, 0X472C, 0X75B7, 0X643E,
0X9CC9, 0X8D40, 0XBFDB, 0XAE52, 0XDAED, 0XCB64, 0XF9FF, 0XE876,
0X2102, 0X308B, 0X0210, 0X1399, 0X6726, 0X76AF, 0X4434, 0X55BD,
0XAD4A, 0XBCC3, 0X8E58, 0X9FD1, 0XEB6E, 0XFAE7, 0XC87C, 0XD9F5,
0X3183, 0X200A, 0X1291, 0X0318, 0X77A7, 0X662E, 0X54B5, 0X453C,
0XBDCB, 0XAC42, 0X9ED9, 0X8F50, 0XFBEF, 0XEA66, 0XD8FD, 0XC974,
0X4204, 0X538D, 0X6116, 0X709F, 0X0420, 0X15A9, 0X2732, 0X36BB,
0XCE4C, 0XDFC5, 0XED5E, 0XFCD7, 0X8868, 0X99E1, 0XAB7A, 0XBAF3,
0X5285, 0X430C, 0X7197, 0X601E, 0X14A1, 0X0528, 0X37B3, 0X263A,
0XDECD, 0XCF44, 0XFDDF, 0XEC56, 0X98E9, 0X8960, 0XBBFB, 0XAA72,
0X6306, 0X728F, 0X4014, 0X519D, 0X2522, 0X34AB, 0X0630, 0X17B9,
0XEF4E, 0XFEC7, 0XCC5C, 0XDDD5, 0XA96A, 0XB8E3, 0X8A78, 0X9BF1,
0X7387, 0X620E, 0X5095, 0X411C, 0X35A3, 0X242A, 0X16B1, 0X0738,
0XFFCF, 0XEE46, 0XDCDD, 0XCD54, 0XB9EB, 0XA862, 0X9AF9, 0X8B70,
0X8408, 0X9581, 0XA71A, 0XB693, 0XC22C, 0XD3A5, 0XE13E, 0XF0B7,
0X0840, 0X19C9, 0X2B52, 0X3ADB, 0X4E64, 0X5FED, 0X6D76, 0X7CFF,
0X9489, 0X8500, 0XB79B, 0XA612, 0XD2AD, 0XC324, 0XF1BF, 0XE036,
0X18C1, 0X0948, 0X3BD3, 0X2A5A, 0X5EE5, 0X4F6C, 0X7DF7, 0X6C7E,
0XA50A, 0XB483, 0X8618, 0X9791, 0XE32E, 0XF2A7, 0XC03C, 0XD1B5,
0X2942, 0X38CB, 0X0A50, 0X1BD9, 0X6F66, 0X7EEF, 0X4C74, 0X5DFD,
0XB58B, 0XA402, 0X9699, 0X8710, 0XF3AF, 0XE226, 0XD0BD, 0XC134,
0X39C3, 0X284A, 0X1AD1, 0X0B58, 0X7FE7, 0X6E6E, 0X5CF5, 0X4D7C,
0XC60C, 0XD785, 0XE51E, 0XF497, 0X8028, 0X91A1, 0XA33A, 0XB2B3,
0X4A44, 0X5BCD, 0X6956, 0X78DF, 0X0C60, 0X1DE9, 0X2F72, 0X3EFB,
0XD68D, 0XC704, 0XF59F, 0XE416, 0X90A9, 0X8120, 0XB3BB, 0XA232,
0X5AC5, 0X4B4C, 0X79D7, 0X685E, 0X1CE1, 0X0D68, 0X3FF3, 0X2E7A,
0XE70E, 0XF687, 0XC41C, 0XD595, 0XA12A, 0XB0A3, 0X8238, 0X93B1,
0X6B46, 0X7ACF, 0X4854, 0X59DD, 0X2D62, 0X3CEB, 0X0E70, 0X1FF9,
0XF78F, 0XE606, 0XD49D, 0XC514, 0XB1AB, 0XA022, 0X92B9, 0X8330,
0X7BC7, 0X6A4E, 0X58D5, 0X495C, 0X3DE3, 0X2C6A, 0X1EF1, 0X0F78,
};



// calculate 16 bits CRC of the given length data.

U16 GetCrc16(const U8* pData, int nLength)
{
    U16 fcs = 0xffff; // Initialize
    while(nLength&gt;0)
    {
        fcs = (fcs &gt;&gt; 8) ^ crctab16[(fcs ^ *pData) &amp; 0xff];
        nLength--;
        pData++;
    }
    return ~fcs; // Negate
}

// Check whether the 16 bits CRC of the given length data is right.
BOOL IsCrc16Good(const U8* pData, int nLength)
{
    U16 fcs = 0xffff;
    // Initialize
    while(nLength&gt;0)
    {
        fcs = (fcs &gt;&gt; 8) ^ crctab16[(fcs ^ *pData) &amp; 0xff];
        nLength--;
        pData++;
    }
    return (fcs == 0xf0b8); // 0xf0b8 is CRC-ITU 的""Magic Value""
}
</code></pre>

<p>But the rewriten code in Python below is computing the crc value as <code>-26351</code> </p>

<pre><code>CRC16_TABLE = [
    0x0000, 0x1189, 0x2312, 0x329b, 0x4624, 0x57ad, 0x6536, 0x74bf,
    0x8c48, 0x9dc1, 0xaf5a, 0xbed3, 0xca6c, 0xdbe5, 0xe97e, 0xf8f7,
    0x1081, 0x0108, 0x3393, 0x221a, 0x56a5, 0x472c, 0x75b7, 0x643e,
    0x9cc9, 0x8d40, 0xbfdb, 0xae52, 0xdaed, 0xcb64, 0xf9ff, 0xe876,
    0x2102, 0x308b, 0x0210, 0x1399, 0x6726, 0x76af, 0x4434, 0x55bd,
    0xad4a, 0xbcc3, 0x8e58, 0x9fd1, 0xeb6e, 0xfae7, 0xc87c, 0xd9f5,
    0x3183, 0x200a, 0x1291, 0x0318, 0x77a7, 0x662e, 0x54b5, 0x453c,
    0xbdcb, 0xac42, 0x9ed9, 0x8f50, 0xfbef, 0xea66, 0xd8fd, 0xc974,
    0x4204, 0x538d, 0x6116, 0x709f, 0x0420, 0x15a9, 0x2732, 0x36bb,
    0xce4c, 0xdfc5, 0xed5e, 0xfcd7, 0x8868, 0x99e1, 0xab7a, 0xbaf3,
    0x5285, 0x430c, 0x7197, 0x601e, 0x14a1, 0x0528, 0x37b3, 0x263a,
    0xdecd, 0xcf44, 0xfddf, 0xec56, 0x98e9, 0x8960, 0xbbfb, 0xaa72,
    0x6306, 0x728f, 0x4014, 0x519d, 0x2522, 0x34ab, 0x0630, 0x17b9,
    0xef4e, 0xfec7, 0xcc5c, 0xddd5, 0xa96a, 0xb8e3, 0x8a78, 0x9bf1,
    0x7387, 0x620e, 0x5095, 0x411c, 0x35a3, 0x242a, 0x16b1, 0x0738,
    0xffcf, 0xee46, 0xdcdd, 0xcd54, 0xb9eb, 0xa862, 0x9af9, 0x8b70,
    0x8408, 0x9581, 0xa71a, 0xb693, 0xc22c, 0xd3a5, 0xe13e, 0xf0b7,
    0x0840, 0x19c9, 0x2b52, 0x3adb, 0x4e64, 0x5fed, 0x6d76, 0x7cff,
    0x9489, 0x8500, 0xb79b, 0xa612, 0xd2ad, 0xc324, 0xf1bf, 0xe036,
    0x18c1, 0x0948, 0x3bd3, 0x2a5a, 0x5ee5, 0x4f6c, 0x7df7, 0x6c7e,
    0xa50a, 0xb483, 0x8618, 0x9791, 0xe32e, 0xf2a7, 0xc03c, 0xd1b5,
    0x2942, 0x38cb, 0x0a50, 0x1bd9, 0x6f66, 0x7eef, 0x4c74, 0x5dfd,
    0xb58b, 0xa402, 0x9699, 0x8710, 0xf3af, 0xe226, 0xd0bd, 0xc134,
    0x39c3, 0x284a, 0x1ad1, 0x0b58, 0x7fe7, 0x6e6e, 0x5cf5, 0x4d7c,
    0xc60c, 0xd785, 0xe51e, 0xf497, 0x8028, 0x91a1, 0xa33a, 0xb2b3,
    0x4a44, 0x5bcd, 0x6956, 0x78df, 0x0c60, 0x1de9, 0x2f72, 0x3efb,
    0xd68d, 0xc704, 0xf59f, 0xe416, 0x90a9, 0x8120, 0xb3bb, 0xa232,
    0x5ac5, 0x4b4c, 0x79d7, 0x685e, 0x1ce1, 0x0d68, 0x3ff3, 0x2e7a,
    0xe70e, 0xf687, 0xc41c, 0xd595, 0xa12a, 0xb0a3, 0x8238, 0x93b1,
    0x6b46, 0x7acf, 0x4854, 0x59dd, 0x2d62, 0x3ceb, 0x0e70, 0x1ff9,
    0xf78f, 0xe606, 0xd49d, 0xc514, 0xb1ab, 0xa022, 0x92b9, 0x8330,
    0x7bc7, 0x6a4e, 0x58d5, 0x495c, 0x3de3, 0x2c6a, 0x1ef1, 0x0f78,
]

def check(data):
    _str = ''
    i = 0
    for i in range(len(data) / 2):
        _str = _str + data[2 * i:2 * i + 2] + ','
    return _str[0:-1]

def compute(data):
    crc = 0xffff
    for byte in check(data).split("",""):
        crc = ((crc &gt;&gt; 8) ^ CRC16_TABLE[(crc ^ (int(byte, 16) &amp; 0xff)) &amp; 0xff])
    return ~crc
</code></pre>

<p>Does anybody have an idea?</p>
","1057473","-1","2017-05-23 10:33:47","C CRC Calculation method rewritten in Python seems wrong","<python><c><crc><crc16>","4","4","6268"
"50649645","2018-06-01 19:02:21","1","","<p>You are converting a class to a dictionary using var(md) but md is an instance of a class rather than a dictionary. Classes can access their attributes using dot notation (for example md.path) but dictionaries cannot. </p>

<p>If you look at the <a href=""https://github.com/fastai/fastai/blob/c040e7cf151dfb7f72dd6f20ac7ae49ff92d1e98/fastai/learner.py"" rel=""nofollow noreferrer"">Learner class</a> (which StructuredLearner inherits from) you'll see these two lines:</p>

<pre><code>self.data_,self.models,self.metrics = data,models,metrics
self.models_path = models_name if os.path.isabs(models_name) else os.path.join(self.data.path, models_name)
</code></pre>

<p>The key parts of that are:</p>

<pre><code>self.data = data
</code></pre>

<p>and</p>

<pre><code>self.data.path
</code></pre>

<p>So you can see it is trying to access the data (md) using dot notation.</p>

<p>If you really want to convert your dict to a class you can follow this: <a href=""https://codeyarns.com/2017/02/27/how-to-convert-python-dict-to-class-object-with-fields/"" rel=""nofollow noreferrer"">https://codeyarns.com/2017/02/27/how-to-convert-python-dict-to-class-object-with-fields/</a></p>

<p>or </p>

<p><a href=""https://stackoverflow.com/questions/1305532/convert-nested-python-dict-to-object"">Convert nested Python dict to object?</a></p>

<p>I'd recommend reading the docs, reading the library's code and creating your own small projects rather than trying to break something that is designed to work together into parts however it is up to you to figure out how you learn best.</p>
","1830793","","","1","1571","Zev","2012-11-16 20:51:03","2334","215","463","115","50648853","50649645","2018-06-01 18:04:00","0","63","<p>This is from the <a href=""https://github.com/fastai/fastai/tree/master/fastai"" rel=""nofollow noreferrer"">fastai</a> library.
So this function call:</p>

<pre><code>md = ColumnarModelData(PATH, ColumnarDataset.from_data_frame(trn_df, cat_flds=cat_vars, y=trn_y),
                    ColumnarDataset.from_data_frame(val_df, cat_flds=cat_vars, y=val_y), bs=128, test_ds=test_ds)
vars(md)
</code></pre>

<p>Gives this result:</p>

<pre><code>{'path': 'data/rossmann/',
 'test_dl': &lt;fastai.dataloader.DataLoader at 0x112c93d68&gt;,
 'trn_dl': &lt;fastai.dataloader.DataLoader at 0x112c93e80&gt;,
 'val_dl': &lt;fastai.dataloader.DataLoader at 0x112c93a20&gt;}
</code></pre>

<p>But I can build the same result by doing:</p>

<pre><code>md = {'path':PATH, 
      'test_dl':DataLoader(test_ds, batch_size=128, shuffle=False, num_workers=1), 
      'trn_dl':DataLoader(trn_df, batch_size=128, shuffle=False, num_workers=1), 
      'val_dl':DataLoader(val_df, batch_size=128*2, shuffle=False, num_workers=1)}

md
</code></pre>

<p>Which gives:</p>

<pre><code>{'path': 'data/rossmann/',
 'test_dl': &lt;fastai.dataloader.DataLoader at 0x1c20e9cc88&gt;,
 'trn_dl': &lt;fastai.dataloader.DataLoader at 0x1c20d5f8d0&gt;,
 'val_dl': &lt;fastai.dataloader.DataLoader at 0x1c20d5f320&gt;}
</code></pre>

<p>However they behave very differently when trying to use them in other functions. As in:</p>

<pre><code>m = StructuredLearner(md, StructuredModel(to_gpu(model)), opt_fn=optim.Adam)
</code></pre>

<p>This runs fine when I use the initial method of <code>md = ColumnarModelData()</code> but does not work when I build it on my own, gives this error:</p>

<blockquote>
  <p>AttributeError: 'dict' object has no attribute 'path'</p>
</blockquote>

<p>What exactly is going wrong here?</p>
","5380809","5380809","2018-06-01 18:11:50","Difference between dict attributes and dict","<python><dictionary><attributes>","1","9","1783"
"50649656","2018-06-01 19:03:14","0","","<p>Not sure about the difference - maybe GDPR-related since i live in Europe, or because i have set DNT (Do not track) to true in Chrome - but for me, Metacritic autocomplete requests post simply to <code>http://www.metacritic.com/autosearch</code> with the parameters <code>search_term</code> set to the search value and <code>search_filter</code> set to <code>all</code> :</p>

<p><a href=""https://i.stack.imgur.com/7zTf4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7zTf4.png"" alt=""enter image description here""></a></p>

<p>From your screenshots, i think the URL for autocomplete in your browser is constructed with your session id, maybe to avoid stuff like you intend to do :)</p>

<p>So in your case i would try in following order: </p>

<ul>
<li>post to the <code>/autosearch</code> URL and if that doesn't work</li>
<li>figure out the session-id to URL-writing logic, then make an initial request in the code to get a session id and work with that</li>
</ul>
","3820185","8402450","2018-06-05 18:51:40","2","990","wiesion","2014-07-09 11:21:21","2008","165","89","27","50649401","50649656","2018-06-01 18:46:03","0","208","<p>I've had some success using the POST requests in the past on other sites and receiving data from them but for some reason I'm having difficulty with the metacritic site.</p>

<p>Using chrome and the developer tools, I can see that when I begin to type in the search bar, it starts a POST request to the following url.</p>

<pre><code>searchURL = 'http://www.metacritic.com/g00/3_c-6bbb.rjyfhwnynh.htr_/c-6RTWJUMJZX77x24myyux3ax2fx2fbbb.rjyfhwnynh.htrx2ffzytx78jfwhmx3fn65h.rfwpx3dcmw_$/$'
</code></pre>

<p>I also know that my headers need to be the following in order to get a response</p>

<pre><code>headers = {'User-Agent' : ""Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36""}
</code></pre>

<p>When I run this, I get a status code of 200 which indicates it worked but my response text is not what I expected.  I am receiving the content of the entire page when I'm expecting json of search results.  What am I missing here?</p>

<pre><code>title = 'Grand Theft Auto'

#search request using POST
r = requests.post(searchURL, data = {'searchTerm' : title}, headers = headers)

print(r.status_code)

print(r.text)
</code></pre>

<p>You can see in the images below what I'm expecting to get.</p>

<p><a href=""https://i.stack.imgur.com/f1uFu.jpg"" rel=""nofollow noreferrer"">Headers</a></p>

<p><a href=""https://i.stack.imgur.com/E7hkY.jpg"" rel=""nofollow noreferrer"">Response</a></p>
","8402450","","","Return JSON File from Requests in Python","<python><http><python-requests>","1","2","1450"
"50649663","2018-06-01 19:03:41","0","","<p>I have written the following code for calculating crc8:</p>

<pre><code>acklist = [] # a list of your byte string data
x = 0xff
    crc = 0
    for i in range(len(acklist)):
        crc += int(acklist[i], 16) &amp; x

print(crc)

crc = ~crc
crc += 1

crc1 = crc &gt;&gt; 8 &amp; x
crc2 = crc &amp; x
</code></pre>
","2079325","","","1","317","Shakib","2013-02-16 22:01:42","11","17","0","0","6599045","","2011-07-06 15:32:44","0","1026","<p>I need to calc CRC checksumme of binary file.
This file content CRC too and by comparing I find out when file was corrupted.</p>

<p>Bin file is something like long hex string</p>

<pre><code>00200020 595A0008 ......
</code></pre>

<p>But CRC in file was calculated per integer(4.byte little Endian) like this</p>

<pre><code>1.int -  0x20002000
2.int -  0x8000A559
</code></pre>

<p>How can I get the same result without switching bytes in python?
I was trying <a href=""http://www.tty1.net/pycrc/"" rel=""nofollow"">http://www.tty1.net/pycrc/</a> and played with reflect in, but I dont get the same result.</p>

<p>For this two bytes is correct crc <strong>0xEF2B32F8</strong></p>
","55129","","","CRC in python, little Endian","<python><crc><endianness>","2","0","682"
"50649673","2018-06-01 19:04:26","0","","<p>You are trying to concat string and int.</p>

<p>Replace</p>

<pre><code>print(int(index)+"" ""+str(count))
</code></pre>

<p>With</p>

<pre><code>print(str(index)+"" ""+str(count))
</code></pre>

<p><em>You can also simplify your code.</em></p>

<p><strong>Ex:</strong></p>

<pre><code>import re
string=""358:6 1260:2 1533:7 1548:292 1550:48 1561:3 1564:186""
values=[v for v in re.findall('.+?:.+?.', string)]
for g in values:
  index, count =g.split("":"")
  print(index, count)
</code></pre>
","532312","532312","2018-06-01 19:10:15","2","491","Rakesh","2010-12-06 13:07:54","56694","5302","758","1508","50649640","50649942","2018-06-01 19:01:51","0","67","<p>I have list of values as string ""index:count"" I want to extract the index and count in the string as in the below code:</p>

<pre><code>          string=""358:6 1260:2 1533:7 1548:292 1550:48 1561:3 1564:186""
          values=[v for v in re.findall('.+?:.+?.', string)]
          for g in values:
              index=g[:g.index("":"")]
              count=g[g.index("":"")+1:]
              print(int(index)+"" ""+str(count))
</code></pre>

<p>But I got error message</p>

<blockquote>
  <p>ValueError: invalid literal for int() with base 10: '2 1550'</p>
</blockquote>

<p>it seems I wrote the regular expression operations wrongly. any idea how to fix this? </p>
","1360328","","","regular expression python index:count","<python><regex><python-3.x><regex-greedy>","3","7","661"
"50649679","2018-06-01 19:04:56","1","","<p>If you are only interested in certain cities, you can just take the subset of your dataframe that contains the cities you are interested in, take the dummy columns of that using <code>pd.dummies</code>, and then join with the original dataframe:</p>

<pre><code>&gt;&gt;&gt; df
  city_names
0      Paris
1   New York
2      Paris
3      Tokyo
4   New York

dummy_var_list = ['Paris', 'New York']

dummy_df = df.join(pd.get_dummies(df.loc[df.city_names.isin(dummy_var_list)], 
                   prefix='', prefix_sep='')).fillna(0)

&gt;&gt;&gt; dummy_df
  city_names  New York  Paris
0      Paris       0.0    1.0
1   New York       1.0    0.0
2      Paris       0.0    1.0
3      Tokyo       0.0    0.0
4   New York       1.0    0.0
</code></pre>

<p><strong>Edit</strong>: If I understand correctly, you want a dummy column for all your cities in <code>dummy_var_list</code>, even if they don't show up in <code>city_names</code> in your original <code>df</code>. In that case, after the code above, you could loop through and add a column of zeros for cities that didn't show up:</p>

<pre><code>&gt;&gt;&gt; df
  city_names
0      Paris
1   New York
2      Paris
3      Tokyo
4   New York

dummy_var_list = ['Paris', 'New York', 'Los Angeles']

dummy_df = df.join(pd.get_dummies(df.loc[df.city_names.isin(dummy_var_list)], 
        prefix='', prefix_sep='')).fillna(0)

for i in dummy_var_list:
    if i not in dummy_df.columns:
        dummy_df[i] = 0 

&gt;&gt;&gt; dummy_df
  city_names  New York  Paris  Los Angeles
0      Paris       0.0    1.0            0
1   New York       1.0    0.0            0
2      Paris       0.0    1.0            0
3      Tokyo       0.0    0.0            0
4   New York       1.0    0.0            0
</code></pre>
","6671176","6671176","2018-06-01 19:28:08","6","1757","sacuL","2016-08-03 07:43:41","32843","1871","2095","286","50649624","","2018-06-01 19:00:52","0","47","<p>So for example I have a pandas DataFrame that contains a column of city names and I already have a large predefined list of city names that will be used as dummy variables in a model. I would like for each city name in the list to be added as a new column and then filled with a bunch of 0s and 1s where a string in the city name column matches the column name of the dummy variable.</p>

<p>From my perspective, I would need to do something along the lines of:</p>

<pre><code>for dv in dummy_var_list:
    df[dv] = df[df[city_names]==dv]
</code></pre>

<p>I am unsure if this would be an efficient or correct approach. I would need to incorporate some sort of 'if' statement or masking which I am unsure of how to do.</p>

<p>i.e. I have list of city names:</p>

<pre><code>['paris','sydney','orlando','milwaukee']
</code></pre>

<p>and I have a list of predefined dummies I need to make columns of:</p>

<pre><code>['tokyo','berlin','beijing','orlando','paris']
</code></pre>

<p>So some rows will not have any '1's in them because there is no match, but that is ok.</p>
","7664441","6671176","2018-06-01 19:13:05","Best way to create a masking of dummy variables?","<python><pandas>","2","1","1077"
"50649721","2018-06-01 19:09:02","1","","<p>What you are attempting to do is not really how most 2D drafting works.</p>

<p>Imagine a blank sheet of paper.  You pick up a pencil and draw a square.  You decide that this will be a room.  So somewhere near the square that you have drawn, you write some text; ""Room 1"" for instance.  There is no intrinsic relationship or 'link' between the text ""Room 1"" and the square you have drawn.  These are completely disconnected entities.  The relationship only exists in the mind of the drafter, or if the text and square are close together, a relationship might be inferred by someone else viewing the drawing.</p>

<p>This is exactly how the vast majority of 2D drafting (.dxf files) works also.  Unless you set up special objects with attributes, there is no relationship between any piece of text and any other drawing entity in a .dxf file.</p>

<p>If you happen to have drawings where the room label is always placed 'inside' the room, then you can do some math to determine if a particular piece of text lies within a specific set of lines/polylines.</p>
","107899","","","1","1061","Stewbob","2009-05-15 19:21:16","15341","2606","5213","290","50580700","","2018-05-29 09:20:14","1","318","<p>I'm very new to CAD system and trying to extract information from dxf file for my project. I have dxf files for floor plan. My aim is to extract geometries for the room and label associated with it.</p>

<p>Though i was able to extract room geometries and room labels separately, but i'm finding it hard to map the both. Is there a tag that i've missed that has link between geometry and labels?</p>

<p><strong><em>Room Label:</em></strong></p>

<pre><code>0

INSERT

5

53CF

330

2

100

AcDbEntity

8

__X_Polyline_Data

100

AcDbBlockReference

66

 1
2

ROOM_DATA

10

11.15367175915704

20

10.40315868785525

30

0.0

1001

AEC_XDATA_BOUND_SPACE

1070

100

1070

 1
0

ATTRIB

5

53D1

330

53CF

100

AcDbEntity

8

__X_Polyline_Data

6

Continuous

100

AcDbText

10

11.12027175915704

20

10.44215868785524

30

0.0

40

0.25

1

AE.22

100

AcDbAttribute

280

 0
2

PART_AOID

70

 0
280

 1
</code></pre>

<p><strong><em>Room Text :</em></strong></p>

<pre><code>0

TEXT

5

62C

330

2

100

AcDbEntity

8

X_Raumnummer

370

 0
100

AcDbText

10

11.15367175915704

20

10.40315868785525

30

0.0

40

0.8

1

AE.22
</code></pre>

<p><strong><em>Room Coordinates :</em></strong></p>

<pre><code>0

LWPOLYLINE

5

239B

330

2

100

AcDbEntity

8

A_DEC_Durchbrüche

370

 0
100

AcDbPolyline

90

    4
70

 1
43

0.0

10

10.95

20

12.215

10

14.6

20

12.215

10

14.6

20

12.48499999999999

10

10.95

20

12.48499999999999
</code></pre>

<p>Code that i've written to extract the data:</p>

<pre><code>lines = [entity for entity in dwg.entities if entity.dxftype == 'LWPOLYLINE']
room_lines = [entity for entity in lines if entity.layer == '__X_Polyline']
lines_data = [entity for entity in dwg.entities if entity.dxftype == 'TEXT']
room_text = [entity for entity in lines_data if entity.layer == 'X_Raumnummer']  
</code></pre>

<p>I've used dxfgrabber library to extract information.</p>

<p>I'm sorry if my question is stupid, I'm not able to find the link and i'm fairly new to this field. I was able to extract AE.22 and it's 4 coordinates, but now able to map them together.</p>
","2619295","","","Understanding DXF file format. Relation between Label and Geometry","<python><dxf>","1","1","2112"
"50649742","2018-06-01 19:11:03","2","","<p>Use: <a href=""https://pandas.pydata.org/pandas-docs/version/0.21/generated/pandas.crosstab.html"" rel=""nofollow noreferrer"">.crosstab()</a></p>

<pre><code>pd.crosstab(df.id,df.fruit)
Out[251]: 
fruit  apple  grapes  lemon  orange  watermelon
id                                             
101        1       0      0       1           1
102        1       0      0       1           0
103        0       1      0       0           0
104        0       0      2       0           0
105        1       0      1       0           0
</code></pre>
","7964527","9609447","2018-06-01 19:27:54","4","547","WeNYoBen","2017-05-04 16:45:29","164847","15327","4764","689","50649700","50649742","2018-06-01 19:06:48","1","39","<p>I have a very large pandas dataframe like this:</p>

<pre><code>id   fruit
---|------
101  apple
102  apple
101  watermelon
101  orange
102  orange
104  lemon
105  lemon
104  lemon
105  apple
103  grapes
</code></pre>

<p>How can I create a count dataframe, where each id is representing the frequency of fruits across the columns, something like this:</p>

<pre><code>     apple   watermelon  orange  lemon  grapes

101    1         1         1       0      0
102    2         0         1       0      0
103    0         0         0       0      1
104    0         0         0       0      2
105    1         0         0       0      0
</code></pre>

<p>I tried to:</p>

<pre><code>new_df = df.groupby(['id','fruit']).count()
new_df
</code></pre>

<p>And</p>

<pre><code>new_df = df[['id','fruit']].groupby(['id','fruit']).count()
new_df
</code></pre>

<p>And</p>

<pre><code>new_df = df[['id','fruit']].groupby(df['fruit'].tolist()).count()
new_df
</code></pre>

<p>However, I am not getting the expected output. Any idea of how to create the desired output from the dataframe?</p>
","4140027","","","How to return the frequency of each value for every item inside of a pandas dataframe?","<python><python-3.x><pandas>","1","0","1087"
"50649747","2018-06-01 19:11:30","6","","<p>In the interactive console, it's easy to do:</p>

<pre><code>data_all2.columns.tolist()
</code></pre>

<p>Or this within a script:</p>

<pre><code>print(data_all2.columns.tolist())
</code></pre>
","1584793","","","0","198","EEE","2011-04-02 18:27:03","133","9","296","0","49188960","49189503","2018-03-09 07:49:04","35","77304","<p>I have a dataframe that consist of hundreds of columns, and I need too see all column names.</p>

<p>What I did:</p>

<pre><code>In[37]:
data_all2.columns
</code></pre>

<p>The output is:</p>

<pre><code>Out[37]:
Index(['customer_id', 'incoming', 'outgoing', 'awan', 'bank', 'family', 'food',
       'government', 'internet', 'isipulsa',
       ...
       'overdue_3months_feature78', 'overdue_3months_feature79',
       'overdue_3months_feature80', 'overdue_3months_feature81',
       'overdue_3months_feature82', 'overdue_3months_feature83',
       'overdue_3months_feature84', 'overdue_3months_feature85',
       'overdue_3months_feature86', 'loan_overdue_3months_total_y'],
      dtype='object', length=102)
</code></pre>

<p>How do I show <em>all</em> columns, instead of a truncated list?</p>
","7585973","10526730","2018-12-14 11:39:52","How to show all of columns name on pandas dataframe?","<python><pandas><dataframe><show>","9","0","802"
"50649758","2018-06-01 19:12:37","0","","<p>You try to break <code>ans</code> into smaller chunks, but notice that each iteration of this loop discards the previous content of <code>chunks</code> so you loose all but the last chunk of data. </p>

<pre><code>#separating the answers into lists
for i in range(0, len(ans), 100):
    chunk = ans[i:i+100]    # overwrites previous chunk
</code></pre>

<p>This is why you only get 20 items in the list... its only the final chunk. Since you want <code>final_string</code> to hold all of the text nodes, there is no need to chunk and I just removed it.</p>

<p>Next, and this is just tightening up the code, you don't need to both iterate the values of the list and track an index just to get the same value you are indexing. Working on <code>ans</code> because we are no longer chunking,</p>

<pre><code>finalans=[]
l=0
for i in ans:
    stri=ans[l]
    finalans.append(stri.text)
    l+=1
    continue
</code></pre>

<p>becomes</p>

<pre><code>finalans=[]
for item in ans:
    finalans.append(item.text)
</code></pre>

<p>or more susinctly</p>

<pre><code>finalans = [item.text for item in ans]
</code></pre>

<p>So the program is</p>

<pre><code>#importing the libraries
import urllib.request as urllib2

from bs4 import BeautifulSoup

#getting the page url
quote_page=""https:abcdef.com""

page=urllib2.urlopen(quote_page)

#parsing the html
soup = BeautifulSoup(page,""html.parser"")

# Take out the &lt;div&gt; of name and get its value
name_box = soup.find(""div"", attrs={""class"": ""AnswerListDiv""})

#finding all  the tags in the page
ans=name_box.find_all(""div"", attrs={""class"": ""u-serif-font-main--large""},recursive=True)

#extracting all the answers and putting into a list 
finalans = [item.text for item in ans]

final_string = '\n'.join(finalans)

#final output
print(final_string)
</code></pre>
","642070","","","3","1807","tdelaney","2011-03-02 22:27:36","37874","2589","1647","92","50649425","","2018-06-01 18:47:48","0","69","<pre><code>#importing the libraries
import urllib.request as urllib2

from bs4 import BeautifulSoup

#getting the page url
quote_page=""https://www.quora.com/What-is-the-best-advice-you-can-give-to-a-junior-programmer""

page=urllib2.urlopen(quote_page)

#parsing the html
soup = BeautifulSoup(page,""html.parser"")

# Take out the &lt;div&gt; of name and get its value
name_box = soup.find(""div"", attrs={""class"": ""AnswerListDiv""})

#finding all  the tags in the page
ans=name_box.find_all(""div"", attrs={""class"": ""u-serif-font-main--large""},recursive=True)

#separating the answers into lists
for i in range(0, len(ans), 100):
    chunk = ans[i:i+100]

#extracting all the answers and putting into a list 
finalans=[]
l=0
for i in chunk:
    stri=chunk[l]
    finalans.append(stri.text)
    l+=1
    continue

final_string = '\n'.join(finalans)

#final output
print(final_string)
</code></pre>

<p>I am not able to get more than 20 entries into this list. What is wrong with this code? (I am a beginner and I have used some references to write this program)
Edit: I have added the URL  I want to scrape.</p>
","9255539","9255539","2018-06-01 19:09:47","Returns a list with only 20 entries. Does not go beyond that","<python><python-3.x><web-scraping><beautifulsoup>","1","6","1104"
"50649762","2018-06-01 19:13:05","10","","<p>Try <a href=""https://atom.io/packages/atom-ide-debugger-python"" rel=""noreferrer"">atom-ide-debugger-python</a>.  It runs with atom's <a href=""https://atom.io/packages/atom-ide-debugger-python"" rel=""noreferrer"">ide-python</a> and allows variable watching as well as breakpoint setting. </p>
","6571327","","","1","292","Steven Kalt","2016-07-10 14:00:07","590","28","214","6","41539235","","2017-01-08 23:56:15","24","20517","<p>Any package or IDE for Atom that will allow me to watch variables when debugging?</p>

<p>I tried <a href=""https://github.com/webBoxio/atom-hashrocket"" rel=""noreferrer"">https://github.com/webBoxio/atom-hashrocket</a>
but this does not let me go step by step</p>

<p>I tried <a href=""https://atom.io/packages/python-debugger"" rel=""noreferrer"">https://atom.io/packages/python-debugger</a>
But it has no watched variables.</p>

<p>Any suggestions?</p>
","6854832","6729812","2017-06-23 18:42:44","Debugging python in Atom?","<python><debugging><atom-editor>","2","0","452"
"50649769","2018-06-01 19:13:41","3","","<p>You are already using regex - why not simply use groupings and create a dict from it?</p>

<pre><code>import re

s=""358:6 1260:2 1533:7 1548:292 1550:48 1561:3 1564:186""

values= dict(re.findall('(\d+):(\d+) ?', s)) # use capturing groups

for g in values:
    print(g, values[g])
</code></pre>

<p>Output:</p>

<pre><code> 358 6
1260 2
1533 7
1548 292
1550 48
1561 3
</code></pre>

<p>You have your key/value pairs conveniently inside a dictionary (all as strings).
You are loosing your ordering by that, but for key/values that should be no problem. </p>

<p>If you need this ordering, just use the returned list of findall:  </p>

<pre><code>values = re.findall('(\d+):(\d+) ?', s) # use capturing groups
</code></pre>

<p>which gives you a list of tuples with your matches returned:</p>

<pre><code>[('358', '6'), ('1260', '2'), ('1533', '7'), ('1548', '292'),
 ('1550', '48'), ('1561', '3'), ('1564', '186')]
</code></pre>
","7505395","","","0","931","Patrick Artner","2017-02-02 10:46:51","30736","5120","3506","4713","50649640","50649942","2018-06-01 19:01:51","0","67","<p>I have list of values as string ""index:count"" I want to extract the index and count in the string as in the below code:</p>

<pre><code>          string=""358:6 1260:2 1533:7 1548:292 1550:48 1561:3 1564:186""
          values=[v for v in re.findall('.+?:.+?.', string)]
          for g in values:
              index=g[:g.index("":"")]
              count=g[g.index("":"")+1:]
              print(int(index)+"" ""+str(count))
</code></pre>

<p>But I got error message</p>

<blockquote>
  <p>ValueError: invalid literal for int() with base 10: '2 1550'</p>
</blockquote>

<p>it seems I wrote the regular expression operations wrongly. any idea how to fix this? </p>
","1360328","","","regular expression python index:count","<python><regex><python-3.x><regex-greedy>","3","7","661"
"50649772","2018-06-01 19:13:54","0","","<p>Try using the <code>find</code> method in bs4</p>

<p><strong>Ex:</strong></p>

<pre><code>from bs4 import BeautifulSoup
html = """"""&lt;li class=""hide"" style=""display: list-item;""&gt;
    &lt;div class=""name""&gt;Name&lt;/div&gt;
    &lt;span class=""value""&gt;TEST TEST&lt;/span&gt;
&lt;/li&gt;""""""

soup = BeautifulSoup(html, ""html.parser"")
print( soup.find(""li"", class_=""hide"").text.strip() )
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>Name
TEST TEST
</code></pre>

<ul>
<li>After you find the required element use <code>.text</code> to extract the string. </li>
</ul>
","532312","","","0","587","Rakesh","2010-12-06 13:07:54","56694","5302","758","1508","50649616","50649772","2018-06-01 19:00:12","-1","44","<p>I need help in python. I need to find my code in this --></p>

<pre><code>&lt;li class=""hide"" style=""display: list-item;""&gt;
    &lt;div class=""name""&gt;Name&lt;/div&gt;
    &lt;span class=""value""&gt;TEST TEST&lt;/span&gt;
&lt;/li&gt;
</code></pre>

<p>These words:Name, TEST TEST.</p>
","9882852","1121864","2018-06-01 20:59:56","Python: String filter","<python><beautifulsoup>","1","5","290"
"50649789","2018-06-01 19:15:43","1","","<p><code>.find()</code> only supports basic xpath. </p>

<p>Try <code>.xpath()</code> instead. </p>

<p>Example (untested)... </p>

<pre><code>regular = item.xpath('.//span[contains(@class,""Regular"")]')[0].text 
</code></pre>

<p>See <a href=""http://lxml.de/xpathxslt.html"" rel=""nofollow noreferrer"">http://lxml.de/xpathxslt.html</a> for more details.</p>
","317052","","","0","356","Daniel Haley","2010-04-15 00:08:03","42044","2887","4003","190","50649171","50649789","2018-06-01 18:27:57","1","39","<p>I've written a script in python in combination with lxml libary to parse some <code>price</code> (80 and 100 in this case) out of a chunk of <code>html elements</code>. I used <code>xpaths</code> to do the job. When I go for using <code>.fromstring()</code> then both the <code>xpaths</code> I've used within my below scraper work flwlessly. However, when I choose to go with <code>HTML</code> imported from <code>lxml.etree</code> then the xpath containig <code>contains()</code> expression fails. Turns out that when I use multiple <code>class</code> names within the scraper, it works but when choose a <code>single class name</code> out of <code>compound class names</code> then it throws an error.</p>

<p>How can I handle such situation without using <code>compound class names</code>;rather, using a <code>single class name</code> following <code>.contains()</code> pattern or something?</p>

<p>This is my try:</p>

<pre><code>from lxml.etree import HTML

elements =\
""""""
    &lt;li class=""ProductPrice""&gt;
      &lt;span class=""Regular Price""&gt;80.00&lt;/span&gt;
    &lt;/li&gt;
    &lt;li class=""ProductPrice""&gt;
      &lt;span class=""Regular Price""&gt;100.00&lt;/span&gt;
    &lt;/li&gt;
""""""
root = HTML(elements)
for item in root.findall("".//*[@class='ProductPrice']""):
    # regular = item.find('.//span[@class=""Regular Price""]').text
    regular = item.find('.//span[contains(@class,""Regular"")]').text
    print(regular)
</code></pre>

<p>Btw, the commented out <code>xpath</code> used within above script is working fine. But can't go fo <code>.contains()</code> expression which throws the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\WCS\AppData\Local\Programs\Python\Python36-32\SO.py"", line 15, in &lt;module&gt;
    regular = item.find('.//span[contains(@class,""Regular"")]').text
  File ""src\lxml\etree.pyx"", line 1526, in lxml.etree._Element.find
  File ""src\lxml\_elementpath.py"", line 311, in lxml._elementpath.find
  File ""src\lxml\_elementpath.py"", line 300, in lxml._elementpath.iterfind
  File ""src\lxml\_elementpath.py"", line 283, in lxml._elementpath._build_path_iterator
  File ""src\lxml\_elementpath.py"", line 229, in lxml._elementpath.prepare_predicate
SyntaxError: invalid predicate
</code></pre>

<p>One last thing: I do not wish to use <code>compound class names</code> cause few websites produce them dynamically. Thanks.</p>
","9189799","","","Scraper throws an error even if right xpath is used","<python><python-3.x><xpath><web-scraping><lxml>","1","2","2409"
"50649800","2018-06-01 19:16:14","0","","<p>Where possible, don't try to fill in a string template to build a command, as you are unlikely to get the quoting correct. Instead, do as much in Python as possible, using the <code>subprocess</code> module where necessary.</p>

<pre><code>from subprocess import Popen, call, PIPE

def processFile(name, pattern, output):
    p = Popen([""gunzip"", ""-c"", name], stdout=PIPE)
    call([""egrep"", ""-i"", pattern], stdin=p1.stdout, stdout=output)

with open(""test"", ""w"") as fh:
    p = Popen([""python"", ""{0}_getSyslogs.py"".format(a), b, c, d], stdout=PIPE)
    for x in p.stdout:
        x = x.strip()
        processFile(os.path.join(d, x), e, fh)
</code></pre>
","1126841","","","0","659","chepner","2012-01-02 21:41:39","295038","14033","16849","5091","50649316","50649402","2018-06-01 18:38:57","2","58","<p>This makes no sense to me. I define 5 variables:</p>

<pre><code>a='a'
b='b'
c='c'
d='d'
e='e'
</code></pre>

<p>Then I try to build a command using those variables:</p>

<pre><code>command = ""for i in \`python {0}_getSyslogs.py {1} {2} {3}\`\ndo\ngunzip -c {3}/\$i | egrep -i '{4}' &gt;&gt; "" .format(a,b,c,d,e)
</code></pre>

<p>This works as expected, with the resulting command:</p>

<pre><code>""for i in \\`python a_getSyslogs.py b c d\\`\ndo\ngunzip -c d/\\$i | egrep -i 'e' &gt;&gt; ""
</code></pre>

<p>What gets me is if I add one more component to the string (ie ""test""), the whole thing falls apart in that there is no more substitution going on:</p>

<pre><code>command = ""for i in \`python {0}_getSyslogs.py {1} {2} {3}\`\ndo\ngunzip -c {3}/\$i | egrep -i '{4}' &gt;&gt; "" + ""test"" .format(a,b,c,d,e)
</code></pre>

<p>The resulting command is as follow:</p>

<pre><code>""for i in \\`python {0}_getSyslogs.py {1} {2} {3}\\`\ndo\ngunzip -c {3}/\\$i | egrep -i '{4}' &gt;&gt; test""
</code></pre>

<p>This is probably a ""can't see the forest through the trees"" problem, but I've been trying all kinds of different combinations and nothing works. </p>

<p>I'm running python 2.7.10 on CentOS:</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>sys.version
      '2.7.10 (default, Oct  6 2017, 22:29:07) \n[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.31)]'</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>What am I doing wrong?</p>
","4403891","","","python .format not working as expected","<python><format>","4","1","1465"
"50649804","2018-06-01 19:16:32","5","","<p>Don't cleanup these with <code>shutil</code>.  The <code>tempfile.TemporaryDirectory</code> class provides a <code>cleanup()</code> method, just call that if you want to opt-in to an explicit cleanup.</p>

<p>The reason you get the crash with your code is that the <code>TemporaryDirectory</code> class is designed to clean up after itself once it goes out of scope (ref count to zero).  However, since you've already removed the directory from your filesystem manually, the tear down fails when the instance subsequently tries to delete itself.  The ""No such file or directory"" error is from <code>TemporaryDirectory</code>'s own tear down, it's not from your <code>shutil.rmtree</code> line!</p>
","674039","674039","2018-06-01 19:30:22","5","701","wim","2011-03-23 23:40:27","187587","12233","9064","5087","50649701","50649804","2018-06-01 19:06:49","3","382","<p>Consider this test</p>

<pre><code>import shutil, tempfile
from os import path
import unittest

from pathlib import Path

class TestExample(unittest.TestCase):
    def setUp(self):
        # Create a temporary directory
        self.test_dir = tempfile.TemporaryDirectory()
        self.test_dir2 = tempfile.mkdtemp()

    def tearDown(self):
        # Remove the directory after the  test
        shutil.rmtree(self.test_dir2) 
        shutil.rmtree(self.test_dir.name) #throws error

    def test_something(self):
        self.assertTrue(Path(self.test_dir.name).is_dir())
        self.assertTrue(Path(self.test_dir2).is_dir())

if __name__ == '__main__':
    unittest.main()
</code></pre>

<p>In <code>tearDown</code> however an error is raised</p>

<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpxz7ts7a7'
</code></pre>

<p>which refers to <code>self.test_dir.name</code>.</p>

<p>According to <a href=""https://github.com/python/cpython/blob/1df877caad584135263ef94d83a28251e162e643/Lib/tempfile.py#L779"" rel=""nofollow noreferrer"">the source code for <code>tempfile</code></a>, both elements are the same.</p>

<pre><code>    def __init__(self, suffix=None, prefix=None, dir=None):
        self.name = mkdtemp(suffix, prefix, dir)
        self._finalizer = _weakref.finalize(
            self, self._cleanup, self.name,
            warn_message=""Implicitly cleaning up {!r}"".format(self))
</code></pre>

<p>And I'm not using it within a context, so <code>__exit__()</code> shouldn't be called as far as I understand.</p>

<p>What is happening?</p>
","5819113","1222951","2018-06-01 19:32:25","`shutil.rmtree` does not work on `tempfile.TemporaryDirectory()`","<python><python-3.x><python-unittest><temporary-files><shutil>","2","4","1584"
"50649844","2018-06-01 19:19:41","5","","<p>It's not context related:</p>

<pre><code>import tempfile,os

t = tempfile.TemporaryDirectory()
s = t.name
print(os.path.isdir(s))
# os.rmdir(s) called here triggers error on the next line
t = None
print(os.path.isdir(s))
</code></pre>

<p>it prints</p>

<pre><code>True
False
</code></pre>

<p>So as soon as the reference of <code>t</code> is set to <code>None</code> the object is garbage collected and the directory is removed, as the <a href=""https://docs.python.org/3/library/tempfile.html#tempfile.TemporaryDirectory"" rel=""nofollow noreferrer"">documentation</a> states:</p>

<blockquote>
  <p>On completion of the context <strong>or destruction of the temporary directory object</strong> the newly created temporary directory and all its contents are removed from the filesystem.</p>
</blockquote>

<p>Uncommenting <code>os.rmdir(s)</code> in the snippet below throws exception when object is finalized:</p>

<pre><code>Exception ignored in: &lt;finalize object at 0x20b20f0; dead&gt;
Traceback (most recent call last):
  File ""L:\Python34\lib\weakref.py"", line 519, in __call__
    return info.func(*info.args, **(info.kwargs or {}))
  File ""L:\Python34\lib\tempfile.py"", line 698, in _cleanup
    _shutil.rmtree(name)
  File ""L:\Python34\lib\shutil.py"", line 482, in rmtree
    return _rmtree_unsafe(path, onerror)
  File ""L:\Python34\lib\shutil.py"", line 364, in _rmtree_unsafe
    onerror(os.listdir, path, sys.exc_info())
  File ""L:\Python34\lib\shutil.py"", line 362, in _rmtree_unsafe
    names = os.listdir(path)
</code></pre>

<p>So your call probably succeeds, but you get the exception at the finalization of the object (just afterwards)</p>

<p>Calling <code>cleanup()</code> object method instead of <code>rmtree</code> solves the issue, because the object internal state is updated for <em>not</em> to try to remove the directory when finalized (if you ask me, the object <em>should</em> test if directory exists before trying to clean it up, but even that doesn't always work since it's not an atomic operation)</p>

<p>So replace</p>

<pre><code>shutil.rmtree(self.test_dir.name)
</code></pre>

<p>by</p>

<pre><code>self.test_dir.cleanup()
</code></pre>

<p>or by nothing at all, let the object clean the directory on deletion.</p>
","6451573","6451573","2018-06-01 19:48:41","4","2257","Jean-François Fabre","2016-06-10 19:19:53","113106","37329","9248","14670","50649701","50649804","2018-06-01 19:06:49","3","382","<p>Consider this test</p>

<pre><code>import shutil, tempfile
from os import path
import unittest

from pathlib import Path

class TestExample(unittest.TestCase):
    def setUp(self):
        # Create a temporary directory
        self.test_dir = tempfile.TemporaryDirectory()
        self.test_dir2 = tempfile.mkdtemp()

    def tearDown(self):
        # Remove the directory after the  test
        shutil.rmtree(self.test_dir2) 
        shutil.rmtree(self.test_dir.name) #throws error

    def test_something(self):
        self.assertTrue(Path(self.test_dir.name).is_dir())
        self.assertTrue(Path(self.test_dir2).is_dir())

if __name__ == '__main__':
    unittest.main()
</code></pre>

<p>In <code>tearDown</code> however an error is raised</p>

<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpxz7ts7a7'
</code></pre>

<p>which refers to <code>self.test_dir.name</code>.</p>

<p>According to <a href=""https://github.com/python/cpython/blob/1df877caad584135263ef94d83a28251e162e643/Lib/tempfile.py#L779"" rel=""nofollow noreferrer"">the source code for <code>tempfile</code></a>, both elements are the same.</p>

<pre><code>    def __init__(self, suffix=None, prefix=None, dir=None):
        self.name = mkdtemp(suffix, prefix, dir)
        self._finalizer = _weakref.finalize(
            self, self._cleanup, self.name,
            warn_message=""Implicitly cleaning up {!r}"".format(self))
</code></pre>

<p>And I'm not using it within a context, so <code>__exit__()</code> shouldn't be called as far as I understand.</p>

<p>What is happening?</p>
","5819113","1222951","2018-06-01 19:32:25","`shutil.rmtree` does not work on `tempfile.TemporaryDirectory()`","<python><python-3.x><python-unittest><temporary-files><shutil>","2","4","1584"
"50649904","2018-06-01 19:24:56","1","","<p>I'm going to run into this same problem later while working on a Pyramid app, so I googled and found <a href=""http://docrep.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">docrep</a>.  I think it'll satisfy both of our needs.</p>
","4892237","4892237","2018-06-01 20:29:06","1","238","Jason Klas","2015-05-12 15:47:47","76","10","42","0","50649830","","2018-06-01 19:18:44","4","34","<p>I'm writing some Python code that's being checked with pylint with the <a href=""http://pylint.pycqa.org/en/1.9/technical_reference/extensions.html?highlight=docparams"" rel=""nofollow noreferrer"">docparams</a> extension. This requires documenting function parameters in their docstrings. I have a tendency to try to <a href=""https://en.wikipedia.org/wiki/Functional_decomposition"" rel=""nofollow noreferrer"">decompose functions</a> as much as possible and end up with lots of <a href=""https://softwareengineering.stackexchange.com/questions/141251/functions-that-only-call-other-functions-is-this-a-good-practice"">functions that call other functions</a> - often sharing many of the same parameters. This can lead to lots of redundant (copy-pasted) documentation in the docstrings. Is there a recommended way of dealing with this type of situation?</p>
","4451345","4451345","2018-06-01 19:35:01","Reducing redundancy in Python doc-strings while using pylint docparams","<python><pylint><docstring>","1","0","852"
"50649928","2018-06-01 19:26:56","0","","<p>So I managed to get it using (outside of loop, of course)</p>

<p><code>get_img_class = driver.find_elements_by_class_name('img')[1].get_attribute('class')</code></p>

<p>Just like that I am able to parse the <code>Class ID</code> and store it for a later use. Thanks so much for everyones help. All ideas are great and noted for later use. </p>
","8965047","","","0","349","uzdisral","2017-11-19 05:16:27","79","25","13","0","50632242","50632361","2018-05-31 20:23:34","0","583","<p>It looks like the <code>&lt;class id&gt;</code> for <code>&lt;img class&gt;</code> on Instagram's web page is changing every day. Right now it is <code>FFVAD</code> and tomorrow it will be something else. For example (I made it shorter, links are long): </p>

<p><code>&lt;img class=""FFVAD"" alt=""Tag your best friend"" decoding=""auto"" style="""" sizes=""293px"" src=""https://scontent-lax3-2.cdninstagram.com/vp/0436c00a3ac9428b2b8c977b45abd022/5BAB3EBC/t51.2885-15/s640x640/sh0.08/e35/33110483_592294374461447_8669459880035221504_n.jpg""&gt;</code></p>

<p>By saying that, I need to fix the script and hardcode the <code>Class ID</code> in order to be able scrape the web-page.</p>

<p><code>var = driver.find_elements_by_class_name('FFVAD')</code></p>

<p>Somebody told me that I could use <code>img.get_attribute('class')</code> to find the <code>class ID</code> and store it for later. But I still don't understand how this can be achieved, so selenium or soup could grab the <code>Class ID</code> from the <code>html tag</code> and store or parse it later.</p>

<p>All I got now is this. It's little dirty, and not right, but the idea is there.</p>

<pre><code>import requests
import selenium.webdriver as webdriver

url = ('https://www.instagram.com/kitties')
driver = webdriver.Firefox()
driver.get(url)
last_height = driver.execute_script(""return document.body.scrollHeight"")

while True:
    imgs_dedupe = driver.find_elements_by_class_name('FFVAD')

    for img in imgs_dedupe:
        posts = img.get_attribute('class')
        print posts

    driver.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")
    time.sleep(scroll_delay)
    new_height = driver.execute_script(""return document.body.scrollHeight"")

    if new_height == last_height:
        break
    last_height = new_height
</code></pre>

<p>When I run it, I get this output, and because there are 3 images on the page, I get 3x <code>Class ID</code></p>

<pre><code>python tag_print.py 
FFVAD
FFVAD
FFVAD
</code></pre>
","8965047","648265","2018-06-01 07:54:27","Get element with a randomized class name","<python><selenium><web-scraping>","2","12","2004"
"50649933","2018-06-01 19:27:43","67","","<p>In one of your dataframes the year is a string and the other it is an int64 
you can convert it first and then join  (e.g. <code>df['year']=df['year'].astype(int)</code> or as RafaelC suggested <code>df.year.astype(int)</code>)</p>
","1018659","1018659","2019-02-24 13:06:26","5","235","Arnon Rotem-Gal-Oz","2011-10-28 15:41:27","21113","3036","950","17","50649853","50649933","2018-06-01 19:20:07","40","66892","<p>These are my two dataframes saved in two variables:</p>

<pre><code>&gt; print(df.head())
&gt;
          club_name  tr_jan  tr_dec  year
    0  ADO Den Haag    1368    1422  2010
    1  ADO Den Haag    1455    1477  2011
    2  ADO Den Haag    1461    1443  2012
    3  ADO Den Haag    1437    1383  2013
    4  ADO Den Haag    1386    1422  2014
&gt; print(rankingdf.head())
&gt;
           club_name  ranking  year
    0    ADO Den Haag    12    2010
    1    ADO Den Haag    13    2011
    2    ADO Den Haag    11    2012
    3    ADO Den Haag    14    2013
    4    ADO Den Haag    17    2014
</code></pre>

<p>I'm trying to merge these two using this code:</p>

<pre><code>new_df = df.merge(ranking_df, on=['club_name', 'year'], how='left')
</code></pre>

<p>The how='left' is added because I have less datapoints in my ranking_df than in my standard df.</p>

<p>The expected behaviour is as such:</p>

<pre><code>&gt; print(new_df.head()) 
&gt; 

      club_name  tr_jan  tr_dec  year    ranking
0  ADO Den Haag    1368    1422  2010    12
1  ADO Den Haag    1455    1477  2011    13
2  ADO Den Haag    1461    1443  2012    11
3  ADO Den Haag    1437    1383  2013    14
4  ADO Den Haag    1386    1422  2014    17
</code></pre>

<p>But I get this error:</p>

<blockquote>
  <p>ValueError: You are trying to merge on object and int64 columns. If
  you wish to proceed you should use pd.concat</p>
</blockquote>

<p>But I do not wish to use concat since I want to merge the trees not just add them on.</p>

<p>Another behaviour that's weird in my mind is that my code works if I save the first df to .csv and then load that .csv into a dataframe.</p>

<p>The code for that:</p>

<pre><code>df = pd.DataFrame(data_points, columns=['club_name', 'tr_jan', 'tr_dec', 'year'])
df.to_csv('preliminary.csv')

df = pd.read_csv('preliminary.csv', index_col=0)

ranking_df = pd.DataFrame(rankings, columns=['club_name', 'ranking', 'year'])

new_df = df.merge(ranking_df, on=['club_name', 'year'], how='left')
</code></pre>

<p>I think that it has to do with the index_col=0 parameter. But I have no idea to fix it without having to save it, it doesn't matter much but is kind of an annoyance that I have to do that.</p>
","8580574","832230","2018-11-07 21:03:46","Trying to merge 2 dataframes but get ValueError","<python><pandas><dataframe>","3","0","2219"
"50649942","2018-06-01 19:28:25","2","","<p>I think you won't need the <code>?</code> lazy modifier at the end of the regex pattern. The <code>?</code> lazy modifier you put there can actually produce more noise than capturing the right data</p>

<p><strong>EDIT NOTE:</strong> the pattern <code>.+:.+</code> I introduced in previous edits was a wrong or even a bad regex pattern to capture the desired pattern. Please use the <code>\d+:\d+</code> pattern instead. However, I leave it be because it still can solve the OP's problem using another workaround.</p>

<p>As long as your data is not malformed or contain noise and is neatly separated with a whitespace, I think <code>'.+:.+'</code> is sufficient to find your <code>index:count</code> format. Probably the best way is to use <code>\d+:\d+</code> since you know it is at least one <code>digit</code> separated by a <code>:</code> and followed by another <code>digit</code>.</p>

<p>Here are good links <a href=""https://regexr.com/"" rel=""nofollow noreferrer"">regexr</a> and <a href=""https://regex101.com/"" rel=""nofollow noreferrer"">regex101</a> to better design/visualize your regex pattern.</p>

<p>If you use the <code>.+:.+</code> pattern, it will return you the string as a whole since it matches the string as a whole. You need to preprocess the result since <a href=""https://docs.python.org/2/library/re.html#re.findall"" rel=""nofollow noreferrer""><code>re.findall</code></a> returns a <code>list</code>, in this example, it returns only 1 element.</p>

<pre><code>In [  ]: string=""358:6 1260:2 1533:7 1548:292 1550:48 1561:3 1564:186""
    ...: values=[v for v in re.findall('.+:.+', string)]
    ...: print(values)
['358:6 1260:2 1533:7 1548:292 1550:48 1561:3 1564:186']
</code></pre>

<p>Since it returns a list with only one element, you can use <a href=""https://docs.python.org/2/tutorial/datastructures.html"" rel=""nofollow noreferrer""><code>pop()</code></a> to take the only <code>str</code> element out and print it nicely with <code>str</code> function <a href=""https://docs.python.org/2/library/string.html#string.split"" rel=""nofollow noreferrer""><code>split()</code></a>.</p>

<pre><code>In [  ]: print(values.pop().split())
['358:6', '1260:2', '1533:7', '1548:292', '1550:48', '1561:3', '1564:186']
</code></pre>

<p>If you are using <code>\d+:\d+</code> pattern, it will directly return you a nicely separated list since it correctly finds them. Therefore, you can directly print its value.</p>

<pre><code>In [  ]: string=""358:6 1260:2 1533:7 1548:292 1550:48 1561:3 1564:186""
    ...: values=[v for v in re.findall('\d+:\d+', string)]
    ...: print(values)
['358:6', '1260:2', '1533:7', '1548:292', '1550:48', '1561:3', '1564:186']
</code></pre>

<p>Finally, you can print the result nicely with built-in <a href=""https://pyformat.info/"" rel=""nofollow noreferrer"">string formatting</a>. <strong>Disclaimer:</strong> I do not own that website, I just found it useful for beginner me :)</p>

<pre><code>In [  ]: for s in values:
    ...:     index, count = s.split("":"")
    ...:     print(""Index: {:&gt;8} Count: {:&gt;8}"".format(index, count))
    ...:     
Index:      358 Count:        6
Index:     1260 Count:        2
Index:     1533 Count:        7
Index:     1548 Count:      292
Index:     1550 Count:       48
Index:     1561 Count:        3
Index:     1564 Count:      186
</code></pre>
","7434285","7434285","2018-06-01 20:05:39","6","3332","ToleLee","2017-01-18 07:16:15","167","26","238","0","50649640","50649942","2018-06-01 19:01:51","0","67","<p>I have list of values as string ""index:count"" I want to extract the index and count in the string as in the below code:</p>

<pre><code>          string=""358:6 1260:2 1533:7 1548:292 1550:48 1561:3 1564:186""
          values=[v for v in re.findall('.+?:.+?.', string)]
          for g in values:
              index=g[:g.index("":"")]
              count=g[g.index("":"")+1:]
              print(int(index)+"" ""+str(count))
</code></pre>

<p>But I got error message</p>

<blockquote>
  <p>ValueError: invalid literal for int() with base 10: '2 1550'</p>
</blockquote>

<p>it seems I wrote the regular expression operations wrongly. any idea how to fix this? </p>
","1360328","","","regular expression python index:count","<python><regex><python-3.x><regex-greedy>","3","7","661"
"50649980","2018-06-01 19:31:45","0","","<p>You can do so:</p>

<pre><code>import numpy as np

df = pd.DataFrame(columns=['tokyo','berlin','beijing','orlando','paris'])
df['city'] = ['paris','sydney','orlando','milwaukee']
</code></pre>

<p>Initial df:</p>

<pre><code>  tokyo berlin beijing orlando paris       city
0   NaN    NaN     NaN     NaN   NaN      paris
1   NaN    NaN     NaN     NaN   NaN     sydney
2   NaN    NaN     NaN     NaN   NaN    orlando
3   NaN    NaN     NaN     NaN   NaN  milwaukee

for col in df.columns:
    df.loc[df['city'] == col, col] = 1
df = df.replace(np.NaN, 0)
</code></pre>

<p>Output:</p>

<pre><code>   tokyo  berlin  beijing  orlando  paris       city
0      0       0        0        0      1      paris
1      0       0        0        0      0     sydney
2      0       0        0        1      0    orlando
3      0       0        0        0      0  milwaukee
</code></pre>
","5178905","","","1","879","Joe","2015-07-31 17:53:47","7327","442","644","3","50649624","","2018-06-01 19:00:52","0","47","<p>So for example I have a pandas DataFrame that contains a column of city names and I already have a large predefined list of city names that will be used as dummy variables in a model. I would like for each city name in the list to be added as a new column and then filled with a bunch of 0s and 1s where a string in the city name column matches the column name of the dummy variable.</p>

<p>From my perspective, I would need to do something along the lines of:</p>

<pre><code>for dv in dummy_var_list:
    df[dv] = df[df[city_names]==dv]
</code></pre>

<p>I am unsure if this would be an efficient or correct approach. I would need to incorporate some sort of 'if' statement or masking which I am unsure of how to do.</p>

<p>i.e. I have list of city names:</p>

<pre><code>['paris','sydney','orlando','milwaukee']
</code></pre>

<p>and I have a list of predefined dummies I need to make columns of:</p>

<pre><code>['tokyo','berlin','beijing','orlando','paris']
</code></pre>

<p>So some rows will not have any '1's in them because there is no match, but that is ok.</p>
","7664441","6671176","2018-06-01 19:13:05","Best way to create a masking of dummy variables?","<python><pandas>","2","1","1077"
"50649987","2018-06-01 19:32:19","1","","<ul>
<li><code>ResNet50</code> has a parameter <code>include_top</code> exactly for that purpose -- set it to <code>False</code>  to skip the last fully connected layer. (It then outputs a feature vector of length 2048).</li>
<li><p>The best way to reduce your image size is to resample the images, e.g. using the dedicated function <a href=""https://www.tensorflow.org/api_docs/python/tf/image/resize_images"" rel=""nofollow noreferrer""><code>tf.image.resample_images</code></a>.</p>

<ul>
<li><p>Also, I did not notice at first that your input images have only three channels, thx @Daniel. I suggest you build your 3-channel grayscale image on the GPU (not on the host using numpy) to avoid tripling your data transfer to GPU memory, using <code>tf.tile</code>:</p>

<pre><code>im3 = tf.tile(im, (1, 1, 1, 3))
</code></pre></li>
</ul></li>
</ul>
","1735003","1735003","2018-06-01 20:27:47","0","845","P-Gn","2012-10-10 13:22:24","14425","1153","273","168","50646426","50649987","2018-06-01 15:10:05","0","517","<p>I'm trying to use a pretrained network such as <code>tf.keras.applications.ResNet50</code> but I have two problems:</p>

<p>I just want to obtain the top embedding layers at the end of the network, because I don't want to do any image classification. So due to this there is no need for a classes number I think.</p>

<ul>
<li><p><code>tf.keras.applications.ResNet50</code> takes a default parameter <code>'classes=1000'</code></p>

<ul>
<li>Is there a way how I can omit this parameter?</li>
</ul></li>
<li><p>My input pictures are <code>128*128*1</code> pixels and not <code>224*224*3</code></p>

<ul>
<li>What is the best way to fix my input data shape?</li>
</ul></li>
</ul>

<p>My goal is to make a triplet loss network with the output of a <code>resnet</code> network.</p>

<p>Thanks a lot!</p>
","9731056","1735003","2018-06-01 19:35:47","How to use pre-trained models without classes in Tensorflow?","<python><tensorflow><keras><resnet>","2","2","804"
"50650039","2018-06-01 19:37:12","2","","<p>Not sure what is your intention, but you're mixing models and tensors in the same list <code>models = [inputs, model1, model2]</code>. This is the cause of the error. </p>

<p>Now, we have no idea about what kinds of input you have, so we cannot help further, but assuming a few things (that may be wrong) this code can help you:</p>

<pre><code>inputForEmbedding = Input((length,)) #where length seems to be 1
extraInput = Input(shapeOfTheExtraInput) #I don't know this shape

model1Out = Embedding(1115, 10, input_length=1 name='Hb_{}'.format(i))(inputForEmbedding)
model1Out = Reshape((10,))(model1Out)

....you must make the shapes compatible for `model1Out` and `extraInput`...

outputs = Concatenate(axis=...)([model1Out,extraInput])
outputs = Dense(1000, kernel_initializer='uniform')(outputs)
outputs = Activation('relu')(outputs)
outputs = Dense(500, kernel_initializer='uniform')(outputs)
outputs = Activation('relu')(outputs)
outputs = Dense(1)(outputs)
outputs = Activation('sigmoid')(outputs)

model = Model([inputForEmbedding, extraInput])
model.summary()   
</code></pre>
","2097240","2097240","2018-06-01 19:56:56","1","1090","Daniel Möller","2013-02-21 21:35:32","48862","7561","2736","22","50649794","50650039","2018-06-01 19:15:52","0","55","<p>Is it possible in keras add external input to Merge layer ? I have simple embedding wich I would like to combine with external values, but every time I try to do I always get error. Is there way to add external input to Keras layers ?</p>

<pre><code>models = []
inputs =Input(shape=(10,))
models.append(inputs)
for i in range(2):
    model_s = Sequential()
    model_s.add(Embedding(1115, 10, input_length=1, name='Hb_{}'.format(i)))
    model_s.add(Reshape(target_shape=(10,)))
    models.append(model_s)



model = Sequential()
model.add(Merge(models, mode='concat'))
model.add(Dense(1000, kernel_initializer='uniform'))
model.add(Activation('relu'))
model.add(Dense(500, kernel_initializer='uniform'))
model.add(Activation('relu'))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.summary()   
</code></pre>

<p>Error</p>

<pre><code>Tensor' object has no attribute 'get_output_shape_at'
</code></pre>

<p>This is the small code for testing.</p>

<pre><code>model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])


m={'Hb_0_input': np.array([0,1, 2, 3, 4, 5, 6, 7, 8, 9]),'Hb_1_input': np.array([0,1, 2, 3, 4, 5, 6, 7, 8, 9]), 'x': np.array([0,1, 2, 3, 4, 5, 6, 7, 8, 9])}
y=np.array([0, 1, 0, 1, 0, 1, 0, 0, 0, 0])
model.fit(m, y)
</code></pre>
","5745211","5745211","2018-06-01 19:33:18","Combine Layers with Input","<python><tensorflow><keras>","1","1","1321"
"50650049","2018-06-01 19:38:03","0","","<p>AWS Glue is useful for this kind of task. You can create a glue job to convert json format day to parquet format and save it to a S3 bucket of your choice. <a href=""https://aws.amazon.com/blogs/big-data/build-a-data-lake-foundation-with-aws-glue-and-amazon-s3/"" rel=""nofollow noreferrer"">https://aws.amazon.com/blogs/big-data/build-a-data-lake-foundation-with-aws-glue-and-amazon-s3/</a></p>
","5043127","","","1","395","Abhishek Upadhyaya","2015-06-24 05:36:55","121","43","340","0","50650005","50650049","2018-06-01 19:33:49","0","587","<p>I have a bit over 1200 JSON-files in AWS S3 that I need to convert to Parquet and split into smaller files (I am preparing them for Redshift Spectrum). I have tried to create a Lambda-function that does this for me per file. But the function takes too long to complete or consumes to much memory and therefore ends before completion. The files are around 3-6 GB.</p>

<p>Btw. I use Python.</p>

<p>I do not want to fire up a EC2 for this, since that takes forever to complete.</p>

<p>I would like some advise on how to accomplish this.</p>
","5759828","","","Convert and split large JSON files to smaller Parquet files","<python><json><amazon-web-services><aws-lambda><parquet>","1","0","544"
"50650052","2018-06-01 19:38:09","2","","<p>Your problem is, that the csv-module <code>writerows</code> has its own ""newline""-logic. It interferes with the default newline behaviour of <code>open()</code>:</p>

<p>Fix like this:</p>

<pre><code>with open('rutasAeropuertos.csv', 'w', newline='' ) as archivo_rutas:
#                                      ^^^^^^^^^^
</code></pre>

<p>This is also documented in the example in the documentation: <a href=""https://docs.python.org/3/library/csv.html#csv.writer"" rel=""nofollow noreferrer"">csv.writer(csvfile, dialect='excel', **fmtparams)</a>:</p>

<blockquote>
  <p>If <em>csvfile</em> is a file object, it should be opened with <code>newline=''</code> [1]</p>
</blockquote>

<p>with a link to a footnote telling you:</p>

<blockquote>
  <p><strong>[1]</strong>   If newline='' is not specified, newlines embedded inside quoted fields will not be interpreted correctly, and on platforms that use <code>\r\n</code> linendings on write an extra <code>\r</code> will be added. It should always be safe to specify newline='', since the csv module does its own (universal) newline handling.</p>
</blockquote>

<p>You are using windows which does use <code>\r\n</code> which adds another \r which leads to your ""wrong"" output.</p>

<hr>

<p>Full code with some optimizations:</p>

<pre><code>import csv
import random

def dict_ID_aeropuertos():
  with open('AeropuertosArg.csv') as archivo_csv:
    leer = csv.reader(archivo_csv)
    dic_ID = {}
    for linea in leer:
      dic_ID.setdefault(linea[0],linea[1]) 
  return dic_ID

def ruteoAleatorio():
  dic_ID = dict_ID_aeropuertos()
  lista_ID = list(dic_ID.keys())
  lista_rutas = set()            # a set only holds unique values 
  while (len(lista_rutas) &lt; 50): # simply check the length of the set
    r1,r2 = random.sample(lista_ID, k=2)  # draw 2 different ones
    lista_rutas.add( (r1,r2) )            # you can not add duplicates, no need to check    
  with open('rutasAeropuertos.csv', 'w', newline='' ) as archivo_rutas:
    escribir = csv.writer(archivo_rutas)
    escribir.writerows(lista_rutas)

ruteoAleatorio()
</code></pre>

<p>Output:</p>

<pre><code>9,3
16,10
15,6
[snipp lots of values]
13,14
13,7
20,4
</code></pre>
","7505395","7505395","2018-06-01 19:44:53","1","2193","Patrick Artner","2017-02-02 10:46:51","30736","5120","3506","4713","50649754","50650052","2018-06-01 19:12:05","1","41","<p>I have a problem with me file csv. It's saving with spaces in middle of each row. I don't know why. How do I solve this problem? I'm asking because I don't find any answer and solutions to this.</p>

<p>Here is the code: </p>

<pre><code>import csv
import random

def dict_ID_aeropuertos():
  with open('AeropuertosArg.csv') as archivo_csv:
    leer = csv.reader(archivo_csv)
    dic_ID = {}
    for linea in leer:
      dic_ID.setdefault(linea[0],linea[1])
  archivo_csv.close()
  return dic_ID

def ruteoAleatorio():
  dic_ID = dict_ID_aeropuertos()
  lista_ID = list(dic_ID.keys())
  cont = 0
  lista_rutas = []
  while (cont &lt; 50):
    r1 = random.choice(lista_ID)
    r2 = random.choice(lista_ID)
    if (r1 != r2):
      t = (r1,r2)
      if (t not in lista_rutas):
        lista_rutas.append(t)
        cont += 1

  with open('rutasAeropuertos.csv', 'w') as archivo_rutas:
    escribir = csv.writer(archivo_rutas)
    escribir.writerows(lista_rutas)

  archivo_rutas.close()

ruteoAleatorio()
</code></pre>

<p>Here is the file csv AeropuertosArg.cvs: </p>

<pre><code>1,Aeroparque Jorge Newbery,Ciudad Autonoma de Buenos Aires,Ciudad Autonoma de Buenos Aires,-34.55803,-58.417009
2,Aeropuerto Internacional Ministro Pistarini,Ezeiza,Buenos Aires,-34.815004,-58.5348284
3,Aeropuerto Internacional Ingeniero Ambrosio Taravella,Cordoba,Cordoba,-31.315437,-64.21232
4,Aeropuerto Internacional Gobernador Francisco Gabrielli,Ciudad de Mendoza,Mendoza,-32.827864,-68.79849
5,Aeropuerto Internacional Teniente Luis Candelaria,San Carlos de Bariloche,Rio Negro,-41.146714,-71.16203
6,Aeropuerto Internacional de Salta Martin Miguel de Guemes,Ciudad de Salta,Salta,-24.84423,-65.478412
7,Aeropuerto Internacional de Puerto Iguazu,Puerto Iguazu,Misiones,-25.731778,-54.476181
8,Aeropuerto Internacional Presidente Peron,Ciudad de Neuquen,Neuquen,-38.952137,-68.140484
9,Aeropuerto Internacional Malvinas Argentinas,Ushuaia,Tierra del Fuego,-54.842237,-68.309701
10,Aeropuerto Internacional Rosario Islas Malvinas,Rosario,Santa Fe,-32.916887,-60.780391
11,Aeropuerto Internacional Comandante Armando Tola,El Calafate,Santa Cruz,-50.283977,-72.053641
12,Aeropuerto Internacional General Enrique Mosconi,Comodoro Rivadavia,Chubut,-45.789435,-67.467498
13,Aeropuerto Internacional Teniente General Benjamin Matienzo,San Miguel de Tucuman,Tucuman,-26.835888,-65.108361
14,Aeropuerto Comandante Espora,Bahia Blanca,Buenos Aires,-38.716152,-62.164955
15,Aeropuerto Almirante Marcos A. Zar,Trelew,Chubut,-43.209957,-65.273405
16,Aeropuerto Internacional de Resistencia,Resistencia,Chaco,-27.444926,-59.048739
17,Aeropuerto Internacional Astor Piazolla,Mar del Plata,Buenos Aires,-37.933205,-57.581518
18,Aeropuerto Internacional Gobernador Horacio Guzman,San Salvador de Jujuy,Jujuy,-24.385987,-65.093755
19,Aeropuerto Internacional Piloto Civil Norberto Fernandez,Rio Gallegos,Santa Cruz,-51.611788,-69.306315
20,Aeropuerto Domingo Faustino Sarmiento,San Juan,San Juan,-31.571814,-68.422568
</code></pre>
","8802004","7505395","2018-06-01 19:27:04","Spaces in middle of each row in file csv","<python><python-3.x><file><csv>","1","12","3002"
"50650072","2018-06-01 19:39:18","16","","<p>Let's break down the components of your question:</p>

<ol>
<li><p>Your expectation of regularisation is probably in line with a feed-forward network where yes the penalty term is applied to the weights of the overall network. But this is not necessarily the case when you have RNNs mixed with CNNs etc so Keras opts give fine grain control. Perhaps for easy setup, a regularisation at model level could be added to the API for all weights.</p></li>
<li><p>When you use layer regularisation, the base <code>Layer</code> class actually <a href=""https://github.com/keras-team/keras/blob/7365a99f6e832847808c7aa28718d32fbc744b21/keras/engine/base_layer.py#L255"" rel=""noreferrer"">adds</a> the regularising term to the loss which at training time penalises the corresponding layer's weights etc.</p></li>
<li><p>Now in Keras you can often apply regularisation to 3 different things as in <a href=""https://keras.io/layers/core/#dense"" rel=""noreferrer"">Dense</a> layer. Every layer has different kernels such recurrent etc, so for the question let's look at the ones you are interested in but the same roughly applies to all layers:</p>

<ol>
<li><strong>kernel</strong>: this applies to actual <em>weights</em> of the layer, in Dense it is the <em>W</em> of <em>Wx+b</em>.</li>
<li><strong>bias</strong>: this is the bias vector of the weights, so you can apply a different regulariser for it, the <em>b</em> in <em>Wx+b</em>.</li>
<li><strong>activity</strong>: is applied to the output vector, the <em>y</em> in <em>y = f(Wx + b)</em>.</li>
</ol></li>
</ol>
","9758922","","","0","1557","nuric","2018-05-08 13:57:46","6956","503","353","45","50649831","50650072","2018-06-01 19:18:45","10","3950","<p>I am trying to understand why regularization syntax in Keras looks the way that it does.</p>

<p>Roughly speaking, regularization is way to reduce overfitting by adding a penalty term to the loss function proportional to some function of the model weights.  Therefore, I would expect that regularization would be defined as part of the specification of the model's loss function.</p>

<p>However, in Keras the regularization is defined on a per-layer basis.  For instance, consider this regularized DNN model:</p>

<pre><code>input = Input(name='the_input', shape=(None, input_shape))
x = Dense(units = 250, activation='tanh', name='dense_1', kernel_regularizer=l2, bias_regularizer=l2, activity_regularizer=l2)(x)
x = Dense(units = 28, name='dense_2',kernel_regularizer=l2, bias_regularizer=l2, activity_regularizer=l2)(x)
y_pred = Activation('softmax', name='softmax')(x)
mymodel= Model(inputs=input, outputs=y_pred)
mymodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
</code></pre>

<p>I would have expected that the regularization arguments in the Dense layer were not needed and I could just write the last line more like:</p>

<pre><code>mymodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'], regularization='l2')
</code></pre>

<p>This is obviously wrong syntax, but I was hoping someone could elaborate for me a bit on why the regularizes are defined this way and what is actually happening when I use layer-level regularization.</p>

<p>The other thing I don't understand is under what circumstances would I use each or all of the three regularization options: <code>(kernel_regularizer, activity_regularizer, bias_regularizer)</code>?</p>
","2690677","2690677","2018-06-01 19:51:39","Understanding Regularization in Keras","<python><keras>","1","1","1735"
"50650168","2018-06-01 19:48:22","0","","<p>I'm guessing here, but it looks like the reason why you are doing this is a performance concern.   </p>

<p>Python threads aren't performant in any meaningful way for this use case.   Instead, use sqlite transactions, which are super fast.   </p>

<p>If you do all your updates in a transaction, you'll find an order of magnitude speedup. </p>
","627042","","","0","347","Erik Aronesty","2011-02-21 17:26:44","6611","1758","1302","87","22739590","22739924","2014-03-30 02:31:57","10","26847","<p>I am trying to write a multi-threaded Python application in which a single SQlite connection is shared among threads.  I am unable to get this to work.  The real application is a cherrypy web server, but the following simple code demonstrates my problem.</p>

<p>What change or changes to I need to make to run the sample code, below, successfully?</p>

<p>When I run this program with THREAD_COUNT set to 1 it works fine and my database is updated as I expect (that is, letter ""X"" is added to the text value in the SectorGroup column).</p>

<p>When I run it with THREAD_COUNT set to anything higher than 1, all threads but 1 terminate prematurely with SQLite related exceptions.  Different threads throw different exceptions (with no discernible pattern) including:</p>

<pre><code>OperationalError: cannot start a transaction within a transaction 
</code></pre>

<p>(occurs on the <code>UPDATE</code> statement)</p>

<pre><code>OperationalError: cannot commit - no transaction is active 
</code></pre>

<p>(occurs on the .commit() call)</p>

<pre><code>InterfaceError: Error binding parameter 0 - probably unsupported type. 
</code></pre>

<p>(occurs on the <code>UPDATE</code> and the <code>SELECT</code> statements)</p>

<pre><code>IndexError: tuple index out of range
</code></pre>

<p>(this one has me completely puzzled, it occurs on the statement <code>group = rows[0][0] or ''</code>, but only when multiple threads are running)</p>

<p>Here is the code:</p>

<pre><code>CONNECTION = sqlite3.connect('./database/mydb', detect_types=sqlite3.PARSE_DECLTYPES, check_same_thread = False)
CONNECTION.row_factory = sqlite3.Row

def commands(start_id):

    # loop over 100 records, read the SectorGroup column, and write it back with ""X"" appended.
    for inv_id in range(start_id, start_id + 100):

        rows = CONNECTION.execute('SELECT SectorGroup FROM Investment WHERE InvestmentID = ?;', [inv_id]).fetchall()
        if rows:
            group = rows[0][0] or ''
            msg = '{} inv {} = {}'.format(current_thread().name, inv_id, group)
            print msg
            CONNECTION.execute('UPDATE Investment SET SectorGroup = ? WHERE InvestmentID = ?;', [group + 'X', inv_id])

        CONNECTION.commit()

if __name__ == '__main__':

    THREAD_COUNT = 10

    for i in range(THREAD_COUNT):
        t = Thread(target=commands, args=(i*100,))
        t.start()
</code></pre>
","180368","","","How to share single SQLite connection in multi-threaded Python application","<python><multithreading><sqlite>","3","4","2396"
"50650177","2018-06-01 19:49:16","2","","<p>You're not <em>storing</em> the instances you create.</p>

<pre><code>for user in dict:
    u    = dict[user]
    user = User(u['firstname'], u['lastname'], u['age'])
</code></pre>

<p>^^ Create a bunch of <code>User</code>'s, but assign each one to the same name, <code>user</code>.  That means that users 1 and 2 are lost as as you continue iterating through your loop.</p>

<p>If you want to have access to each of these users, you need to store the reference to the created instance:</p>

<pre><code>users = []
for user in dict:
    u    = dict[user]
    user = User(u['firstname'], u['lastname'], u['age'])
    users.append(user)
</code></pre>

<p>Then, to print results:</p>

<pre><code>for user in users:
    print(user)
</code></pre>

<p>*<em>PS</em>: When you write <code>for user in dict</code> you are iterating over the <em>keys</em> of <code>dict</code>, which is why you get <code>User1, User2, User3</code>.</p>

<p>**<em>PPS</em>: You should absolutely not use <code>dict</code> as a name, that shadows the builtin <code>dict()</code> function, and will cause you problems later.</p>
","351031","","","3","1103","g.d.d.c","2010-05-26 14:52:13","36506","1946","372","193","50650133","50650177","2018-06-01 19:44:50","0","184","<p>Preface: I've spent many HOURS trying to read similar posts like this one and try lots of different things, but for the life of me, I can't understand what I'm doing wrong, or why this doesn't work. Can someone please give me an ELI5 answer as to either what I'm doing wrong, or why this doesn't work the way I'm expecting?</p>

<pre><code>class User:

    def __init__(self, first, last, age):
        self.firstname = first
        self.lastname  = last
        self.age       = age


dict = {
    'User1': {'id': 'id001', 'firstname': 'User', 'lastname': 'One', 'age': 11},
    'User2': {'id': 'id002', 'firstname': 'User', 'lastname': 'Two', 'age': 22},
    'User3': {'id': 'id003', 'firstname': 'User', 'lastname': 'Three', 'age': 33}
}


for user in dict:
    u    = dict[user]
    user = User(u['firstname'], u['lastname'], u['age'])


try:
    print(User1.lastname)
except Exception as e:
    print(e)
finally:
    print(user.lastname)
</code></pre>

<p>Which returns:</p>

<pre><code>name 'User1' is not defined
Three
</code></pre>

<p>Clearly <code>user</code> is getting instantiated three times with the data from <code>dict</code>, with <code>User3</code>'s data being the last of it. </p>

<p>If I do a:</p>

<pre><code>for user in dict:
    print(user)
</code></pre>

<p>it prints out:</p>

<pre><code>User1
User2
User3
</code></pre>

<p>So my expectation would be that the for loop would run, and then I'd be able to call <code>User1.firstname</code>, <code>User2.lastname</code>, or <code>User3.age</code>, and it'd give me the same data that's in <code>dict</code> (User, Two, 33, respectively), but that's clearly not the case.</p>

<p>Help?</p>
","8142768","","","Python - Instantiate Objects from dictionary with for loop","<python><loops><class>","2","6","1668"
"50650184","2018-06-01 19:49:49","0","","<p>I realize that recursion isn't popular, but would something like this work? Also, uncertain if adding recursion to the mix counts as just using slices.</p>

<pre><code>def get_elements(A, m, n):
    if(len(A) &lt; m):
        return A
    else:
        return A[:m] + get_elements(A[n:], m, n)
</code></pre>

<p>A is the array, m and n are defined as in the question. The first if covers the base case, where you have an array with length less than the number of elements you're trying to retrieve, and the second if is the recursive case. I'm somewhat new to python, please forgive my poor understanding of the language if this doesn't work properly, though I tested it and it seems to work fine.</p>
","2452594","2452594","2018-06-01 20:05:32","0","705","GnoveltyGnome","2013-06-04 16:31:09","115","8","16","0","50647167","50647227","2018-06-01 15:54:16","26","1388","<p>I am trying to get <em>m</em> values while stepping through every <em>n</em> elements of an array. For example, for <em>m</em> = 2 and <em>n</em> = 5, and given</p>

<pre><code>a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
</code></pre>

<p>I want to retrieve</p>

<pre><code>b = [1, 2, 6, 7]
</code></pre>

<p>Is there a way to do this using slicing? I can do this using a nested list comprehension, but I was wondering if there was a way to do this using the indices only. For reference, the list comprehension way is:</p>

<pre><code> b = [k for j in [a[i:i+2] for i in range(0,len(a),5)] for k in j]
</code></pre>
","9877290","9209546","2018-06-08 18:00:57","Stepping with multiple values while slicing an array in Python","<python><arrays><list>","7","2","611"
"50650247","2018-06-01 19:56:05","1","","<p>The proper schema definition for ""src"" should have been:</p>

<pre><code>attributetype ( 1.3.6.4.2.7888.5.1.16 NAME 'src'
                DESC 'ATT source path'
                EQUALITY caseExactMatch
                SYNTAX 1.3.6.1.4.1.1466.115.121.1.15{512}
                X-ORIGIN 'user defined' )
</code></pre>

<p>The ""equality"" clause was missing. Thats what the NAMING_VIOLATION was explaining.</p>
","8216902","","","0","409","nverkland","2017-06-26 17:45:30","27","8","2","0","50649825","50650247","2018-06-01 19:18:22","0","94","<p>I have a set of diff's that need to be ""saved"" (they are all new records). The following code is used for committing the set of changes:</p>

<pre><code>def commit(self):
    l = ldap.initialize(self.ldapURL)
    l.simple_bind_s(self.ldapUser,self.ldapPass)
    for dn,ldif in self.ldapAdds.iteritems():
        try:
            print json.dumps(ldif,indent=4)
            l.add_s(dn,ldif)
            print ""a"",
        except ldap.ALREADY_EXISTS:
            pass

    for dn,ldif in self.ldapMods.iteritems():
        l.modify_s(dn,ldif)
        print ""m"",
    print """"
    l.unbind_s()
    self.ldapAdds = dict()
    self.ldapMods = dict()
</code></pre>

<p>Unfortunately, I am getting the following error:</p>

<blockquote>
  <p>Traceback (most recent call last):   File ""./ldapUpdate.py"", line 868,
  in 
      lMods.commit()   File ""./ldapUpdate.py"", line 769, in commit
      l.add_s(dn,ldif)   File ""/sites/utils/Python/lib/python2.7/site-packages/ldap/ldapobject.py"",
  line 216, in add_s
      return self.add_ext_s(dn,modlist,None,None)   File ""/sites/utils/Python/lib/python2.7/site-packages/ldap/ldapobject.py"",
  line 202, in add_ext_s
      resp_type, resp_data, resp_msgid, resp_ctrls = self.result3(msgid,all=1,timeout=self.timeout)   File
  ""/sites/utils/Python/lib/python2.7/site-packages/ldap/ldapobject.py"",
  line 519, in result3
      resp_ctrl_classes=resp_ctrl_classes   File ""/sites/utils/Python/lib/python2.7/site-packages/ldap/ldapobject.py"",
  line 526, in result4
      ldap_result = self._ldap_call(self._l.result4,msgid,all,timeout,add_ctrls,add_intermediates,add_extop)
  File
  ""/sites/utils/Python/lib/python2.7/site-packages/ldap/ldapobject.py"",
  line 108, in _ldap_call
      result = func(*args,**kwargs) ldap.NAMING_VIOLATION: {'info': ""naming attribute 'src' has no equality matching rule"", 'desc':
  'Naming violation'}</p>
</blockquote>

<p>The failed ldiff record looks like this:</p>

<pre><code>[
    [  ""src"",   ""ecare/ecare-self.ear"" ], 
    [  ""modname"",  ""ecare-self""  ], 
    [  ""dest"",   ""/sites/MODULES/ecare/ecare-self.ear""], 
    [  ""objectClass"",  [  ""ctlapp"", ""ctlmodule"", ""top"" ] ], 
    [  ""action"",  ""rsync"" ], 
    [  ""depot"",  ""DEPOT"" ]
]
</code></pre>

<p>What is it about the ""src"" field that SLAPD doesn't like? Does someone have more insight into NAMING_VIOLATIONs?</p>

<p>""src"" has this definition in schema</p>

<pre><code>attributetype ( 1.3.6.4.2.7888.5.1.16 NAME 'src'
                SYNTAX 1.3.6.1.4.1.1466.115.121.1.15
                X-ORIGIN 'user defined' )
</code></pre>

<p>""ctlapp"" has this definition in schema</p>

<pre><code>objectclass ( 1.3.6.4.2.7888.5.1.22 NAME 'ctlapp'
                DESC 'ATT deployable component'
                SUP ctlmodule STRUCTURAL
                MUST ( src $ depot $ dest $ action )
                X-ORIGIN 'user defined' )
</code></pre>
","8216902","1222951","2018-06-02 11:17:38","Understanding Naming Violations in Python-ldap","<python><python-2.7><openldap><ldap-query>","1","0","2861"
"50650269","2018-06-01 19:57:46","0","","<p>The event you're looking for is <a href=""http://nullege.com/codes/search/Tkinter.OptionMenu.bind"" rel=""nofollow noreferrer""><code>Activate</code></a>:</p>

<pre><code>optmenu.bind('&lt;Activate&gt;', onactivate)
</code></pre>

<p>Your <code>onactivate</code> callback takes an <code>Activate</code> event, but you probably don't care about its attributes.</p>

<p>The second half of your problem is how to update the menu. To do this, you use the <code>menu</code> attribute, which is a <a href=""http://nullege.com/codes/search/Tkinter.Menu"" rel=""nofollow noreferrer""><code>Menu</code></a> object, on which you can call <a href=""http://nullege.com/codes/search/Tkinter.Menu.delete"" rel=""nofollow noreferrer""><code>delete</code></a> and <a href=""http://nullege.com/codes/search/Tkinter.Menu.add"" rel=""nofollow noreferrer""><code>add</code></a> and whatever else you want. So, for example:</p>

<pre><code>def onactivate(evt):
    menu = optmenu['menu']
    menu.delete(0, tkinter.END)
    menu.add_command(label='new choice 1')
    menu.add_command(label='new choice 2')
    menu.add_separator()
    menu.add_command(label='new choice 3')
    optvar.set('new choice 1')
</code></pre>

<p>(However, notice that, while <code>set</code>ting the var at the end does cause that to become the new default selection, and does show up in the menu header, it doesn't cause the selected item to be highlighted if the cursor isn't over any of the menu items. If you want that, it's tricky, so hopefully you don't.)</p>
","908494","","","0","1509","abarnert","2011-08-23 20:55:30","272076","19605","5576","4509","50650012","50650269","2018-06-01 19:34:54","0","784","<p>I need to update my list when I click the button to drop down list:</p>

<p><a href=""https://i.stack.imgur.com/QQv02.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QQv02.png"" alt=""enter image description here""></a></p>

<p>How can I bind the button to some function?</p>
","9882954","3266847","2018-06-01 21:04:00","Drop down button in Tkinter OptionMenu","<python><tkinter><optionmenu>","2","2","294"
"50650287","2018-06-01 19:59:09","13","","<p>Try to</p>

<pre><code>source ~/anaconda3/bin/activate root
anaconda-navigator
</code></pre>

<p><a href=""https://github.com/ContinuumIO/anaconda-issues/issues/1580"" rel=""noreferrer"">https://github.com/ContinuumIO/anaconda-issues/issues/1580</a></p>
","5955187","","","1","253","Александр Поплавский","2016-02-20 11:32:29","147","12","2","0","44841470","","2017-06-30 07:56:26","25","137770","<p>Anaconda (listed as ""Python 3.6.0 (Anaconda 4.3.1 64 bit)"" ) is in my programs and features list, but there is seeming <strong>no Anaconda Navigator desktop app, as their seems to be no icon on my desktop and I am unable to search for it through ""Start""</strong>. Could this be because I have the 32-bit version of Anaconda downloaded and I have a 64-bit OS (I thought I should do this because Python on my computer was 64-bit) or because I downloaded Anaconda under ""users"" instead of Desktop. I also downloaded Anaconda twice, if that could be causing some of the problem. I have a Windows 10 laptop, if that is any help.</p>
","8232942","8232942","2017-06-30 10:50:49","Anaconda Installed but Cannot Launch Navigator","<python><windows><python-3.x><windows-10><anaconda>","19","7","631"
"50650288","2018-06-01 19:59:19","2","","<p>Change this line:</p>

<pre><code>for numbers in a:
</code></pre>

<p>To:</p>

<pre><code>for i, numbers in enumerate(a):
</code></pre>

<p>And then change how you print the indices:</p>

<pre><code>print(""Number"", n, ""is located in the list and the position is:"", (i+1))
</code></pre>

<p>Sample output:</p>

<pre><code>[1, 2, 2, 5, 5, 5, 6, 7, 8, 8, 8, 10, 10, 10, 10, 10, 11, 11, 16, 17, 17, 19, 19]
What number are you looking for? 8
Number 8 is located in the list and the position is: 9
Number 8 is located in the list and the position is: 10
Number 8 is located in the list and the position is: 11
</code></pre>
","3483203","","","2","622","user3483203","2014-04-01 00:22:53","39972","4672","2822","1887","50650250","50650288","2018-06-01 19:56:06","2","43","<p>So I have written a code that generates a random list with random number of values. Then asks the user what number the user is looking for and if its in the list, it will tell the user what position in the list the number is in.</p>

<pre><code>import random

a = [random.randint(1, 20) for i in range(random.randint(8, 30))]

a.sort()
print(a)


def askUser():

    n = input(""What number are you looking for?"")
    while not n.isdigit():
        n = input(""What number are you looking for?"")

    n = int(n)
    s = 0

    for numbers in a:
        if numbers == n:
            s += 1
            print(""Number"", n, ""is located in the list and the position is:"", (a.index(n)+1))
            # Something here to skip this index next time it goes through the loop
        else:
            pass
    if s == 0:
        print(""Your number could not be found"")


askUser()
</code></pre>

<p>I would like to add something that will skip the index it found first time and then look for the index of the duplicate if there is one.</p>

<p>Current result</p>

<pre><code>[2, 4, 8, 9, 10, 10, 16, 19, 20, 20]
What number are you looking for?20
Number 20 is located in the list and the position is: 9
Number 20 is located in the list and the position is: 9
</code></pre>

<p>Desired result</p>

<pre><code>[2, 4, 8, 9, 10, 10, 16, 19, 20, 20]
What number are you looking for?20
Number 20 is located in the list and the position is: 9
Number 20 is located in the list and the position is: 10
</code></pre>
","9041256","","","Checking duplicates in list from user input","<python><python-3.x><list><input>","4","1","1499"
"50650303","2018-06-01 20:01:16","1","","<p>You are returning the class itself from <code>errormanger.__new__</code>, not an instance of that class. <code>__new__</code> should look something like</p>

<pre><code>class error_manager:
    def __new__(cls):
        inst = object.__new__(cls)
        inst._error = 0
        inst._er_string = ""None""
        return inst
</code></pre>

<p>As is, though, it would be simpler to override <code>__init__</code> instead of <code>__new__</code>, unless there is other work not shown that requires <code>__new__</code> to be overridden.</p>

<pre><code>class error_manager:
    def __init__(self):
        self._error = 0
        self._er_string = ""None""
</code></pre>
","1126841","1126841","2018-06-01 20:22:34","2","669","chepner","2012-01-02 21:41:39","295038","14033","16849","5091","50650218","50650303","2018-06-01 19:54:13","0","33","<p>I've already looked into this link: <a href=""http://infohost.nmt.edu/tcc/help/pubs/python/web/new-new-method.html"" rel=""nofollow noreferrer"">http://infohost.nmt.edu/tcc/help/pubs/python/web/new-new-method.html</a></p>

<p>But even tough I followed these instructions there's something bothering me on my code:</p>

<p>error_manager.py:</p>

<pre><code>class error_manager:
    def __new__(cls):
        cls._error = 0
        cls._er_string = ""None""
        return cls
</code></pre>

<p>database.py:</p>

<pre><code>from error_manager import error_manager as EM

class Database(EM):
    def __new__(cls, table_name="""", database_name=""""):
        inst = EM.__new__(cls) #THIS LINE
        return inst

    def __init__(self, table_name="""", database_name=""""):
        print(""init dtb"")
        self.table_name = table_name
        self.database_name = database_name

    def __str__(self):
        return self.table_name + "" "" + self.database_name

    def __repr__(self):
        return self.table_name + "" "" + self.database_name
</code></pre>

<p>run:</p>

<pre><code>dtb = Database(""tbl"", ""dtb"")
print(dtb)
</code></pre>

<p>This outputs:</p>

<pre><code>&lt;class '__main__.Database'&gt;
</code></pre>

<p>Why it's not calling either __str_ or __repr_ methods from the subclass?</p>

<p>Plus on the line #THIS LINE it says ""Too many arguments positional arguments for method call. But if I remove the arguments the script won't run. What do I do?</p>
","9878921","9878921","2018-06-01 20:00:34","Parent class methods bein called despise declaration on subclass","<python>","1","0","1456"
"50650306","2018-06-01 20:01:35","1","","<p><code>QObject</code> instances can be created in any thread, of course. And they are, and Qt would be quite useless without it. What you're looking for is the global application object - and its thread:</p>

<pre><code>myObj = QObject()
mainThread = QCoreApplication.instance().thread()
myObj.moveToThread(mainThread)
</code></pre>

<p>It is undefined behavior to call, from the current thread, any non-thread-safe methods on the object after the <code>moveToThread</code> call. Past this call, the object's non-thread-safe methods can be safely used from the target thread only.</p>
","1329652","1329652","2018-06-04 16:51:33","0","587","Kuba Ober","2012-04-12 16:24:40","74972","8871","6744","776","50648367","50650306","2018-06-01 17:25:05","0","186","<p>I know that in general, <code>QObject()</code> instances should be created in the main thread. I also know that - once created - you can move a <code>QObject()</code> from the main thread to another one:</p>

<p>official Qt docs can be found <a href=""http://doc.qt.io/qt-5/qthread.html"" rel=""nofollow noreferrer"">here</a></p>

<pre><code>    # Worker-object approach
    # -----------------------
    # Note: these codelines execute in the main thread.
    workerThread = QThread()
    workerObj = WorkerObj()
    workerObj.moveToThread(workerThread)
</code></pre>

<p>&nbsp;<br>
Currently I am facing the exact opposite problem. I have a <code>QObject()</code> instantiated in a (non-main) <code>QThread</code>. I want to move it to the main thread, like this:</p>

<pre><code>    # Move a QObject() to the main thread
    # ------------------------------------
    # Note: these codelines execute in some QThread
    myObj = QObject()
    myObj.moveToThread(threading.main_thread())
</code></pre>

<p>I get the following error:</p>

<blockquote>
  <p>TypeError: moveToThread(self, QThread): argument 1 has unexpected type '_MainThread'</p>
</blockquote>

<p>I probably get the error because the main thread is not a genuine <code>QThread</code>. What should I do to make it work?</p>

<hr>

<p><b>EDIT:</b><br>
Apparently the answer was right <a href=""https://doc.qt.io/qt-5/qobject.html#moveToThread"" rel=""nofollow noreferrer"">in the documentation</a> of the <code>moveToThread()</code> function. This is pretty embarrassing. My sincere apologies. I'll be more careful next time.</p>

<hr>
","6178507","6178507","2018-06-02 08:44:43","PyQt5: How to move a QObject to the main thread?","<python><multithreading><qt><qt5><pyqt5>","1","5","1596"
"50650316","2018-06-01 20:02:41","0","","<p><a href=""https://stackoverflow.com/a/14947384/6451573"">this answer</a> from <a href=""https://stackoverflow.com/questions/14947238/change-first-line-of-a-file-in-python"">change first line of a file in python</a> you copied from doesn't work in windows</p>

<p>On Linux, you can open a file for reading &amp; writing at the same time. The system ensures that there's no conflict, but behind the scenes, 2 <em>different</em> file objects are being handled. And this method is <em>very</em> unsafe: if the program crashes while reading/writing (power off, disk full)... the file has a great chance to be truncated/corrupt.</p>

<p>Anyway, in Windows, you cannot open a file for reading and writing at the same time using 2 handles. It just destroys the contents of the file.</p>

<p>So there are 2 options, which are portable and safe:</p>

<ol>
<li>create a file in the same directory, once copied, delete first file, and rename the new one</li>
</ol>

<p>Like this:</p>

<pre><code>import os
import shutil

filepath = ""test.txt""

with open(filepath) as from_file, open(filepath+"".new"",""w"") as to_file:
    data = from_file.readline()
    to_file.write(""something else\n"")
    shutil.copyfileobj(from_file, to_file)
os.remove(filepath)
os.rename(filepath+"".new"",filepath)
</code></pre>

<p>This doesn't take much longer, because the <code>rename</code> operation is instantaneous. Besides, if the program/computer crashes at any point, one of the files (old or new) is valid, so it's safe.</p>

<ol start=""2"">
<li>if patterns have the same length, use read/write mode</li>
</ol>

<p>like this:</p>

<pre><code>filepath = ""test.txt""

with open(filepath,""r+"") as rw_file:
    data = rw_file.readline()
    data = ""h""*(len(data)-1) + ""\n""
    rw_file.seek(0)
    rw_file.write(data)
</code></pre>

<p>Here we, read the line, replace the first line by the same amount of <code>h</code> characters, rewind the file and write the first line back, overwriting previous contents, keeping the rest of the lines. This is also safe, and even if the file is huge, it's very fast. The only constraint is that the pattern must be of the exact same size (else you would have remainders of the previous data, or you would overwrite the next line(s) since no data is shifted)</p>
","6451573","","","2","2263","Jean-François Fabre","2016-06-10 19:19:53","113106","37329","9248","14670","50649985","50650316","2018-06-01 19:32:16","1","57","<p>I have been searching for a solution for this and haven't been able to find one. I have a directory of folders which contain multiple, very-large csv files. I'm looping through each csv in each folder in the directory to replace values of certain headers. I need the headers to be consistent (from file to file) in order to run a different script to process all the data properly.</p>

<p>I found this solution that I though would work: <a href=""https://stackoverflow.com/questions/14947238/change-first-line-of-a-file-in-python"">change first line of a file in python</a>.</p>

<p>However this is not working as expected. My code:</p>

<pre><code>        from_file = open(filepath)
            # for line in f:
            #     if
        data = from_file.readline()
            # print(data)
        # with open(filepath, ""w"") as f:
        print 'DBG: replacing in file', filepath
            # s = s.replace(search_pattern, replacement)
        for i in range(len(search_pattern)):
            data = re.sub(search_pattern[i], replacement[i], data)
            # data = re.sub(search_pattern, replacement, data)
        to_file = open(filepath, mode=""w"")
        to_file.write(data)
        shutil.copyfileobj(from_file, to_file)
</code></pre>

<p>I want to replace the header values in <code>search_pattern</code> with values in <code>replacement</code> without saving or writing to a different file - I want to modify the file. I have also tried</p>

<pre><code>        shutil.copyfileobj(from_file, to_file, -1)
</code></pre>

<p>As I understand it that should copy the whole file rather than breaking it up in chunks, but it doesn't seem to have an effect on my output. Is it possible that the csv is just too big?</p>

<p>I haven't been able to determine a different way to do this or make this way work. Any help would be greatly appreciated!</p>
","9882885","6451573","2018-06-01 19:49:02","Selectively replacing csv header names","<python><windows><csv>","1","5","1860"
"50650321","2018-06-01 20:03:32","1","","<p>This function recursively replaces all strings which equal the value <code>original</code> with the value <code>new</code>.</p>

<p>This function works on the python structure - but of course you can use it on a json file - by using <code>json.load</code></p>

<p>It doesn't replace keys in the dictionary - just the values.</p>

<pre><code>def nested_replace( structure, original, new ):
    if type(structure) == list:
        return [nested_replace( item, original, new) for item in structure]

    if type(structure) == dict:
        return {key : nested_replace(value, original, new)
                     for key, value in structure.items() }

    if structure == original:
        return new
    else:
        return structure

d = [ 'replace', {'key1': 'replace', 'key2': ['replace', 'don\'t replace'] } ]
new_d = nested_replace(d, 'replace', 'now replaced')
print(new_d)
['now replaced', {'key1': 'now replaced', 'key2': ['now replaced', ""don't replace""]}]
</code></pre>
","3426606","3426606","2018-06-01 22:45:45","0","982","Tony Suffolk 66","2014-03-16 19:27:38","4885","525","190","55","50631393","50632306","2018-05-31 19:21:47","2","811","<p>Say that I have a JSON file whose structure is either unknown or may change overtime - I want to replace all values of ""REPLACE_ME"" with a string of my choice in Python.</p>

<p>Everything I have found assumes I know the structure. For example, I can read the JSON in with <code>json.load</code> and walk through the dictionary to do replacements then write it back. This assumes I know Key names, structure, etc.</p>

<p>How can I replace ALL of a given string value in a JSON file with something else?</p>
","2340636","","","Python replace values in unknown structure JSON file","<python><json>","5","1","511"
"50650325","2018-06-01 20:03:39","1","","<p>As a complement to the other answer. You will also need to make your images have three channels, although technically not the best input for Resnet, it is the easiest solution (changing the Resnet model is an option too, if you visit the source code and change the input shape yourself).</p>

<p>Use numpy to pack images in three channels:</p>

<pre><code>images3ch = np.concatenate([images,images,images], axis=-1)
</code></pre>
","2097240","","","0","433","Daniel Möller","2013-02-21 21:35:32","48862","7561","2736","22","50646426","50649987","2018-06-01 15:10:05","0","517","<p>I'm trying to use a pretrained network such as <code>tf.keras.applications.ResNet50</code> but I have two problems:</p>

<p>I just want to obtain the top embedding layers at the end of the network, because I don't want to do any image classification. So due to this there is no need for a classes number I think.</p>

<ul>
<li><p><code>tf.keras.applications.ResNet50</code> takes a default parameter <code>'classes=1000'</code></p>

<ul>
<li>Is there a way how I can omit this parameter?</li>
</ul></li>
<li><p>My input pictures are <code>128*128*1</code> pixels and not <code>224*224*3</code></p>

<ul>
<li>What is the best way to fix my input data shape?</li>
</ul></li>
</ul>

<p>My goal is to make a triplet loss network with the output of a <code>resnet</code> network.</p>

<p>Thanks a lot!</p>
","9731056","1735003","2018-06-01 19:35:47","How to use pre-trained models without classes in Tensorflow?","<python><tensorflow><keras><resnet>","2","2","804"
"50650341","2018-06-01 20:04:30","1","","<p>A simple way might be to use <code>scipy.interpolate.interp1d</code> like so:</p>

<pre><code>&gt;&gt;&gt; from scipy.interpolate import interp1d

&gt;&gt;&gt; def resample(data, n):
...     m = len(data)
...     xin, xout = np.arange(n, 2*m*n, 2*n), np.arange(m, 2*m*n, 2*m)
...     return interp1d(xin, data, 'nearest', fill_value='extrapolate')(xout)
... 
&gt;&gt;&gt; resample(short, new_length)
array([1., 0., 0., 1., 0., 0., 0., 0., 0., 1.])
&gt;&gt;&gt; 
&gt;&gt;&gt; resample(long, new_length)
array([1., 1., 0., 1., 0., 0., 0., 1., 0., 1.])
</code></pre>
","7207392","","","1","567","Paul Panzer","2016-11-24 23:39:00","37645","7185","734","1","50649875","50650341","2018-06-01 19:22:34","2","52","<p>BACKSTORY:</p>

<p>Getting data together to feed into a neural network; starts as a document (long string); gets split into sentences, sentences are reduced to 1 or 0 depending on if they have a feature (in this case, class of word) or not. </p>

<p>The catch is that documents have different numbers of sentences, so it can't be a 1-1 between sentences and input neurons; you have to train to a fixed number of neurons (unless I'm missing something).</p>

<p>So, I'm working on an algo to map arrays to a fixed size, while preserving the frequency and position of those 1's in the array as much as possible (as that's what the NN is making its decisions off of.</p>

<p>CODE: </p>

<p>say we're aiming for a fixed length of 10 sentences or neurons, and need to be able to handle arrays of smaller and larger size.</p>

<pre><code>new_length = 10
short = [1,0,1,0,0,0,0,1]
long  = [1,1,0,0,1,0,0,0,0,1,0,0,1]

def map_to_fixed_length(arr, new_length):
    arr_length = len(arr)
    partition_size = arr_length/new_length
    res = []
    for i in range(new_length):
        slice_start_index = int(math.floor(i * partition_size))
        slice_end_index = int(math.ceil(i * partition_size))
        partition = arr[slice_start_index:slice_end_index]
        val = sum(partition)
        res.append([slice_start_index, slice_end_index, partition])
        if val &gt; 0:
            res.append(1)
        else:
            res.append(0)
    return res
</code></pre>

<p>Not very pythonic probably. Anyways, the issue is that this is ommits certain index-slices. For example, the last index of <code>short</code> is omitted, and due to the rounding various indexes get omitted as well.</p>

<p>This is a simplified version of what I've been working on, which is mainly just adding if-statements to address all the gaps this leaves. But is there a better way to do this? Something a bit more statistically sound?</p>

<p>I was looking through numpy but all the resize functions are just padding with zeros or something pretty arbitrarily.</p>
","7964289","7964289","2018-06-01 19:28:13","Mapping arbitrary length list to fixed length, preserving frequency and position of internal results (as much as possible)","<python><python-3.x><algorithm>","2","3","2043"
"50650378","2018-06-01 20:07:33","1","","<p>You could simplify this code using <code>numpy</code> to remove your loops.</p>

<pre><code>a = np.array([random.randint(1,20) for i in range(random.randint(8,30))])
</code></pre>

<p>Then you can use <code>np.where</code> to determine if the user selected a value in your array <code>a</code> of random values:</p>

<pre><code>idx_selections = np.where(a == n)[0]
</code></pre>

<p>Then you can handle if the user matched an answer or not:</p>

<pre><code>if len(idx_selections) == 0:
    print(""Your number could not be found"")
else:
    for i in idx_selections:
        print(""Number"", n, ""is located in the list and the position is:"", i)
</code></pre>
","2703802","","","0","659","vealkind","2013-08-21 13:27:49","1586","127","128","11","50650250","50650288","2018-06-01 19:56:06","2","43","<p>So I have written a code that generates a random list with random number of values. Then asks the user what number the user is looking for and if its in the list, it will tell the user what position in the list the number is in.</p>

<pre><code>import random

a = [random.randint(1, 20) for i in range(random.randint(8, 30))]

a.sort()
print(a)


def askUser():

    n = input(""What number are you looking for?"")
    while not n.isdigit():
        n = input(""What number are you looking for?"")

    n = int(n)
    s = 0

    for numbers in a:
        if numbers == n:
            s += 1
            print(""Number"", n, ""is located in the list and the position is:"", (a.index(n)+1))
            # Something here to skip this index next time it goes through the loop
        else:
            pass
    if s == 0:
        print(""Your number could not be found"")


askUser()
</code></pre>

<p>I would like to add something that will skip the index it found first time and then look for the index of the duplicate if there is one.</p>

<p>Current result</p>

<pre><code>[2, 4, 8, 9, 10, 10, 16, 19, 20, 20]
What number are you looking for?20
Number 20 is located in the list and the position is: 9
Number 20 is located in the list and the position is: 9
</code></pre>

<p>Desired result</p>

<pre><code>[2, 4, 8, 9, 10, 10, 16, 19, 20, 20]
What number are you looking for?20
Number 20 is located in the list and the position is: 9
Number 20 is located in the list and the position is: 10
</code></pre>
","9041256","","","Checking duplicates in list from user input","<python><python-3.x><list><input>","4","1","1499"
"50650390","2018-06-01 20:08:38","15","","<p>Python relies on the underlying platform for its floating-point arithmetic. I hypothesize that Python’s <code>**</code> operator uses a <code>pow</code> implementation (as used in C) (confirmed by <a href=""https://stackoverflow.com/users/2357112/user2357112"">user2357112</a> referring to Python <a href=""https://github.com/python/cpython/blob/v2.7.15/Objects/floatobject.c#L921"" rel=""noreferrer"">2.7.15 source code</a>).</p>

<p>Generally, <code>pow</code> is implemented by using (approximations of) logarithms and exponentials, in part. This is necessary since <code>pow</code> supports non-integer arguments. (Of course, this general implementation does not preclude specializations for subsets of its domain.)</p>

<p>Microsoft’s <code>pow</code> implementation is notoriously not good. Hence, for <code>pow(a, 2)</code>, it may be returning a result not equal to <code>a*a</code>.</p>
","298225","298225","2018-06-01 20:14:16","5","893","Eric Postpischil","2010-03-21 00:38:50","94840","8126","1948","2731","50650277","50650390","2018-06-01 19:57:58","13","920","<pre><code>$ python --version
Python 2.7.15

$ type test.py
import random

while True:
    a = random.uniform(0, 1)
    b = a ** 2
    c = a * a
    if b != c:
        print ""a = {}"".format(a)
        print ""a ** 2 = {}"".format(b)
        print ""a * a = {}"".format(c)
        break

$ python test.py
a = 0.145376687586
a ** 2 = 0.0211343812936
a * a = 0.0211343812936
</code></pre>

<p>I was only able to reproduce this on a Windows build of Python - to be precise: <code>Python 2.7.15 (v2.7.15:ca079a3ea3, Apr 30 2018, 16:30:26) [MSC v.1500 64 bit (AMD64)] on win32</code>. On my <a href=""http://en.wikipedia.org/wiki/Arch_Linux"" rel=""nofollow noreferrer"">Arch Linux</a> box installation of Python (<code>Python 2.7.15 (default, May 1 2018, 20:16:04) [GCC 7.3.1 20180406] on linux2</code>) the loop does not seem to terminate indicating that the <code>a**2 = a * a</code> invariant holds there.</p>

<p>What is going on here? I know that IEEE floats come with a plethora of misconceptions and idiosyncrasies (<a href=""https://stackoverflow.com/questions/588004/is-floating-point-math-broken"">this</a>, for example, does not answer my question), but I fail to see what part of the specification or what kind of implementation of <code>**</code> could possibly allow for this.</p>

<p>To address the duplicate flagging: This is most likely not directly an IEEE floating point math problem and more of a implementation issue of the <code>**</code> operator. Therefore, this is not a duplicate of questions which are only asking about floating point issues such as precision or associativity.</p>
","1658887","63550","2018-06-02 09:41:38","Why is a**2 != a * a for some floats?","<python><floating-point><ieee>","2","7","1594"
"50650397","2018-06-01 20:09:14","1","","<p>Why not just use:</p>

<pre><code>topics_dict = { ""title"":[],
            ""score"":[],
            ""id"":[],
            ""url"":[], 
            ""comms_num"": [],
            ""created"": [],
            ""body"":[]}
</code></pre>

<p>and remove the <code>\</code> it does exactly the same thing.</p>

<p>This is perfectly valid for a <a href=""https://docs.python.org/3/tutorial/datastructures.html#dictionaries"" rel=""nofollow noreferrer"">dictionary</a> and is not affected by new lines like strings are (as long as indentation is correct).</p>
","8372104","","","0","540","Simon","2017-07-26 19:03:27","6383","1766","5398","222","50650372","50650397","2018-06-01 20:06:55","0","83","<p>I am trying to create a dataframe to store reddit data:</p>

<pre><code>topics_dict = { ""title"":[],\
            ""score"":[],\
            ""id"":[], ""url"":[],\ -&gt; Error 
            ""comms_num"": [],\
            ""created"": [],\
            ""body"":[]}
</code></pre>
","3937973","104349","2018-06-01 20:10:23","Python Error: Unexpected character after line continuation character","<python>","2","2","269"
"50650399","2018-06-01 20:09:34","1","","<p>Use a list of dictionary to store your data.</p>

<p><strong>Demo:</strong></p>

<pre><code>import requests
from bs4 import BeautifulSoup
base_url=""http://cbcs.fastvturesults.com/student/1sp15me00""
res = []
for page in range(1,10,1):
    r=requests.get(base_url+str(page))
    c=r.content
    soup=BeautifulSoup(c,""html.parser"")
    items=soup.find(class_=""text-muted"")
    if items:
        res.append({""Name"": items.previous_sibling, ""USN"": items.text.replace(""("","""").replace("")"","""")})
print(res)
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>[{'USN': u'1sp15me001', 'Name': u'Agnello Fernandes A '}, {'USN': u'1sp15me002', 'Name': u'Ajay Kumar V '}, {'USN': u'1sp15me003', 'Name': u'Ajay Rajendiran '}, {'USN': u'1sp15me004', 'Name': u'Amit Singh Yadav '}, {'USN': u'1sp15me006', 'Name': u'Ankit Mahato '}, {'USN': u'1sp15me008', 'Name': u'Antony Levin Fernandez D '}, {'USN': u'1sp15me009', 'Name': u'Ashish S '}]
</code></pre>
","532312","","","0","948","Rakesh","2010-12-06 13:07:54","56694","5302","758","1508","50650342","50650399","2018-06-01 20:04:30","1","39","<p>I was extracting 9 items from a website. All was good but when I tried to pass those items into the dictionary then only the last item was saved into the dictionary.</p>

<pre><code>import requests
from bs4 import BeautifulSoup
base_url=""http://cbcs.fastvturesults.com/student/1sp15me00""
d={}
for page in range(1,10,1):
    r=requests.get(base_url+str(page))
    c=r.content
    soup=BeautifulSoup(c,""html.parser"")
    items=soup.find(class_=""text-muted"")
    if items:
        d[""Name""]=items.previous_sibling
        d[""USN""]=items.text.replace(""("","""").replace("")"","""")
d
</code></pre>

<p>How can I save all the items into a dictionary?</p>
","9569600","","","Web Scraping; Unable to pass all the items in a dictionary","<python><web-scraping><beautifulsoup>","2","2","646"
"50650407","2018-06-01 20:10:17","9","","<p><code>a ** 2</code> uses a <em>floating point</em> power function (like the one you can find in the standard C math lib), which is able to elevate any number to any power.</p>

<p><code>a * a</code> is just multiplying <em>once</em>, it's more suitable for this case, and not liable to precision errors, (even more true for integers), like <code>a ** 2</code> would be.</p>

<p>For floating point <code>a</code>, if you want to elevate to a power of, say, 5, by using</p>

<pre><code>a * a * a * a * a
</code></pre>

<p>you'd be better off with <code>a**5</code> because repeated multiplication is now liable to floating point accumulation error, and it's much slower.</p>

<p><code>a ** b</code> is more interesting when <code>b</code> is large, because it's more efficient, for instance. But the precision may differ because it uses a floating point algorithm.</p>
","6451573","6451573","2018-06-01 20:12:25","5","870","Jean-François Fabre","2016-06-10 19:19:53","113106","37329","9248","14670","50650277","50650390","2018-06-01 19:57:58","13","920","<pre><code>$ python --version
Python 2.7.15

$ type test.py
import random

while True:
    a = random.uniform(0, 1)
    b = a ** 2
    c = a * a
    if b != c:
        print ""a = {}"".format(a)
        print ""a ** 2 = {}"".format(b)
        print ""a * a = {}"".format(c)
        break

$ python test.py
a = 0.145376687586
a ** 2 = 0.0211343812936
a * a = 0.0211343812936
</code></pre>

<p>I was only able to reproduce this on a Windows build of Python - to be precise: <code>Python 2.7.15 (v2.7.15:ca079a3ea3, Apr 30 2018, 16:30:26) [MSC v.1500 64 bit (AMD64)] on win32</code>. On my <a href=""http://en.wikipedia.org/wiki/Arch_Linux"" rel=""nofollow noreferrer"">Arch Linux</a> box installation of Python (<code>Python 2.7.15 (default, May 1 2018, 20:16:04) [GCC 7.3.1 20180406] on linux2</code>) the loop does not seem to terminate indicating that the <code>a**2 = a * a</code> invariant holds there.</p>

<p>What is going on here? I know that IEEE floats come with a plethora of misconceptions and idiosyncrasies (<a href=""https://stackoverflow.com/questions/588004/is-floating-point-math-broken"">this</a>, for example, does not answer my question), but I fail to see what part of the specification or what kind of implementation of <code>**</code> could possibly allow for this.</p>

<p>To address the duplicate flagging: This is most likely not directly an IEEE floating point math problem and more of a implementation issue of the <code>**</code> operator. Therefore, this is not a duplicate of questions which are only asking about floating point issues such as precision or associativity.</p>
","1658887","63550","2018-06-02 09:41:38","Why is a**2 != a * a for some floats?","<python><floating-point><ieee>","2","7","1594"
"50650417","2018-06-01 20:11:02","1","","<p>For completeness - it is possible to create global variables in python thanks to <code>globals()[key] = value</code> - but everyone with a bit of experience would never advise you to do that - if not for very specific and well-thought-out reasons there is no justification for doing so. Nonetheless here you can see how it is achieved given your example:</p>

<p><a href=""https://repl.it/repls/InternationalShamelessBug"" rel=""nofollow noreferrer"">https://repl.it/repls/InternationalShamelessBug</a></p>
","3820185","","","4","506","wiesion","2014-07-09 11:21:21","2008","165","89","27","50650133","50650177","2018-06-01 19:44:50","0","184","<p>Preface: I've spent many HOURS trying to read similar posts like this one and try lots of different things, but for the life of me, I can't understand what I'm doing wrong, or why this doesn't work. Can someone please give me an ELI5 answer as to either what I'm doing wrong, or why this doesn't work the way I'm expecting?</p>

<pre><code>class User:

    def __init__(self, first, last, age):
        self.firstname = first
        self.lastname  = last
        self.age       = age


dict = {
    'User1': {'id': 'id001', 'firstname': 'User', 'lastname': 'One', 'age': 11},
    'User2': {'id': 'id002', 'firstname': 'User', 'lastname': 'Two', 'age': 22},
    'User3': {'id': 'id003', 'firstname': 'User', 'lastname': 'Three', 'age': 33}
}


for user in dict:
    u    = dict[user]
    user = User(u['firstname'], u['lastname'], u['age'])


try:
    print(User1.lastname)
except Exception as e:
    print(e)
finally:
    print(user.lastname)
</code></pre>

<p>Which returns:</p>

<pre><code>name 'User1' is not defined
Three
</code></pre>

<p>Clearly <code>user</code> is getting instantiated three times with the data from <code>dict</code>, with <code>User3</code>'s data being the last of it. </p>

<p>If I do a:</p>

<pre><code>for user in dict:
    print(user)
</code></pre>

<p>it prints out:</p>

<pre><code>User1
User2
User3
</code></pre>

<p>So my expectation would be that the for loop would run, and then I'd be able to call <code>User1.firstname</code>, <code>User2.lastname</code>, or <code>User3.age</code>, and it'd give me the same data that's in <code>dict</code> (User, Two, 33, respectively), but that's clearly not the case.</p>

<p>Help?</p>
","8142768","","","Python - Instantiate Objects from dictionary with for loop","<python><loops><class>","2","6","1668"
"50650478","2018-06-01 20:16:09","0","","<p>Keep it very short, as simple as:</p>

<pre><code>from selenium import webdriver
import time

driver = webdriver.Firefox()

driver.get('URL')
while True:
    time.sleep(20)
    driver.refresh()
driver.quit()
</code></pre>
","8965047","","","0","225","uzdisral","2017-11-19 05:16:27","79","25","13","0","16399355","","2013-05-06 13:01:55","10","31873","<p>I'm using Python to gather some information, construct a very simple html page, save it locally and display the page in my browser using webbrowser.open('file:///c:/testfile.html'). I check for new information every minute. If the information changes, I rewrite the local html file and would like to reload the displayed page.</p>

<p>The problem is that webbrowser.open opens a new tab in my browser every time I run it.  How do I refresh the page rather than reopen it?  I tried new=0, new=1 and new=2, but all do the same thing. Using controller() doesn't work any better.</p>

<p>I suppose I could add something like &lt; META HTTP-EQUIV=""refresh"" CONTENT=""60"" > to the &lt; head > section of the html page to trigger a refresh every minute whether or not the content changed, but would prefer finding a better way.</p>

<p>Exact time interval is not important.</p>

<p>Python 2.7.2, chrome 26.0.1410.64 m, Windows 7 64.</p>
","173922","","","Refresh a local web page using Python","<python><html><refresh>","5","2","932"
"50650490","2018-06-01 20:17:06","5","","<p>If you don't want to bother with external libraries, you can get this done with just the stdlib (although it may well be slower than some alternatives):</p>

<pre><code>import collections
import itertools

def gen_ngrams(sentence):
    words = sentence.split() # or re.findall('\b\w+\b'), or whatever
    n_words = len(words)
    for i in range(n_words - 2):
        for j in range(i + 3, n_words):
            yield ' '.join(words[i: j]) # Assume normalization of spaces


def count_ngrams(sentences):
    return collections.Counter(
        itertools.chain.from_iterable(
            gen_ngrams(sentence) for sentence in sentences
        )
    )

counts = count_ngrams(errList)
dict(counts.most_common(10))
</code></pre>

<p>Which gets you:</p>

<pre><code>{'but didnt have': 11,
 'ate lunch but': 7,
 'ate lunch but didnt': 7,
 'ate lunch but didnt have': 7,
 'lunch but didnt': 7,
 'lunch but didnt have': 7,
 'icecream but didnt': 4,
 'icecream but didnt have': 4,
 'ate lunch and': 4,
 'ate lunch and icecream': 4}
</code></pre>
","2697110","2697110","2018-06-14 02:39:58","2","1039","Oliver Sherouse","2013-08-19 16:15:37","191","14","2","0","50494956","50620223","2018-05-23 18:17:54","11","846","<p>I have a list of sentences such as this:</p>

<pre><code>errList = [ 'Ragu ate lunch but didnt have Water for drinks',
            'Rams ate lunch but didnt have Gatorade for drinks',
            'Saya ate lunch but didnt have :water for drinks',
            'Raghu ate lunch but didnt have water for drinks',
            'Hanu ate lunch but didnt have -water for drinks',
            'Wayu ate lunch but didnt have water for drinks',
            'Viru ate lunch but didnt have .water 4or drinks',

            'kk ate lunch &amp; icecream but did have Water for drinks',
            'M ate lunch &amp;and icecream but did have Gatorade for drinks',
            'Parker ate lunch icecream but didnt have :water for drinks',
            'Sassy ate lunch and icecream but didnt have water for drinks',
            'John ate lunch and icecream but didnt have -water for drinks',
            'Pokey ate lunch and icecream but didnt have Water for drinks',
            'Laila ate lunch and icecream but did have water 4or drinks',
        ]
</code></pre>

<p>I want to find out count of longest phrases/part (phrase must be more than 2 words) of sentences in each element of list? In following example, output will look closer to this (longest phrase as key and count as value):</p>

<pre><code>{ 'ate lunch but didnt have': 7,
  'water for drinks': 7,
  'ate lunch and icecream': 4,
  'didnt have water': 3,
  'didnt have Water': 2    # case sensitives
}
</code></pre>

<p>Using re module is out of question since problem is close to sequence matching or perhaps using nltk or perhaps scikit-learn ? I have some familiarity with NLP and scikit but not enough to solve this? If I solve this, I will publish it here.</p>
","1251918","1251918","2018-05-30 13:55:26","sequence matching algorithm in python","<python><scikit-learn><nltk><difflib>","4","2","1718"
"50650498","2018-06-01 20:17:53","2","","<p>If you feel fancy you can convert some of your loops into list comprehensions:</p>

<pre><code>def askUser():

    n = input(""What number are you looking for?"")
    while not n.isdigit():
        n = input(""What number are you looking for?"")

    n = int(n)

    # get a list of all indexes that match the number
    foundAt = [p+1 for p,num in enumerate(a) if num == n]

    if foundAt:
        # print text by creating a list of texts to print and decompose them
        # printing with a seperator of linefeed
        print( *[f""Number {n} is located in the list and the position is: {q}"" for 
                 q in foundAt], sep=""\n"")
    else: 
        print(""Your number could not be found"")
</code></pre>

<p>Edit: as Chrisz pointed out <code>f""""</code> format strings came with <a href=""https://www.python.org/dev/peps/pep-0498/?"" rel=""nofollow noreferrer"">PEP-498</a> for Python 3.6 (wasn't aware :o/ ) - so for earlier 3.x Python would have to use </p>

<pre><code>print( *[""Number {} is located in the list and the position is: {}"".format(n,q) for 
                 q in foundAt], sep=""\n"")
</code></pre>
","7505395","7505395","2018-06-01 20:38:07","1","1119","Patrick Artner","2017-02-02 10:46:51","30736","5120","3506","4713","50650250","50650288","2018-06-01 19:56:06","2","43","<p>So I have written a code that generates a random list with random number of values. Then asks the user what number the user is looking for and if its in the list, it will tell the user what position in the list the number is in.</p>

<pre><code>import random

a = [random.randint(1, 20) for i in range(random.randint(8, 30))]

a.sort()
print(a)


def askUser():

    n = input(""What number are you looking for?"")
    while not n.isdigit():
        n = input(""What number are you looking for?"")

    n = int(n)
    s = 0

    for numbers in a:
        if numbers == n:
            s += 1
            print(""Number"", n, ""is located in the list and the position is:"", (a.index(n)+1))
            # Something here to skip this index next time it goes through the loop
        else:
            pass
    if s == 0:
        print(""Your number could not be found"")


askUser()
</code></pre>

<p>I would like to add something that will skip the index it found first time and then look for the index of the duplicate if there is one.</p>

<p>Current result</p>

<pre><code>[2, 4, 8, 9, 10, 10, 16, 19, 20, 20]
What number are you looking for?20
Number 20 is located in the list and the position is: 9
Number 20 is located in the list and the position is: 9
</code></pre>

<p>Desired result</p>

<pre><code>[2, 4, 8, 9, 10, 10, 16, 19, 20, 20]
What number are you looking for?20
Number 20 is located in the list and the position is: 9
Number 20 is located in the list and the position is: 10
</code></pre>
","9041256","","","Checking duplicates in list from user input","<python><python-3.x><list><input>","4","1","1499"
"50650506","2018-06-01 20:18:43","0","","<p>So, what I was planning to do was not ideal . .
Apache Tika requires to scan physical files to fetch metadata. I made a bridge and started from sessions pulling files to the server Tika code was hosted.</p>
","7340744","","","0","210","Lasit Pant","2016-12-26 00:18:45","70","48","12","0","49776224","50650506","2018-04-11 13:19:41","0","38","<p>I have a tika code in a server. I want to create an SFTP session with another server with files and run Apache tika on that server. I am using python as back end. Will this work ? is my approach correct ?</p>

<p>Thanks</p>
","7340744","","","using apache tika for scanning documents on servers using sftp","<python><sftp><apache-tika>","1","5","227"
"50650524","2018-06-01 20:20:46","1","","<p>Assuming the ""can't"" above is not really in your code, if possible update your question to omit the ""'"" as it messes up the readability or change to some other placeholder text.</p>

<p>As the error indicates, you have not assigned a value yet to counter, the method ""on_status"" will try to increment counter, but this is only local to the method, not the object, thus it fails.</p>

<pre><code>def on_status(self, status):
        #prints status text. can be replaced with a counter probably.
        counter = counter + 1
        print(status.text)
</code></pre>

<p>You should initialize the counter in an <strong>init</strong> method and then use self.counter instead.</p>

<p>Add</p>

<pre><code>...
class MySteamListener(tweepy.StreamListener):

    def __init__(self):
        # Not sure if necessary, to make this work, but you could
        # Initialize the inherited class as well (this may work only in Python 3)
        # super().__init__()
        self.counter = 0
...
</code></pre>

<p>Modify on_status to </p>

<pre><code>def on_status(self, status):
        #prints status text. can be replaced with a counter probably.
        self.counter = self.counter + 1
        # Can be written as 'self.counter += 1'
        print(status.text)
</code></pre>
","9802392","","","2","1268","d parolin","2018-05-16 19:48:35","184","23","11","0","50650345","50650524","2018-06-01 20:04:54","0","80","<pre class=""lang-py prettyprint-override""><code>from tweepy import OAuthHandler

import tweepy
from tweepy import StreamListener
from tweepy import Stream


import time



consumer_key = 'super secret consumer key'
consumer_secret = 'shhhhh can't tell anyone this!'
access_token = 'hmmmmmmmmmmmmm'
access_secret = 'arrays should start at 0'

auth = OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_secret)

api = tweepy.API(auth)
print('')
print('starting...')
time.sleep(3)

class MySteamListener(tweepy.StreamListener):







    def on_status(self, status):
        #prints status text. can be replaced with a counter probably.
        counter = counter + 1
        print(status.text)


    def on_error(self, status_code):
        if status_code == 420:
            print('420 error')
            #Ends stream in case of rate limiting
            return False


mySteamListener = MySteamListener()

myStream = tweepy.Stream(auth = api.auth, listener = mySteamListener)

myStream.filter(track = ['Warriors'])
</code></pre>

<p>I'm new to tweepy, and the first thing I'm trying to do is make a program that scans all tweets for certain words. Everything was working well until I tried to add a counter for the number of instances for that word. No matter where I assign the counter, I always get a 'UnboundLocalError: local variable 'counter' referenced before assignment' error. Where should I assign the counter in this program? </p>
","9659161","","","Can't get a counter working in tweepy","<python><tweepy>","1","0","1477"
"50650532","2018-06-01 20:21:35","8","","<p>This is a partial answer to my own question. I'm saying partial, because I do believe there is a better way.</p>

<p>Building on this post <a href=""http://swig.10945.n7.nabble.com/Pass-a-Swig-wrapped-C-class-to-embedded-Python-code-td8812.html"" rel=""noreferrer"">http://swig.10945.n7.nabble.com/Pass-a-Swig-wrapped-C-class-to-embedded-Python-code-td8812.html</a>
I generated the swig runtime header, as described here, section 15.4: <a href=""http://www.swig.org/Doc2.0/Modules.html#Modules_external_run_time"" rel=""noreferrer"">http://www.swig.org/Doc2.0/Modules.html#Modules_external_run_time</a></p>

<p>Including the generated header in the C++ code above, allow the following code to be written:</p>

<pre><code>    PyObject* pObj = SWIG_NewPointerObj((void*)&amp;obj, SWIG_TypeQuery(""_p_MyClass""),  0 ); 
</code></pre>

<p>This code is using information from the Swig python wrap source files, namely the ""swig"" name of the type MyClass, i.e. <em>_p_MyClass</em>.</p>

<p>With the above PyObject* as an argument to the  PyObject_CallObject function, the python <strong>execute()</strong> function in the code above executes fine, and the Python code, using the generated python module, do have proper access to the MyClass objects internal data. This is great.</p>

<p>Although the above code illustrate how to pass, and retrieve data between C++ and Python in a quite simple fashion, its not ideal, in my opinion.</p>

<p>The usage of the swig header file in the C++ code is really not that pretty, and in addition, it requires a user to ""manually"" look into swig generated wrapper code in order to find the ""_p_MyClass"" code. </p>

<p>There must be a better way!? Perhaps something should be added to the swig interface file in order to get this looking nicer?</p>
","1927753","1927753","2018-06-01 21:48:45","3","1772","Totte Karlsson","2012-12-25 06:44:25","489","204","166","11","50613010","","2018-05-30 20:38:19","28","1809","<p>This question is about <em>how to pass a C++ object to a python function that is called in a (C++) embedded Python interpreter</em>. </p>

<p>The following C++ class (MyClass.h) is designed for testing:</p>

<pre><code>#ifndef MyClassH
#define MyClassH
#include &lt;string&gt;

using std::string;
class MyClass
{
    public:
                        MyClass(const string&amp; lbl): label(lbl) {}
                        ~MyClass(){}
        string          getLabel() {return label;}

    private:
        string          label;
};
#endif
</code></pre>

<p>A python module, exposing the C++ class, can be generated by the following Swig interface file:</p>

<pre><code>%module passmetopython

%{    #include ""MyClass.h""    %}

%include ""std_string.i""

//Expose to Python
%include ""MyClass.h""
</code></pre>

<p>Below is a Python script using the python module</p>

<pre><code>import passmetopython as pmtp

def execute(obj):
    #This function is to be called from C/C++, with a
    #MyClass object as an argument
    print (""Entering execute function"")
    lbl = obj.getLabel();
    print (""Printing from within python execute function. Object label is: "" + lbl)
    return True

def main():
    c = pmtp.MyClass(""Test 1"")
    retValue = execute(c)
    print(""Return value: "" + str(retValue))

#Test function from within python
if __name__ == '__main__':
    main()
</code></pre>

<p>This question is about how to get the python <em>execute()</em> function working, when called from c++, with a C++ object as an argument.</p>

<p>The following C++ program was written to test the functions (minimum amount of error checking):</p>

<pre><code>#include ""Python.h""
#include &lt;iostream&gt;
#include &lt;sstream&gt;
#include ""MyClass.h""

using namespace std;

int main()
{
    MyClass obj(""In C++"");
    cout &lt;&lt; ""Object label: \"""" &lt;&lt; obj.getLabel() &lt;&lt; ""\"""" &lt;&lt; endl;

    //Setup the Python interpreter and eventually call the execute function in the
    //demo python script
    Py_Initialize();

    //Load python Demo script, ""passmetopythonDemo.py""
    string PyModule(""passmetopythonDemo"");
    PyObject* pm = PyUnicode_DecodeFSDefault(PyModule.c_str());

    PyRun_SimpleString(""import sys"");
    stringstream cmd;
    cmd &lt;&lt; ""sys.path.append(\"""" &lt;&lt; ""."" &lt;&lt; ""\"")"";
    PyRun_SimpleString(cmd.str().c_str());
    PyObject* PyModuleP = PyImport_Import(pm);
    Py_DECREF(pm);

    //Now create PyObjects for the Python functions that we want to call
    PyObject* pFunc = PyObject_GetAttrString(PyModuleP, ""execute"");

    if(pFunc)
    {
        //Setup argument
        PyObject* pArgs = PyTuple_New(1);

        //Construct a PyObject* from long
        PyObject* pObj(NULL);

        /* My current attempt to create avalid argument to Python */
        pObj = PyLong_FromLong((long) &amp;obj);


        PyTuple_SetItem(pArgs, 0, pObj);

        /***** Calling python here *****/
        cout&lt;&lt;endl&lt;&lt;""Calling function with an MyClass argument\n\n"";
        PyObject* res = PyObject_CallObject(pFunc, pArgs);
        if(!res)
        {
            cerr &lt;&lt; ""Failed calling function.."";
        }
    }

    return 0;
}
</code></pre>

<p>When running the above code, the <strong>execute()</strong> python function, with a MyClass object as an argument, fails and returns NULL. However, the Python function is entered, as I can see the output (<em>Entering execute function</em>) in the console output, indicating that the object passed is not, <strong>indeed</strong>, a valid MyClass object.</p>

<p>There are a lot of examples on how to pass simple types, like ints, doubles or string types to Python from C/C++. But there are very few example showing how to pass a C/C++ object/ pointer, which is kind of puzzling.</p>

<p>The above code, with a CMake file, can be checked out from github: 
<a href=""https://github.com/TotteKarlsson/miniprojects/tree/master/passMeToPython"" rel=""nofollow noreferrer"">https://github.com/TotteKarlsson/miniprojects/tree/master/passMeToPython</a></p>

<p>This code is not to use any boost python or other API's. Cython sounds interesting though, and if it can be used to simplify on the C++ side, it could be acceptable.</p>
","1927753","1927753","2018-06-22 19:45:34","Passing a C++ object to Python","<python><c++><swig>","2","4","4220"
"50650550","2018-06-01 20:22:49","1","","<p>The approach that you are looking for is called</p>

<h2>Frequent Itemset Mining</h2>

<p>It finds frequent subsets, given a list of sets.</p>
","1060350","","","1","146","Anony-Mousse","2011-11-22 17:47:24","62917","9956","809","7405","50565772","50566577","2018-05-28 11:50:54","1","57","<p>Sorry for the broad title, I just do not know how to name this.</p>

<p>I have a list of integers, let's say:</p>

<pre><code>X = [20, 30, 40, 50, 60, 70, 80, 100]
</code></pre>

<p>And a second list of tuples of size 2 to 6 made from this integers:</p>

<pre><code>Y = [(20, 30), (40, 50, 80, 100), (100, 100, 100), ...]
</code></pre>

<p>Some of the numbers come back quite often in <code>Y</code> and I'd like to identify the group of integers coming back often.</p>

<p>Right now, I'm counting the number of apparition of each integer. It gives me some information, but nothing about the groups.</p>

<p>Example:</p>

<pre><code>Y = [(20, 40, 80), (30, 60, 80), (60, 80, 100), (60, 80, 100, 20), (40, 60, 80, 20, 100), ...]
</code></pre>

<p>On that example <code>(60, 80)</code> and <code>(60, 80, 100)</code> are combinations which come back often.</p>

<p>I could use <code>itertools.combinations_with_replacement()</code> to generate every combinations and then count the number of apparition, but is there any other better way to do this?</p>

<p>Thanks.</p>
","8324480","","","Cluster analysis within a set of integers","<python><combinations><cluster-analysis>","2","0","1071"
"50650553","2018-06-01 20:22:59","2","","<p>It is illegal to have anything between the line-continuation backslash and the newline.<sup>1</sup> (See <a href=""https://docs.python.org/3/reference/lexical_analysis.html#explicit-line-joining"" rel=""nofollow noreferrer"">Explicit line joining</a> for details.)</p>

<p>Usually, this problem is pretty obvious:</p>

<pre><code>&gt;&gt;&gt; 'abc' \stuff
    'abc' \stuff
                ^
SyntaxError: unexpected character after line continuation character
</code></pre>

<p>… but it's a lot harder to see, even though just as illegal, if all you have is whitespace:</p>

<pre><code>&gt;&gt;&gt; 'abc' \ 
    'abc' \
            ^
SyntaxError: unexpected character after line continuation character
</code></pre>

<p>It can be even worse if you're mixing Windows <code>\r\n</code> and Unix <code>\n</code> newlines in the same source. Then, you can have a backslash followed immediately by a <code>\r\n</code>, which looks perfectly fine—but the <code>\r</code> is treated as illegal whitespace, not part of the newline, because it's treating your script as <code>\n</code>-terminated.<sup>2</sup></p>

<p>A decent editor will make these problems either hard to create, easy to spot, or both. Pretty much any editor besides Notepad and TextEdit counts as ""decent"" for these purposes, including lots of free ones for every platform.</p>

<hr>

<p>The solution is to remove whatever whitespace you have after the <code>\</code>.</p>

<p>Or, better still, to avoid using backslash continuation when you don't need it—as you don't here, because any expression inside <code>{}</code>, <code>[]</code>, or <code>()</code> is automatically continued, without needing a backslash. (See <a href=""https://docs.python.org/3/reference/lexical_analysis.html#implicit-line-joining"" rel=""nofollow noreferrer"">Implicit line joining</a> for details, but it almost always just works as you'd expect.)</p>

<hr>

<p><sub>1. Technically, I don't think a backslash followed by something other than a newline is a line continuation at all. So the actual error is just that a backslash on its own is an invalid token. But if so, the tokenizer is being nice by providing a more comprehensible error message here, so let's not complain…</sub></p>

<p><sub>2. I believe this one may no longer be true as of… somewhere around 2.6, or maybe 3.0? See <a href=""https://docs.python.org/3/reference/lexical_analysis.html#physical-lines"" rel=""nofollow noreferrer"">Physical lines</a>. But still, better to use consistent endings than to hope that Python guesses correctly what you were trying to do…</sub></p>
","908494","908494","2018-06-01 20:28:56","0","2577","abarnert","2011-08-23 20:55:30","272076","19605","5576","4509","50650372","50650397","2018-06-01 20:06:55","0","83","<p>I am trying to create a dataframe to store reddit data:</p>

<pre><code>topics_dict = { ""title"":[],\
            ""score"":[],\
            ""id"":[], ""url"":[],\ -&gt; Error 
            ""comms_num"": [],\
            ""created"": [],\
            ""body"":[]}
</code></pre>
","3937973","104349","2018-06-01 20:10:23","Python Error: Unexpected character after line continuation character","<python>","2","2","269"
"50650567","2018-06-01 20:24:39","3","","<p>Mypy extends PEP 484 by providing a <a href=""http://mypy.readthedocs.io/en/latest/more_types.html#typeddict"" rel=""nofollow noreferrer""><code>TypedDict</code></a> type. This allows specifying specific attributes of a dict type. In your case you can do the following:</p>

<pre><code>from mypy_extensions import TypedDict

# you can also do HasX = TypedDict('HasX', {'x': str})
class HasX(TypedDict):
    x: str

def f(x: HasX) -&gt; None:
    reveal_type(d[""x""])  # error: Revealed type is 'builtins.str'
</code></pre>
","9688107","","","0","521","ethanhs","2018-04-23 21:58:50","350","17","17","0","50648538","50648582","2018-06-01 17:38:19","3","588","<p>Suppose I have a function which takes a dictionary as a parameter:</p>

<pre><code>def f(d: dict) -&gt; None:
    x = d[""x""]
    print(x)
</code></pre>

<p>Can I specify that this dictionary <em>must</em> have the key <code>""x""</code> to mypy? I'm looking for something similar to <a href=""https://www.typescriptlang.org/docs/handbook/interfaces.html"" rel=""nofollow noreferrer"">interface from typescript</a>, without changing <code>d</code> to a class.</p>

<p>The reason I don't want to change <code>d</code> to a class, is because I am modifying a large existing codebase to add <code>mypy</code> type checking and this dictionary is used in many places. I would have to modify a lot of code if I had to change all instances of <code>d[""x""]</code> to <code>d.x</code>.</p>
","755934","100297","2018-06-01 17:40:08","mypy set dictionary keys / interface","<python><python-3.x><type-hinting><mypy>","2","0","778"
"50650574","2018-06-01 20:25:08","1","","<p>You can do it like this</p>

<pre><code>class Employee {

  String firstName;
  String lastName;
  num pay;

  Employee(this.firstName, this.lastName, this.pay);
}

class Developer extends Employee{

  List&lt;String&gt; languages;

  Developer(String firstName, String lastName, num pay, this.languages): super(firstName, lastName, pay);
}

void main(){ 
  var developer = new Developer('John', 'Doe', 5000, ['Python', 'Java']);
}
</code></pre>
","1285830","","","0","449","pulkit-singhal","2012-03-22 11:52:51","682","35","23","6","50650459","50650574","2018-06-01 20:14:40","1","61","<p>I've recently learned object oriented python, and have been trying to learn Dart in the past few days. However, I'm having trouble learning how to make subclasses.</p>

<p>This is the result I want to achieve, in python:</p>

<pre><code>class Employee:

  def __init__(self, fistName, lastName, pay):
    self.firstName = firstName
    self.lastName = lastName
    self.pay = pay

class Developer(Employee):

  def __init__(self, firstName, lastName, pay, languages):
    super().__init__(firstName, lastName, pay)
    self.languages = languages

developer = Developer('John', 'Doe', 5000, ['Python', 'Java'])
</code></pre>

<p>In dart, this is the code I have so far:</p>

<pre><code>class Employee {

  String firstName;
  String lastName;
  num pay;

  Employee(this.firstName, this.lastName, this.pay);
}

class Developer extends Employee{

  List&lt;String&gt; languages;

  Developer(?);
}

void main(){ 
  var developer = new Developer('John', 'Doe', 5000, ['Python', 'Java'])
}
</code></pre>

<p>How do I complete the code so that I get the same result as in python?</p>
","9156326","","","How do I properly use super method for subclasses in Dart?","<python><class><dart>","1","0","1082"
"50650611","2018-06-01 20:28:41","2","","<p>Have a look at <a href=""https://docs.djangoproject.com/en/2.0/topics/forms/modelforms/#inline-formsets"" rel=""nofollow noreferrer"">Django Inline Formsets</a>. This allows you to include multiple forms per view and the inline formset allows you to work with objects related by a foreign key. </p>

<p>It is relatively easier when you know the exact amount of forms you need to include in your template. However, if you want to dynamically include forms in your template then you will need to look into adding some javascript to provide this functionality.</p>
","8582797","","","1","561","HoneyNutIchiros","2017-09-09 03:49:58","326","15","3","1","50650291","","2018-06-01 19:59:47","0","34","<p>I'm trying to create a form to input scores for a round of golf. A standard round of golf is 18 holes, so the form should create 18 instances of 'Score' once submitted. How would I go about creating a form that contains a single dropdown for 'player' and 18 text fields for strokes on each hole? Below are the models being used:</p>

<pre><code>class Score(models.Model):
    hole = models.ForeignKey(Hole, on_delete=models.CASCADE)
    player = models.ForeignKey(Player, on_delete=models.CASCADE)
    strokes = models.IntegerField(default=0)

class Player(models.Model):
    user = models.OneToOneField(User, on_delete=models.CASCADE)

class Hole(models.Model):
    number = models.IntegerField()
</code></pre>
","9883009","","","Multiple Django Forms on Single Page","<python><django>","1","2","715"
"50650658","2018-06-01 20:33:33","0","","<p>As the error message says, you'll need to install a BLAS+LAPACK library.</p>

<p>The build instructions for Scipy on CPython <a href=""https://scipy.github.io/devdocs/building/windows.html"" rel=""nofollow noreferrer"">https://scipy.github.io/devdocs/building/windows.html</a> probably are applicable here, but precisely what you need to do depends on what compiler you used for Pypy.</p>
","108184","","","0","388","pv.","2009-05-16 15:33:22","25125","1423","34","60","50522488","","2018-05-25 06:03:29","0","439","<p>I am trying to install SciPy with PyPy on Windows. I installed PyPy with the windows 32-bit zip on the website. I installed numpy using <code>python setup.py install</code>. I tried to install scipy with <code>pip install scipy</code> or related commands, I always get a ""NotFoundError: no lapack/blas resources found"" error. Note I cannot install with wheels because they are meant for CPython 2.7 not PyPy 2.7</p>

<p>How can I solve this problem, especially since most of SciPy is compatible with PyPy?</p>
","4368746","","","installing SciPy with PyPy on Windows","<python><scipy><pypy>","1","2","513"
"50650684","2018-06-01 20:35:17","2","","<p>Looks like some of these answers are pretty old, I just tested this on numpy 1.13.3:</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; a = np.array([[1,1,3],[1,0,4],[1,2,2]])
&gt;&gt;&gt; a
array([[1, 1, 3],
       [1, 0, 4],
       [1, 2, 2]])
&gt;&gt;&gt; a = a - a.mean(axis=0)
&gt;&gt;&gt; a
array([[ 0.,  0.,  0.],
       [ 0., -1.,  1.],
       [ 0.,  1., -1.]])
</code></pre>

<p>I think this is much cleaner and simpler. Have a try and let me know if this is somehow inferior than the other answers.</p>
","7100714","","","0","526","tjann","2016-11-01 16:38:37","21","11","19","0","8423051","8423212","2011-12-07 21:47:26","21","18863","<p>I have a numpy matrix <code>A</code> where the data is organised column-vector-vise i.e <code>A[:,0]</code> is the first data vector, <code>A[:,1]</code> is the second and so on. I wanted to know whether there was a more elegant way to zero out the mean from this data. I am currently doing it via a <code>for</code> loop:</p>

<pre><code>mean=A.mean(axis=1)
for k in range(A.shape[1]):
    A[:,k]=A[:,k]-mean
</code></pre>

<p>So does numpy provide a function to do this? Or can it be done more efficiently another way?</p>
","748865","","","Remove mean from numpy matrix","<python><numpy>","4","0","528"
"50650716","2018-06-01 20:38:58","2","","<p>This solution attempts to find pagination <code>a</code> tags. If any pagination is found, all the pages are scraped when the user iterates over the instance of the class <code>PageScraper</code>. If not, only the first result (the single page) will be crawled:</p>

<pre><code>import requests
from bs4 import BeautifulSoup as soup
import contextlib
def has_pagination(f):
  def wrapper(cls):
     if not cls._pages:
       raise ValueError('No pagination found')
     return f(cls)
  return wrapper

class PageScraper:
   def __init__(self, url:str):
     self.url = url
     self._home_page = requests.get(self.url).text
     self._pages = [i.text for i in soup(self._home_page, 'html.parser').find('div', {'class':'pagination'}).find_all('a')][:-1]
   @property
   def first_page(self):
      return [i.find('h2', {'class':'table-row-heading'}).text for i in soup(self._home_page, 'html.parser').find_all('td', {'class':'table-row-price'})]
   @has_pagination
   def __iter__(self):
     for p in self._pages:
        _link = requests.get(f'{self.url}/page/{p}').text
        yield [i.find('h2', {'class':'table-row-heading'}).text for i in soup(_link, 'html.parser').find_all('td', {'class':'table-row-price'})]
   @classmethod
   @contextlib.contextmanager
   def feed_link(cls, link):
      results = cls(link)
      try:
        yield results.first_page
        for i in results:
          yield i
      except:
         yield results.first_page
</code></pre>

<p>The constructor of the class will find any pagination, and the <code>__iter__</code> method garners all pages, only if pagination links are found. For instance, <a href=""https://www.mobilehome.net/mobile-home-park-directory/rhode-island/all"" rel=""nofollow noreferrer"">https://www.mobilehome.net/mobile-home-park-directory/rhode-island/all</a> has no pagination. Thus:</p>

<pre><code>r = PageScraper('https://www.mobilehome.net/mobile-home-park-directory/rhode-island/all')
pages = [i for i in r]
</code></pre>

<blockquote>
  <p>ValueError: No pagination found</p>
</blockquote>

<p>However, the first page contents can be found:</p>

<pre><code>print(r.first_page)
['Forest Park MHP', 'Gansett Mobile Home Park', 'Meadowlark Park', 'Indian Cedar Mobile Homes Inc', 'Sherwood Valley Adult Mobile', 'Tripp Mobile Home Park', 'Ramblewood Estates', 'Countryside Trailer Park', 'Village At Wordens Pond', 'Greenwich West Inc', 'Dadson Mobile Home Estates', ""Oliveira's Garage"", 'Tuckertown Village Clubhouse', 'Westwood Estates']
</code></pre>

<p>However, for pages with complete pagination, all resulting pages can be scraped:</p>

<pre><code>r = PageScraper('https://www.mobilehome.net/mobile-home-park-directory/maine/all')
d = [i for i in r]
</code></pre>

<p><code>PageScraper.feed_link</code> will accomplish this check automatically, and output the first page, with all subsequent results should pagination be found, or just the first page if no pagination exists in the result:</p>

<pre><code>urls = {'https://www.mobilehome.net/mobile-home-park-directory/maine/all', 'https://www.mobilehome.net/mobile-home-park-directory/rhode-island/all', 'https://www.mobilehome.net/mobile-home-park-directory/vermont/all', 'https://www.mobilehome.net/mobile-home-park-directory/new-hampshire/all'}
for url in urls:
   with PageScraper.feed_link(url) as r:
      print(r)
</code></pre>
","7326738","","","2","3354","Ajax1234","2016-12-21 16:39:57","49079","3709","2930","360","50611496","50651780","2018-05-30 18:42:10","11","301","<p>I've written a scraper in python using BeautifulSoup library to parse all the names traversing different pages of a website. I could manage it if it were not for more than one urls with different pagination, meaning some urls have pagination some does not as the content are few. </p>

<p>My question is: how could I manage to compile them within a function to handle whether they have pagination or not?</p>

<p>My initial attempt (it is able to parse the content from each url's first page only):</p>

<pre><code>import requests 
from bs4 import BeautifulSoup

urls = {
    'https://www.mobilehome.net/mobile-home-park-directory/maine/all',
    'https://www.mobilehome.net/mobile-home-park-directory/rhode-island/all',
    'https://www.mobilehome.net/mobile-home-park-directory/new-hampshire/all',
    'https://www.mobilehome.net/mobile-home-park-directory/vermont/all'
}

def get_names(link):
    r = requests.get(link)
    soup = BeautifulSoup(r.text,""lxml"")
    for items in soup.select(""td[class='table-row-price']""):
        name = items.select_one(""h2 a"").text
        print(name)

if __name__ == '__main__':
    for url in urls:
        get_names(url)
</code></pre>

<p>I could have managed to do the whole thing, if there is a single url with pagination like below:</p>

<pre><code>from bs4 import BeautifulSoup 
import requests

page_no = 0
page_link = ""https://www.mobilehome.net/mobile-home-park-directory/new-hampshire/all/page/{}""

while True:
    page_no+=1
    res = requests.get(page_link.format(page_no))
    soup = BeautifulSoup(res.text,'lxml')
    container = soup.select(""td[class='table-row-price']"")
    if len(container)&lt;=1:break 

    for content in container:
        title = content.select_one(""h2 a"").text
        print(title)
</code></pre>

<p>But, all the urls do not have pagination. So, how can i manage to grab all of them whether there is any pagination or not?</p>
","9189799","","","Unable to exhaust the content of all the identical urls used within my scraper","<python><python-3.x><web-scraping><beautifulsoup>","2","0","1908"
"50650723","2018-06-01 20:39:46","1","","<p>I just released <a href=""http://pepy.tech/"" rel=""nofollow noreferrer"">http://pepy.tech/</a> to view the downloads of a package. I use the official data which is stored in BigQuery. I hope you will find it interesting :-) </p>

<p>Also the site is open source <a href=""https://github.com/psincraian/pepy"" rel=""nofollow noreferrer"">https://github.com/psincraian/pepy</a></p>
","1993508","","","0","376","petrusqui","2013-01-19 19:57:14","865","22","1","0","37531231","","2016-05-30 17:47:05","6","1686","<p>Up until recently, it was possible to see how many times a python module indexed on <a href=""https://pypi.python.org/pypi"" rel=""noreferrer"">https://pypi.python.org/pypi</a> had been downloaded (each module listed downloads for the past 24hrs, week and month). Now that information seems to be missing.</p>

<p>Download numbers are very helpful information when evaluating whether to build code off of one module or another. They also seem to be referenced by sites such as <a href=""https://img.shields.io/"" rel=""noreferrer"">https://img.shields.io/</a> </p>

<p>Does anyone know what happened? And/or, where I can view/retrieve that information?</p>
","4941585","","","Total Downloads of Module Missing on PyPi","<python><pypi>","4","0","652"
"50650791","2018-06-01 20:46:16","0","","<p>You can use <code>numpy.r_</code> to combine ranges with scalars. The only complication is you need to use <code>pd.DataFrame.iloc</code> instead, but this can be facilitated via <code>df.columns.get_loc</code>.</p>

<p>Here's a demo:</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame(columns=['column'+str(i) for i in range(1, 82)])

colidx = df.columns.get_loc

res = df.iloc[:, np.r_[colidx('column1'):colidx('column5'), colidx('column80')]]

print(res.columns)

Index(['column1', 'column2', 'column3', 'column4', 'column80'], dtype='object')
</code></pre>
","9209546","","","0","587","jpp","2018-01-12 14:47:22","109049","18235","7890","3496","50647832","","2018-06-01 16:44:20","6","682","<p>Suppose I want to select a range of columns from a dataframe: Call them 'column_1' through 'column_60'. I know I could use loc like this:
<code>df.loc[:, 'column_1':'column_60']</code>
That will give me all rows in columns 1-60. </p>

<p>But what if I wanted that range of columns plus 'column_81'. This <i>doesn't</i> work:
<code>df.loc[:, 'column_1':'column_60', 'column_81']</code></p>

<p>It throws a ""Too many indexers"" error. 
Is there another way to state this using loc? Or is loc even the best function to use in this case? </p>

<p>Many thanks.</p>
","9782611","9209546","2018-06-01 20:47:52","Can you use loc to select a range of columns plus a column outside of the range?","<python><python-3.x><pandas><dataframe>","3","1","562"
"50650814","2018-06-01 20:48:18","0","","<p>found a solution</p>

<pre><code>A = K.constant(a)
B = K.constant(b)
mxidx = K.argmax(B, axis=1)
c = K.map_fn(lambda i: A[i, mxidx[i], :], K.arange(A.shape[0], dtype='int64'))
print K.eval(c)
array([[ 2.,  7.],
   [ 8.,  1.],
   [ 5.,  7.],
   [ 9.,  9.],
   [ 3.,  6.],
   [ 6.,  4.],
   [ 6.,  5.]], dtype=float32)
</code></pre>

<p><strong>EDIT</strong>: adding runtime info</p>

<pre><code>%timeit K.eval(c)
The slowest run took 9.76 times longer than the fastest. This could mean 
that an intermediate result is being cached.
100000 loops, best of 3: 12.2 µs per loop
</code></pre>
","8453556","","","0","590","Sahil Puri","2017-08-12 03:45:31","179","30","15","0","50648271","","2018-06-01 17:17:31","1","341","<p>I have 2 tensors <code>a</code> and <code>b</code> which have the following shapes</p>

<pre><code>&gt;&gt;K.int_shape(a)
(None, 5 , 2)
&gt;&gt;K.int_shape(b)
(None, 5)
</code></pre>

<p>What I want to get is a tensor <code>c</code> </p>

<pre><code>&gt;&gt;K.int_shape(c)
(None, 2)
</code></pre>

<p>such that along axis 0, you pick the index of largest element in <code>b</code> and use that to index <code>a</code> along axis 1.</p>

<p>Example - say I have </p>

<pre><code>a = np.array([[[2, 7],
    [6, 5],
    [9, 9],
    [4, 2],
    [5, 9]],

   [[8, 1],
    [8, 8],
    [3, 9],
    [9, 2],
    [9, 1]],

   [[3, 9],
    [6, 4],
    [5, 7],
    [5, 2],
    [5, 6]],

   [[7, 5],
    [9, 9],
    [9, 5],
    [9, 8],
    [5, 7]],

   [[6, 3],
    [1, 7],
    [3, 6],
    [8, 2],
    [3, 2]],

   [[6, 4],
    [5, 9],
    [8, 6],
    [5, 2],
    [5, 2]],

   [[2, 6],
    [6, 5],
    [3, 1],
    [6, 2],
    [6, 4]]])
</code></pre>

<p>and I have</p>

<pre><code>b = np.array([[ 0.27,  0.25,  0.23,  0.06,  0.19],
[ 0.3 ,  0.13,  0.17,  0.2 ,  0.2 ],
[ 0.08,  0.04,  0.40,  0.36,  0.12],
[ 0.3 ,  0.33,  0.11,  0.07,  0.19],
[ 0.15,  0.21,  0.30,  0.12,  0.22],
[ 0.3 ,  0.13,  0.23,  0.1 ,  0.23],
[ 0.26,  0.35 ,  0.25 ,  0.07,  0.07]])
</code></pre>

<p>What I expect <code>c</code> to be</p>

<pre><code>c = np.zeros((7,2))
for i in range(7):
    ind = np.argmax(b[i, :])
    c[i, :] = a[i, ind, :]
c
array([[ 2.,  7.],
   [ 8.,  1.],
   [ 5.,  7.],
   [ 9.,  9.],
   [ 3.,  6.],
   [ 6.,  4.],
   [ 6.,  5.]])
</code></pre>
","8453556","","","How do I index based on another array in Keras","<python><indexing><keras><theano>","2","3","1537"
"50650817","2018-06-01 20:48:42","8","","<p>I would use a <code>Toplevel()</code> window to build my own customer error box.</p>

<p>I think using <code>ttk</code> buttons here would be a good idea and with a combination of frames and weights we can get the window to look decent enough.</p>

<p>Keeping the window from being resized by the user I also had to set up a way to toggle the details textbox. With a tracking variable and the use of a if/else statement that was easy enough to set up.</p>

<p>Finally, we can disable the textbox with <code>.config(state=""disabled"")</code></p>

<pre><code>import tkinter as tk
import tkinter.ttk as ttk
import traceback


class MyApp(tk.Frame):
    def __init__(self, master):
        tk.Frame.__init__(self, master)
        tk.Button(self, text=""test error"", command=self.run_bad_math).pack()

    def run_bad_math(self):
        try:
            1/0
        except Exception as error:
            title = ""Traceback Error""
            message = ""An error has occurred: '{}'."".format(error)
            detail = traceback.format_exc()
            topErrorWindow(title, message, detail)


class topErrorWindow(tk.Toplevel):
    def __init__(self, title, message, detail):
        tk.Toplevel.__init__(self)
        self.details_expanded = False
        self.title(title)
        self.geometry(""350x75"")
        self.minsize(350, 75)
        self.maxsize(425, 250)
        self.rowconfigure(0, weight=0)
        self.rowconfigure(1, weight=1)
        self.columnconfigure(0, weight=1)

        button_frame = tk.Frame(self)
        button_frame.grid(row=0, column=0, sticky=""nsew"")
        button_frame.columnconfigure(0, weight=1)
        button_frame.columnconfigure(1, weight=1)

        text_frame = tk.Frame(self)
        text_frame.grid(row=1, column=0, padx=(7, 7), pady=(7, 7) ,sticky=""nsew"")
        text_frame.rowconfigure(0, weight=1)
        text_frame.columnconfigure(0, weight=1)

        ttk.Label(button_frame, text=message).grid(row=0, column=0, columnspan=2, pady=(7, 7))
        ttk.Button(button_frame, text=""OK"", command=self.destroy).grid(row=1, column=0, sticky=""e"")
        ttk.Button(button_frame, text=""Details"", command=self.toggle_details).grid(row=1, column=1, sticky=""w"")

        self.textbox = tk.Text(text_frame, height=6)
        self.textbox.insert(""1.0"", detail)
        self.textbox.config(state=""disabled"")
        self.scrollb = tk.Scrollbar(text_frame, command=self.textbox.yview)
        self.textbox.config(yscrollcommand=self.scrollb.set)

    def toggle_details(self):
        if self.details_expanded == False:
            self.textbox.grid(row=0, column=0, sticky='nsew')
            self.scrollb.grid(row=0, column=1, sticky='nsew')
            self.geometry(""350x160"")
            self.details_expanded = True

        else:
            self.textbox.grid_forget()
            self.scrollb.grid_forget()
            self.geometry(""350x75"")
            self.details_expanded = False


if __name__ == ""__main__"":
    root = tk.Tk()
    App = MyApp(root).pack()
    root.mainloop()
</code></pre>

<p>Results:</p>

<p><a href=""https://i.stack.imgur.com/x4VXl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/x4VXl.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/6Dem7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6Dem7.png"" alt=""enter image description here""></a></p>

<p>Now with resizing :D</p>

<p><a href=""https://i.stack.imgur.com/0eio1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0eio1.png"" alt=""enter image description here""></a></p>

<p>Update:</p>

<p>In response to your statement below:</p>

<blockquote>
  <p>The error window will not display if a Tk instance hasn't been initialized first.</p>
</blockquote>

<p>If we set up the class as its own <code>Tk()</code> instance it can be used as a stand alone error pop-up. I have also added some alignment changes and some resizing control to make this class a bit more conformative to the standard error messages you mention in the comments.</p>

<p>See below code.</p>

<pre><code>import tkinter as tk
import tkinter.ttk as ttk
import traceback


class topErrorWindow(tk.Tk):
    def __init__(self, title, message, detail):
        tk.Tk.__init__(self)
        self.details_expanded = False
        self.title(title)
        self.geometry(""350x75"")
        self.minsize(350, 75)
        self.maxsize(425, 250)
        self.resizable(False, False)
        self.rowconfigure(0, weight=0)
        self.rowconfigure(1, weight=1)
        self.columnconfigure(0, weight=1)

        button_frame = tk.Frame(self)
        button_frame.grid(row=0, column=0, sticky=""nsew"")
        button_frame.columnconfigure(0, weight=1)
        button_frame.columnconfigure(1, weight=1)

        text_frame = tk.Frame(self)
        text_frame.grid(row=1, column=0, padx=(7, 7), pady=(7, 7) ,sticky=""nsew"")
        text_frame.rowconfigure(0, weight=1)
        text_frame.columnconfigure(0, weight=1)

        ttk.Label(button_frame, text=message).grid(row=0, column=0, columnspan=3, pady=(7, 7), padx=(7, 7), sticky=""w"")
        ttk.Button(button_frame, text=""OK"", command=self.destroy).grid(row=1, column=1, sticky=""e"")
        ttk.Button(button_frame, text=""Details"", command=self.toggle_details).grid(row=1, column=2, padx=(7, 7), sticky=""e"")

        self.textbox = tk.Text(text_frame, height=6)
        self.textbox.insert(""1.0"", detail)
        self.textbox.config(state=""disabled"")
        self.scrollb = tk.Scrollbar(text_frame, command=self.textbox.yview)
        self.textbox.config(yscrollcommand=self.scrollb.set)
        self.mainloop()

    def toggle_details(self):
        if self.details_expanded == False:
            self.textbox.grid(row=0, column=0, sticky='nsew')
            self.scrollb.grid(row=0, column=1, sticky='nsew')
            self.resizable(True, True)
            self.geometry(""350x160"")
            self.details_expanded = True

        else:
            self.textbox.grid_forget()
            self.scrollb.grid_forget()
            self.resizable(False, False)
            self.geometry(""350x75"")
            self.details_expanded = False
</code></pre>

<p>Results:</p>

<p><a href=""https://i.stack.imgur.com/B6yyu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B6yyu.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/n0c88.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/n0c88.png"" alt=""enter image description here""></a></p>

<p>You can add an image as well using canvas with the type of error image you want.</p>
","7475225","7475225","2018-06-07 15:59:26","7","6616","Mike - SMT","2017-01-26 17:53:11","11286","1306","856","369","49072942","50650817","2018-03-02 16:06:59","10","1052","<p>I have a Python script which uses <code>tkinter.messagebox</code> to display an error message with traceback details if an unexpected exception occurs.</p>

<pre><code>import tkinter.messagebox as tm
import traceback

try:
    1/0
except Exception as error:
    tm.showerror(title=""Error"",
                 message=""An error has occurred: '"" + str(error) + ""'."",
                 detail=traceback.format_exc())
</code></pre>

<p><a href=""https://i.stack.imgur.com/XEmTp.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/XEmTp.png"" alt=""Standard tkinter error""></a></p>

<p>Displaying tracebacks this way has a few drawbacks.</p>

<ul>
<li>Traceback details <a href=""https://softwareengineering.stackexchange.com/a/245258/168744"">aren't helpful for the average user</a>.</li>
<li>Testers can't easily select and copy text from a messagebox</li>
<li>Complex errors can have <a href=""https://i.stack.imgur.com/iVEqE.png"" rel=""noreferrer"">large tracebacks</a> which span dozens of lines.</li>
</ul>

<p>Instead of displaying error details by default, I would like to add a ""show details"" button which would display more information in a read-only text field.</p>

<p><a href=""https://i.stack.imgur.com/P6YXH.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/P6YXH.png"" alt=""Detailed error for &quot;division by zero&quot;""></a></p>

<p>How can I add a ""show details"" button to a tkinter messagebox?</p>
","3357935","3357935","2018-06-01 15:51:41","How can I add a ""show details"" button to a tkinter messagebox?","<python><tkinter><tkmessagebox>","1","2","1418"
"50650818","2018-06-01 20:48:45","1","","<p>The <code>.timestamp()</code> method was added in python version 3.3 [<a href=""https://docs.python.org/3/library/datetime.html#datetime.datetime.timestamp"" rel=""nofollow noreferrer"">source</a>], so you can't use <code>.timestamp()</code> in Python 2.</p>
","5666087","","","0","258","Jakub","2015-12-10 20:15:54","718","138","731","23","50650704","","2018-06-01 20:36:57","11","20045","<p><strong>Please Help - I keep receiving the following Traceback Error:</strong></p>

<p>Currently Running Python 2.0 </p>

<p>I'm attempting to utilize Python's Plotly library to display an infographic illustrating bitcoin prices. I've tried importing datetime at the top of my code but this doesn't appear to solve the problem. </p>

<pre><code>Traceback (most recent call last):
  File ""project_one.py"", line 165, in &lt;module&gt;
    crypto_price_df = get_crypto_data(coinpair)
  File ""project_one.py"", line 155, in get_crypto_data
    json_url = base_polo_url.format(poloniex_pair, start_date.timestamp(), end_date.timestamp(), pediod)
AttributeError: 'datetime.datetime' object has no attribute 'timestamp'
</code></pre>

<p><strong>My Code Starts Here</strong></p>

<pre><code>import numpy as np
import pandas as pd
from pandas import Series, DataFrame, Panel
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
import seaborn as sns
import sklearn as sk
import scipy as sp
import os
import pickle
import quandl
import datetime
import plotly.plotly as py
import plotly.graph_objs as go
import plotly.figure_factory as ff
from plotly import tools
from plotly.offline import iplot, init_notebook_mode
from IPython.display import display, HTML
init_notebook_mode(connected=True)


def get_quandl_data(quandl_id):

    cache_path = '{}.pkl'.format(quandl_id).replace('/','-')
    try:
        f = open(cache_path, 'rb')
        df = pickle.load(f)   
        print('Loaded {} from cache'.format(quandl_id))
    except (OSError, IOError) as e:
        print('Downloading {} from Quandl'.format(quandl_id))
        df = quandl.get(quandl_id, returns=""pandas"")
        df.to_pickle(cache_path)
        print('Cached {} at {}'.format(quandl_id, cache_path))
    return df


btc_usd_price_kraken = get_quandl_data('BCHARTS/KRAKENUSD')



exchanges = ['COINBASE','BITSTAMP','ITBIT']

exchange_data = {}

exchange_data['KRAKEN'] = btc_usd_price_kraken

for exchange in exchanges:
    exchange_code = 'BCHARTS/{}USD'.format(exchange)
    btc_exchange_df = get_quandl_data(exchange_code)
    exchange_data[exchange] = btc_exchange_df

def merge_dfs_on_column(dataframes, labels, col):

    series_dict = {}
    for index in range(len(dataframes)):
        series_dict[labels[index]] = dataframes[index][col]

    return pd.DataFrame(series_dict) 


btc_usd_datasets = merge_dfs_on_column(list(exchange_data.values()), 
list(exchange_data.keys()), 'Weighted Price')



def df_scatter(df, title, seperate_y_axis=False, y_axis_label='', 
scale='linear', initial_hide=False):

    label_arr = list(df)
    series_arr = list(map(lambda col: df[col], label_arr))

    layout = go.Layout(
        title=title,
        legend=dict(orientation=""h""),
        xaxis=dict(type='date'),
        yaxis=dict(
            title=y_axis_label,
            showticklabels= not seperate_y_axis,
            type=scale
        )
    )

    y_axis_config = dict(
        overlaying='y',
        showticklabels=False,
        type=scale )

    visibility = 'visible'
    if initial_hide:
        visibility = 'legendonly'


    trace_arr = []
    for index, series in enumerate(series_arr):
        trace = go.Scatter(
            x=series.index, 
            y=series, 
            name=label_arr[index],
            visible=visibility
        )


        if seperate_y_axis:
            trace['yaxis'] = 'y{}'.format(index + 1)
            layout['yaxis{}'.format(index + 1)] = y_axis_config    
        trace_arr.append(trace)

    fig = go.Figure(data=trace_arr, layout=layout)
    py.plot(fig)



df_scatter(btc_usd_datasets, 'Bitcoin Price (USD) By Exchange')


btc_usd_datasets.replace(0, np.nan, inplace=True)


df_scatter(btc_usd_datasets, 'Bitcoin Price (USD) By Exchange')


btc_usd_datasets['avg_btc_price_usd'] = btc_usd_datasets.mean(axis=1)



btc_trace = go.Scatter(x=btc_usd_datasets.index, 
y=btc_usd_datasets['avg_btc_price_usd'])
py.plot([btc_trace])



def get_json_data(json_url, cache_path):

    try:        
        f = open(cache_path, 'rb')
        df = pickle.load(f)   
        print('Loaded {} from cache'.format(json_url))
    except (OSError, IOError) as e:
        print('Downloading {}'.format(json_url))
        df = pd.read_json(json_url)
        df.to_pickle(cache_path)
        print('Cached {} at {}'.format(json_url, cache_path))
    return df

# Helper Function that Generates Poloniex API HTTP requests
base_polo_url = 'https://poloniex.com/public? 
command=returnChartData&amp;currencyPair={}&amp;start={}&amp;end={}&amp;period={}'
start_date = datetime.datetime.strptime('2015-01-01', '%Y-%m-%d') # get 
data from the start of 2015
end_date = datetime.datetime.now() # up until today
pediod = 86400 # pull daily data (86,400 seconds per day)

def get_crypto_data(poloniex_pair):

    json_url = base_polo_url.format(poloniex_pair, start_date.timestamp(), end_date.timestamp(), pediod)
    data_df = get_json_data(json_url, poloniex_pair)
    data_df = data_df.set_index('date') 
    return data_df


altcoins = ['ETH','LTC','XRP','ETC','STR','DASH','SC','XMR','XEM']
altcoin_data = {}
for altcoin in altcoins:
    coinpair = 'BTC_{}'.format(altcoin)
    crypto_price_df = get_crypto_data(coinpair)
    altcoin_data[altcoin] = crypto_price_df
</code></pre>
","6431344","","","AttributeError: 'datetime.datetime' object has no attribute 'timestamp'","<python>","5","2","5286"
"50650827","2018-06-01 20:49:16","0","","<p>Works like the common examples, but instead of specifying the file in ExcelWriter, it uses the standard library's BytesIO to store in a variable (<code>processed_data</code>):</p>

<pre><code>from io import BytesIO

import pandas as pd


df = pd.DataFrame({""a"": [1, 2, 3], ""b"": [4, 5, 6})

output = BytesIO()
writer = pd.ExcelWriter(output)
df.to_excel(writer)  # plus any **kwargs
writer.save()
processed_data = output.getvalue()
</code></pre>
","7619676","7619676","2018-06-01 20:57:34","0","448","ZaxR","2016-08-09 17:54:43","1752","111","641","11","50650826","","2018-06-01 20:49:16","1","468","<h1>Update #1</h1>

<p>This question was created along with the answer (see below) in an effort to provide a solution to a problem that I couldn't find addressed elsewhere on SO. For those that have downvoted, I ask that you please explain why and/or provide an alternative answer. Thank you.</p>

<h1>Original Question</h1>

<p>I recently had to take a dataframe and prepare it to output to an Excel file. However, I didn't want to save it to the local system, but rather pass the prepared data to a separate function that saves to the cloud based on a URI. After searching through a number of ExcelWriter examples, I couldn't find what I was looking for. </p>

<p>The goal is to take the dataframe, e.g.:</p>

<pre><code>df = pd.DataFrame({""a"": [1, 2, 3], ""b"": [4, 5, 6})
</code></pre>

<p>And temporarily store it as bytes in a variable, e.g.:</p>

<pre><code>processed_data = &lt;bytes representing the excel output&gt;
</code></pre>

<p>The solution I came up with is provided in the answers and hopefully will help someone else. Would love to see others' solutions as well!</p>

<h1>Update #2 - Example Use Case</h1>

<p>In my case, I created an io module that allows you to use URIs to specify different cloud destinations. For example, ""paths"" starting with gs:// get sent to Google Storage (using gsutils-like syntax). I process the data as my first step, and then pass that processed data to a ""save"" function, which itself filters to determine the right path.</p>

<p><a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html?highlight=to_csv#pandas.DataFrame.to_csv"" rel=""nofollow noreferrer"">df.to_csv()</a> actually works with no path and automatically returns a string (at least in recent versions), so this is my solution to allow to_excel() to do the same.</p>
","7619676","7619676","2018-06-01 21:12:35","Pandas to_excel as variable (without destination file)","<python><excel><pandas><pandas.excelwriter>","1","0","1813"
"50650865","2018-06-01 20:52:39","0","","<p>As you are using Anaconda, you can go for <strong>conda</strong> </p>

<pre><code>python -m conda install --upgrade pyodbc 
</code></pre>

<p>or </p>

<pre><code>python -m conda install --upgrade pyodbc=4.0.23
</code></pre>

<p>Since conda is a package manager for any software (installation, upgrade and uninstallation), it will resolve the issue you are facing, which is mostly due to conflicting site-packages such as sqlalchemy, older versions on pyodbc, etc.</p>
","5973377","","","1","471","Nishant Patel","2016-02-24 08:59:22","349","67","109","13","50650433","50650865","2018-06-01 20:12:55","0","725","<p>I would like to upgrade a module in python called ""pyodbc"". 
I use python 3.6 with spider in Anaconda and my version of ""pyodbc"" is <strong>4.0.22</strong> and the one of my coworker is <strong>3.0.16</strong>.
I would like to upgrade to <strong>4.0.23</strong>. </p>

<p>I tried with a batch file : </p>

<pre><code>C: 
cd path to Anaconda3
python -m pip install --user -U pyodbc
</code></pre>

<p>and I tried manually with the anaconda navigator but nothing works. It's always for me <strong>4.02.22</strong> unfortunately. </p>

<p>Thank you in advance for the help.</p>
","9876578","6738015","2018-06-01 20:30:58","python 3.6 // Update pyodbc","<python><batch-file><pip><anaconda><pyodbc>","1","3","577"
"50650878","2018-06-01 20:53:28","8","","<p>The <a href=""https://docs.python.org/3/library/datetime.html#datetime.datetime.timestamp"" rel=""noreferrer""><code>timestamp</code></a> method was added in Python 3.3. So if you're using Python 2.0, or even 2.7, you don't have it. </p>

<p>There are backports of current <code>datetime</code> to older Python versions on PyPI, but none of them seems to be official, or up-to-date; you might want to try searching for yourself.</p>

<p>There are also a number of third-party replacement libraries that add functionality that isn't in (2.x) <code>datetime</code>, including the ability to convert to Unix timestamps.</p>

<hr>

<p>You can just <a href=""https://github.com/python/cpython/blob/3.6/Lib/datetime.py#L1547"" rel=""noreferrer"">copy the function out of the source code from 3.3 or later</a>:</p>

<pre><code>def timestamp(self):
    ""Return POSIX timestamp as float""
    if self._tzinfo is None:
        s = self._mktime()
        return s + self.microsecond / 1e6
    else:
        return (self - _EPOCH).total_seconds()
</code></pre>

<p>… but you will have to modify things a bit to get them to work, because:</p>

<ul>
<li><code>_EPOCH</code> is deleted at the end of the module.</li>
<li>The 3.x <code>_EPOCH</code> is a tz-aware object built with a proper UTC timezone, which you don't have in 2.x unless you're using a third-party library like <code>pytz</code>.</li>
<li>The <code>_mktime</code> method and <code>_tzinfo</code> attribute don't exist on 2.x <code>datetime</code>, so you need to simulate what they do as well.</li>
</ul>

<p>If you don't need the same function to work equally well for naive, GMT, and tz-aware datetimes, it won't be that hard, but it's still not quite trivial—and if you do need the full functionality, it's going to be more painful.</p>

<hr>

<p>Or it may be easier to port the equivalent code given in <a href=""https://docs.python.org/3/library/datetime.html#datetime.datetime.timestamp"" rel=""noreferrer"">the docs</a>.</p>

<p>For aware <code>datetime</code> instances:</p>

<pre><code>(dt - datetime(1970, 1, 1, tzinfo=timezone.utc)).total_seconds()
</code></pre>

<p>Of course you still don't have that <code>timezone.utc</code>, but for this purpose, you don't need a full timezone object; you can use an instance of the example <code>UTC</code> class in the <a href=""https://docs.python.org/2/library/datetime.html#datetime.tzinfo.fromutc"" rel=""noreferrer"">2.x <code>tzinfo</code> docs</a>.</p>

<p>… for naive:</p>

<pre><code>timestamp = dt.replace(tzinfo=timezone.utc).timestamp()
</code></pre>

<p>… or:</p>

<pre><code>timestamp = (dt - datetime(1970, 1, 1)) / timedelta(seconds=1)
</code></pre>

<p>Since you don't have aware datetimes, that last one is all you need.</p>

<hr>

<p>If your Python is old enough, <code>timedelta</code> may not have a <code>__div__</code> method. In that case (if you haven't found a backport), you have to do division manually as well, by calling <code>total_seconds</code> on each one, making sure at least one of them is a float, and dividing the numbers:</p>

<pre><code>timestamp = ((dt - datetime(1970, 1, 1)).total_seconds() / 
    float(timedelta(seconds=1).total_seconds()))
</code></pre>

<p>But in this particular case, it should be pretty obvious that the divisor is just going to be 1.0, and dividing by 1.0 is the same as doing nothing, so:</p>

<pre><code>timestamp = (dt - datetime(1970, 1, 1)).total_seconds()
</code></pre>
","908494","908494","2018-08-28 23:00:54","3","3435","abarnert","2011-08-23 20:55:30","272076","19605","5576","4509","50650704","","2018-06-01 20:36:57","11","20045","<p><strong>Please Help - I keep receiving the following Traceback Error:</strong></p>

<p>Currently Running Python 2.0 </p>

<p>I'm attempting to utilize Python's Plotly library to display an infographic illustrating bitcoin prices. I've tried importing datetime at the top of my code but this doesn't appear to solve the problem. </p>

<pre><code>Traceback (most recent call last):
  File ""project_one.py"", line 165, in &lt;module&gt;
    crypto_price_df = get_crypto_data(coinpair)
  File ""project_one.py"", line 155, in get_crypto_data
    json_url = base_polo_url.format(poloniex_pair, start_date.timestamp(), end_date.timestamp(), pediod)
AttributeError: 'datetime.datetime' object has no attribute 'timestamp'
</code></pre>

<p><strong>My Code Starts Here</strong></p>

<pre><code>import numpy as np
import pandas as pd
from pandas import Series, DataFrame, Panel
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
import seaborn as sns
import sklearn as sk
import scipy as sp
import os
import pickle
import quandl
import datetime
import plotly.plotly as py
import plotly.graph_objs as go
import plotly.figure_factory as ff
from plotly import tools
from plotly.offline import iplot, init_notebook_mode
from IPython.display import display, HTML
init_notebook_mode(connected=True)


def get_quandl_data(quandl_id):

    cache_path = '{}.pkl'.format(quandl_id).replace('/','-')
    try:
        f = open(cache_path, 'rb')
        df = pickle.load(f)   
        print('Loaded {} from cache'.format(quandl_id))
    except (OSError, IOError) as e:
        print('Downloading {} from Quandl'.format(quandl_id))
        df = quandl.get(quandl_id, returns=""pandas"")
        df.to_pickle(cache_path)
        print('Cached {} at {}'.format(quandl_id, cache_path))
    return df


btc_usd_price_kraken = get_quandl_data('BCHARTS/KRAKENUSD')



exchanges = ['COINBASE','BITSTAMP','ITBIT']

exchange_data = {}

exchange_data['KRAKEN'] = btc_usd_price_kraken

for exchange in exchanges:
    exchange_code = 'BCHARTS/{}USD'.format(exchange)
    btc_exchange_df = get_quandl_data(exchange_code)
    exchange_data[exchange] = btc_exchange_df

def merge_dfs_on_column(dataframes, labels, col):

    series_dict = {}
    for index in range(len(dataframes)):
        series_dict[labels[index]] = dataframes[index][col]

    return pd.DataFrame(series_dict) 


btc_usd_datasets = merge_dfs_on_column(list(exchange_data.values()), 
list(exchange_data.keys()), 'Weighted Price')



def df_scatter(df, title, seperate_y_axis=False, y_axis_label='', 
scale='linear', initial_hide=False):

    label_arr = list(df)
    series_arr = list(map(lambda col: df[col], label_arr))

    layout = go.Layout(
        title=title,
        legend=dict(orientation=""h""),
        xaxis=dict(type='date'),
        yaxis=dict(
            title=y_axis_label,
            showticklabels= not seperate_y_axis,
            type=scale
        )
    )

    y_axis_config = dict(
        overlaying='y',
        showticklabels=False,
        type=scale )

    visibility = 'visible'
    if initial_hide:
        visibility = 'legendonly'


    trace_arr = []
    for index, series in enumerate(series_arr):
        trace = go.Scatter(
            x=series.index, 
            y=series, 
            name=label_arr[index],
            visible=visibility
        )


        if seperate_y_axis:
            trace['yaxis'] = 'y{}'.format(index + 1)
            layout['yaxis{}'.format(index + 1)] = y_axis_config    
        trace_arr.append(trace)

    fig = go.Figure(data=trace_arr, layout=layout)
    py.plot(fig)



df_scatter(btc_usd_datasets, 'Bitcoin Price (USD) By Exchange')


btc_usd_datasets.replace(0, np.nan, inplace=True)


df_scatter(btc_usd_datasets, 'Bitcoin Price (USD) By Exchange')


btc_usd_datasets['avg_btc_price_usd'] = btc_usd_datasets.mean(axis=1)



btc_trace = go.Scatter(x=btc_usd_datasets.index, 
y=btc_usd_datasets['avg_btc_price_usd'])
py.plot([btc_trace])



def get_json_data(json_url, cache_path):

    try:        
        f = open(cache_path, 'rb')
        df = pickle.load(f)   
        print('Loaded {} from cache'.format(json_url))
    except (OSError, IOError) as e:
        print('Downloading {}'.format(json_url))
        df = pd.read_json(json_url)
        df.to_pickle(cache_path)
        print('Cached {} at {}'.format(json_url, cache_path))
    return df

# Helper Function that Generates Poloniex API HTTP requests
base_polo_url = 'https://poloniex.com/public? 
command=returnChartData&amp;currencyPair={}&amp;start={}&amp;end={}&amp;period={}'
start_date = datetime.datetime.strptime('2015-01-01', '%Y-%m-%d') # get 
data from the start of 2015
end_date = datetime.datetime.now() # up until today
pediod = 86400 # pull daily data (86,400 seconds per day)

def get_crypto_data(poloniex_pair):

    json_url = base_polo_url.format(poloniex_pair, start_date.timestamp(), end_date.timestamp(), pediod)
    data_df = get_json_data(json_url, poloniex_pair)
    data_df = data_df.set_index('date') 
    return data_df


altcoins = ['ETH','LTC','XRP','ETC','STR','DASH','SC','XMR','XEM']
altcoin_data = {}
for altcoin in altcoins:
    coinpair = 'BTC_{}'.format(altcoin)
    crypto_price_df = get_crypto_data(coinpair)
    altcoin_data[altcoin] = crypto_price_df
</code></pre>
","6431344","","","AttributeError: 'datetime.datetime' object has no attribute 'timestamp'","<python>","5","2","5286"
"50650937","2018-06-01 20:59:46","0","","<p>It's because, NaN value is a float, but True and False are bool. There are mixed dtypes in one column, so Pandas will automatically convert it into object.  </p>

<p>Another instance of this is, if you have a column with all integer values and append a value with float, then pandas change entire column to float by adding '.0' to the remaining values.</p>

<hr>

<p><strong>Edit</strong></p>

<p>Based on comments, Another hacky way to convert object to bool dtype. </p>

<pre><code>df = pandas.DataFrame({
    'name': ['Bob', 'Sue', 'Tom'],
    'age': [45, 40, 10],
    'weight': [143.2, 130.2, 34.9],
    'has_children': [True, True, False]
})
row = {'name': 'Cindy', 'age': 12}
df = df.append(row, ignore_index=True)
df['has_children'] = df['has_children'].fillna(False).astype('bool')
</code></pre>

<p><strong>Now the new dataframe looks like this :</strong></p>

<pre><code>    age has_children    name    weight
 0  45  True             Bob    143.2
 1  40  True             Sue    130.2
 2  10  False            Tom    34.9
 3  12  False            Cindy  NaN
</code></pre>
","6308318","6308318","2018-06-05 01:57:12","2","1086","Venkata Gogu","2016-05-09 02:23:30","606","135","208","32","50650850","50651265","2018-06-01 20:51:36","2","1256","<p>What's the best way to insert new rows into an existing pandas DataFrame while maintaining column data types and, at the same time, giving user-defined fill values for columns that aren't specified?  Here's an example:</p>

<pre><code>df = pd.DataFrame({
    'name': ['Bob', 'Sue', 'Tom'],
    'age': [45, 40, 10],
    'weight': [143.2, 130.2, 34.9],
    'has_children': [True, True, False]
})
</code></pre>

<p>Assume that I want to add a new record passing just <code>name</code> and <code>age</code>.  To maintain data types, I can copy rows from <code>df</code>, modify values and then append <code>df</code> to the copy, e.g.</p>

<pre><code>columns = ('name', 'age')
copy_df = df.loc[0:0, columns].copy()
copy_df.loc[0, columns] = 'Cindy', 42
new_df = copy_df.append(df, sort=False).reset_index(drop=True)
</code></pre>

<p>But that converts the <code>bool</code> column to an object.</p>

<p>Here's a really hacky solution that doesn't feel like the ""right way"" to do this:</p>

<pre><code>columns = ('name', 'age')
copy_df = df.loc[0:0].copy()

missing_remap = {
    'int64': 0,
    'float64': 0.0,
    'bool': False,
    'object': ''
}
for c in set(copy_df.columns).difference(columns)):
    copy_df.loc[:, c] = missing_remap[str(copy_df[c].dtype)]

new_df = copy_df.append(df, sort=False).reset_index(drop=True)
new_df.loc[0, columns] = 'Cindy', 42
</code></pre>

<p>I know I must be missing something.</p>
","510151","","","Insert rows into pandas DataFrame while maintaining column data types","<python><pandas><dataframe><append>","2","0","1420"
"50650959","2018-06-01 21:02:44","4","","<p>As the other answers state, <code>datetime.timestamp()</code> was added on <strong>Python 3.3</strong>.</p>

<p>To get a similar behavior on Python &lt; 3.3, you need to use <code>time.mktime()</code>:</p>

<pre><code>import time

def to_seconds(date):
    return time.mktime(date.timetuple())
</code></pre>

<p>And then, instead of calling <code>start_date.timestamp()</code>, you just call <code>to_seconds(start_date)</code></p>
","2401769","","","2","435","Matias Cicero","2013-05-20 12:52:52","14282","1237","283","581","50650704","","2018-06-01 20:36:57","11","20045","<p><strong>Please Help - I keep receiving the following Traceback Error:</strong></p>

<p>Currently Running Python 2.0 </p>

<p>I'm attempting to utilize Python's Plotly library to display an infographic illustrating bitcoin prices. I've tried importing datetime at the top of my code but this doesn't appear to solve the problem. </p>

<pre><code>Traceback (most recent call last):
  File ""project_one.py"", line 165, in &lt;module&gt;
    crypto_price_df = get_crypto_data(coinpair)
  File ""project_one.py"", line 155, in get_crypto_data
    json_url = base_polo_url.format(poloniex_pair, start_date.timestamp(), end_date.timestamp(), pediod)
AttributeError: 'datetime.datetime' object has no attribute 'timestamp'
</code></pre>

<p><strong>My Code Starts Here</strong></p>

<pre><code>import numpy as np
import pandas as pd
from pandas import Series, DataFrame, Panel
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
import seaborn as sns
import sklearn as sk
import scipy as sp
import os
import pickle
import quandl
import datetime
import plotly.plotly as py
import plotly.graph_objs as go
import plotly.figure_factory as ff
from plotly import tools
from plotly.offline import iplot, init_notebook_mode
from IPython.display import display, HTML
init_notebook_mode(connected=True)


def get_quandl_data(quandl_id):

    cache_path = '{}.pkl'.format(quandl_id).replace('/','-')
    try:
        f = open(cache_path, 'rb')
        df = pickle.load(f)   
        print('Loaded {} from cache'.format(quandl_id))
    except (OSError, IOError) as e:
        print('Downloading {} from Quandl'.format(quandl_id))
        df = quandl.get(quandl_id, returns=""pandas"")
        df.to_pickle(cache_path)
        print('Cached {} at {}'.format(quandl_id, cache_path))
    return df


btc_usd_price_kraken = get_quandl_data('BCHARTS/KRAKENUSD')



exchanges = ['COINBASE','BITSTAMP','ITBIT']

exchange_data = {}

exchange_data['KRAKEN'] = btc_usd_price_kraken

for exchange in exchanges:
    exchange_code = 'BCHARTS/{}USD'.format(exchange)
    btc_exchange_df = get_quandl_data(exchange_code)
    exchange_data[exchange] = btc_exchange_df

def merge_dfs_on_column(dataframes, labels, col):

    series_dict = {}
    for index in range(len(dataframes)):
        series_dict[labels[index]] = dataframes[index][col]

    return pd.DataFrame(series_dict) 


btc_usd_datasets = merge_dfs_on_column(list(exchange_data.values()), 
list(exchange_data.keys()), 'Weighted Price')



def df_scatter(df, title, seperate_y_axis=False, y_axis_label='', 
scale='linear', initial_hide=False):

    label_arr = list(df)
    series_arr = list(map(lambda col: df[col], label_arr))

    layout = go.Layout(
        title=title,
        legend=dict(orientation=""h""),
        xaxis=dict(type='date'),
        yaxis=dict(
            title=y_axis_label,
            showticklabels= not seperate_y_axis,
            type=scale
        )
    )

    y_axis_config = dict(
        overlaying='y',
        showticklabels=False,
        type=scale )

    visibility = 'visible'
    if initial_hide:
        visibility = 'legendonly'


    trace_arr = []
    for index, series in enumerate(series_arr):
        trace = go.Scatter(
            x=series.index, 
            y=series, 
            name=label_arr[index],
            visible=visibility
        )


        if seperate_y_axis:
            trace['yaxis'] = 'y{}'.format(index + 1)
            layout['yaxis{}'.format(index + 1)] = y_axis_config    
        trace_arr.append(trace)

    fig = go.Figure(data=trace_arr, layout=layout)
    py.plot(fig)



df_scatter(btc_usd_datasets, 'Bitcoin Price (USD) By Exchange')


btc_usd_datasets.replace(0, np.nan, inplace=True)


df_scatter(btc_usd_datasets, 'Bitcoin Price (USD) By Exchange')


btc_usd_datasets['avg_btc_price_usd'] = btc_usd_datasets.mean(axis=1)



btc_trace = go.Scatter(x=btc_usd_datasets.index, 
y=btc_usd_datasets['avg_btc_price_usd'])
py.plot([btc_trace])



def get_json_data(json_url, cache_path):

    try:        
        f = open(cache_path, 'rb')
        df = pickle.load(f)   
        print('Loaded {} from cache'.format(json_url))
    except (OSError, IOError) as e:
        print('Downloading {}'.format(json_url))
        df = pd.read_json(json_url)
        df.to_pickle(cache_path)
        print('Cached {} at {}'.format(json_url, cache_path))
    return df

# Helper Function that Generates Poloniex API HTTP requests
base_polo_url = 'https://poloniex.com/public? 
command=returnChartData&amp;currencyPair={}&amp;start={}&amp;end={}&amp;period={}'
start_date = datetime.datetime.strptime('2015-01-01', '%Y-%m-%d') # get 
data from the start of 2015
end_date = datetime.datetime.now() # up until today
pediod = 86400 # pull daily data (86,400 seconds per day)

def get_crypto_data(poloniex_pair):

    json_url = base_polo_url.format(poloniex_pair, start_date.timestamp(), end_date.timestamp(), pediod)
    data_df = get_json_data(json_url, poloniex_pair)
    data_df = data_df.set_index('date') 
    return data_df


altcoins = ['ETH','LTC','XRP','ETC','STR','DASH','SC','XMR','XEM']
altcoin_data = {}
for altcoin in altcoins:
    coinpair = 'BTC_{}'.format(altcoin)
    crypto_price_df = get_crypto_data(coinpair)
    altcoin_data[altcoin] = crypto_price_df
</code></pre>
","6431344","","","AttributeError: 'datetime.datetime' object has no attribute 'timestamp'","<python>","5","2","5286"
"50650963","2018-06-01 21:03:20","0","","<p>Use Regex.</p>

<p><strong>Ex:</strong></p>

<pre><code>import re
import string

text = ""Hello ! #$%&amp;'()*+,-./:;&lt;=&gt;?@[\]^_`{|}~ World""
print(re.sub(""["" + re.escape(string.punctuation) + ""]"", """", text))
#or
print( re.sub(r'[^a-zA-Z0-9\s]','',text) )
</code></pre>
","532312","532312","2018-06-01 21:28:32","2","276","Rakesh","2010-12-06 13:07:54","56694","5302","758","1508","50650921","","2018-06-01 20:58:31","0","734","<p>Made a function to count 20 most common words in a book that I downloaded as a plain text format. The python textbook I am going off of said to use the <code>import string</code> and then the <code>replace</code> or the <code>translate</code> method to remove any punctuation, but when I print out the lines after the replace step, all the lines still have punctuation in it. I tried moving around the <code>line = line.strip()</code> and the <code>line = line.replace(string.punctuation,'')</code> step, but that did not work. I have never used replace so I may be using it wrong for all I know. Rest of my program works, just that step is frustrating me.</p>

<pre><code>import string
def function():
    infile = open('gutbook.txt','r',encoding='utf-8')
    count = dict()
    list2 = list()
    for line in infile:
        line = line.strip()
        line = line.replace(string.punctuation,'')
        line = line.lower().split()
        if line== []:
            continue
        for i in line:
            count[i] = count.get(i,0) + 1
    for key,value in count.items():
        newtuple = (value,key)
        list2.append(newtuple)
    list3 = sorted(list2,reverse = True)
    print(list3[:20])



function()
</code></pre>
","9883150","3562273","2018-06-01 21:42:30","How to use string.punctuation to remove punctuation in a text file","<python><replace>","1","3","1234"
"50651079","2018-06-01 21:15:08","1","","<p>You can't order a <code>dict</code>, but you can create a list of the <code>items</code> and have that sorted:</p>

<pre><code>sorted(data.iteritems(), key=lambda x:x[1][u'rank'])
</code></pre>
","8472976","","","0","197","MegaIng","2017-08-16 13:56:11","3958","373","232","57","50651015","50651121","2018-06-01 21:07:56","0","32","<p>I'm loading data into a python dict, pulled from coinmarketcap's api and then I want to be able to sort it by the rank.
I've had a look online and although I've seen some examples or ordering I just can't get it to work, any help would be greatly appreciated.
Can anyone help?</p>

<pre><code>{
  'data': {
    1: {
      u'last_updated': 1527886475,
      u'name': u'Bitcoin',
      u'symbol': u'BTC',
      u'rank': 1,
      u'total_supply': '17068825.0',
      u'quotes': {
        u'USD': {
          u'market_cap': '127372692798.0',
          u'percent_change_7d': '0.0',
          u'price': '7462.3',
          u'percent_change_24h': '-1.29',
          u'volume_24h': '4962840000.0',
          u'percent_change_1h': '0.03'
        }
      },
      u'max_supply': '21000000.0',
      u'circulating_supply': '17068825.0',
      u'website_slug': u'bitcoin',
      u'id': 1
    },
    825: {
      u'last_updated': 1527886449,
      u'name': u'Tether',
      u'symbol': u'USDT',
      u'rank': 13,
      u'total_supply': '2830109970.0',
      u'quotes': {
        u'USD': {
          u'market_cap': '2508920883.0',
          u'percent_change_7d': '-0.03',
          u'price': '1.00071',
          u'percent_change_24h': '0.01',
          u'volume_24h': '2542980000.0',
          u'percent_change_1h': '0.1'
        }
      },
      u'max_supply': None,
      u'circulating_supply': '2507140814.0',
      u'website_slug': u'tether',
      u'id': 825
    },
    1027: {
      u'last_updated': 1527886459,
      u'name': u'Ethereum',
      u'symbol': u'ETH',
      u'rank': 2,
      u'total_supply': '99807496.0',
      u'quotes': {
        u'USD': {
          u'market_cap': '56961236045.0',
          u'percent_change_7d': '-2.88',
          u'price': '570.711',
          u'percent_change_24h': '-1.62',
          u'volume_24h': '1986760000.0',
          u'percent_change_1h': '-0.39'
        }
      },
      u'max_supply': None,
      u'circulating_supply': '99807496.0',
      u'website_slug': u'ethereum',
      u'id': 1027
    }
  }
}
</code></pre>
","8665303","","","Ordering DICT in Python","<python><sorting>","2","1","2059"
"50651080","2018-06-01 21:15:14","7","","<p>I am getting the time delta in seconds and dividing it by 3600 to get hours</p>

<p><code>round(td.total_seconds() / 3600)</code> .</p>

<p>When I tested in jupyter notebook this approach works faster</p>

<pre><code>%timeit td / np.timedelta64(1, 'h') 
The slowest run took 19.10 times longer than the fastest. This could mean that an intermediate result is being cached. 
100000 loops, best of 3: 4.58 µs per loop

%timeit round(td.total_seconds() / 3600)
The slowest run took 18.08 times longer than the fastest. This could mean that an intermediate result is being cached.
1000000 loops, best of 3: 401 ns per loop
</code></pre>
","5977874","","","0","636","Hrushikesh Dhumal","2016-02-25 02:32:57","301","3","92","1","31283001","31283068","2015-07-08 03:24:53","26","21931","<p>How can I get the total number of hours in a Pandas timedelta?</p>

<p>For example:</p>

<pre><code>&gt;&gt;&gt; td = pd.Timedelta('1 days 2 hours')
&gt;&gt;&gt; td.get_total_hours()
26
</code></pre>

<p>Note: as per the documentation, the <code>.hours</code> attribute will return the hours <em>component</em>:</p>

<pre><code>&gt;&gt;&gt; td.hours
2
</code></pre>
","71522","","","Get total number of hours from a Pandas Timedelta?","<python><pandas>","3","0","369"
"50651099","2018-06-01 21:18:11","1","","<blockquote>
  <p>To expand on the answer by Nick ODell</p>
</blockquote>

<p><strong>You must be on Windows for DLLs to work, they are not portable.</strong></p>

<p><em>However the code below is cross platform and all platforms support run-times so this can be re-compiled for each platform you need it to work on.</em></p>

<p>Python does not (yet) provide an easy tool to create a dll, however you can do it in C/C++</p>

<hr>

<p>First you will need a compiler (Windows does not have one by default) notably Cygwin, MinGW or Visual Studio.  </p>

<p>A basic knowledge of C is also necessary (since we will be coding mainly in C).</p>

<p><em>You will also need to include the necessary headers, I will skip this so it does not become horribly long, and will assume everything is set up correctly.</em></p>

<hr>

<p>For this demonstration I will print a traditional hello world:</p>

<p><strong>Python code we will be converting to a DLL:</strong></p>

<pre><code>def foo(): print(""hello world"")
</code></pre>

<p><strong>C code:</strong></p>

<pre><code>#include ""Python.h"" // Includes everything to use the Python-C API

int foo(void); // Declare foo

int foo(void) { // Name of our function in our DLL

    Py_Initialize(); // Initialise Python

    PyRun_SimpleString(""print('hello world')""); // Run the Python commands

    return 0; // Finish execution
}
</code></pre>

<p>Here is the <a href=""https://docs.python.org/3/extending/embedding.html"" rel=""nofollow noreferrer"">tutorial</a> for embedding Python.  There are a few extra things that should be added here, but for brevity I have left those out.</p>

<p>Compile it and you should have a DLL. :)</p>

<hr>

<p>That is not all.  You will need to distribute whatever dependencies are needed,  that will mean the <code>python36.dll</code> run-time and some other components to run the Python script.</p>

<blockquote>
  <p>My C coding is not perfect, so if anyone can spot any improvements please comment and I will do my best to fix the it. </p>
</blockquote>

<hr>

<p>It might also be possible in C# from this answer <a href=""https://stackoverflow.com/questions/13231913/how-do-i-call-a-specific-method-from-a-python-script-in-c"">How do I call a specific Method from a Python Script in C#?</a>, since C# can create DLLs, and you can call Python functions from C#.</p>
","8372104","","","0","2335","Simon","2017-07-26 19:03:27","6383","1766","5398","222","10859369","","2012-06-02 02:33:23","21","66685","<p>Well, I have a Python package. I need to compile it as dll before distribute it in a way easily importable. <strong>How?</strong> You may suggest that <code>*.pyc</code>. But I read somewhere any <code>*.pyc</code> can be easily decompiled!</p>

<p><strong>Update:</strong>
Follow these:<br>
1) I wrote a python package<br>
2) want to distribute it<br>
3) do NOT want distribute the source<br>
4) *.pyc is decompilable >> source can be extracted!<br>
5) dll is standard  </p>
","566326","566326","2012-06-02 09:16:35","How to compile a Python package to a dll","<python><dll><module><compilation>","9","2","479"
"50651100","2018-06-01 21:18:13","2","","<p>This can also happen if the binary is not meant to run on your system.</p>

<p>I'm on OSX, but the binary I was running is not meant for OSX, as I saw from using <code>file path/to/binary</code>:</p>

<pre><code>webui/bin/wkhtmltopdf: ELF 64-bit LSB executable, x86-64, version 1 (GNU/Linux), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, for GNU/Linux 2.6.18, BuildID[sha1]=b6566a9e44c43a0eebf18d8c1dc6cb616801a77e, stripped
</code></pre>
","5258518","","","0","458","Joe Flack","2015-08-24 02:17:09","404","31","22","0","26807937","","2014-11-07 18:42:21","35","52225","<p>Yesterday, I wrote and ran a python <code>script</code> which executes a <code>shell</code> using <code>subprocess.Popen(command.split())</code> where command is string which constitutes <code>.sh</code> script and its argument. This script was working fine until yesterday. Today, I ran the same script and now I am continuously hitting this error.</p>

<pre><code>p=subprocess.Popen(shell_command.split())
File ""/usr/lib/python2.7/subprocess.py"", line 679, in __init__
errread, errwrite)
File ""/usr/lib/python2.7/subprocess.py"", line 1249, in _execute_child
raise child_exception
OSError: [Errno 8] Exec format error
</code></pre>

<p>I know there are similar questions that have been asked before related to this question, but in my case I tried everything which doesn't solve my purpose. Using <code>shell=True</code> does not work because my shell script calls an another shell script before which some environment has to be set in order to run that script. I am badly stuck in this. I just restart my system once. I am using <code>ubuntu 12.04</code></p>

<p><strong>EDIT:</strong></p>

<pre><code> import subprocess
 import os
 import sys

 arg1=sys.argv[1]
 arg2=sys.argve[2]

 shell_command = 'my_path/my_shell.sh ' + arg1 + ' '+ arg2
 P = subprocess.Popen(shell_command.split())
 P.wait()
</code></pre>

<p><strong>my_shell.sh:</strong></p>

<pre><code>  arg1=$1
  arg2=$2

  cd $TOP
  setup the environment and run shell script
  build the kernel ...
  execute shell command .....
</code></pre>
","2708608","3911457","2017-09-12 05:57:14","subprocess.Popen(): OSError: [Errno 8] Exec format error in python?","<python><linux><shell>","7","8","1509"
"50651121","2018-06-01 21:20:26","2","","<p>Using <code>collections.OrderedDict</code></p>

<p><strong>Demo:</strong></p>

<pre><code>from collections import OrderedDict
print( OrderedDict(sorted(data['data'].items(), key=lambda (x, y): y['rank'])) )
print( OrderedDict(sorted(data['data'].items(), key=lambda (x, y): y['rank'], reverse=True)) )  #Descending order
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>OrderedDict([(1, {u'total_supply': '17068825.0', u'quotes': {u'USD': {u'market_cap': '127372692798.0', u'percent_change_7d': '0.0', u'price': '7462.3', u'percent_change_1h': '0.03', u'volume_24h': '4962840000.0', u'percent_change_24h': '-1.29'}}, u'max_supply': '21000000.0', u'last_updated': 1527886475, u'name': u'Bitcoin', u'website_slug': u'bitcoin', u'symbol': u'BTC', u'id': 1, u'rank': 1, u'circulating_supply': '17068825.0'}), (1027, {u'total_supply': '99807496.0', u'quotes': {u'USD': {u'market_cap': '56961236045.0', u'percent_change_7d': '-2.88', u'price': '570.711', u'percent_change_1h': '-0.39', u'volume_24h': '1986760000.0', u'percent_change_24h': '-1.62'}}, u'max_supply': None, u'last_updated': 1527886459, u'name': u'Ethereum', u'website_slug': u'ethereum', u'symbol': u'ETH', u'id': 1027, u'rank': 2, u'circulating_supply': '99807496.0'}), (825, {u'total_supply': '2830109970.0', u'quotes': {u'USD': {u'market_cap': '2508920883.0', u'percent_change_7d': '-0.03', u'price': '1.00071', u'percent_change_1h': '0.1', u'volume_24h': '2542980000.0', u'percent_change_24h': '0.01'}}, u'max_supply': None, u'last_updated': 1527886449, u'name': u'Tether', u'website_slug': u'tether', u'symbol': u'USDT', u'id': 825, u'rank': 13, u'circulating_supply': '2507140814.0'})])
OrderedDict([(825, {u'total_supply': '2830109970.0', u'quotes': {u'USD': {u'market_cap': '2508920883.0', u'percent_change_7d': '-0.03', u'price': '1.00071', u'percent_change_1h': '0.1', u'volume_24h': '2542980000.0', u'percent_change_24h': '0.01'}}, u'max_supply': None, u'last_updated': 1527886449, u'name': u'Tether', u'website_slug': u'tether', u'symbol': u'USDT', u'id': 825, u'rank': 13, u'circulating_supply': '2507140814.0'}), (1027, {u'total_supply': '99807496.0', u'quotes': {u'USD': {u'market_cap': '56961236045.0', u'percent_change_7d': '-2.88', u'price': '570.711', u'percent_change_1h': '-0.39', u'volume_24h': '1986760000.0', u'percent_change_24h': '-1.62'}}, u'max_supply': None, u'last_updated': 1527886459, u'name': u'Ethereum', u'website_slug': u'ethereum', u'symbol': u'ETH', u'id': 1027, u'rank': 2, u'circulating_supply': '99807496.0'}), (1, {u'total_supply': '17068825.0', u'quotes': {u'USD': {u'market_cap': '127372692798.0', u'percent_change_7d': '0.0', u'price': '7462.3', u'percent_change_1h': '0.03', u'volume_24h': '4962840000.0', u'percent_change_24h': '-1.29'}}, u'max_supply': '21000000.0', u'last_updated': 1527886475, u'name': u'Bitcoin', u'website_slug': u'bitcoin', u'symbol': u'BTC', u'id': 1, u'rank': 1, u'circulating_supply': '17068825.0'})])
</code></pre>
","532312","","","1","2961","Rakesh","2010-12-06 13:07:54","56694","5302","758","1508","50651015","50651121","2018-06-01 21:07:56","0","32","<p>I'm loading data into a python dict, pulled from coinmarketcap's api and then I want to be able to sort it by the rank.
I've had a look online and although I've seen some examples or ordering I just can't get it to work, any help would be greatly appreciated.
Can anyone help?</p>

<pre><code>{
  'data': {
    1: {
      u'last_updated': 1527886475,
      u'name': u'Bitcoin',
      u'symbol': u'BTC',
      u'rank': 1,
      u'total_supply': '17068825.0',
      u'quotes': {
        u'USD': {
          u'market_cap': '127372692798.0',
          u'percent_change_7d': '0.0',
          u'price': '7462.3',
          u'percent_change_24h': '-1.29',
          u'volume_24h': '4962840000.0',
          u'percent_change_1h': '0.03'
        }
      },
      u'max_supply': '21000000.0',
      u'circulating_supply': '17068825.0',
      u'website_slug': u'bitcoin',
      u'id': 1
    },
    825: {
      u'last_updated': 1527886449,
      u'name': u'Tether',
      u'symbol': u'USDT',
      u'rank': 13,
      u'total_supply': '2830109970.0',
      u'quotes': {
        u'USD': {
          u'market_cap': '2508920883.0',
          u'percent_change_7d': '-0.03',
          u'price': '1.00071',
          u'percent_change_24h': '0.01',
          u'volume_24h': '2542980000.0',
          u'percent_change_1h': '0.1'
        }
      },
      u'max_supply': None,
      u'circulating_supply': '2507140814.0',
      u'website_slug': u'tether',
      u'id': 825
    },
    1027: {
      u'last_updated': 1527886459,
      u'name': u'Ethereum',
      u'symbol': u'ETH',
      u'rank': 2,
      u'total_supply': '99807496.0',
      u'quotes': {
        u'USD': {
          u'market_cap': '56961236045.0',
          u'percent_change_7d': '-2.88',
          u'price': '570.711',
          u'percent_change_24h': '-1.62',
          u'volume_24h': '1986760000.0',
          u'percent_change_1h': '-0.39'
        }
      },
      u'max_supply': None,
      u'circulating_supply': '99807496.0',
      u'website_slug': u'ethereum',
      u'id': 1027
    }
  }
}
</code></pre>
","8665303","","","Ordering DICT in Python","<python><sorting>","2","1","2059"
"50651123","2018-06-01 21:20:47","0","","<p>This is a very typical situation with evolutionary algorithms. Success rate is a quite common metric, and 30% is a decent result.</p>

<p>Just an example, recently I implemented a <a href=""https://github.com/werediver/Sandbox"" rel=""nofollow noreferrer"">GP/GE solver</a> for <a href=""https://en.wikipedia.org/wiki/Santa_Fe_Trail_problem"" rel=""nofollow noreferrer"">Santa Fe Trail problem</a>, and it demonstrates the success rate of 30% or less.</p>

<h3>How to improve success rate</h3>

<p><em>A personal interpretation of the problem based on limited experience follows.</em></p>

<p>An evolutionary algorithm fails to find a close to global optimum solution when it converges around a local optimum or gets stuck on a great plateau, and has not enough diversity in its population to escape this trap by finding a better region.</p>

<p>You may try to supply your algorithm with more diversity by increasing the size of the population. Or you may look into techniques like <a href=""http://eplex.cs.ucf.edu/noveltysearch/userspage/"" rel=""nofollow noreferrer"">novelty search</a>, and <a href=""https://www.frontiersin.org/articles/10.3389/frobt.2016.00040/full"" rel=""nofollow noreferrer"">quality diversity</a>.</p>

<p>By the way, here is a very nice interactive demonstration of novelty search vs. fitness search: <a href=""http://eplex.cs.ucf.edu/noveltysearch/userspage/demo.html"" rel=""nofollow noreferrer"">http://eplex.cs.ucf.edu/noveltysearch/userspage/demo.html</a></p>
","3541063","3541063","2018-06-02 10:06:37","0","1476","werediver","2014-04-16 11:53:55","3673","193","125","42","50634857","","2018-06-01 01:22:57","1","38","<p>I wrote a code that implements a simple genetic algorithm to maximize:</p>

<pre><code>f(x) = 15x - x^2
</code></pre>

<p>The function has its maximum at 7.5, so the code output should be 7 or 8 since the population are integers.
When I run the code 10 times I get 7 or 8 around three times out of 10.
What modification should I make to further improve the algorithm and what are different types of genetic algorithms?</p>

<p>Here is the code:</p>

<pre><code>from random import *
import numpy as np

#fitness function
def fit(x):
    return 15*x -x**2
#covert binary list to decimal number
def to_dec(x):
    return int("""".join(str(e) for e in x), 2)

#picks pairs from the original population
def gen_pairs(populationl, prob):
    pairsl = []
    test = [0, 1, 2, 3, 4, 5]
    for i in range(3):
        pair = []
        for j in range(2):
            temp = np.random.choice(test, p=prob)
            pair.append(populationl[temp].copy())
        pairsl.append(pair)

    return pairsl

#mating function
def cross_over(prs, mp):
    new = []
    for pr in prs:
        if mp[prs.index(pr)] == 1:
            index = np.random.choice([1,2,3], p=[1/3, 1/3, 1/3])
            pr[0][:index], pr[1][:index] = pr[1][:index], pr[0][:index]

    for pr in prs:
        new.append(pr[0])
        new.append(pr[1])

    return new


#mutation
def mutation(x):
    for chromosome in x:
        for gene in chromosome:
            mutation_prob = np.random.choice([0, 1], p=[0.999, .001])
            if mutation_prob == 1:
                #m_index = np.random.choice([0,1,2,3])
                if gene == 0:
                    gene = 1
                else:
                    gene = 0
    #generate initial population
    randlist = lambda n:[randint(0,1) for b in range(1, n+1)]

for j in range(10):
    population = [randlist(4) for i in range(6)]
    for _ in range(20):
        fittness = [fit(to_dec(y)) for y in population]

        s = sum(fittness)
        prob = [e/s for e in fittness]
        pairsg = gen_pairs(population.copy(), prob)

        mating_prob = []
        for i in pairsg:
            mating_prob.append(np.random.choice([0,1], p=[0.4,0.6]))

        new_population = cross_over(pairsg, mating_prob)
        mutated = mutation(new_population)
        decimal_p = [to_dec(i)for i in population]
        decimal_new = [to_dec(i)for i in new_population]
        # print(decimal_p)
        # print(decimal_new)
        population = new_population
    print(decimal_new)
</code></pre>
","8269980","","","Different forms of genetic algorithim","<python><genetic-algorithm>","1","1","2506"
"50651153","2018-06-01 21:23:07","1","","<p>As far as I understand, you want the dead time window to increase each time the sensor is hit by an event (a 'one'), when it's already in the paralyzed state. I believe the following code does that. The trick here is to use an inner while loop, so that you can dynamically change the loop boundaries, as it's not really possible to do so with a for loop in python. I did not delete the print statements so that it's easier to see where do the outputs given later come from.</p>

<pre><code>import numpy as np

np.random.seed(468316)
random_set = np.random.poisson(lam=1, size = 30) #The source is making the elements with Poisson distribution
#lam could be any other value 
d = 2 #dead time, could be any other integers

#Saturation effect
#detector could not detect more than one elements at a time
random_set[random_set&gt;1] = 1

print('###initial random set')
print(random_set)
#set max index
max_index = random_set.shape[0] - 1
print('i', '\t', 'j', '\t', 'dt', '\t',' i+dt+1')

#Paralyzable dead time effect
for i,val in enumerate(random_set):
    #see if current value is an event
    if val == 1:
        #if so, set next d elements to zero
        dt = d
        #emulate 'for j in range(i+1, i+dt+1):' with a while loop
        j = i+1 if i &lt; max_index else max_index        
        while j &lt; i+dt+1:
            print(i, '\t',j, '\t', dt, '\t', i+dt+1)
            #if an event is foud within the d window, increase dt by another d
            if random_set[j]==1:
                random_set[j]=0
                dt += d

           #dont let the i+dt+1 to get out of the bounds of the random_set 
            if i+dt+1 &gt; max_index:
                dt =   max_index - i
            j += 1
print('###final random set')
print(random_set)
</code></pre>

<p>This produces the following random_set (i've used a different seed and lenght for a presentable example). In the list below, <em>i</em> indicates the index of the outer loop and <em>j</em> of the inner while loop. That means that <em>i</em> indicates the 'ones' that are going to stay in the final set and <em>j's</em> indicate the range within the which the 'ones' are going to be deleted. <em>dt</em> is the size of the 'dead time' since the initial event hit the sensor. It increases each time when a 'one' is found within the [j,i+dt+1] range. Hereby <em>i+dt+1</em> indicates the outer boundary after which the paralyzation should stop.</p>

<pre><code>###initial random set
[1 0 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1]
i    j   dt  i+dt+1
0    1   2   3
0    2   2   3
4    5   2   7
4    6   4   9
4    7   4   9
4    8   6   11
4    9   8   13
4    10  10  15
4    11  12  17
4    12  14  19
4    13  14  19
4    14  14  19
4    15  14  19
4    16  14  19
4    17  14  19
4    18  16  21
4    19  16  21
4    20  18  23
4    21  18  23
4    22  18  23
23   24  2   26
23   25  4   28
23   26  6   30
23   27  6   30
23   28  6   30
23   29  6   30
###final random set
[1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]
</code></pre>

<p>I hope that solves it. Please let me know if misunderstood the problem in one or another way.</p>
","4357954","4357954","2018-06-02 07:18:04","2","3149","user59271","2014-12-13 19:43:17","171","15","67","0","50593079","","2018-05-29 21:16:52","3","107","<p>As a part of a bigger model, I need to have couple of lines of code that could be implementing the Paralazable system model for me.</p>

<p>The whole model is on a detector. This detector could record signals based on the elements that are arriving. The detectors have this inefficiency that it could lose its sensitivity for a certain amount of time after one element hits it.</p>

<p>In Paralyzable models the detector will be dead when an element hits it, even if it did not detect it. It means that certain number that we have as the dead time, could be changed if another element hit the detector in that time and it will add another dead time to that.  </p>

<p>There is this link that you could read a little more about this topic:
<a href=""https://en.wikipedia.org/wiki/Dead_time"" rel=""nofollow noreferrer"">Dead time</a> </p>

<p>I have made a random Poisson sample using numpy as something that could have the same behavior as the source (The source is making the elements based on Poisson distribution), then due to the fact that the detector could only detect one element at a time, we need to replace all the values more than one with one.</p>

<p>Then the last step is the main part that is actually applying the Paralzable dead time effect, which will remove the values with the dead time distance of the detected values.</p>

<pre><code>import numpy as np
np.random.seed(2)
random_set = np.random.poisson(lam=1, size = 500) #The source is making the elements with Poisson distribution
#lam could be any other value 
d = 2 #dead time, could be any other integers

#Saturation effect
#detector could not detect more than one elements at a time
random_set[random_set&gt;1] = 1

index = 1 
#Paralyzable dead time effect
for i in range(1, (random_set.shape[0])):
    for j in range(index, index + d+1):

        if random_set[j]==1:
            index = j
            random_set[j]=0
</code></pre>

<p>It does not make any errors but it is certainly not doing what I am looking for.
Is there a fast way to make it work?</p>

<p>Thanks.</p>
","8727632","","","Paralyzable systems model in python","<python><numpy><poisson>","2","1","2052"
"50651156","2018-06-01 21:23:20","0","","<p>Try this to get an answer:</p>

<pre><code>&gt;&gt;&gt;d = ('1', '2') 
&gt;&gt;&gt; print(""Value: %s"" %(d))
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
TypeError: not all arguments converted during string formatting
</code></pre>

<p>If we put only-one tuple inside (), it makes a tuple itself:</p>

<pre><code>&gt;&gt;&gt; (d)
('1', '2')
</code></pre>

<p>This means the above print statement will look like: 
print(""Value: %s"" %('1', '2')) which is an error!</p>

<p>Hence:</p>

<pre><code>&gt;&gt;&gt; (d,)
(('1', '2'),)
&gt;&gt;&gt; 
</code></pre>

<p>Above will be fed correctly to the print's arguments.</p>
","9883252","","","0","658","Yogi","2018-06-01 21:23:20","13","7","0","0","1455602","1455623","2009-09-21 17:03:45","116","183102","<p>So, i have this problem.
I got tuple (1,2,3) which i should print with string formatting.
eg.</p>

<pre><code>tup = (1,2,3)
print ""this is a tuple %something"" % (tup)
</code></pre>

<p>and this should print tuple representation with brackets, like</p>

<blockquote>
  <p>This is a tuple (1,2,3)</p>
</blockquote>

<p>But I get <code>TypeError: not all arguments converted during string formatting</code> instead.</p>

<p>How in the world am I able to do this? Kinda lost here so if you guys could point me to a right direction :)</p>
","176683","281545","2014-09-17 21:09:35","Printing tuple with string formatting in Python","<python>","10","0","537"
"50651241","2018-06-01 21:33:14","1","","<p>The problem is with the location of static files. Actually, server can't find the static files needed to render ckeditor's widgets.</p>

<p><strong>The simple solution</strong>:</p>

<p>You need to upload the contents of the static folder inside the ckeditor package into the static folder of your project (albeit manually!).</p>
","4884196","","","0","333","Omid Reza Abbasi","2015-05-10 11:15:00","30","32","103","0","49020840","","2018-02-28 01:58:33","1","835","<p><strong>I tried to install CKeditor but couldn't figure out a way.</strong></p>

<p>Steps I tried</p>

<ul>
<li>pip install django-ckeditor</li>
<li>Add 'ckeditor' to your INSTALLED_APPS setting.</li>
<li>Run the collectstatic</li>
<li>CKEDITOR_BASEPATH = ""/static/ckeditor/ckeditor"" to my settings</li>
</ul>

<p>and </p>

<pre><code>from ckeditor.fields import RichTextField

class MyModel(models.Model):
    myfield = RichTextField()
</code></pre>

<p><strong><em>when I requested the admin page for proper model, I can only get blank white page.
There is no error on terminal. CKeditor files returns 302 HTTP.</em></strong></p>

<p>Screenshot from my admin page</p>

<p><a href=""https://i.stack.imgur.com/8EAtZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8EAtZ.png"" alt=""Screenshot from my admin page""></a></p>

<p><strong><em>it is strange I can see related ckeditor things on DOM.
am I missing something?</em></strong></p>
","6630195","","","Django 2.0.1 with CKEditor doesn't work on admin page","<python><django><static><ckeditor>","3","0","955"
"50651265","2018-06-01 21:35:28","0","","<p>As you found, since <code>NaN</code> is a <code>float</code>, adding <code>NaN</code> to a series may cause it to be either upcasted to <code>float</code> or converted to <code>object</code>. You are right in determining this is not a desirable outcome.</p>

<p>There is no straightforward approach. My suggestion is to store your input row data in a dictionary and combine it with a dictionary of defaults before appending. Note that this works because <code>pd.DataFrame.append</code> accepts a <code>dict</code> argument.</p>

<p>In Python 3.6, you can use the syntax <code>{**d1, **d2}</code> to combine two dictionaries with preference for the second.</p>

<pre><code>default = {'name': '', 'age': 0, 'weight': 0.0, 'has_children': False}

row = {'name': 'Cindy', 'age': 42}

df = df.append({**default, **row}, ignore_index=True)

print(df)

   age  has_children   name  weight
0   45          True    Bob   143.2
1   40          True    Sue   130.2
2   10         False    Tom    34.9
3   42         False  Cindy     0.0

print(df.dtypes)

age               int64
has_children       bool
name             object
weight          float64
dtype: object
</code></pre>
","9209546","","","8","1173","jpp","2018-01-12 14:47:22","109049","18235","7890","3496","50650850","50651265","2018-06-01 20:51:36","2","1256","<p>What's the best way to insert new rows into an existing pandas DataFrame while maintaining column data types and, at the same time, giving user-defined fill values for columns that aren't specified?  Here's an example:</p>

<pre><code>df = pd.DataFrame({
    'name': ['Bob', 'Sue', 'Tom'],
    'age': [45, 40, 10],
    'weight': [143.2, 130.2, 34.9],
    'has_children': [True, True, False]
})
</code></pre>

<p>Assume that I want to add a new record passing just <code>name</code> and <code>age</code>.  To maintain data types, I can copy rows from <code>df</code>, modify values and then append <code>df</code> to the copy, e.g.</p>

<pre><code>columns = ('name', 'age')
copy_df = df.loc[0:0, columns].copy()
copy_df.loc[0, columns] = 'Cindy', 42
new_df = copy_df.append(df, sort=False).reset_index(drop=True)
</code></pre>

<p>But that converts the <code>bool</code> column to an object.</p>

<p>Here's a really hacky solution that doesn't feel like the ""right way"" to do this:</p>

<pre><code>columns = ('name', 'age')
copy_df = df.loc[0:0].copy()

missing_remap = {
    'int64': 0,
    'float64': 0.0,
    'bool': False,
    'object': ''
}
for c in set(copy_df.columns).difference(columns)):
    copy_df.loc[:, c] = missing_remap[str(copy_df[c].dtype)]

new_df = copy_df.append(df, sort=False).reset_index(drop=True)
new_df.loc[0, columns] = 'Cindy', 42
</code></pre>

<p>I know I must be missing something.</p>
","510151","","","Insert rows into pandas DataFrame while maintaining column data types","<python><pandas><dataframe><append>","2","0","1420"
"50651267","2018-06-01 21:35:30","-1","","<p>You need to change your pattern so that you don't have two capturing groups. When you have two groups, you'll get a 2-tuple of matching strings, even if one of them is empty.</p>

<p>An easy fix is to make your groups non-capturing, with <code>(?: )</code> instead of plain <code>( )</code>:</p>

<pre><code>example_re = re.compile(r'(?:\(.*?\))|(?:\[.*?\])')
</code></pre>
","1405065","1405065","2018-06-01 21:42:24","9","377","Blckknght","2012-05-19 12:23:30","70324","3947","2807","517","50651146","","2018-06-01 21:22:36","-1","230","<p>I apologize if this is a repeated question, but I tried for a while and couldn't figure out what search to use. </p>

<p>Previously, I had a regular expression:</p>

<pre><code>example_re = re.compile(r'[\(\[].*?[\)\]]')
</code></pre>

<p>which was supposed to capture (text that looked like this) and [this], but would also incorrectly capture text that [looked like this).</p>

<p>I fixed it:</p>

<pre><code>example_re = re.compile(r'(\(.*?\))|(\[.*?\])')
</code></pre>

<p>but now, when I call example_re.findall(text), tests are breaking because where the first expression returns a list of strings, the second returns a list of tuples, whenever there is a nested expression [like (this)].</p>

<p>How do I fix this so that findall returns only have the outermost match?</p>

<p><strong>Edit:</strong> Whoever marked this question as a duplicate really isn't helping anybody. The title of the question that this is supposedly a duplicate of is 'Python re.findall behaves weird'. How was I (or anyone else) supposed to find that? Just by virtue of the fact that I phrased the question differently makes it a non-duplicate.</p>
","5009004","5009004","2018-06-07 19:27:21","How do you only capture outermost matches (throw away groups) in python regex?","<python><regex>","1","6","1134"
"50651288","2018-06-01 21:38:00","0","","<p>is because he's using django 2 
According to the <a href=""https://docs.djangoproject.com/en/2.0/ref/urls/"" rel=""nofollow noreferrer"">documentation</a> you can use 
<a href=""https://docs.djangoproject.com/en/2.0/ref/urls/"" rel=""nofollow noreferrer"">path and re_path</a> example :</p>

<pre><code>from django.urls import path , re_path

urlpatterns = [
     path('tinymce/', include('tinymce.urls')),

    # or
    #path('tinymce/', include('tinymce.urls')),

]
</code></pre>
","8168045","","","0","477","NEFEGAGO","2017-06-15 19:06:43","128","20","19","0","50636257","50636302","2018-06-01 04:48:46","0","1099","<p>I am trying to get TinyMCE working in Django. Here is what I did:</p>

<ul>
<li>Using this package as a reference: <a href=""https://github.com/romanvm/django-tinymce4-lite"" rel=""nofollow noreferrer"">django-tinymce4-lite</a></li>
<li>Successfully ran <code>pip install django-tinymce4-lite</code>; package installs fine</li>
<li>Added tinymce to INSTALLED_APPS in settings.py </li>
</ul>

<p>Then here it gets tricky:</p>

<pre><code>Add tinymce.urls to urls.py for your project:

urlpatterns = [
    ...
    url(r'^tinymce/', include('tinymce.urls')),
    ...
]
</code></pre>

<p>When I do this, I get this error:</p>

<pre><code>url(r'^tinymce/', include('tinymce.urls')),  
NameError: name 'url' is not defined
</code></pre>

<p>I have tried the following:</p>

<ul>
<li>Restarting django</li>
<li>Instead of placing this in my project's urls.py I have tried my app's urls.py</li>
<li>I have tried to convert this to ""<em>path('tinymce/', include('tinymce.urls')),</em>"" because all other entries use 'path' and not 'url', but that didn't work either (ModuleNotFoundError: No module named 'tinymce.urls)</li>
<li>I have tried <a href=""https://github.com/aljosa/django-tinymce"" rel=""nofollow noreferrer"">another tinymce plugin</a> </li>
</ul>

<p>None of this helped. Any suggestions? </p>

<p><strong>UPDATE</strong></p>

<p>As per the suggestions, I updated url to path. Now I have a new error:</p>

<pre><code>ModuleNotFoundError: No module named 'tinymce.urls'
</code></pre>

<p>Here is my urls.py:</p>

<pre><code>from django.urls import include, path
from django.contrib import admin

from django.conf import settings
from django.conf.urls.static import static

urlpatterns = [
    path('', include('core.urls')),
    path('tinymce/', include('tinymce.urls')),
]

if settings.DEBUG:
    urlpatterns += static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)

urlpatterns += static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)
</code></pre>

<p>This error made me doubt if I had installed the plugin correctly. But it seems I have:</p>

<pre><code>pip install django-tinymce4-lite
Requirement already satisfied: django-tinymce4-lite in /usr/local/lib/python3.6/site-packages
Requirement already satisfied: Django&gt;=1.8.0 in /usr/local/lib/python3.6/site-packages (from django-tinymce4-lite)
Requirement already satisfied: jsmin in /usr/local/lib/python3.6/site-packages (from django-tinymce4-lite)
Requirement already satisfied: pytz in /usr/local/lib/python3.6/site-packages (from Django&gt;=1.8.0-&gt;django-tinymce4-lite)
</code></pre>
","","","2018-06-01 04:56:57","Django and TinyMCE: NameError: name 'url' is not defined","<python><django><tinymce>","3","1","2568"
"50651294","2018-06-01 21:38:22","4","","<p>I don't know you data size or if you require a built-in solution or not.</p>

<p>However, one simple solution involves using <code>pandas</code>. If you have</p>

<pre><code>mydata = [""0:00:00.618000"",
          ""0:00:00.593000"",
          ""0:00:00.569000"",
          ""0:00:00.572000"",
          ""0:00:00.636000"",
          ""0:00:01"",
          ""0:00:01"",
          ""0:00:00.546000"",
          ""0:00:00.400000""]
</code></pre>

<p>You can use <code>pd.to_timedelta</code> and <code>mean</code> and just do</p>

<pre><code>pd.Series(pd.to_timedelta(mydata)).mean()
</code></pre>

<p>Might be/might not be overkill, but its really readable and straightforward.</p>
","2535611","","","3","665","rafaelc","2013-06-30 01:12:12","38531","2317","1924","318","50651216","","2018-06-01 21:30:21","-2","311","<p>My data is in the format of HH:MM:SS.milliseconds. How can I calculate average of such data in Python? I need average in form of miliseconds. I looked at several other similar postings but they do not answer my question.</p>

<pre><code>My data =  0:00:00.618000
           0:00:00.593000
           0:00:00.569000
           0:00:00.572000
           0:00:00.636000
           0:00:01
           0:00:01
           0:00:00.546000
           0:00:00.400000
</code></pre>
","8280454","9209546","2018-06-01 21:57:33","How to calculate average of seconds & milliseconds in Python?","<python><python-2.7><timedelta>","5","6","474"
"50651310","2018-06-01 21:40:25","1","","<p>I'm assuming that each of these is a string, you can do the following using no libraries in both python 2 and 3</p>

<pre><code>def mean(numbers):
    return float(sum(numbers)) / max(len(numbers), 1)

def timestamp_to_millis(timestamp):
    hour, min, sec = map(float, timestamp.split(':'))
    mills = (((hour * 60 + min) * 60) + sec) * 1000
    return millis


my_data = # [ timestamps ... ]
my_mean = mean(map(timestamp_to_millis, my_data))
</code></pre>
","3033441","3033441","2018-06-01 21:46:02","0","462","Lakshay Garg","2013-11-25 17:52:06","1693","305","604","1","50651216","","2018-06-01 21:30:21","-2","311","<p>My data is in the format of HH:MM:SS.milliseconds. How can I calculate average of such data in Python? I need average in form of miliseconds. I looked at several other similar postings but they do not answer my question.</p>

<pre><code>My data =  0:00:00.618000
           0:00:00.593000
           0:00:00.569000
           0:00:00.572000
           0:00:00.636000
           0:00:01
           0:00:01
           0:00:00.546000
           0:00:00.400000
</code></pre>
","8280454","9209546","2018-06-01 21:57:33","How to calculate average of seconds & milliseconds in Python?","<python><python-2.7><timedelta>","5","6","474"
"50651318","2018-06-01 21:41:15","2","","<p>The first step is to parse all those timestamps into something that you can perform arithmetic on. This can be <a href=""https://docs.python.org/3/library/datetime.html#timedelta-objects"" rel=""nofollow noreferrer""><code>timedelta</code></a> objects, or integer microseconds (or milliseconds, since your times all have 0 micros), or float seconds, or any other reasonable type.</p>

<p>For example, assuming that input is one big string:</p>

<pre><code>ts = []
for h, m, s, u in re.findall(r'(\d+):(\d+):(\d+)(?:\.(\d+))?', bigstring):
    h, m, s = int(h), int(m), int(s)
    u = int(u) if u else 0
    ts.append(datetime.timedelta(hours=h, minutes=m, seconds=s, microseconds=u))
</code></pre>

<p>If it's a list of strings, or a file object, etc. just change it to iterate that and do <code>re.search</code> on each one, instead of iterating <code>re.findall</code>.</p>

<p>Then we can average them the same as any other values:</p>

<pre><code>sum(ts, datetime.timedelta()) / len(ts)
</code></pre>

<p>Since I used <code>timedelta</code> values, that's what the result will be:</p>

<pre><code>datetime.timedelta(0, 0, 659333)
</code></pre>

<p>… or, if you <code>print</code> it:</p>

<pre><code>0:00:00.659333
</code></pre>

<p>… or, if you want it as, say, a number of seconds, just call its <code>total_seconds()</code> method:</p>

<pre><code>0.659333
</code></pre>
","908494","","","0","1377","abarnert","2011-08-23 20:55:30","272076","19605","5576","4509","50651216","","2018-06-01 21:30:21","-2","311","<p>My data is in the format of HH:MM:SS.milliseconds. How can I calculate average of such data in Python? I need average in form of miliseconds. I looked at several other similar postings but they do not answer my question.</p>

<pre><code>My data =  0:00:00.618000
           0:00:00.593000
           0:00:00.569000
           0:00:00.572000
           0:00:00.636000
           0:00:01
           0:00:01
           0:00:00.546000
           0:00:00.400000
</code></pre>
","8280454","9209546","2018-06-01 21:57:33","How to calculate average of seconds & milliseconds in Python?","<python><python-2.7><timedelta>","5","6","474"
"50651341","2018-06-01 21:45:02","1","","<p>It looks like the input consists of strings. Those should be converted to <a href=""https://docs.python.org/3/library/datetime.html#datetime-objects"" rel=""nofollow noreferrer""><code>datetime.datetime</code></a> objects. Use <a href=""https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime"" rel=""nofollow noreferrer""><code>datetime.datetime.strptime</code></a> to do that.</p>

<p>After that, the average of anything is calculated as <code>sum(values) / len(values)</code>, but unfortunately you cannot sum dates. What you can is sum date differences, so you'll have to make some conversions</p>

<p>For example:</p>

<pre><code>dates = [datetime.datetime(1951, 1, 5),
         datetime.datetime(1951, 1, 7),
         datetime.datetime(1951, 1, 7)]

base_datetime = datetime.datetime.now()  # really, anything

relative_dates = [d-base_datetime for d in dates]

average_relative_datetime = sum(relative_dates, datetime.timedelta()) / len(relative_dates)

result = base_datetime + average_relative_datetime  # datetime.datetime(1951, 1, 6, 8, 0)
</code></pre>
","389289","","","0","1079","zvone","2010-07-12 08:25:02","11303","980","601","663","50651216","","2018-06-01 21:30:21","-2","311","<p>My data is in the format of HH:MM:SS.milliseconds. How can I calculate average of such data in Python? I need average in form of miliseconds. I looked at several other similar postings but they do not answer my question.</p>

<pre><code>My data =  0:00:00.618000
           0:00:00.593000
           0:00:00.569000
           0:00:00.572000
           0:00:00.636000
           0:00:01
           0:00:01
           0:00:00.546000
           0:00:00.400000
</code></pre>
","8280454","9209546","2018-06-01 21:57:33","How to calculate average of seconds & milliseconds in Python?","<python><python-2.7><timedelta>","5","6","474"
"50651353","2018-06-01 21:46:14","1","","<p>Imho you can do this with supervised learning, no unsupervised learning (=clustering) needed.</p>

<ol>
<li><p>Transform the feature <code>Event Date</code> to <code>number of days since start</code> by considering the earliest date for each unitID as the first day (at this day this feature would be 0). </p></li>
<li><p>Transform the features <code>Model</code> and <code>SoftwareVersion</code> to one-hot-coded, categorial features.</p></li>
<li><p>Normalize all numerical features so the values are floating point numbers in a range of [0, 1] or [-1, 1] or something in that value range.</p></li>
<li><p>Make a set of all unitIDs.</p></li>
<li><p>Remove 10% of that set and name it ""validation set"".</p></li>
<li><p>Remove another 10% of that set and name it ""test set"". Name the remaining 80% ""training set"".</p></li>
<li><p>Compute a model using the entries belonging to the <code>unitID</code>s in the training set. Use a neural network if you have at least 10-100k entries, or a gaussian process if you have less than that. The model takes the features <code>number of days since start</code>, <code>Model</code>, <code>YearsOwned</code> and <code>SoftwareVersion</code> as inputs and targets to predict <code>energyConsumptionRate</code>. </p></li>
<li><p>Do early stopping regarding the validation set (in case you use NNs).</p></li>
<li><p>Apply the resulting model on all entries belonging to the <code>unitID</code>s from the test set.</p></li>
<li><p>Do this 10-fold so every data point has been in the <code>test set</code> once and your table with the test-set-based model predictions from step <code>9</code> covers every <code>unitID</code>.</p></li>
<li><p>Make a query to find all <code>unitID</code>s where the actual <code>energyConsumptionRate</code> is considerably higher than what the models say.</p></li>
</ol>

<p>You probably only need a very tiny model for that, with few free parameters. Maybe you can also achieve your goal with a sophisticated SQL query and basic statistics, not using machine learning at all.</p>
","2068759","2068759","2018-06-01 21:51:24","1","2051","schreon","2013-02-13 14:49:12","845","65","56","3","50650066","","2018-06-01 19:39:02","-1","55","<p>I have a time series dataset, which contains the energy consumption of a product by each unit over time. It also contains the unit dimentions like model, years of usage, product version etc.</p>

<p>We want to find clusters of units which consistently shows higher consumption. </p>

<p>The dataset looks like this:</p>

<pre><code>EventDate, UnitID, energyConsumptionRate, Model, YearsOwned, SoftwareVersion

5/1/2018      100            103             M3            1            2.1

5/2/2018      100             42             M3            1            2.1

5/3/2018      100             78             M3            1            2.1
....
</code></pre>

<p>One each day, the unit will report one event. Where should I start?</p>

<p>-Ch</p>
","9882963","1848654","2018-06-01 20:08:50","How to handle this in ML mind?","<python><machine-learning>","1","10","750"
"50651379","2018-06-01 21:49:43","0","","<p>How do you create file in python? Can you please show full example of your code? I am also working on Mac, but no issues. Please see one of simple examples below on how it works:</p>

<p>create file:</p>

<pre><code>file = open('test9.txt', 'w')
file.write('first \n second line \n aaa')
file.close()
</code></pre>

<p>read file:</p>

<pre><code>with open('test9.txt') as f:
    lines = f.readlines()
    print(lines)
</code></pre>

<p>created file is a plain text document with .txt</p>
","8974516","","","1","491","Blacho","2017-11-20 22:31:04","42","6","10","0","50650837","","2018-06-01 20:50:04","0","274","<p>I write some files as .txt with python3.6 in mac. And then, when I try to read them using:</p>

<pre><code>f = open(...,'r')
lines = f.readlines()
</code></pre>

<p>I got this error: </p>

<blockquote>
  <p>UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe8 in position 10: invalid continuation byte</p>
</blockquote>

<p>Can anyone help? 
Thank you.</p>
","8935585","3483203","2018-06-01 20:51:17","UnicodeDecodeError in mac with python3 when reading .txt files","<python><python-3.x>","1","0","364"
"50651382","2018-06-01 21:50:39","2","","<p>Here's what I managed to come up with:</p>

<p>Import pandas and setup the dataframe</p>

<pre><code>import pandas as pd
df = pd.DataFrame(list(range(91)), pd.date_range('2015-04-01', '2015-6-30'), columns=['foo']).resample('B')
</code></pre>

<p>Start with a pure list of marker dates, since I'm guessing that what you're really starting with:</p>

<pre><code>marker_dates = [
    pd.to_datetime('2015-04-17', format='%Y-%m-%d'),
    pd.to_datetime('2015-05-18', format='%Y-%m-%d'),
    pd.to_datetime('2015-06-19', format='%Y-%m-%d')
]
marker_df = pd.DataFrame([], columns=['marker', 'start', 'end', 'avg'])
marker_df['marker'] = marker_dates
</code></pre>

<p>For the case where you want to just test ranges, input the start and end manually here instead of calculating it. If you want to change the range you can change the arguments to shift():</p>

<pre><code>marker_df['start'] = df.index.shift(-1)[df.index.isin(marker_df['marker'])]
marker_df['end'] = df.index.shift(1)[df.index.isin(marker_df['marker'])]
</code></pre>

<p>Finally, use DataFrame.apply() to do a row by row calculation of averages:</p>

<pre><code>marker_df.apply(
    lambda x: df[(x['start'] &lt;= df.index) &amp; (df.index &lt;= x['end'])]['foo'].mean(), 
    axis=1
)
</code></pre>

<p>Which gives us this result:</p>

<pre><code>      marker      start        end        avg
0 2015-04-17 2015-04-16 2015-04-20  17.000000
1 2015-05-18 2015-05-15 2015-05-19  46.666667
2 2015-06-19 2015-06-18 2015-06-22  80.000000
</code></pre>
","8239386","","","0","1511","John Aaron","2017-06-30 22:35:47","163","11","15","6","50528475","50559402","2018-05-25 11:45:03","5","315","<p>I have a pandas dataframe with a business-day-based DateTimeIndex.  For each month that's in the index, I also have a single 'marker' day specified.  </p>

<p>Here's a toy version of that dataframe:</p>

<pre><code># a dataframe with business dates as the index
df = pd.DataFrame(list(range(91)), pd.date_range('2015-04-01', '2015-6-30'), columns=['foo']).resample('B').last()

# each month has an single, arbitrary marker day specified
marker_dates = [df.index[12], df.index[33], df.index[57]]
</code></pre>

<p>For each month in the index, I need to calculate average of the <code>foo</code> column  in specific slice of rows in that month.  </p>

<p>There are two different ways I need to be able to specify those slices:</p>

<p><strong>1)  m'th day to n'th day.</strong>  </p>

<p>Example might be (2rd to 4th business day in that month).  So april would be the average of 1 (apr2), 4 (apr3), and 5 (apr 6) = 3.33.  May would be 33 (may 4), 34 (may 5), 35 (may 6) = 34.    I don't consider the weekends/holidays that don't occur in the index as days.</p>

<p><strong>2) m'th day before/after the marker date to the n'th day before/after the marker date.</strong> </p>

<p>Example might be ""average of the slice from 1 day before the marker date to 1 day after the marker date in each month""   Eg. In April, the marker date is 17Apr.   Looking at the index, we want the average of apr16, apr17, and apr20.</p>

<p>For Example 1, I had an ugly solution that foreach month I would slice the rows of that month away, and then apply   <code>df_slice.iloc[m:n].mean()</code></p>

<p>Whenever I start doing iterative things with pandas, I always suspect I'm doing it wrong.  So I imagine there is a cleaner, pythonic/vectorized way to make this result for all the months</p>

<p>For Example 2, I don't not know a good way to do this slice-averaging based on arbitrary dates across many months.</p>
","3556757","","","Tricky slicing specifications on business-day datetimeindex","<python><pandas>","4","2","1899"
"50651388","2018-06-01 21:51:04","3","","<p>The difference is not int the type of the object, but in what your code does to it.</p>

<p>There is a big difference between these two:</p>

<pre><code>self.tricks = trick
</code></pre>

<p>and:</p>

<pre><code>self.tricks.append(trick)
</code></pre>

<p>The first one <code>self.tricks = trick</code> assigns a value to attribute <code>tricks</code> of <code>self</code>.</p>

<p>The second one <code>self.tricks.append(trick)</code> retrieves <code>self.tricks</code> and calls a method on it (which here modifies its values).</p>

<hr>

<p>The problem, in your case, is that there is no <code>tricks</code> defined on <code>self</code> instance, so <code>self.tricks.append</code> gets the <code>tricks</code> attribute of the class and modifies it, but <code>self.tricks = ...</code> creates a new attribute on <code>self</code> instead.</p>

<p>The fact that one of them is a string and the other is a list is not really relevant. It would be the same if both were lists.
<em>Note that they could not both be strings because strings are immutable and thus have no append method</em></p>

<h2>How to fix it?</h2>

<p>This is wrong:</p>

<pre><code>def add_trick(self, trick):
    self.tricks = trick
</code></pre>

<p>If <code>tricks</code> is a class attribute, <code>add_trick</code> should be a class method:</p>

<pre><code>@classmethod
def add_trick(cls, trick):
    cls.tricks = trick
</code></pre>

<p>If there are reasons for <code>add_trick</code> to be an instance method, then simply do this:</p>

<pre><code>def add_trick(self, trick):
    DogStr.tricks = trick
</code></pre>
","389289","389289","2018-06-01 21:57:15","1","1596","zvone","2010-07-12 08:25:02","11303","980","601","663","50651364","50651388","2018-06-01 21:47:18","1","43","<p>Say that I have the following Python code:</p>

<pre><code>import sys

class DogStr:
    tricks = ''

    def add_trick(self, trick):
        self.tricks = trick

class DogList:
    tricks = []

    def add_trick(self, trick):
        self.tricks.append(trick)

# Dealing with DogStr
d = DogStr()
e = DogStr()
d.add_trick('trick d')
e.add_trick('trick e')
print(d.tricks)
print(e.tricks)

# Dealing with DogList
d = DogList()
e = DogList()
d.add_trick('trick d')
e.add_trick('trick e')
print(d.tricks)
print(e.tricks)
</code></pre>

<p>Running this code with Python 3.6.5, I get the following output:</p>

<pre><code>trick d
trick e
['trick d', 'trick e']
['trick d', 'trick e']
</code></pre>

<p>The difference between DogStr and DogList is that I treat <code>tricks</code> as a string on former and as a list on the latter.</p>

<p>When dealing with DogStr, tricks is behaving as an instance variable. BUT with DogList tricks is behaving as a class variable.</p>

<p>I was expecting to see the same behaviour on both calls, i.e.: if the two last lines of the output are identical, so should be the first two.</p>

<p>So I wonder. What is the explanation for that?</p>
","408195","","","Why the Python scope seems to behave differently for list variable and string variable?","<python><python-3.x><programming-languages>","1","5","1173"
"50651396","2018-06-01 21:51:58","0","","<p>I created a snippet which adds an onclick-event to each of your buttons. If clicked, the script will check how many buttons are behind the clicked one and returns the number within an alert.
I actually don't know how you're using Python within JS, therefore I used only plain old JS.</p>

<p><a href=""https://jsfiddle.net/Syndesi/9wh0rvn7/"" rel=""nofollow noreferrer"">fiddle</a></p>

<pre><code>// code from the fiddle
var buttons = document.getElementsByClassName('KUBKM');
for (var i = 0, len = buttons.length; i &lt; len; i++) {
    buttons[i].onclick = function(e){
        var li = e.target.parentNode.parentNode;
        var i = 0;
        while(li != null){
            li = li.nextSibling;
            if(li != null){
                if(li.tagName == 'LI'){
                    i++;
                }
            }
        }
        //console.log('Button '+i+' was clicked');
        alert('Button '+i+' was clicked');
    }
}
</code></pre>
","4417327","","","8","951","Syndesi","2015-01-04 13:39:23","5","3","0","0","50650673","","2018-06-01 20:34:18","1","251","<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>    &lt;ul&gt;
      &lt;li&gt;
        &lt;span class=""_1OSdk""&gt;
          &lt;button class=""_5f5mN    -fzfL    KUBKM      yZn4P   ""&gt;Segui già&lt;/button&gt;
        &lt;/span&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;span class=""_1OSdk""&gt;
          &lt;button class=""_5f5mN    -fzfL    KUBKM      yZn4P   ""&gt;Segui già&lt;/button&gt;
        &lt;/span&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;span class=""_1OSdk""&gt;
          &lt;button class=""_5f5mN    -fzfL    KUBKM      yZn4P   ""&gt;Segui già&lt;/button&gt;
        &lt;/span&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;span class=""_1OSdk""&gt;
          &lt;button class=""_5f5mN    -fzfL    KUBKM      yZn4P   ""&gt;Segui già&lt;/button&gt;
        &lt;/span&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;span class=""_1OSdk""&gt;
          &lt;button class=""_5f5mN    -fzfL    KUBKM      yZn4P   ""&gt;Segui già&lt;/button&gt;
        &lt;/span&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;span class=""_1OSdk""&gt;
          &lt;button class=""_5f5mN    -fzfL    KUBKM      yZn4P   ""&gt;Segui già&lt;/button&gt;
        &lt;/span&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;span class=""_1OSdk""&gt;
          &lt;button class=""_5f5mN    -fzfL    KUBKM      yZn4P   ""&gt;Segui già&lt;/button&gt;
        &lt;/span&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;span class=""_1OSdk""&gt;
          &lt;button class=""_5f5mN    -fzfL    KUBKM      yZn4P   ""&gt;Segui già&lt;/button&gt;
        &lt;/span&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;span class=""_1OSdk""&gt;
          &lt;button class=""_5f5mN    -fzfL    KUBKM      yZn4P   ""&gt;Segui già&lt;/button&gt;
        &lt;/span&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;span class=""_1OSdk""&gt;
          &lt;button class=""_5f5mN    -fzfL    KUBKM      yZn4P   ""&gt;Segui già&lt;/button&gt;
        &lt;/span&gt;
      &lt;/li&gt;
    &lt;/ul&gt;</code></pre>
</div>
</div>
</p>

<p>so this code display a list of buttons, i need a code that let me click from the last button to the first button, in the code that i wrote i found the buttons by css_selector that it's equal for all the buttons.
The code that i wrote goes good but it click from the first button to the last button and i don't know how to invert this.</p>

<p>The code that i wrote is this </p>

<pre><code>    from selenium import webdriver
    import time
    from selenium.webdriver.common.keys import Keys
    from selenium.webdriver.firefox.options import Options
    from selenium.common.exceptions import NoSuchElementException
    from selenium.webdriver.support import expected_conditions
    from selenium.webdriver import ActionChains
    while a==0 :
    try:
        #GO TO THE BOTTOM OF THE TABLE
        driver.find_element_by_xpath (""/html/body/div[3]/div/div[2]/div"").click ()
        elem = driver.find_element_by_css_selector ("".j6cq2"")
        elem.send_keys (Keys.ARROW_DOWN)
        elem.send_keys (Keys.ARROW_DOWN)
        time.sleep (2)
        elem.send_keys (Keys.ARROW_DOWN)
        elem.send_keys (Keys.ARROW_DOWN)
        time.sleep (2)
        elem.send_keys (Keys.ARROW_DOWN)
        elem.send_keys (Keys.ARROW_DOWN)
        time.sleep (2)

        #SEARCH FOR THE BUTTON THAT IT HAS TO CLICK
        Segui_Già = driver.find_element_by_css_selector(""._5f5mN.-fzfL.KUBKM.yZn4P"")

        #IF THE BUTTON IT'S NOT FOUND IT SCROLL UP THE TABLE
    except NoSuchElementException :
        driver.find_element_by_xpath (""/html/body/div[3]/div/div[2]/div"").click ()
        elem = driver.find_element_by_css_selector ("".j6cq2"")
        elem.send_keys (Keys.ARROW_UP)
        elem.send_keys (Keys.ARROW_UP)

        #IF THE BUTTON IT'S FOUND IT CLICKS THE THE FIRST BUTTON THAT HE FOUNDS
    else:
        Segui_Già.click()
        a=a+1
        print(a)
        time.sleep(5)
</code></pre>

<p>i have a table with all this buttons displayed and i need that the code click from the last button to the first.
This code click from the first button to the last so the reverse of what i need.
(On the site where i have to run this code when a button it's clicked it changes the css_selector so the code that i wrote doesn't stop on the first element for this)</p>
","9859146","9859146","2018-06-02 12:16:46","Random click button","<python><selenium>","2","2","4471"
"50651431","2018-06-01 21:57:22","1","","<p>Here's one approach using <code>datetime.timedelta</code>. The tricky part is converting strings into <code>timedelta</code> objects. Sequence unpacking makes this easier and efficient to implement.</p>

<pre><code>from datetime import timedelta

data = ['0:00:00.618000', '0:00:00.593000', '0:00:00.569000',
        '0:00:00.572000', '0:00:00.636000', '0:00:01',
        '0:00:01', '0:00:00.546000', '0:00:00.400000']

def converter(x):
    if '.' not in x:
        x += '.000000'
    hrs, mins, secs, millis = map(int, x[:-3].replace('.', ':').split(':'))
    return timedelta(hours=hrs, minutes=mins, seconds=secs, milliseconds=millis)

res = sum(map(converter, data), timedelta(0)) / len(data)

print(res)

0:00:00.659333
</code></pre>

<p>Note that <code>sum</code> only appears to work with <code>timedelta</code> objects with the added <code>timedelta(0)</code> argument, this trick <a href=""https://stackoverflow.com/a/3617540/9209546"">courtesy of @JochenRitzel</a>.</p>
","9209546","","","0","982","jpp","2018-01-12 14:47:22","109049","18235","7890","3496","50651216","","2018-06-01 21:30:21","-2","311","<p>My data is in the format of HH:MM:SS.milliseconds. How can I calculate average of such data in Python? I need average in form of miliseconds. I looked at several other similar postings but they do not answer my question.</p>

<pre><code>My data =  0:00:00.618000
           0:00:00.593000
           0:00:00.569000
           0:00:00.572000
           0:00:00.636000
           0:00:01
           0:00:01
           0:00:00.546000
           0:00:00.400000
</code></pre>
","8280454","9209546","2018-06-01 21:57:33","How to calculate average of seconds & milliseconds in Python?","<python><python-2.7><timedelta>","5","6","474"
"50651437","2018-06-01 21:58:06","2","","<p>This is how I usually do it:</p>

<pre><code>&gt;&gt;&gt; import multiprocessing
... q = multiprocessing.Queue()
... q.get_nowait()
... 
... 
---------------------------------------------------------------------------
Empty                                     Traceback (most recent call last)
&lt;...snip...&gt;
&gt;&gt;&gt; import sys
&gt;&gt;&gt; err = sys.last_value
&gt;&gt;&gt; err.__module__
'queue'
&gt;&gt;&gt; from queue import Empty
&gt;&gt;&gt; isinstance(err, Empty)
True
</code></pre>

<p>There is no foolproof way that works for all modules in the generic case, because they don't usually know (or care) about all their dependencies exception hierarchy. An exception in 3rd party code would just bubble up the stack, and there is generally no point to catch it unless one can actually do something to handle it and continue. Good projects will usually document the exception hierarchy clearly in their API guide.</p>
","674039","674039","2018-06-01 22:06:06","0","935","wim","2011-03-23 23:40:27","187587","12233","9064","5087","50651378","50651437","2018-06-01 21:49:37","2","58","<p>Python modules often have their own exceptions. I often find myself wanting to import those exceptions to be able to properly catch them (properly as in not just cacthing <code>Exception</code> and hoping for the best).</p>

<p>However, I often find myself spending time to figure out exactly where in a module the exceptions are located, or if they're imported from another module, etc. I'm curious if there's a general way to find this out, i.e. given <code>SomeModulespecificException</code> is there a simple way to figure out how to import it?</p>

<p>Here's an example from the multiprocessing module:</p>

<pre><code>import multiprocessing
q = multiprocessing.Queue()
q.get_nowait()
</code></pre>

<p>The above code raises an <code>Empty</code> Exception. In this case, I found out from <a href=""https://stackoverflow.com/questions/13941562/why-can-i-not-catch-a-queue-empty-exception-from-a-multiprocessing-queue#answer-13941865"">this answer</a> that the exception is imported from the <code>Queue</code> module, so in this particular case, you need <code>from Queue import Empty</code> to import the exception.</p>

<p>Is there an easy way to figure this out in the general case?</p>
","3752268","","","How to import and catch module-specific exceptions in Python?","<python>","1","1","1196"
"50651441","2018-06-01 21:58:42","2","","<p>Just remove credentials.json file from the folder where you run this script from and rerun your script with the final desired list of scopes such as SCOPES = ['<a href=""https://www.googleapis.com/auth/gmail.send"" rel=""nofollow noreferrer"">https://www.googleapis.com/auth/gmail.send</a>', '<a href=""https://www.googleapis.com/auth/gmail.labels"" rel=""nofollow noreferrer"">https://www.googleapis.com/auth/gmail.labels</a>']. Authorize, at the resulting pop up browser window, the list of scopes for the desired gmail account. Then you should be all set.</p>

<p>Background story: It is likely you had authorized less permissive scopes (such as readonly) via the browser screen that popped up when your run_flow() ran first time. This would have created a file called credentials.json in the folder where you ran the script. This credentials.json is not again regenerated per your script above. It is just frozen with the first time scope. Just for curiosity, you can open and read the credentials.json and look for the key 'scopes'. Of course, you cannot add more scopes in this file manually and expect newer scopes to work. This has to be regenerated for different set of scopes if any.</p>

<p>Unrelated cautionary note: Avoid the too permissive scope <a href=""https://mail.google.com/"" rel=""nofollow noreferrer"">https://mail.google.com/</a> if at all possible:)</p>
","4398837","4398837","2018-06-01 22:32:53","0","1370","SoundaR","2014-12-28 00:30:05","170","20","5","0","50531623","50651441","2018-05-25 14:34:54","1","1098","<p>i am trying to run the gmail API to get email from a specif user. 
However i always receive the following error:
googleapiclient.errors.HttpError: https://www.googleapis.com/gmail/v1/users/me/settings/filters?alt=json returned ""Insufficient Permission""></p>

<p>I have tried all the different options suggested from changing the scope to enabling the less secure applications in the gmail setting.
The last thing is that my code could be wrong:</p>

<pre><code>from __future__ import print_function
from googleapiclient import discovery
from googleapiclient.discovery import build
from httplib2 import Http
from oauth2client import file, client, tools

# Setup the Gmail API
SCOPES = 'https://mail.google.com/'
store = file.Storage('credentials.json')
creds = store.get()
if not creds or creds.invalid:
    flow = client.flow_from_clientsecrets('client_secret.json', SCOPES)
    creds = tools.run_flow(flow, store)
service = build('gmail', 'v1', http=creds.authorize(Http()))

# Call the Gmail API
    # results = service.users().labels().list(userId='me').execute()
    # labels = results.get('labels', [])
    # if not labels:
    #     print('No labels found.')
    # else:
    #     print('Labels:')
    #     for label in labels:
    #         print(label['name'])
label_id = 'mine' # ID of user label to add
filter = {
    'criteria': {
        'from': 'string@test.com'
    }#,
    #'action': {
        #'addLabelIds': [label_id],
        #'removeLabelIds': ['INBOX']
    #}
}
result = service.users().settings().filters().create(userId='me', body=filter).execute()
print ('Created filter: %s' % result.get('id'))
</code></pre>

<p>Could please help me?</p>

<p>Thanks</p>
","7430063","","","403 insufficient permission for gmail api with pythin","<python><api><gmail>","2","0","1683"
"50651452","2018-06-01 22:00:12","1","","<p>This should help. ->Using a simple iterattion, <code>str.startswith</code> and <code>str.split</code></p>

<p><strong>Demo:</strong></p>

<pre><code>res = []
with open(filename, ""r"") as infile:
    for line in infile:
        if line.startswith(""&gt;""):
            val = line.split(""|"")
            res.append(val[5])
print(res)
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>['RNF14', 'GAPDH', 'ACTB', 'HELLE']
</code></pre>

<p><em>In you code Replace</em> </p>

<pre><code>res = [el[5:] for s in k for el in s.split(""|"")]
</code></pre>

<p>with</p>

<pre><code>res = [s.split(""|"")[5] for s in k ] #Should work.
</code></pre>
","532312","532312","2018-06-01 22:09:39","3","644","Rakesh","2010-12-06 13:07:54","56694","5302","758","1508","50651429","","2018-06-01 21:56:53","0","35","<p>I have a text file like this:</p>

<pre><code>&gt;ENST00000511961.1|ENSG00000013561.13|OTTHUMG00000129660.5|OTTHUMT00000370661.3|RNF14-003|RNF14|278
MSSEDREAQEDELLALASIYDGDEFRKAESVQGGETRIYLDLPQNFKIFVSGNSNECLQNSGFEYTICFLPPLVLNFELPPDYPSSSPPSFTLSGKWLSPTQLSALCKHLDNLWEEHRGSVVLFAWMQFLKEETLAYLNIVSPFELKIGSQKKVQRRTAQASPNTELDFGGAAGSDVDQEEIVDERAVQDVESLSNLIQEILDFDQAQQIKCFNSKLFLCSICFCEKLGSECMYFLECRHVYCKACLKDYFEIQIRDGQVQCLNCPEPKCPSVATPGQ
&gt;ENST00000506822.1|ENSG00000013561.13|OTTHUMG00000129660.5|OTTHUMT00000370662.1|RNF14-004|GAPDH|132
MSSEDREAQEDELLALASIYDGDEFRKAESVQGGETRIYLDLPQNFKIFVSGNSNECLQNSGFEYTICFLPPLVLNFELPPDYPSSSPPSFTLSGKWLSPTQLSALCKHLDNLWEEHRGSVVLFAWMQFLKE
&gt;ENST00000513019.1|ENSG00000013561.13|OTTHUMG00000129660.5|OTTHUMT00000370663.1|RNF14-005|ACTB|99
MSSEDREAQEDELLALASIYDGDEFRKAESVQGGETRIYLDLPQNFKIFVSGNSNECLQNSGFEYTICFLPPLVLNFELPPDYPSSSPPSFTLSGKWLS
&gt;ENST00000356143.1|ENSG00000013561.13|OTTHUMG00000129660.5|-|RNF14-202|HELLE|474
MSSEDREAQEDELLALASIYDGDEFRKAESVQGGETRIYLDLPQNFKIFVSGNSNECLQNSGFEYTICFLPPLVLNFELPPDYPSSSPPSFTLSGKWLSPTQLSALCKHLDNLWEEHRGSVVLFAWMQFLKEETLAYLNIVSPFELKIGSQKKVQRRTAQASPNTELDFGGAAGSDVDQEEIVDERAVQDVESLSNLIQEILDFDQAQQIKCFNSKLFLCSICFCEKLGSECMYFLECRHVYCKACLKDYFEIQIRDGQVQCLNCPEPKCPSVATPGQVKELVEAELFARYDRLLLQSSLDLMADVVYCPRPCCQLPVMQEPGCTMGICSSCNFAFCTLCRLTYHGVSPCKVTAEKLMDLRNEYLQADEANKRLLDQRYGKRVIQKAL
</code></pre>

<p>I want to make a <code>list</code> in <code>python</code> for the 6th element of the lines that start with ""<code>&gt;</code>"".
to do so, I first make a <code>dictionary</code> in python and then the keys should be the <code>list</code> that I want. like this:</p>

<pre><code>from itertools import groupby
with open('infile.txt') as f:
    groups = groupby(f, key=lambda x: not x.startswith(""&gt;""))
    d = {}
    for k,v in groups:
        if not k:
            key, val = list(v)[0].rstrip(), """".join(map(str.rstrip,next(groups)[1],""""))
            d[key] = val


k = d.keys()
res = [el[5:] for s in k for el in s.split(""|"")]
</code></pre>

<p>but it returns all elements in the line starts with <code>""&gt;"".</code></p>

<p>do you know how to fix it?</p>

<p>here is expected output:</p>

<pre><code>[""RNF14"", ""GAPDH"", ""ACTB"", ""HELLE""]
</code></pre>
","9578831","","","editing a text file in python and making a new one","<python>","2","1","2214"
"50651462","2018-06-01 22:01:13","1","","<p>The typical solution for this would be either <a href=""https://hackernoon.com/a-gentle-introduction-to-tmux-8d784c404340?gi=d0b4ca6c999b"" rel=""nofollow noreferrer""><code>tmux</code></a> or <a href=""https://www.howtoforge.com/linux_screen"" rel=""nofollow noreferrer""><code>screen</code></a>. I prefer <code>tmux</code>, so I'll give instructions for that.</p>

<p>Start by installing <code>tmux</code></p>

<pre><code>sudo apt-get install tmux
</code></pre>

<p>Then start a session:</p>

<pre><code>tmux new -s mybot
</code></pre>

<p>Then start your bot with whatever command you would normally use. Detach from the screen with <code>Ctrl-a Ctr-d</code>. You can now exit your ssh session and the bot will still be running.</p>

<p>To reattach to the session (to shut down the bot or whatever), just run:</p>

<pre><code>tmux attach -t mybot
</code></pre>
","6627323","","","1","859","James","2016-07-22 21:40:48","2323","191","93","38","50651416","50651462","2018-06-01 21:54:33","0","103","<p>So currently I'm trying to host a little bot I made in Python. It's meant to be running 24/7 so I tried Google cloud platform. I have a Ubuntu dist installed on a small scale VM server and I can run the bot perfectly. However when I exit out of my ssh session the python stops running. I've tried searching for solutions but I've found nothing.</p>

<p>So, how do I keep running python 24/7 on my Ubuntu VM? </p>
","6248826","","","Run python constantly on the Google cloud platform","<python><google-cloud-platform>","1","2","416"
"50651486","2018-06-01 22:03:39","1","","<p>You want to <a href=""https://stackoverflow.com/questions/952914/making-a-flat-list-out-of-list-of-lists-in-python"">flatten</a> the list.  For instance:</p>

<pre><code>a = [[0, 1], [0, 1, 2], [1]]
flat_list = [item for sublist in a for item in sublist]


In [5]: flat_list
</code></pre>

<blockquote>
  <p>Out[5]: [0, 1, 0, 1, 2, 1]</p>
</blockquote>

<p>In the case of your particular code, you could do:</p>

<pre><code>rind = [[i for i in range(len(C)) if item in C[i]]
        for item in item_list]
rind = [item for sublist in rind for item in sublist]
</code></pre>

<p>Alternatively, you could do it in one line like this:</p>

<pre><code>rind = list(map(set, [[i for i in range(len(C)) if item in C[i]]
        for item in item_list]))
</code></pre>
","1586231","1586231","2018-06-01 22:09:20","3","761","Max von Hippel","2012-08-09 00:08:44","2058","1119","1854","121","50651443","","2018-06-01 21:58:49","0","21","<p>say,</p>

<pre><code>C = [ [1,2,3],[1,3,4],[3,5,6]]
item_list=[1,3,4]
</code></pre>

<p>I used the following code to accomplish what I wanted :</p>

<pre><code>    rind = [[i for i in range(len(C)) if item in C[i]]
        for item in item_list]
</code></pre>

<p>I got the rind to be 
[[0, 1], [0, 1, 2], [1]]</p>

<p>I actually want my o/p to be as a 1d array like [0 1 0 1 2 1]</p>

<p>Could you either suggest a completely alternative approach to obtain row indices or advise me on how  to convert the list of arrays to a 1D array ?</p>

<p>Please note that the actual size of C is 2 M * 4 and item_list is 20000.</p>
","5129061","5129061","2018-06-01 22:17:52","How do I efficiently find the row indices of multiple elements in a 2D matrix?","<python><arrays><python-2.7><arraylist><indexing>","1","0","625"
"50651508","2018-06-01 22:06:44","0","","<p>I verified by setting a trace in the the <code>pybrake.Notifier</code>'s <code>send_notice</code> method that it does not get called in the unit test, whereas it does when I call the function 'manually' from the Django shell.</p>

<p>To still get test coverage, I worked around this problem simply asserting that <code>logger.error</code> gets called:</p>

<pre><code>from django.test import TestCase
from unittest.mock import patch

class SessionRecommendationTestCase(TestCase):
    @patch('lucy_web.lib.session_recommendation.logger.error')
    def test_create_session_from_non_existent_session_type_title_triggers_error_logging(self, mock_error):
        title = ""Being Awesome""    # Title of a non-existent session type
        _create_sessions([title])
        mock_error.assert_called_with(
            f""Tried to create a session of type '{title}', but no such session type was found in the database."")
</code></pre>

<p>where this is the actual function I'm trying to test:</p>

<pre><code>def _create_sessions(title_list, starting_number=1, **kwargs):
    '''Auxiliary function to create sessions from a list of SessionType titles'''
    session_number = starting_number
    for title in title_list:
        session_type = SessionType.objects.filter(title=title).first()
        if session_type:
            Session.objects.create(
                session_type=session_type,
                session_number=session_number,
                **kwargs)
            session_number += 1
        else:
            logger.error(
                f""Tried to create a session of type ""
                f""'{title}', but no such session type was found in the database."")
</code></pre>

<p>This test passes:</p>

<pre><code>(venv) Kurts-MacBook-Pro-2:lucy-web kurtpeek$ python manage.py test lucy_web.tests.test_session_recommendation.SessionRecommendationTestCase.test_create_session_from_non_existent_session_type_title_triggers_error_logging
Creating test database for alias 'default'...
System check identified no issues (0 silenced).
.
----------------------------------------------------------------------
Ran 1 test in 0.004s

OK
Destroying test database for alias 'default'...
</code></pre>
","995862","","","0","2197","Kurt Peek","2011-10-14 17:09:09","14006","2290","5406","6","50635038","50651508","2018-06-01 01:50:45","3","59","<p>I've configured a Django app called <code>lucy_web</code> to log errors to Airbrake using <a href=""https://github.com/airbrake/pybrake"" rel=""nofollow noreferrer""><code>pybrake</code></a>. In a module in the <code>lucy_web</code> hierarchy, <code>lucy_web.lib.session_recommendations</code>, I've defined a testing function:</p>

<pre><code>import logging

logger = logging.getLogger(__name__)

def log_something():
    logger.error(""Logging something..."")
</code></pre>

<p>If I call this function from the Django shell, like so:</p>

<pre><code>(venv) Kurts-MacBook-Pro-2:lucy-web kurtpeek$ python manage.py shell
Python 3.6.4 (v3.6.4:d48ecebad5, Dec 18 2017, 21:07:28) 
Type 'copyright', 'credits' or 'license' for more information
IPython 6.3.1 -- An enhanced Interactive Python. Type '?' for help.

In [1]: from lucy_web.lib.session_recommendation import *

In [2]: log_something()
</code></pre>

<p>I see an error email appear:</p>

<p><a href=""https://i.stack.imgur.com/boACX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/boACX.png"" alt=""enter image description here""></a></p>

<p>However, if I define a test, and try to call it from there, I don't see any new instances of the error appear:</p>

<pre><code>from django.test import TestCase
from django.core import mail


class SessionRecommendationTestCase(TestCase):
    def test_airbrake_notification_if_session_type_does_not_exist(self):
        from lucy_web.lib.session_recommendation import log_something
        log_something()
        import ipdb; ipdb.set_trace()
</code></pre>

<p>Running this did not cause any new instances of the error appear in the Airbrake dashboard. Also, <code>mail.outbox</code> is empty:</p>

<pre><code>(venv) Kurts-MacBook-Pro-2:lucy-web kurtpeek$ python manage.py test lucy_web.tests.test_session_recommendation.SessionRecommendationTestCase.test_airbrake_notification_if_session_type_does_not_exist
Creating test database for alias 'default'...
System check identified no issues (0 silenced).
--Return--
None
&gt; /Users/kurtpeek/Documents/Dev/lucy2/lucy-web/lucy_web/tests/test_session_recommendation.py(377)test_airbrake_notification_if_session_type_does_not_exist()
    375         from lucy_web.lib.session_recommendation import log_something
    376         log_something()
--&gt; 377         import ipdb; ipdb.set_trace()

ipdb&gt; mail.outbox
[]
</code></pre>

<p>As I understand from <a href=""https://docs.djangoproject.com/en/2.0/topics/testing/tools/#email-services"" rel=""nofollow noreferrer"">https://docs.djangoproject.com/en/2.0/topics/testing/tools/#email-services</a>, within a test, Django redirects all email sent by Django to a dummy outbox, <code>mail.outbox</code>. I would not expect this to the case for Airbrake, though, since the emails are probably sent by their backend - and if it were the case, I wouldn't expect <code>mail.outbox</code> to be empty.</p>

<p>Why am I not getting any Airbrake notification for the test? Is it something more general, like that all logging is disabled/captured during the test?</p>
","995862","","","Airbrake notifications disabled in Django tests?","<python><django><logging><airbrake>","1","0","3062"
"50651510","2018-06-01 22:06:56","3","","<p>Look at the third line of <code>get_students_titlecase</code>. You should be appending to the array, not setting it to a value.</p>

<pre><code>students_titlecase.append(student[""name""].title())
</code></pre>
","945873","","","0","212","itdoesntwork","2011-09-15 02:29:44","2587","244","567","48","50651499","","2018-06-01 22:05:16","3","46","<p>I recently began a Python course through PluralSight and one of th early modules on list creation and inputs has me a bit confused. I made the same code that the instructor presented but it's not acting as I think it should.</p>

<p>We were asked to take a code that asked for 2 inputs, a name and a student ID number, and add to it to create a program that asks for those 2 inputs and then ask if the user wants to add another name and number. If they do, it would ask for another name, number and if the user wants to add more. If the user says no, it should print out a list of names (but not numbers). </p>

<p>I came up with a code that does that, but when I print out the list, it only prints out the most recent name that was input. I can't figure out how to print a list of multiple names, any advice? Thanks!</p>

<pre><code>students = []

def get_students_titlecase():
    students_titlecase = []
    for student in students:
        students_titlecase = student[""name""].title()
    return students_titlecase

def print_students_titlecase():
    students_titlecase = get_students_titlecase()
    print(students_titlecase)

def add_student(name, student_id=332):
    student = {""name"": name, ""student_id"": student_id}
    students.append(student)


student_list = get_students_titlecase()



student_name = input(""Enter Student name: "")
student_id = input(""Enter Student id: "")
add_student(student_name, student_id)
add_more = input(""Add another student? [y/n]: "")


while add_more == ""y"":
    student_name = input(""Enter Student name: "")
    student_id = input(""Enter Student id: "")
    add_student(student_name, student_id)
    add_more = input(""Add another student? [y/n]: "")


if add_more == ""n"":
    print_students_titlecase()
</code></pre>
","9883321","","","Python list generation from inputs","<python>","1","0","1758"
"50651540","2018-06-01 22:10:39","0","","<p>I think you could use <code>pyautogui</code> to do this.</p>

<pre><code>import pyautogui

pyautogui.position() # gets mouse position returns pixel value E.G: (975,400)
pyautogui.moveTo(975,400)   # move mouse to this location
</code></pre>

<p>You could put these in a function as needed and map it to a button as needed.</p>

<p>Make one that centers the mouse, grabs the position, place it in a variable for later use.</p>

<p>Just an idea.</p>
","9883354","5674223","2018-06-01 22:34:15","1","451","pythonpizza","2018-06-01 22:10:39","1","0","0","0","45108057","","2017-07-14 16:50:33","0","128","<p>To make a long story short, I have an oculus dk2 vr headset, a blue tooth adapter and psmove motion controller. I play a game doom 3 fully possesed in vr, and you can control your aiming separetely from your view with the mouse.</p>

<p>Now I was able to make my psmove act like a mouse with the built in gyroscope, which sounds more difficult than it is. I use an app called psmoveservice, wich can connect the psmove to the pc through bluetooth, then I use another app called psmovefreepiebridge which sends the raw data to an app called freepie.</p>

<p>Freepie is based on python syntax and you can import libraries. I started off with this code  which assigns some buttons to the psmove, and makes the psmove act like a mouse.</p>

<pre><code>def update():
   #define globals

   #define constants
   mag = 1000
   dz = 0.005
   i=0
   j=0

   #bind left mouse to cross button
   #Right mouse to circle button
   #Middle mouse to move
   mouse.leftButton = joystick[j].getDown(14)
   mouse.rightButton = joystick[j].getDown(13)
   mouse.middleButton = joystick[j].getDown(19)

   #Mouse movement using Gryoscope
   # Only moves when the trigger is held down
   mdX = -1 * filters.deadband(filters.delta(freePieIO[i].yaw),dz) * mag
   mdY = -1 * filters.deadband(filters.delta(freePieIO[i].pitch),dz) *   mag
   if joystick[j].getDown(20):
      mouse.deltaX = mdX
      mouse.deltaY = mdY


if starting:
   freePieIO[0].update += update
</code></pre>

<p>Now of course because the psmove doesn't use any positional tracking here it loses alignment with the aiming in game a lot, especially after changing direction. I can just align it back by aiming where the gun is and holding a button but I thought this was a bit cumbersome and changed this button in a toggle button. It works perfectly but the problem is that sometimes the aim is out of my view, and that makes it quite annoying when I have to search where my gun is.</p>

<p>What I would want is that when I press a button the aim moves to the centre, since you mostly aim where you looking, I know what you're thinking, why not align the headset with the crosshair, but the thing is, if you want to aim at something you will look at it first but the finer aiming you do with your eyeballs. It also isn't as fun as aiming with gun :)</p>

<p>So I thought it would work quite well, and then changed my code so when I press a button the mouse goes to the center of the screen. This is the code (it also has some other code to map buttons)</p>

<pre><code>def update():
   #define globals

   #define constants
   mag = 1000
   dz = 0.005
   i=0
   j=0

   #bind left mouse to trigger button
   #Right mouse to circle
   #Middle mouse to triangle
   #up arrow key to square
   #down arrow key to cross
   #B Key to select ps button
   #N key to select button
   #Esc key to start button

   mouse.leftButton = joystick[j].getDown(20)
   mouse.rightButton = joystick[j].getDown(13)
   mouse.middleButton = joystick[j].getDown(12)
   keyboard.setKey(Key.UpArrow, joystick[j].getDown(15))
   keyboard.setKey(Key.DownArrow, joystick[j].getDown(14))
   keyboard.setKey(Key.B, joystick[j].getDown(16))
   keyboard.setKey(Key.N, joystick[j].getDown(0))
   keyboard.setKey(Key.Escape, joystick[j].getDown(3))

   #Mouse movement using Gryoscope
   # move button centers aim
   mdX = -1 * filters.deadband(filters.delta(freePieIO[i].yaw),dz) * mag
   mdY = -1 * filters.deadband(filters.delta(freePieIO[i].pitch),dz) * mag
   mouse.deltaX = mdX
   mouse.deltaY = mdY

      if joystick[j].getDown(19):    
          import ctypes

          ctypes.windll.user32.SetCursorPos(1000, 500)

if starting:
   freePieIO[0].update += update
</code></pre>

<p>Now the command used to set the mouse to the center of the screen is the setcursor command, which works perfectly, only it doesn't work in game.</p>

<p>With doing some research I realized games don't use the mouse position of windows but rather use the raw data from the mouse driver, or something like that anyway.</p>

<p>So I think I can only solve this problem by using a code that remembers the mouse position when I press a button and then goes back to that position when I press another button. I can figure out the button mapping, the code for remembering and going back to a certain position I cannot.</p>

<p>It's either that or communicating with one of the drivers (mouse driver, directinput) which is even harder.</p>

<p>So if anyone would have any idea where I need to start I would be very happy :)</p>
","8309047","","","0 How do I assign a button to a mouse position and bring the mouse back to that position with another button press (for VR purposes/python/freepie)","<python><mousemove><virtual-reality><oculus>","1","0","4525"
"50651549","2018-06-01 22:11:38","5","","<p>Your rules look a little bit odd. Can you remove all of them and try this instead?</p>

<pre><code>SUBSYSTEM==""usb"", ATTRS{idVendor}==""0683"", MODE=""0666""
</code></pre>
","28128","","","0","171","David Grayson","2008-10-15 06:16:24","61831","4202","3036","304","50625363","50651549","2018-05-31 13:20:53","2","3470","<p>This problem is old as the world. There are discussions and solutions available. It all boils down to update the rules file and give permissions. So I have followed the recipe. But I still have the same problem. here are screenshots showing I follow instructions.</p>

<p>Versions:</p>

<pre><code>Python 2.7.12 (default, Dec  4 2017, 14:50:18) 
usb.__version__ '1.0.2'
</code></pre>

<p>Error:</p>

<pre><code>    Traceback (most recent call last):
  File ""/media/psf/Home/All-Projects-on-femto/LaserLab/Software/usb_4108.py"", line 19, in &lt;module&gt;
    dev.set_configuration()
  File ""/usr/local/lib/python2.7/dist-packages/usb/core.py"", line 869, in set_configuration
    self._ctx.managed_set_configuration(self, configuration)
  File ""/usr/local/lib/python2.7/dist-packages/usb/core.py"", line 102, in wrapper
    return f(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/usb/core.py"", line 147, in managed_set_configuration
    self.managed_open()
  File ""/usr/local/lib/python2.7/dist-packages/usb/core.py"", line 102, in wrapper
    return f(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/usb/core.py"", line 120, in managed_open
    self.handle = self.backend.open_device(self.dev)
  File ""/usr/local/lib/python2.7/dist-packages/usb/backend/libusb1.py"", line 786, in open_device
    return _DeviceHandle(dev)
  File ""/usr/local/lib/python2.7/dist-packages/usb/backend/libusb1.py"", line 643, in __init__
    _check(_lib.libusb_open(self.devid, byref(self.handle)))
  File ""/usr/local/lib/python2.7/dist-packages/usb/backend/libusb1.py"", line 595, in _check
    raise USBError(_strerror(ret), ret, _libusb_errno[ret])
USBError: [Errno 13] Access denied (insufficient permissions)
</code></pre>

<p>Rules. file (location /etc/udev/rules.d/99-usbftdi.rules)</p>

<pre><code># For FTDI FT232 &amp; FT245 USB devices with Vendor ID = 0x0403, Product ID = 0xabc
SYSFS{idProduct}==""4108"", SYSFS{idVendor}==""0683"", RUN+=""/sbin/modprobe -q ftdi-sio product=0x4108 vendor=0x0683""
#SYSFS{idProduct}==""2450"", SYSFS{idVendor}==""0683"", RUN+=""/sbin/modprobe -q ftdi-sio product=0x2450 vendor=0x0683""
SYSFS{idVendor}==""0683"", SYSFS{idProduct}==""4108"", MODE=""0666""
</code></pre>

<p>and strip down code:</p>

<pre><code>import usb.core
import usb.util
dev = usb.core.find(idVendor=0x0683, idProduct=0x4108)
dev.reset() 
</code></pre>

<p>However, it doesn't give me necessary permissions.
My stripped down code that still produces the error:</p>

<pre><code>import usb.core
import usb.util
dev = usb.core.find(idVendor=0x0683, idProduct=0x4108)
dev.reset()
</code></pre>

<p>The strange thing that if I start IDLE from the terminal as superuser I get permissions (terminal: <code>sudo idle</code>).</p>
","8436767","8436767","2018-06-01 18:14:27","USBError: [Errno 13] Access denied (insufficient permissions)","<python><usb><ubuntu-16.04>","1","4","2742"
"50651562","2018-06-01 22:13:06","3","","<p>@zvone is correct.  For instance:</p>

<pre><code>banana = True
tom = 4
hotdog = None
goo = hotdog or tom or banana
shazam = banana or hotdog or tom
</code></pre>

<p>In this example, <code>goo</code> is <code>4</code>, and <code>shazam</code> is <code>True</code>.  So you can mix types!  Cool stuff.</p>
","1586231","1586231","2018-06-01 22:44:59","2","309","Max von Hippel","2012-08-09 00:08:44","2058","1119","1854","121","50651518","50651562","2018-06-01 22:08:15","0","44","<p>In JavaScript I can do</p>

<pre><code>const foo = bar || baz || bloop || blop || bleep || null
</code></pre>

<p>If I want <code>foo</code> to be equal to the first one of the above arguments that is not falsey / null</p>

<p>Is there an idiomatic Python equivalent? I was just going to loop through a list of args, but thought I'd check.</p>
","9035107","","","Python equivalent to picking first non-null argument","<javascript><python>","1","2","347"
"50651627","2018-06-01 22:19:19","1","","<p>The dimensions are: (row,col,channel)
Yes it often makes sense to feed a 1D array into a Neural Net, for example if you use a fully connected network.
To reshape you have multiple options:</p>

<ol>
<li><p>Use the reshape function   </p>

<p><code>data.reshape(-1)</code></p></li>
<li><p>Use the flatten function</p>

<p><code>data.flatten()</code></p></li>
</ol>
","9280994","9280994","2018-06-01 22:23:44","2","367","Jonathan R","2018-01-28 21:47:13","1615","128","51","6","50651581","50651627","2018-06-01 22:14:21","0","32","<p>In this code (courtesy to this <a href=""https://stackoverflow.com/a/7769424/7902058"">answer</a>):</p>

<pre><code>from PIL import Image
import numpy as np


def load_image(infilename):
    img = Image.open(infilename)
    img.load()
    data = np.asarray(img, dtype=""int32"")
    return data


def save_image(npdata, outfilename):
    img = Image.fromarray(np.asarray(np.clip(npdata, 0, 255), dtype=""uint8""), ""L"")
    img.save(outfilename)

data = load_image('cat.0.jpg')
print(data.shape)
</code></pre>

<p>The value of <code>print(data.shape)</code> is a tuple of three dim <code>(374, 500, 3)</code>. Thus, I have these questions:</p>

<ol>
<li>What does this tuple represent? </li>
<li>To be used for machine learning classification purpose, does it make sense to convert such a tuple data into <strong>one dimension vector</strong>? If so, how?</li>
</ol>

<p>Thank you very much.</p>
","7902058","","","Is it possible to convert 3D image to one vector?","<python><python-3.x><machine-learning><scikit-learn><python-imaging-library>","2","2","892"
"50651629","2018-06-01 22:19:20","0","","<ol>
<li><p>374 rows of 500 columns of RGB (3) values (or pixels), or some permutation of these dimensions.</p></li>
<li><p>Maybe. Though remember that any 1D encoding of this discards two-dimensional distance information between different pixels. If you're working with neural networks, look into convolutional neural networks to see how they deal with this problem.</p></li>
</ol>
","945873","","","1","383","itdoesntwork","2011-09-15 02:29:44","2587","244","567","48","50651581","50651627","2018-06-01 22:14:21","0","32","<p>In this code (courtesy to this <a href=""https://stackoverflow.com/a/7769424/7902058"">answer</a>):</p>

<pre><code>from PIL import Image
import numpy as np


def load_image(infilename):
    img = Image.open(infilename)
    img.load()
    data = np.asarray(img, dtype=""int32"")
    return data


def save_image(npdata, outfilename):
    img = Image.fromarray(np.asarray(np.clip(npdata, 0, 255), dtype=""uint8""), ""L"")
    img.save(outfilename)

data = load_image('cat.0.jpg')
print(data.shape)
</code></pre>

<p>The value of <code>print(data.shape)</code> is a tuple of three dim <code>(374, 500, 3)</code>. Thus, I have these questions:</p>

<ol>
<li>What does this tuple represent? </li>
<li>To be used for machine learning classification purpose, does it make sense to convert such a tuple data into <strong>one dimension vector</strong>? If so, how?</li>
</ol>

<p>Thank you very much.</p>
","7902058","","","Is it possible to convert 3D image to one vector?","<python><python-3.x><machine-learning><scikit-learn><python-imaging-library>","2","2","892"
"50651644","2018-06-01 22:20:45","1","","<p>For this case you can use <code>getattr</code>:</p>

<p><strong>test.kv</strong></p>

<pre><code>BoxLayout:
    orientation: ""vertical""
    Label:
        id: 1
    Label:
        id: 2
    Label:
        id: 3
    Label:
        id: 4
    Label:
        id: 5
    Button:
        text: ""press me""
        on_press: app.testFn()
</code></pre>

<p><strong>main.py</strong></p>

<pre><code>from kivy.app import App

class TestApp(App):
    def testFn(self):
        for i in range(1, 6):
            getattr(self.root.ids, str(i)).text = str(5*i)

if __name__ == '__main__':
    TestApp().run()
</code></pre>

<hr>

<p>Or take advantage of that self.ids is a dictionary where the key is the id and the value is the widget.</p>

<pre><code>from kivy.app import App

class TestApp(App):
    def testFn(self):
        for i in range(1, 6):
            self.root.ids[str(i)].text = str(5*i)

if __name__ == '__main__':
    TestApp().run()
</code></pre>

<hr>

<p><strong>Note:</strong> keep in mind that the past to <code>self.ids[]</code> must be a string.</p>

<p><strong>Update:</strong></p>

<p><strong>calculation.kv</strong></p>

<pre><code>BoxLayout:

    CustomLabel1:
        id: 1

    CustomLabel2:
        id: 2

    CustomLabel3:
        id: 3

    CustomLabel4:
        id: 4

    CustomLabel5:
        id: 5

    Button:
        text: ""Calculate values""
        on_press: app.calculate_values(2,4)

&lt;CustomLabel1@Label&gt;:
&lt;CustomLabel2@Label&gt;:
&lt;CustomLabel3@Label&gt;:
&lt;CustomLabel4@Label&gt;:
&lt;CustomLabel5@Label&gt;:
</code></pre>

<p><strong>main.py</strong></p>

<pre><code>from kivy.app import App

class TestApp(App):
    def calculate_values(self, start, end):
        for i in range(start, end):
            self.root.ids[str(i)].text = str(5*i)

if __name__ == '__main__':
    TestApp().run()
</code></pre>
","6622587","6622587","2018-06-01 22:32:28","0","1848","eyllanesc","2016-07-21 23:29:11","114264","27275","2584","21000","50651466","50651644","2018-06-01 22:02:00","0","263","<p>I have a kv file with several widgets, their IDs being numbers (in string representation). Let's say it's numbers from 1 - 10.</p>

<p>Can I somehow access those widgets from Python with a loop by using their ID in the method call (which is an integer in string form) instead of using the ID name explicitly?
For example (as illustration, it doesn't really work), I would like to use something like:</p>

<pre><code>for i in range (1, 11)
    self.root.ids.str(i).text = str(i*5)
</code></pre>

<p>instead of:</p>

<pre><code>self.root.ids.1.text = str(5)
self.root.ids.2.text = str(10)
self.root.ids.3.text = str(15)
... etc
</code></pre>

<p>The reason is this list of widgets can grow large. Also the ranges (slices) I want to access can vary.</p>
","3055594","3055594","2018-06-01 22:42:46","Access kivy widgets from Python code using dynamic IDs","<python><widget><kivy>","1","0","754"
"50651646","2018-06-01 22:21:02","2","","<p>You cannot use <code>csrf_exempt</code> as a class decorator.</p>

<p>DRF views do not have csrf protection anyway. You should just delete the decorators.</p>
","104349","","","0","162","Daniel Roseman","2009-05-10 12:36:13","489411","52610","12851","10717","50651557","50651646","2018-06-01 22:12:46","3","2894","<p>I am relatively new to web application development and am trying my hand at using Django. I implemented an API using Django Rest Framework and it seemed to work fine at first.</p>

<p>I tried adding some bootstrap to hook up to my frontend (React) and now when I run Django with ""python manage.py runserver"", I receive an AttributeError. I'm not sure how to go about debugging this error.</p>

<p>Below is the full stack trace, my views.py and urls.py for context.</p>

<p><strong>Stack Trace</strong>                                                                                                                                               </p>

<pre><code>Unhandled exception in thread started by &lt;function check_errors.&lt;locals&gt;.wrapper
 at 0x000000560AAAC488&gt;
Traceback (most recent call last):
  File ""C:\Users\aidancain\envs\usaproject\lib\site-packages\django\utils\autore
load.py"", line 225, in wrapper
    fn(*args, **kwargs)
  File ""C:\Users\aidancain\envs\usaproject\lib\site-packages\django\core\managem
ent\commands\runserver.py"", line 120, in inner_run
    self.check(display_num_errors=True)
  File ""C:\Users\aidancain\envs\usaproject\lib\site-packages\django\core\managem
ent\base.py"", line 364, in check
    include_deployment_checks=include_deployment_checks,
  File ""C:\Users\aidancain\envs\usaproject\lib\site-packages\django\core\managem
ent\base.py"", line 351, in _run_checks
    return checks.run_checks(**kwargs)
  File ""C:\Users\aidancain\envs\usaproject\lib\site-packages\django\core\checks\
registry.py"", line 73, in run_checks
    new_errors = check(app_configs=app_configs)
  File ""C:\Users\aidancain\envs\usaproject\lib\site-packages\django\core\checks\
urls.py"", line 13, in check_url_config
    return check_resolver(resolver)
  File ""C:\Users\aidancain\envs\usaproject\lib\site-packages\django\core\checks\
urls.py"", line 23, in check_resolver
    return check_method()
  File ""C:\Users\aidancain\envs\usaproject\lib\site-packages\django\urls\resolve
rs.py"", line 397, in check
    for pattern in self.url_patterns:
  File ""C:\Users\aidancain\envs\usaproject\lib\site-packages\django\utils\functi
onal.py"", line 36, in __get__
    res = instance.__dict__[self.name] = self.func(instance)
  File ""C:\Users\aidancain\envs\usaproject\lib\site-packages\django\urls\resolve
rs.py"", line 536, in url_patterns
    patterns = getattr(self.urlconf_module, ""urlpatterns"", self.urlconf_module)
  File ""C:\Users\aidancain\envs\usaproject\lib\site-packages\django\utils\functi
onal.py"", line 36, in __get__
    res = instance.__dict__[self.name] = self.func(instance)
  File ""C:\Users\aidancain\envs\usaproject\lib\site-packages\django\urls\resolve
rs.py"", line 529, in urlconf_module
    return import_module(self.urlconf_name)
  File ""C:\Users\aidancain\envs\usaproject\lib\importlib\__init__.py"", line 126,
 in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""&lt;frozen importlib._bootstrap&gt;"", line 994, in _gcd_import
  File ""&lt;frozen importlib._bootstrap&gt;"", line 971, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 955, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 665, in _load_unlocked
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 678, in exec_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 219, in _call_with_frames_removed
  File ""C:\Users\ATC\envs\usaproject\Scripts\BaldEagle\usabaseball\urls.py"", lin
e 24, in &lt;module&gt;
    path('api/', include('api.urls')),
  File ""C:\Users\aidancain\envs\usaproject\lib\site-packages\django\urls\conf.py
"", line 34, in include
    urlconf_module = import_module(urlconf_module)
  File ""C:\Users\aidancain\envs\usaproject\lib\importlib\__init__.py"", line 126,
 in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""&lt;frozen importlib._bootstrap&gt;"", line 994, in _gcd_import
  File ""&lt;frozen importlib._bootstrap&gt;"", line 971, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 955, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 665, in _load_unlocked
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 678, in exec_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 219, in _call_with_frames_removed
  File ""C:\Users\ATC\envs\usaproject\Scripts\BaldEagle\api\urls.py"", line 16, in
 &lt;module&gt;
    urlpatterns += router.urls
  File ""C:\Users\aidancain\envs\usaproject\lib\site-packages\rest_framework\rout
ers.py"", line 101, in urls
    self._urls = self.get_urls()
  File ""C:\Users\aidancain\envs\usaproject\lib\site-packages\rest_framework\rout
ers.py"", line 363, in get_urls
    urls = super(DefaultRouter, self).get_urls()
  File ""C:\Users\aidancain\envs\usaproject\lib\site-packages\rest_framework\rout
ers.py"", line 261, in get_urls
    routes = self.get_routes(viewset)
  File ""C:\Users\aidancain\envs\usaproject\lib\site-packages\rest_framework\rout
ers.py"", line 176, in get_routes
    extra_actions = viewset.get_extra_actions()
AttributeError: 'function' object has no attribute 'get_extra_actions'
</code></pre>

<p><strong>api.views.py</strong></p>

<pre><code>from django.contrib.auth.decorators import login_required
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from rest_framework.response import Response
from rest_framework import status
from rest_framework import generics, mixins
from rest_framework import viewsets
from api.models import Playerbios, Testtrackman
from api.serializers import USASerializer, TrackmanSerializer

# @method_decorator(login_required, name='dispatch')
@csrf_exempt
class USAViewset(viewsets.ModelViewSet):
    queryset = Playerbios.objects.all()
    serializer_class = USASerializer
    lookup_field = 'trackmanid'

# @method_decorator(login_required, name='dispatch')
@csrf_exempt
class TrackmanViewset(viewsets.ModelViewSet):
    queryset = Testtrackman.objects.all()
    serializer_class = TrackmanSerializer
    lookup_field = 'pitchuid'
</code></pre>

<p><strong>api.urls.py</strong></p>

<pre><code>from django.conf.urls import url, include
from rest_framework import routers
#from .views import USAView, USAListView
from .views import USAViewset, TrackmanViewset

# urlpatterns = [
#     url('(?P&lt;trackmanid&gt;)/', USAView.as_view(), name='usa-rud-view'),
#     url('', USAListView.as_view(), name='usa-list-view'),
# ]
urlpatterns = [

]
router = routers.DefaultRouter()
router.register(r'bios', USAViewset)
router.register(r'trackman', TrackmanViewset)
urlpatterns += router.urls
</code></pre>
","7758462","7758462","2018-06-01 22:19:00","Django/DRF app receives AttributeError: 'function' object has no attribute 'get_extra_actions'","<python><django><django-rest-framework><attributeerror>","1","0","6644"
"50651658","2018-06-01 22:22:14","5","","<p>From the <a href=""https://docs.python.org/3/library/itertools.html#itertools.combinations"" rel=""noreferrer"">docs</a>:</p>

<blockquote>
  <p>Combinations are emitted in lexicographic sort order. So, if the input
  iterable is sorted, the combination tuples will be produced in sorted
  order.</p>
  
  <p>Elements are treated as unique based on their position, not on their
  value. So if the input elements are unique, there will be no repeat
  values in each combination.</p>
</blockquote>
","7619676","","","2","495","ZaxR","2016-08-09 17:54:43","1752","111","641","11","50651619","","2018-06-01 22:18:38","0","143","<p>Is the order of items obtained from itertools.combinations() deterministic?</p>

<p>I am currently writing a script which produces an excessively large number of objects using itertools.combinations, large enough that I can't retain it all in memory. For each combination, there is a function which returns a value that I store in a numpy array (since those are fairly memory efficient). I have just barely enough memory to store all of those floats.</p>

<p>I then iterate over those floats, and if it's an index of interest, I run itertools.combinations again with a counter variable to access the combination that produced that result (which only takes a few seconds).</p>

<p>I have tested this with various smaller datasets for which I have enough memory, and the entries all are the same in those cases, but I am worried that this is not a ""safe"" approach to doing what I want.</p>
","7886653","","","Is itertools.combinations() deterministic?","<python><functional-programming><combinatorics>","1","0","891"
"50651693","2018-06-01 22:25:16","0","","<p>a solution near yours with filter instead of groupby and map</p>

<pre><code>with open('infile.txt') as f:
    lines = f.readlines()

groups = filter(lambda x: x.startswith(""&gt;""), lines)
res = list(map(lambda x: x.split('|')[5],groups))
</code></pre>
","2105333","","","0","256","bobrobbob","2013-02-24 20:29:06","1165","190","352","263","50651429","","2018-06-01 21:56:53","0","35","<p>I have a text file like this:</p>

<pre><code>&gt;ENST00000511961.1|ENSG00000013561.13|OTTHUMG00000129660.5|OTTHUMT00000370661.3|RNF14-003|RNF14|278
MSSEDREAQEDELLALASIYDGDEFRKAESVQGGETRIYLDLPQNFKIFVSGNSNECLQNSGFEYTICFLPPLVLNFELPPDYPSSSPPSFTLSGKWLSPTQLSALCKHLDNLWEEHRGSVVLFAWMQFLKEETLAYLNIVSPFELKIGSQKKVQRRTAQASPNTELDFGGAAGSDVDQEEIVDERAVQDVESLSNLIQEILDFDQAQQIKCFNSKLFLCSICFCEKLGSECMYFLECRHVYCKACLKDYFEIQIRDGQVQCLNCPEPKCPSVATPGQ
&gt;ENST00000506822.1|ENSG00000013561.13|OTTHUMG00000129660.5|OTTHUMT00000370662.1|RNF14-004|GAPDH|132
MSSEDREAQEDELLALASIYDGDEFRKAESVQGGETRIYLDLPQNFKIFVSGNSNECLQNSGFEYTICFLPPLVLNFELPPDYPSSSPPSFTLSGKWLSPTQLSALCKHLDNLWEEHRGSVVLFAWMQFLKE
&gt;ENST00000513019.1|ENSG00000013561.13|OTTHUMG00000129660.5|OTTHUMT00000370663.1|RNF14-005|ACTB|99
MSSEDREAQEDELLALASIYDGDEFRKAESVQGGETRIYLDLPQNFKIFVSGNSNECLQNSGFEYTICFLPPLVLNFELPPDYPSSSPPSFTLSGKWLS
&gt;ENST00000356143.1|ENSG00000013561.13|OTTHUMG00000129660.5|-|RNF14-202|HELLE|474
MSSEDREAQEDELLALASIYDGDEFRKAESVQGGETRIYLDLPQNFKIFVSGNSNECLQNSGFEYTICFLPPLVLNFELPPDYPSSSPPSFTLSGKWLSPTQLSALCKHLDNLWEEHRGSVVLFAWMQFLKEETLAYLNIVSPFELKIGSQKKVQRRTAQASPNTELDFGGAAGSDVDQEEIVDERAVQDVESLSNLIQEILDFDQAQQIKCFNSKLFLCSICFCEKLGSECMYFLECRHVYCKACLKDYFEIQIRDGQVQCLNCPEPKCPSVATPGQVKELVEAELFARYDRLLLQSSLDLMADVVYCPRPCCQLPVMQEPGCTMGICSSCNFAFCTLCRLTYHGVSPCKVTAEKLMDLRNEYLQADEANKRLLDQRYGKRVIQKAL
</code></pre>

<p>I want to make a <code>list</code> in <code>python</code> for the 6th element of the lines that start with ""<code>&gt;</code>"".
to do so, I first make a <code>dictionary</code> in python and then the keys should be the <code>list</code> that I want. like this:</p>

<pre><code>from itertools import groupby
with open('infile.txt') as f:
    groups = groupby(f, key=lambda x: not x.startswith(""&gt;""))
    d = {}
    for k,v in groups:
        if not k:
            key, val = list(v)[0].rstrip(), """".join(map(str.rstrip,next(groups)[1],""""))
            d[key] = val


k = d.keys()
res = [el[5:] for s in k for el in s.split(""|"")]
</code></pre>

<p>but it returns all elements in the line starts with <code>""&gt;"".</code></p>

<p>do you know how to fix it?</p>

<p>here is expected output:</p>

<pre><code>[""RNF14"", ""GAPDH"", ""ACTB"", ""HELLE""]
</code></pre>
","9578831","","","editing a text file in python and making a new one","<python>","2","1","2214"
"50651766","2018-06-01 22:35:32","0","","<p>This question is too vague.  The process you need to look into is called ""ortho-rectification"".  You should read about the process and then break it down into stages.  Then, figure out the specific pieces you have and don't have.</p>

<p>Fundamentally, in order to create an ortho-image, you need a digital elevation model (DEM), intrinsic camera parameters, and extrinsic parameters (pose).  You can find documentation on the algorithm online or in a standard Remote Sensing book.  </p>

<p>Another option is if your camera provides Rational Polynomial Coefficients (RPCs), which I assume is no. </p>

<p>Generic Amazon Search of Remote Sensing Books</p>

<p><a href=""https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&amp;field-keywords=remote+sensing"" rel=""nofollow noreferrer"">https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&amp;field-keywords=remote+sensing</a></p>
","2142228","","","0","905","msmith81886","2013-03-07 00:10:18","1177","93","29","7","50644447","","2018-06-01 13:21:54","0","72","<p>I've got a processed image array from UAV and want to write it into a projected tiff. I am aware with the array to tiff writting process with python gdal, however not sure how to project it correctly.</p>

<p>I have got the central GPS, UAV height, pixel size of the image array, the array is northward. The orginal UAV image's metadata can not be recognized by gdal, so I have to extract them out and then rearrange them to project the array.</p>

<p>Many thanks!</p>
","6888635","","","Create a projected raster from array with python gdal","<python><projection><gdal>","1","0","472"
"50651780","2018-06-01 22:37:27","2","","<p>It seems I've found out a very robust solutioon to this problem. The approach is iterative. It will first check if there is any <code>next page</code> url available in that page. If it finds one then it will track that url and repeat the process. However, if any link doesn't have any pagination, the scraper will break and try for another link.</p>

<p>Here it is:</p>

<pre><code>import requests
from urllib.parse import urljoin
from bs4 import BeautifulSoup

urls = [
        'https://www.mobilehome.net/mobile-home-park-directory/alaska/all',
        'https://www.mobilehome.net/mobile-home-park-directory/rhode-island/all',
        'https://www.mobilehome.net/mobile-home-park-directory/maine/all',
        'https://www.mobilehome.net/mobile-home-park-directory/vermont/all'
    ]

def get_names(link):
    while True:
        r = requests.get(link)
        soup = BeautifulSoup(r.text,""lxml"")
        for items in soup.select(""td[class='table-row-price']""):
            name = items.select_one(""h2 a"").text
            print(name)

        nextpage = soup.select_one("".pagination a.next_page"")

        if not nextpage:break  #If no pagination url is there, it will break and try another link

        link = urljoin(link,nextpage.get(""href""))

if __name__ == '__main__':
    for url in urls:
        get_names(url)
</code></pre>
","9189799","9189799","2018-06-03 08:43:43","0","1339","SIM","2017-03-26 11:23:11","13061","2013","939","385","50611496","50651780","2018-05-30 18:42:10","11","301","<p>I've written a scraper in python using BeautifulSoup library to parse all the names traversing different pages of a website. I could manage it if it were not for more than one urls with different pagination, meaning some urls have pagination some does not as the content are few. </p>

<p>My question is: how could I manage to compile them within a function to handle whether they have pagination or not?</p>

<p>My initial attempt (it is able to parse the content from each url's first page only):</p>

<pre><code>import requests 
from bs4 import BeautifulSoup

urls = {
    'https://www.mobilehome.net/mobile-home-park-directory/maine/all',
    'https://www.mobilehome.net/mobile-home-park-directory/rhode-island/all',
    'https://www.mobilehome.net/mobile-home-park-directory/new-hampshire/all',
    'https://www.mobilehome.net/mobile-home-park-directory/vermont/all'
}

def get_names(link):
    r = requests.get(link)
    soup = BeautifulSoup(r.text,""lxml"")
    for items in soup.select(""td[class='table-row-price']""):
        name = items.select_one(""h2 a"").text
        print(name)

if __name__ == '__main__':
    for url in urls:
        get_names(url)
</code></pre>

<p>I could have managed to do the whole thing, if there is a single url with pagination like below:</p>

<pre><code>from bs4 import BeautifulSoup 
import requests

page_no = 0
page_link = ""https://www.mobilehome.net/mobile-home-park-directory/new-hampshire/all/page/{}""

while True:
    page_no+=1
    res = requests.get(page_link.format(page_no))
    soup = BeautifulSoup(res.text,'lxml')
    container = soup.select(""td[class='table-row-price']"")
    if len(container)&lt;=1:break 

    for content in container:
        title = content.select_one(""h2 a"").text
        print(title)
</code></pre>

<p>But, all the urls do not have pagination. So, how can i manage to grab all of them whether there is any pagination or not?</p>
","9189799","","","Unable to exhaust the content of all the identical urls used within my scraper","<python><python-3.x><web-scraping><beautifulsoup>","2","0","1908"
"50651804","2018-06-01 22:40:32","1","","<p>You've got two options: either evaluate the Python in the browser or post it back to some server that can spawn a Python process to evaluate the code.</p>

<p>For the former, <a href=""https://www.google.com/search?q=python+implemented+in+JavaScript&amp;oq=python+implemented+in+JavaScript&amp;aqs=chrome..69i57j0.3439j0j7&amp;sourceid=chrome&amp;ie=UTF-8"" rel=""nofollow noreferrer"">there are a few Python-implemented-via-JavaScript solutions</a> out there, which I can't personally vouch for but would be the faster option and wouldn't require you to have servers to execute the code. <a href=""http://pypyjs.org/"" rel=""nofollow noreferrer"">PyPy.js</a> has a REPL in a browser available to play with, so that's worth taking a look at.</p>

<p>For server-side execution, there's a ton of approaches, all depending on your server technologies, which Python interpreter you're using, how you're going to handle security/DOS, etc.</p>

<p>Hopefully that helps you get started.</p>
","119549","","","1","979","Jacob","2009-06-09 01:56:45","59508","4168","4932","36","50651652","50651804","2018-06-01 22:21:41","0","66","<p>Alright, so basically, I own this website: <a href=""https://quarkedit.ga"" rel=""nofollow noreferrer"">https://quarkedit.ga</a> which right now has a HTML/CSS/JS editor using Ace. Now what I need is I also have a python language one, but I don't know how to make the terminal or whatever show up. I'm going for something like <a href=""https://repl.it"" rel=""nofollow noreferrer"">https://repl.it</a> 's python thing. Just wondering if there is any API which i could use, something like</p>

<pre><code>output = evaluatePythonCode(""print(\""test\"")"");
</code></pre>

<p>I basically want to know and find out these things:</p>

<ul>
<li>What API's can I use to do this?</li>
<li>What would the syntax be?</li>
<li>Can I do this with pure HTML/CSS/JS or would I have to use a JS 
Library?</li>
</ul>

<p>I have the input for the code done, and the syntax highlighting (Ace) But what I need is the:</p>

<ul>
<li>Execution</li>
<li><p>Output</p>

<p>All help will be appreciated, and if this question isn't appropriate or anything just comment and I'll remove it. </p></li>
</ul>

<p>Thanks!</p>
","9414889","9414889","2018-06-01 22:26:16","How would I go about adding a python code runner to my website","<javascript><python><html>","2","7","1089"
"50651825","2018-06-01 22:43:35","1","","<p>It's possible Google removed common filler words like 'to' and 'a'. If the file seems otherwise uncorrupt, and checking other words after <code>load()</code> shows that they are present, it'd be reasonable to assume Google discarded the overly-common words as having such diffuse meaning as to be of low-value. </p>

<p>It's unclear and muddled what you're trying to do. You assign to <code>word_to_idx</code> twice - so only the second line matters. </p>

<p>(The first assignment, creating a dict where all words have a <code>0</code> value, has no lingering effect after the 2nd line creates an all-new dict, with only entries where <code>w in model.wv.vocab</code>. The only possible entry with a <code>0</code> after this step would be whatever word in the word-vectors set was already in position <code>0</code> – if and only if that word was also in your <code>corpus_words</code>.)</p>

<p>You seem to want to build new vectors for unknown words based on an average of similar words. However, the <code>most_similar()</code> only works for known-words. It will error if tried on a completely unknown word. So that approach can't work. </p>

<p>And a deeper problem is the <code>gensim</code> <code>KeyedVectors</code> class doesn't have support for dynamically adding new word->vector entries. You would have to dig into its source code and, to add one or a batch of new vectors, modify a bunch of its internal properties (including its <code>vectors</code> array, <code>vocab</code> dict, and <code>index2entity</code> list) in a self-consistent manner to have new entries.</p>
","130288","","","6","1590","gojomo","2009-06-29 07:12:15","25735","1962","345","35","50618993","","2018-05-31 07:31:41","2","998","<p>I want to get word embeddings for the words in a corpus. I decide to use pretrained word vectors in <em>GoogleNews</em> by <em>gensim</em> library. But my corpus contains some words that are not in GoogleNews words. for these missing words, I want to use arithmatic mean of n most similar words to it in GoggoleNews words. First I load GoogleNews and check that the word ""to"" is in it?</p>

<pre><code>#Load GoogleNews pretrained word2vec model
model=word2vec.KeyedVectors.Load_word2vec_format(""GoogleNews-vectors-negative33.bin"",binary=True)
print(model[""to""])
</code></pre>

<p>I receive an error: <code>keyError: ""word 'to' not in vocabulary""</code>
is it possible that such a large dataset doesn't have this word? this is true also for some other common word like ""a""!</p>

<p>For adding missing words to word2vec model,first I want to get indices of words that are in GoogleNews. for missing words I have used index 0.</p>

<pre><code>#obtain index of words
word_to_idx=OrderedDict({w:0 for w in corpus_words})
word_to_idx=OrderedDict({w:model.wv.vocab[w].index for w in corpus_words if w in model.wv.vocab})
</code></pre>

<p>then I calculate the mean of embedding vectors of most similar words to each missing word.</p>

<pre><code>missing_embd={}
for key,value in word_to_idx.items():
    if value==0:
        similar_words=model.wv.most_similar(key)
        similar_embeddings=[model.wv[a[0]] for a in similar_words]
        missing_embd[key]=mean(similar_embeddings)
</code></pre>

<p>And then I add these news embeddings to word2vec model by:</p>

<pre><code>for word,embd in missing_embd.items():
    # model.wv.build_vocab(word,update=True)
    model.wv.syn0[model.wv.vocab[word].index]=embd
</code></pre>

<p>There is an un-consistency. When I print missing_embed, it's empty. As if there were not any missing words.
But when I check it by this:</p>

<pre><code>for w in tokens_lower:
    if(w in model.wv.vocab)==False:
        print(w)
        print(""***********"")
</code></pre>

<p>I found a lot of missing words.
Now, I have 3 questions:
1- why <strong><em>missing_embed</em></strong> is empty while there are some missing words?
2- Is it possible that GoogleNews doesn't have words like ""to""?
3- how can I append new embeddings to word2vec model? I used <strong><em>build_vocab</em></strong> and <strong><em>syn0</em></strong>. Thanks.</p>
","7418216","","","add new words to GoogleNews by gensim","<python><word2vec><gensim><google-news>","2","7","2362"
"50651843","2018-06-01 22:47:24","1","","<p>The <code>company</code> field is serialized to a nested object because you've declared it with a nested serializer.  Try replacing the line</p>

<pre><code>company = CompanySerializer()
</code></pre>

<p>with something like this</p>

<pre><code>company = serializers.HyperlinkedRelatedField(
    many=False,
    read_only=True
)
</code></pre>

<p>You may need to adjust the options according to <a href=""http://www.django-rest-framework.org/api-guide/serializers/#how-hyperlinked-views-are-determined"" rel=""nofollow noreferrer"">the docs</a>.</p>
","2715819","","","1","550","RishiG","2013-08-25 16:38:25","1877","143","190","7","50651509","50651843","2018-06-01 22:06:52","1","331","<p>I have a <code>HyperlinkedModelSerializer</code> of a model with a <code>ForeignKey</code>, and I want to return the hyperlink to the instance referenced in that field, but I get the whole object nested in the json.</p>

<p>These are my models:</p>

<pre><code>class Hotel(models.Model):

    ONE_STAR = '*'
    TWO_STARS = '**'
    THREE_STARS = '***'
    FOUR_STARS = '****'
    FIVE_STARS = '****'
    GRAND_TOURISM = 'GRAND_TOURISM'
    NA = 'NA'
    SPECIAL = 'SPECIAL'
    ECO = 'ECO'
    BOUTIQUE = 'BOUTIQUE'

    HOTEL_CATEGORY_CHOICES = (
        (ONE_STAR, _('*')),
        (TWO_STARS, _('**')),
        (THREE_STARS, _('***')),
        (FOUR_STARS, _('****')),
        (FIVE_STARS, _('*****')),
        (GRAND_TOURISM, _('Grand Tourism')),
        (NA, _('NA')),
        (SPECIAL, _('Special')),
        (ECO, _('Eco-Hotel')),
        (BOUTIQUE, _('Boutique-Hotel'))
    )

    company = models.OneToOneField(Company, on_delete=models.CASCADE, primary_key=True, verbose_name=_('Company'))
    code = models.CharField(max_length=10, verbose_name=_('Code'))
    city = models.ForeignKey(City, on_delete=models.PROTECT, related_name='hotels', verbose_name=_('City'))
    category = models.CharField(max_length=20, choices=HOTEL_CATEGORY_CHOICES, verbose_name=_('Category'))
    capacity = models.IntegerField(verbose_name=_('Capacity'))
    position = models.DecimalField(max_digits=11, decimal_places=2, default=0.00, verbose_name=_('Position'))
    in_pickup = models.BooleanField(default=False, verbose_name=_('In pickup?'))
    is_active = models.BooleanField(default=True, verbose_name=_('Is active?'))

    class Meta:
        verbose_name = _('Hotel')
        verbose_name_plural = _('Hotels')

    def __str__(self):
        return self.company.name

class Company(models.Model):
    name = models.CharField(max_length=40, verbose_name=_('Name'))
    legal_name = models.CharField(max_length=100, null=True, blank=True, verbose_name=_('Legal name'))
    tax_id = models.CharField(max_length=12, null=True, blank=True, verbose_name=_('Tax ID'))
    url = models.URLField(null=True, blank=True, verbose_name=_('URL'))
    address = models.TextField(max_length=400, null=True, blank=True, verbose_name=_('Address'))

    class Meta:
        verbose_name = _('Company')
        verbose_name_plural = _('Companies')

    def __str__(self):
        return ""[{}]{}"".format(self.id, self.name)
</code></pre>

<p>This is my serializer:</p>

<pre><code>class HotelProductsSerializer(serializers.HyperlinkedModelSerializer):
    company = CompanySerializer()
    products = serializers.SerializerMethodField()

    def get_products(self, instance):
        queryset = [product for product in instance.company.products.all()]
        return ProductSerializer(queryset, many=True, context=self.context).data

    class Meta:
        model = models.Hotel
        fields = ('company', 'products')
</code></pre>

<p>This is my ViewSet:</p>

<pre><code>class HotelProductsViewSet(viewsets.ModelViewSet):
    permission_classes = (permissions.IsAuthenticated,)
    queryset = models.Hotel.objects.all()
    serializer_class = serializers.HotelProductsSerializer
</code></pre>

<p>Everything works fine, but the field <code>company</code> results in a nested Company object and I would like to get an hyperlink to the object instead.</p>

<p>Thanks for your help.</p>
","6843153","","","I want to get the url of a ForeignKey in Django Rest Framework","<python><django><django-rest-framework>","1","0","3369"
"50651849","2018-06-01 22:48:53","0","","<p>This is not a csv file.</p>

<ul>
<li><p>When <code>CatPart</code> or <code>CodeItem</code> changes, they are marked with the field at the start of the line, followed by a variable number of spaces and then the value</p></li>
<li><p><code>weight</code> &amp; <code>nb</code> always appear on the same line separated by space.</p></li>
<li><p>A record can be considered filled when the following information exists <code>CatPart</code>, <code>CodeItem</code>, <code>weight</code>, <code>nb</code>. </p></li>
</ul>

<p>One way to restructure the document is to iterate over the lines and yielding records when they are filled.</p>

<pre><code>import io
import re
import pandas as pd


text = """"""CatPart 1
CodeItem    1
12  15
5.5 3

CodeItem    2
7   6
2   7

CatPart 5
CodeItem    0
1   25
1.5 7
CodeItem    8
25  1
22  1""""""

records = []
part, item, weight, nb = '', '', '', ''

for line in io.StringIO(text):
    if line.startswith('CatPart'):
        _, part = re.split('\s+', line.strip())
    if line.startswith('CodeItem'):
        _, item = re.split('\s+', line.strip())
    if re.match('^\d+', line):
        weight, nb = re.split('\s+', line.strip())
        records.append([part, item, weight, nb])
</code></pre>

<p>Note that I've used <code>io.StringIO</code> to create a file in memory for this example. Replace that with something like this:</p>

<pre><code>with open('/path/to/my/file.txt') as in_file:
    for line in in_file:
        ...
</code></pre>

<p>Then pass the records into pandas.DataFrame constructor.</p>

<pre><code>df = pd.DataFrame(records, columns=['CatPart', 'CatItem', 'weight', 'nb'])
</code></pre>

<p>produces the following output:</p>

<pre><code>  CatPart CatItem weight  nb
0       1       1     12  15
1       1       1    5.5   3
2       1       2      7   6
3       1       2      2   7
4       5       0      1  25
5       5       0    1.5   7
6       5       8     25   1
7       5       8     22   1
</code></pre>

<p>However, until this point, I didn't convert any data from string. You can do the conversions as you read the data, or do it after structuring the data in a table using:</p>

<pre><code>df = df.astype(float)
</code></pre>
","2570261","2570261","2018-06-01 22:55:57","0","2188","Haleemur Ali","2013-07-10 21:11:08","14534","823","410","56","50651525","","2018-06-01 22:09:01","0","63","<p>I read a csv file with theses data:</p>

<pre><code>CatPart 1           
CodeItem    1           
12  15  
5.5 3   

CodeItem    2           
7   6   
2   7       

CatPart 5           
CodeItem    0           
1   25  
1.5 7   

CodeItem    8           
25  1   
22  1
</code></pre>

<p>And I would like to have this format in the dataframe</p>

<pre><code>CatPart CodeItem Weight Nb
1 1 12 15   
1 1 5.5 3       
1 2 7 6 
1 2 2 7 
5 0 1 25
5 0 1.5 7   
5 8 25  1
5 8 22  1
</code></pre>

<p>So first I create header with the colums name
df.columns = [""Weight"", ""Nb"", ""CatPart"", ""CodeItem""]
and I removed the empty line. My strategy was to copy the value in the row CatPart to the column Cartpart, this part was okay (see below) and after duplicate the value to replace the nan until the next value (do the same CodeItem) and then delete the row CatPart and CodeItem but I didn't find a way to duplicate the value.</p>

<pre><code>Weight      Nb  CatPart CodeItem
CatPart     1   1   nan
CodeItem    1   nan 1
12          15  nan nan
5.5         3   nan nan
CodeItem    2   2   nan
7           6   nan nan
2           7   nan nan
CatPart     5   5   nan
CodeItem    0   nan 0
1           25  nan nan
1.5         7   nan nan
CodeItem    8   8   nan
25          1   nan nan
22          1   nan nan
</code></pre>

<p>Or maybe their is a easier way to do that but I don't see it.</p>

<p>Thanks for your help!
F.</p>
","9883137","","","Pandas Dataframe - Change the data structure (re-organize)","<python><pandas><dataframe><data-structures>","1","0","1417"
"50651851","2018-06-01 22:49:11","0","","<p>Please do not use <code>format</code> for this. The <code>execute</code> function accepts a 'parameters' argument that you would pass your list to. You can modify it like so:</p>

<pre><code>sql1 = """"""
  SELECT distinct th.name, 'MMS' as Status
  FROM t1 
  JOIN t2 on t1.id = t2.tid
  WHERE t2.station= 'MatS'
  AND t1.name IN (%s)
""""""
cur.execute(sql1, (list_of_things,))
</code></pre>
","5921693","","","8","391","Nick","2016-02-13 07:31:21","4958","421","362","143","50651691","","2018-06-01 22:25:05","-2","77","<p>Here is a part of my script :</p>

<pre><code>sql1 = """"""
                    SELECT distinct th.name, 'MMS' as Status
                    FROM t1 
                    JOIN t2 on t1.id = t2.tid
                    WHERE t2.station= 'MatS'
                    AND t1.name IN ({listthinglist})
        """""".format(listthinglist=', '.join(""'"" + item + ""'"" for item in list_of_things))
    cur.execute(sql1)
</code></pre>

<p>The <code>list_of_things</code> contains 22016 items and I am unable to run this query in python. Is there an alternative to run queries like this in python.</p>
","8544462","6779307","2018-06-01 22:25:37","Python running SQL query With WHERE IN {list} and list is too long","<python><sql><list><pandas>","1","4","585"
"50651870","2018-06-01 22:51:24","0","","<p>Wrap your statement that writes a user to the csv file in an <code>if</code> statement that tests for the correct <code>type</code>.</p>
","535275","","","0","140","Scott Hunter","2010-12-08 16:30:40","37679","5424","831","4721","50650949","","2018-06-01 21:01:02","-3","37","<p>The script I have is exporting all users but I am looking to export users who have a type = xyz. There are two types of users in the directory such as type a and type b and i only want to export users who have type attribute matches b. </p>

<p>Please help me to add a clause/statement in the script so it should only pull users with Type ""B"" and ignore other users with ant other type. </p>

<pre><code>import requests
import json
import re
import sys
import csv


orgName = """"

apiKey = """"

api_token = ""SSWS ""+ apiKey

headers = {'Accept':'application/json','Content-Type':'application/json','Authorization':api_token}

def GetPaginatedResponse(url):

    response = requests.request(""GET"", url, headers=headers)

    returnResponseList = []

    responseJSON = json.dumps(response.json())

    responseList = json.loads(responseJSON)

    returnResponseList = returnResponseList + responseList


    if ""errorCode"" in responseJSON:

        print ""\nYou encountered following Error: \n""
        print responseJSON
        print ""\n""

        return ""Error""

    else:

        headerLink= response.headers[""Link""]

        while str(headerLink).find(""rel=\""next\"""") &gt; -1:

            linkItems = str(headerLink).split("","")

            nextCursorLink = """"
            for link in linkItems:

                if str(link).find(""rel=\""next\"""") &gt; -1:
                    nextCursorLink = str(link)


            nextLink = str(nextCursorLink.split("";"")[0]).strip()
            nextLink = nextLink[1:]
            nextLink = nextLink[:-1]

            url = nextLink

            response = requests.request(""GET"", url, headers=headers)

            responseJSON = json.dumps(response.json())

            responseList = json.loads(responseJSON)

            returnResponseList = returnResponseList + responseList

            headerLink= response.headers[""Link""]


        returnJSON = json.dumps(returnResponseList)

        return returnResponseList



def DownloadSFUsers():

    url = ""https://""+orgName+"".com/api/v1/users""

    responseJSON = GetPaginatedResponse(url)

    if responseJSON != ""Error"":


        userFile = open(""Only-Okta_Users.csv"", ""wb"")

        writer = csv.writer(userFile)

        writer.writerow([""login"",""type""]).encode('utf-8')

        for user in responseJSON:


            login = user[u""profile""][u""login""]
            type  = user[u""credentials""][u""provider""][u""type""]
            row = (""+login+"",""+type).encode('utf-8')

            writer.writerow([login,type])

if __name__ == ""__main__"":

    DownloadSFUsers()
</code></pre>
","3501581","1324033","2018-06-01 21:05:26","Export users with criteria - Python script","<python>","1","2","2579"
"50651878","2018-06-01 22:52:34","0","","<p>You should do it when you set the <code>FileHandler</code> for the logging object. Use <code>datetime</code> instead of time so that you can include the date for each instance of the log in order to differentiate logs on different days at the same time.</p>

<pre><code>fh = logging.FileHandler(""Log""+str(datetime.datetime.now())+'.log')
fh.setLevel(logging.DEBUG)
</code></pre>
","4848801","","","2","382","Ryan Schaefer","2015-04-30 02:00:58","1499","277","242","141","50651797","50667780","2018-06-01 22:39:35","2","49","<p>I am trying to add in multiple log files into a Logs folder, but instead of having to change the code each time you start the program, i want to make the log file's name ""Log(the time).log"". I'm using logger at the moment, but i can switch. I've also imported time.</p>

<p><strong>Edit:</strong> Here is some of the code i am using:</p>

<p><code>import logging</code><br>
<code>logger = logging.getLogger('k')</code><br>
<code>hdlr = logging.FileHandler('Path to the log file/log.log')</code><br>
<code>formatter = logging.Formatter('At %(asctime)s, KPY returned %(message)s at level %(levelname)s</code><br>
<code>hdlr.setFormatter(formatter)</code><br>
<code>logger.addHandler(hdlr)</code><br>
<code>logger.setLevel(logging.DEBUG)</code><br>
<code>logger.info('hello')</code></p>
","9347473","9347473","2018-06-02 12:15:17","Add the time in a log name using logger","<python><logging><time>","3","0","787"
"50651882","2018-06-01 22:53:22","1","","<pre><code>import logging
import time

fname = ""Log({the_time}).log"".format(the_time=time.time())
logging.basicConfig(level=logging.DEBUG, filename=fname)

logging.info('hello')
</code></pre>
","674039","","","0","192","wim","2011-03-23 23:40:27","187587","12233","9064","5087","50651797","50667780","2018-06-01 22:39:35","2","49","<p>I am trying to add in multiple log files into a Logs folder, but instead of having to change the code each time you start the program, i want to make the log file's name ""Log(the time).log"". I'm using logger at the moment, but i can switch. I've also imported time.</p>

<p><strong>Edit:</strong> Here is some of the code i am using:</p>

<p><code>import logging</code><br>
<code>logger = logging.getLogger('k')</code><br>
<code>hdlr = logging.FileHandler('Path to the log file/log.log')</code><br>
<code>formatter = logging.Formatter('At %(asctime)s, KPY returned %(message)s at level %(levelname)s</code><br>
<code>hdlr.setFormatter(formatter)</code><br>
<code>logger.addHandler(hdlr)</code><br>
<code>logger.setLevel(logging.DEBUG)</code><br>
<code>logger.info('hello')</code></p>
","9347473","9347473","2018-06-02 12:15:17","Add the time in a log name using logger","<python><logging><time>","3","0","787"
"50651904","2018-06-01 22:55:56","1","","<p>Here is a simple example of a subclassed QsciScintilla editor with added SublimeText-like commenting by setting multiple selections using the <code>Ctrl+Mouse</code> and then pressing <code>Ctrl+K</code>.</p>

<p><strong>UPDATE:</strong>
Updated the commenting to comment/uncomment at the minimal indentation level of each selection and to merge adjacent selections.</p>

<pre><code># Import the PyQt5 module with some of the GUI widgets
import PyQt5.QtWidgets
import PyQt5.QtGui
import PyQt5.QtCore
# Import the QScintilla module
import PyQt5.Qsci
# Import Python's sys module needed to get the application arguments
import sys

""""""
Custom editor with a simple commenting feature
similar to what SublimeText does
""""""
class MyCommentingEditor(PyQt5.Qsci.QsciScintilla):
    comment_string = ""// ""
    line_ending = ""\n""

    def keyPressEvent(self, event):
        # Execute the superclasses event
        super().keyPressEvent(event)
        # Check pressed key information
        key = event.key()
        key_modifiers = PyQt5.QtWidgets.QApplication.keyboardModifiers()
        if (key == PyQt5.QtCore.Qt.Key_K and 
            key_modifiers == PyQt5.QtCore.Qt.ControlModifier):
                self.toggle_commenting()

    def toggle_commenting(self):
        # Check if the selections are valid
        selections = self.get_selections()
        if selections == None:
            return
        # Merge overlapping selections
        while self.merge_test(selections) == True:
            selections = self.merge_selections(selections)
        # Start the undo action that can undo all commenting at once
        self.beginUndoAction()
        # Loop over selections and comment them
        for i, sel in enumerate(selections):
            if self.text(sel[0]).lstrip().startswith(self.comment_string):
                self.set_commenting(sel[0], sel[1], self._uncomment)
            else:
                self.set_commenting(sel[0], sel[1], self._comment)
        # Select back the previously selected regions
        self.SendScintilla(self.SCI_CLEARSELECTIONS)
        for i, sel in enumerate(selections):
            start_index = self.positionFromLineIndex(sel[0], 0)
            # Check if ending line is the last line in the editor
            last_line = sel[1]
            if last_line == self.lines() - 1:
                end_index = self.positionFromLineIndex(sel[1], len(self.text(last_line)))
            else:
                end_index = self.positionFromLineIndex(sel[1], len(self.text(last_line))-1)
            if i == 0:
                self.SendScintilla(self.SCI_SETSELECTION, start_index, end_index)
            else:
                self.SendScintilla(self.SCI_ADDSELECTION, start_index, end_index)
        # Set the end of the undo action
        self.endUndoAction()

    def get_selections(self):
        # Get the selection and store them in a list
        selections = []
        for i in range(self.SendScintilla(self.SCI_GETSELECTIONS)):
            selection = (
                self.SendScintilla(self.SCI_GETSELECTIONNSTART, i),
                self.SendScintilla(self.SCI_GETSELECTIONNEND, i)
            )
            # Add selection to list
            from_line, from_index = self.lineIndexFromPosition(selection[0])
            to_line, to_index = self.lineIndexFromPosition(selection[1])
            selections.append((from_line, to_line))
        selections.sort()
        # Return selection list
        return selections

    def merge_test(self, selections):
        """"""
        Test if merging of selections is needed
        """"""
        for i in range(1, len(selections)):
            # Get the line numbers
            previous_start_line = selections[i-1][0]
            previous_end_line = selections[i-1][1]
            current_start_line = selections[i][0]
            current_end_line = selections[i][1]
            if previous_end_line == current_start_line:
                return True
        # Merging is not needed
        return False

    def merge_selections(self, selections):
        """"""
        This function merges selections with overlapping lines
        """"""
        # Test if merging is required
        if len(selections) &lt; 2:
            return selections
        merged_selections = []
        skip_flag = False
        for i in range(1, len(selections)):
            # Get the line numbers
            previous_start_line = selections[i-1][0]
            previous_end_line = selections[i-1][1]
            current_start_line = selections[i][0]
            current_end_line = selections[i][1]
            # Test for merge
            if previous_end_line == current_start_line and skip_flag == False:
                merged_selections.append(
                    (previous_start_line, current_end_line)
                )
                skip_flag = True
            else:
                if skip_flag == False:
                    merged_selections.append(
                        (previous_start_line, previous_end_line)
                    )
                skip_flag = False
                # Add the last selection only if it was not merged
                if i == (len(selections) - 1):
                    merged_selections.append(
                        (current_start_line, current_end_line)
                    )
        # Return the merged selections
        return merged_selections

    def set_commenting(self, arg_from_line, arg_to_line, func):
        # Get the cursor information
        from_line = arg_from_line
        to_line = arg_to_line
        # Check if ending line is the last line in the editor
        last_line = to_line
        if last_line == self.lines() - 1:
            to_index = len(self.text(to_line))
        else:
            to_index = len(self.text(to_line))-1
        # Set the selection from the beginning of the cursor line
        # to the end of the last selection line
        self.setSelection(
            from_line, 0, to_line, to_index
        )
        # Get the selected text and split it into lines
        selected_text = self.selectedText()
        selected_list = selected_text.split(""\n"")
        # Find the smallest indent level
        indent_levels = []
        for line in selected_list:
            indent_levels.append(len(line) - len(line.lstrip()))
        min_indent_level = min(indent_levels)
        # Add the commenting character to every line
        for i, line in enumerate(selected_list):
            selected_list[i] = func(line, min_indent_level)
        # Replace the whole selected text with the merged lines
        # containing the commenting characters
        replace_text = self.line_ending.join(selected_list)
        self.replaceSelectedText(replace_text)

    def _comment(self, line, indent_level):
        if line.strip() != """":
            return line[:indent_level] + self.comment_string + line[indent_level:]
        else:
            return line

    def _uncomment(self, line, indent_level):
        if line.strip().startswith(self.comment_string):
            return line.replace(self.comment_string, """", 1)
        else:
            return line
</code></pre>

<p>For the full example, see <a href=""https://github.com/matkuki/qscintilla_docs/blob/master/examples/commenting.py"" rel=""nofollow noreferrer"">https://github.com/matkuki/qscintilla_docs/blob/master/examples/commenting.py</a></p>

<p>I used PyQt5 with QScintilla 2.10.4 and Python 3.6.</p>
","2603918","2603918","2018-06-16 15:53:04","1","7428","Matic Kukovec","2013-07-21 09:56:42","101","21","70","0","50355919","50651904","2018-05-15 17:15:43","6","481","<p>I'm trying to implement a toggle comment feature in QScintilla that works with multiple selection. Unfortunately I don't know very well how to do it, so far I've come up with this code:</p>

<pre><code>import sys
import re
import math

from PyQt5.Qt import *  # noqa

from PyQt5.Qsci import QsciScintilla
from PyQt5 import Qsci
from PyQt5.Qsci import QsciLexerCPP


class Commenter():

    def __init__(self, sci, comment_str):
        self.sci = sci
        self.comment_str = comment_str

    def is_commented_line(self, line):
        return line.strip().startswith(self.comment_str)

    def toggle_comment_block(self):
        sci = self.sci

        line, index = sci.getCursorPosition()

        if sci.hasSelectedText() and self.is_commented_line(sci.text(sci.getSelection()[0])):
            self.uncomment_line_or_selection()
        elif not self.is_commented_line(sci.text(line)):
            self.comment_line_or_selection()
        else:
            start_line = line
            while start_line &gt; 0 and self.is_commented_line(sci.text(start_line - 1)):
                start_line -= 1

            end_line = line
            lines = sci.lines()
            while end_line &lt; lines and self.is_commented_line(sci.text(end_line + 1)):
                end_line += 1

            sci.setSelection(start_line, 0, end_line, sci.lineLength(end_line))
            self.uncomment_line_or_selection()
            sci.setCursorPosition(line, index - len(self.comment_str))

    def comment_line_or_selection(self):
        sci = self.sci

        if sci.hasSelectedText():
            self.comment_selection()
        else:
            self.comment_line()

    def uncomment_line_or_selection(self):
        sci = self.sci

        if sci.hasSelectedText():
            self.uncomment_selection()
        else:
            self.uncomment_line()

    def comment_line(self):
        sci = self.sci

        line, index = sci.getCursorPosition()
        sci.beginUndoAction()
        sci.insertAt(self.comment_str, line, sci.indentation(line))
        sci.endUndoAction()

    def uncomment_line(self):
        sci = self.sci

        line, index = sci.getCursorPosition()

        if not self.is_commented_line(sci.text(line)):
            return

        sci.beginUndoAction()
        sci.setSelection(
            line, sci.indentation(line),
            line, sci.indentation(line) + len(self.comment_str)
        )
        sci.removeSelectedText()
        sci.endUndoAction()

    def comment_selection(self):
        sci = self.sci

        if not sci.hasSelectedText():
            return

        line_from, index_from, line_to, index_to = sci.getSelection()
        if index_to == 0:
            end_line = line_to - 1
        else:
            end_line = line_to

        sci.beginUndoAction()
        for line in range(line_from, end_line + 1):
            sci.insertAt(self.comment_str, line, sci.indentation(line))

        sci.setSelection(line_from, 0, end_line + 1, 0)
        sci.endUndoAction()

    def uncomment_selection(self):
        sci = self.sci

        if not sci.hasSelectedText():
            return

        line_from, index_from, line_to, index_to = sci.getSelection()
        if index_to == 0:
            end_line = line_to - 1
        else:
            end_line = line_to

        sci.beginUndoAction()
        for line in range(line_from, end_line + 1):
            if not self.is_commented_line(sci.text(line)):
                continue

            sci.setSelection(
                line, sci.indentation(line),
                line,
                sci.indentation(line) + len(self.comment_str)
            )
            sci.removeSelectedText()

            if line == line_from:
                index_from -= len(self.comment_str)
            if index_from &lt; 0:
                index_from = 0

            if line == line_to:
                index_to -= len(self.comment_str)
            if index_to &lt; 0:
                index_to = 0

        sci.setSelection(line_from, index_from, line_to, index_to)
        sci.endUndoAction()


class Foo(QsciScintilla):

    def __init__(self, parent=None):
        super().__init__(parent)

        # http://www.scintilla.org/ScintillaDoc.html#Folding
        self.setFolding(QsciScintilla.BoxedTreeFoldStyle)

        # Indentation
        self.setIndentationsUseTabs(False)
        self.setIndentationWidth(4)
        self.setBackspaceUnindents(True)
        self.setIndentationGuides(True)

        # Set the default font
        self.font = QFont()
        self.font.setFamily('Consolas')
        self.font.setFixedPitch(True)
        self.font.setPointSize(10)
        self.setFont(self.font)
        self.setMarginsFont(self.font)

        # Margin 0 is used for line numbers
        fontmetrics = QFontMetrics(self.font)
        self.setMarginsFont(self.font)
        self.setMarginWidth(0, fontmetrics.width(""000"") + 6)
        self.setMarginLineNumbers(0, True)
        self.setMarginsBackgroundColor(QColor(""#cccccc""))

        # Indentation
        self.setIndentationsUseTabs(False)
        self.setIndentationWidth(4)
        self.setBackspaceUnindents(True)

        lexer = QsciLexerCPP()
        lexer.setFoldAtElse(True)
        lexer.setFoldComments(True)
        lexer.setFoldCompact(False)
        lexer.setFoldPreprocessor(True)
        self.setLexer(lexer)

        # Use raw messages to Scintilla here
        # (all messages are documented here: http://www.scintilla.org/ScintillaDoc.html)
        # Ensure the width of the currently visible lines can be scrolled
        self.SendScintilla(QsciScintilla.SCI_SETSCROLLWIDTHTRACKING, 1)
        # Multiple cursor support
        self.SendScintilla(QsciScintilla.SCI_SETMULTIPLESELECTION, True)
        self.SendScintilla(QsciScintilla.SCI_SETMULTIPASTE, 1)
        self.SendScintilla(
            QsciScintilla.SCI_SETADDITIONALSELECTIONTYPING, True)

        # Comment feature goes here
        self.commenter = Commenter(self, ""//"")
        QShortcut(QKeySequence(""Ctrl+7""), self,
                  self.commenter.toggle_comment_block)


def main():
    app = QApplication(sys.argv)
    ex = Foo()
    ex.setText(""""""\
#include &lt;iostream&gt;
using namespace std;

void Function0() {
    cout &lt;&lt; ""Function0"";
}

void Function1() {
    cout &lt;&lt; ""Function1"";
}

void Function2() {
    cout &lt;&lt; ""Function2"";
}

void Function3() {
    cout &lt;&lt; ""Function3"";
}


int main(void) {
    if (1) {
        if (1) {
            if (1) {
                if (1) {
                    int yay;
                }
            }
        }
    }

    if (1) {
        if (1) {
            if (1) {
                if (1) {
                    int yay2;
                }
            }
        }
    }

    return 0;
}\
    """""")
    ex.resize(800, 600)
    ex.show()
    sys.exit(app.exec_())


if __name__ == ""__main__"":
    main()
</code></pre>

<p>Relevant Qscintilla docs live here:</p>

<ul>
<li><a href=""https://www.scintilla.org/ScintillaDoc.html"" rel=""noreferrer"">ScintillaDoc</a></li>
<li><a href=""http://pyqt.sourceforge.net/Docs/QScintilla2"" rel=""noreferrer"">QScintilla2</a></li>
<li><a href=""https://www.scintilla.org/ScintillaDoc.html#MultipleSelectionAndVirtualSpace"" rel=""noreferrer"">MultipleSelectionAndVirtualSpace</a></li>
</ul>

<p>Right now the feature just support one single selection/cursor and the way is commenting is really ugly. As you can see in the code, if you press ctrl while pressing the mouse you'll be able to create multiple cursors/selections already.</p>

<p>There are few things I don't know how to achieve right now though:</p>

<p>1) I'd like the comments to become well-aligned, that is, they should start at the same level of indentation. The existing feature right now produces ugly unaligned comments, example of what I call ""well-aligned"" comments:</p>

<p><a href=""https://i.stack.imgur.com/1e71f.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1e71f.png"" alt=""enter image description here""></a></p>

<p>2) Right now only one cursor/selection is being considered. How do I loop over the cursors/selections to apply a toggle_selection function?</p>

<p><a href=""https://i.stack.imgur.com/5DYhy.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/5DYhy.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/ozck4.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ozck4.png"" alt=""enter image description here""></a></p>

<p>3) I guess if you loop over the selections the result would be than having an even number of cursors in a particular line won't comment the line (comment, uncomment), for instance, something like this:</p>

<p><a href=""https://i.stack.imgur.com/OtBjV.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/OtBjV.png"" alt=""enter image description here""></a></p>

<p>4) An odd number of cursors in a particular line would affect the line because (comment, uncomment, comment), for instance, something like this:</p>

<p><a href=""https://i.stack.imgur.com/8PXgv.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/8PXgv.png"" alt=""enter image description here""></a></p>

<p>5) If you loop over the cursors/selections you'll end up producing output like the below one.</p>

<p><a href=""https://i.stack.imgur.com/WFebw.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/WFebw.png"" alt=""enter image description here""></a></p>

<p>EDIT: 1st draft</p>

<pre><code>class Commenter():

    def __init__(self, sci, comment_str):
        self.sci = sci
        self.comment_str = comment_str

    def selections(self):
        regions = []
        for i in range(self.sci.SendScintilla(QsciScintilla.SCI_GETSELECTIONS)):
            regions.append({
                'begin': self.selection_start(i),
                'end': self.selection_end(i)
            })

        return regions

    def selection_start(self, selection):
        return self.sci.SendScintilla(QsciScintilla.SCI_GETSELECTIONNSTART, selection)

    def selection_end(self, selection):
        return self.sci.SendScintilla(QsciScintilla.SCI_GETSELECTIONNEND, selection)

    def text(self, *args):
        return self.sci.text(*args)

    def run(self):
        send_scintilla = self.sci.SendScintilla

        for region in self.selections():
            print(region)
            print(repr(self.text(region['begin'],region['end'])))
</code></pre>

<p>EDIT2: I've discovered the source code of this feature I'm trying to implement is available on SublimeText Default.sublime-package (zip file), <a href=""https://dl.dropboxusercontent.com/s/6u2f0o7jl8hx44l/2018-05-19_02-48-16.txt"" rel=""noreferrer"">comments.py</a>. That code supports not only normal comments <code>//</code> but also block comments <code>/* ... */</code>. Main problem is porting that code to QScintilla seems to be quite tricky :/</p>
","3809375","3809375","2018-05-19 00:49:03","How to implement a comment feature that works with multiple selections in QScintilla?","<python><pyqt><pyqt5><scintilla><qscintilla>","3","0","10878"
"50651915","2018-06-01 22:57:15","0","","<p>It is possible to specify data types for columns in a behave table, though not necessarily possible for specific rows within the columns. See this <a href=""https://jenisys.github.io/behave.example/tutorials/tutorial10.html"" rel=""nofollow noreferrer"">tutorial</a>. It explains implementing User-Defined Types by using behave's <code>register_type</code>. </p>

<p>Another option is to use behave's <a href=""http://behave.readthedocs.io/en/latest/tutorial.html#step-parameters"" rel=""nofollow noreferrer"">parser</a>. This is as close to what you're looking for as I've found. </p>
","8754516","","","0","581","natn2323","2017-10-10 18:47:48","891","134","123","68","50627578","","2018-05-31 15:13:38","0","895","<p>Here I've written a test to determine if an API is responding to a bad request with the expected content...</p>

<pre><code>Scenario: Unkown user response body properties contain expected content
    Given I have a valid client auth token
    And I request a user with an unknown ""valid"" uuid
    And I get the response json
    Then the expected fields should contain expected content
    | field      | content               |
    | statusCode | 404                   |
    | error      | Not Found             |
    | message    | User record not found |
</code></pre>

<p>This is the corresponding step:</p>

<pre><code>@then(u'the expected fields should contain expected content')
def step_impl(context):
    for row in context.table:
        received_content = str(context.request_json.get(row['field']))
        expected_content = row['content']
        assert_equal(received_content, expected_content)
</code></pre>

<p>It seems that Behave converts table row content to strings.</p>

<p>My question is: Is it possible to specify the data type of a cell in a Behave table?</p>

<p>In the actual response <code>statusCode</code> is an integer, but as you can see in my step function I'm forced to convert the request content to a string in order to validate it. I wouldn't need to do this if I could specify that the <code>404</code> I'm passing in the content column is an integer.</p>
","8506575","8506575","2018-06-01 08:02:51","Specify Behave table row data type","<python><cucumber><bdd><python-behave>","2","0","1397"
"50651923","2018-06-01 22:58:01","0","","<p>Try calling it as separate arguments:</p>

<pre><code>subprocess.call(['msg', '*', 'Hey'], shell=True)
</code></pre>
","4848801","","","2","120","Ryan Schaefer","2015-04-30 02:00:58","1499","277","242","141","50651872","","2018-06-01 22:51:47","1","325","<p>I have been having some problems with subprocess.call(), subprocess.run(), subprocess.Popen(), os.system(), (and other functions to run command prompt commands) as I can't seem to get the <code>msg</code> command to work.</p>

<pre><code>import subprocess
subprocess.call(""msg * Hey"",shell=True)
</code></pre>

<p>In theory, this should send ""Hey"" to every computer on the network unfortunately, this isn't running at all and I'm not quite sure why. I can run it on cmd successful however can't get it to work on python</p>

<p>I'd love to hear why this doesn't work and hopeful a way to fix my code or an alternate solution.</p>

<p>EDIT: Solved, thanks for everyone's help. Upgrading to 64-bit did the trick</p>
","9883398","9883398","2018-06-02 21:12:22","subprocess msg command not working","<python><windows><python-3.x><subprocess>","2","2","717"
"50651928","2018-06-01 22:58:37","1","","<p>Or the way you started could end up something like below:</p>

<pre><code>import requests
from bs4 import BeautifulSoup

base_url=""http://cbcs.fastvturesults.com/student/1sp15me00{}""

data = []

for page in range(1,10,1):
    d = {}
    r = requests.get(base_url.format(page))
    soup = BeautifulSoup(r.content,""html.parser"")
    items = soup.find(class_=""text-muted"")
    if items:
        d[""Name""] = items.previous_sibling
        d[""USN""] = items.text.replace(""("","""").replace("")"","""")
        data.append(d)

print(data)
</code></pre>

<p>Output:</p>

<pre><code>[{'Name': 'Agnello Fernandes A ', 'USN': '1sp15me001'}, {'Name': 'Ajay Kumar V ', 'USN': '1sp15me002'}, {'Name': 'Ajay Rajendiran ', 'USN': '1sp15me003'}, {'Name': 'Amit Singh Yadav ', 'USN': '1sp15me004'}, {'Name': 'Ankit Mahato ', 'USN': '1sp15me006'}, {'Name': 'Antony Levin Fernandez D ', 'USN': '1sp15me008'}, {'Name': 'Ashish S ', 'USN': '1sp15me009'}]
</code></pre>
","9189799","","","0","943","SIM","2017-03-26 11:23:11","13061","2013","939","385","50650342","50650399","2018-06-01 20:04:30","1","39","<p>I was extracting 9 items from a website. All was good but when I tried to pass those items into the dictionary then only the last item was saved into the dictionary.</p>

<pre><code>import requests
from bs4 import BeautifulSoup
base_url=""http://cbcs.fastvturesults.com/student/1sp15me00""
d={}
for page in range(1,10,1):
    r=requests.get(base_url+str(page))
    c=r.content
    soup=BeautifulSoup(c,""html.parser"")
    items=soup.find(class_=""text-muted"")
    if items:
        d[""Name""]=items.previous_sibling
        d[""USN""]=items.text.replace(""("","""").replace("")"","""")
d
</code></pre>

<p>How can I save all the items into a dictionary?</p>
","9569600","","","Web Scraping; Unable to pass all the items in a dictionary","<python><web-scraping><beautifulsoup>","2","2","646"
"50651929","2018-06-01 22:58:38","0","","<p>You should certainly not incorporate e.g. Gmail credentials in the code that is remotely executed on devices you do not control, given I understand correctly the Gmail less secure device issue happens as every ""user"" is running this code and using your credentials. This holds true for any other provider.</p>

<p>Now this won't exactly be simple but one way to go about it would be to create a server side API endpoint that can accept HTTP(s) or any other protocol requests that then will authenticate in a little more secure way on the server side with Gmail.</p>

<p>The concept for emails is:
Bug > Python Script > API call > Email</p>

<p>This could be implemented using Python on the API side (Flask e.g.) using an AWS Lambda Function with Amazon API Gateway, but again that is something to get through and understand by itself which will take a good chunk of time.
You need to touch a lot of concepts, like auth tokens to make this really secure.</p>

<p>Could you elaborate a little on where the code needs to run and if you are willing to try AWS or any other cloud provider, or would have access to an internet connected server ? This makes it easier to provide you with a full example on the solution in a hackish way while I would highlight the problems you could face on the security side.</p>
","9802392","","","1","1310","d parolin","2018-05-16 19:48:35","184","23","11","0","50651811","50777406","2018-06-01 22:41:35","-1","52","<p>Premise: I am a beginner in search for an easy way to send bug reports from users over sea.</p>

<p>I've made a script for some friends that are living on the other side of the sea (<strong>US - EUROPE</strong>)... I will like to gather automatic bug reports whenever they happen. So my first idea was to send myself an email with the <code>smtplib module</code>. It works fine when testing home, but as soon as the sender ""sends"", my email provider (<strong>gmail</strong>) blocks the connection because of course, its from an ""unknown device"". I've already enabled ""Allow less secure apps"" as someone suggested but with no avail. </p>

<p>What I am searching its a simple way of dealing with this. </p>

<p>Yes I could make the script to ignore the error if the email its not being sent, and then go into my google account and enable those devices so at least it will work from the second run.. 
But it doesn't seem what a programmer would do in this case. I am learning so a solution withing the language is what I am after.</p>

<p>A different provider that has no restriction its also a good start but I tried Yahoo, Live, Yandex but I couldn't make them work. Are there any?</p>

<p><strong>So my question is</strong>: how others do? what is the best solution for some one like me?</p>

<p>I've read about sentry or other error/bug tracking but its obviously  way too much for want I need </p>
","9392852","355230","2018-06-02 01:24:04","Gathering bug reports?","<python><python-3.x><gmail><smtplib><bug-tracking>","2","0","1403"
"50651961","2018-06-01 23:02:50","2","","<p>What you're asking for is a cyclic colormap. As of now, matplotlib only provides one single cyclic colormap, namely <code>""hsv""</code>. You may however define your own colormap easily and make sure it's cyclic, e.g. using the colors <code>[""gold"", ""red"", ""black"", ""navy"", ""gold""]</code> where the first and the last color are the same.</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
plt.rcParams['lines.markersize'] = 12

x = np.arange(200)
y = np.ones(200)

fig, ax = plt.subplots()
ax.scatter(x,y, c=x % 52, marker=""|"" )

ax.scatter(x,y-1, c=x % 52, cmap=""hsv"", marker=""|"" )

colors = [""gold"", ""red"", ""black"", ""navy"", ""gold""]
cmap = mcolors.LinearSegmentedColormap.from_list("""", colors)
ax.scatter(x,y-2, c=x % 52, cmap=cmap, marker=""|"" )

ax.set_yticks([-1,0,1])
ax.set_yticklabels([""custom"", ""hsv"", ""viridis"", ])
ax.margins(y=0.4)
plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/n0d2n.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/n0d2n.png"" alt=""enter image description here""></a></p>
","4124317","","","0","1086","ImportanceOfBeingErnest","2014-10-09 07:50:43","173272","28712","2299","3162","50649556","50651961","2018-06-01 18:56:04","1","202","<p>I am trying to set a colorbar, and I understand clim can do that. I cannot figure out how you set it so that the middle colorbar value is the smallest 1, with for example, a range of 1 to 52? </p>

<p>I am doing this because I have week numbers, but I do not want to show big jumps in the color from December to January (values 52 and 1)</p>

<p>EDIT: I am talking about the colors so that the ends of the colorbar are similar in color but the middle is the most different color. I do not want to change the values in the colorbar</p>
","6610946","6610946","2018-06-01 19:44:50","how to set sequential positive colorbar range with lowest value in the middle of colorbar","<python><matplotlib><colorbar>","2","3","538"
"50651975","2018-06-01 23:05:53","2","","<p>The problem was with the numpy version, as stated in the comment by jasonsharper. Upgraded from numpy version 1.12 to 1.14 and the code works fine now.</p>
","1810059","","","0","159","Syed Saad","2012-11-08 17:28:13","75","28","33","0","50647460","50651975","2018-06-01 16:15:33","1","119","<p>I am trying to use numpy to print an eye matrix with extra spaces , such as for input ""3  3"" , i get</p>

<pre><code>[[ 1.  0.  0.]
 [ 0.  1.  0.]
 [ 0.  0.  1.]]
</code></pre>

<p>My code for this program is as follows  </p>

<pre><code> import numpy
    numpy.set_printoptions(sign =' ')
    N, M = input().split(' ')
    print(numpy.eye(int(N), int(M)))
</code></pre>

<p>But I am getting a type error as </p>

<blockquote>
  <p>TypeError: set_printoptions() got an unexpected keyword argument
  'sign'</p>
</blockquote>

<p>I am using python 3.5.2 and PyCharm Community edition 2017.1.3 as an IDE. My code works perfectly fine on online editors for python 3. So is it a problem with my IDE/python installation or a problem in numpy?</p>
","1810059","","","Error in set_printoptions() in numpy","<python><python-3.x><numpy>","1","2","744"
"50651988","2018-06-01 23:08:29","4","","<p>A <code>ttk.Treeview</code> without the tree part can be used to display a table:</p>

<pre><code>tree = ttk.Treeview(master, columns=('Position', 'Name', 'Score'), show='headings')
</code></pre>

<p>Then set the column labels with</p>

<pre><code>tree.heading(&lt;column&gt;, text=""Label"")
</code></pre>

<p>and add rows with</p>

<pre><code>tree.insert("""", ""end"", values=(&lt;position&gt;, &lt;name&gt;, &lt;score&gt;))
</code></pre>

<p>The first argument is the item's parent, since you want a table, all items have the same parent, the root <code>""""</code>. The second argument is the position of the new item in the tree.</p>

<p>Full example:</p>

<pre><code>import tkinter as tk
from tkinter import ttk

def show():

    tempList = [['Jim', '0.33'], ['Dave', '0.67'], ['James', '0.67'], ['Eden', '0.5']]
    tempList.sort(key=lambda e: e[1], reverse=True)

    for i, (name, score) in enumerate(tempList, start=1):
        listBox.insert("""", ""end"", values=(i, name, score))

scores = tk.Tk() 
label = tk.Label(scores, text=""High Scores"", font=(""Arial"",30)).grid(row=0, columnspan=3)
# create Treeview with 3 columns
cols = ('Position', 'Name', 'Score')
listBox = ttk.Treeview(scores, columns=cols, show='headings')
# set column headings
for col in cols:
    listBox.heading(col, text=col)    
listBox.grid(row=1, column=0, columnspan=2)

showScores = tk.Button(scores, text=""Show scores"", width=15, command=show).grid(row=4, column=0)
closeButton = tk.Button(scores, text=""Close"", width=15, command=exit).grid(row=4, column=1)

scores.mainloop()
</code></pre>

<p><a href=""https://i.stack.imgur.com/Elaj9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Elaj9.png"" alt=""screenshot""></a></p>

<p>You can find more details about the <code>Treeview</code> widget <a href=""http://infohost.nmt.edu/tcc/help/pubs/tkinter/web/ttk-Treeview.html"" rel=""nofollow noreferrer"">here</a>.</p>
","6415268","","","0","1907","j_4321","2016-06-02 14:24:15","7006","637","135","106","50625306","","2018-05-31 13:18:10","1","3085","<p>I have written a program which takes data from a text file and displays it in a table style format.</p>

<p>Data from text file:</p>

<pre><code>Jim,0.33
Dave,0.67
James,0.67
Eden,0.5
</code></pre>

<p>Formatted using the program:</p>

<pre><code>Position | Name              |Score
-----------------------------------
1        |Dave               |0.67
2        |James              |0.67
3        |Eden               |0.5
4        |Jim                |0.33
</code></pre>

<p>Without importing Pandas / SQL etc is there a better way of displaying this data?</p>

<p>The code I have written is below:</p>

<pre><code>from tkinter import *

def show():

    tempList= [['Jim', '0.33'], ['Dave', '0.67'], ['James', '0.67'], ['Eden', '0.5']]

    tempList.sort(key=lambda e: e[1], reverse=True)
    listBox.insert(END, ""Position | Name      \t\t |Score\n"")
    listBox.insert(END,""-----------------------------------"")
    listBox.insert(END,""\n"")

    for i in range(len(tempList)):
        listBox.insert(END,(i+1))
        listBox.insert(END,""\t |"")
        listBox.insert(END,tempList[i][0])
        listBox.insert(END,""\t \t|"")
        listBox.insert(END,tempList[i][1])
        listBox.insert(END,""\n"")

scores = Tk() 
label = Label(scores, text=""High Scores"", font = (""Arial"",30)).grid(row = 0, columnspan = 3)
listBox= Text(scores,width = 40)
listBox.grid(row = 1,column= 0, columnspan = 2)
showScores = Button(scores, text = ""Show scores"",width = 15, command = show).grid(row = 4, column = 0)
closeButton = Button(scores, text = ""Close"",width = 15, command = exit).grid(row = 4, column = 1)

scores.mainloop()
</code></pre>
","8247742","8247742","2018-05-31 17:24:56","What's the best way to show data which should be in a table using TKInter python","<python><tkinter><tabular>","1","6","1632"
"50652025","2018-06-01 23:14:52","1","","<p>You cannot catch a syntax error in Python (except if it is raised from an eval, which is not your case)</p>
","4870915","","","1","111","Gelineau","2015-05-06 13:55:15","1462","118","1019","92","50652004","50652026","2018-06-01 23:10:27","0","150","<p>I am trying to catch this error <code>SyntaxError: Non-ASCII character '\xc3'</code> using this code:</p>

<pre><code>try:
    address = 'ÁR11E'
except:
    print 'hello'
</code></pre>

<p>I can never get ""hello"" printed. The error is treated as unhandled and stops the process. How can I catch and handle this type of error?</p>

<p>I only have to catch the error without solving it for now.</p>
","7332645","355230","2018-06-02 01:19:10","How to catch a non-ASCII character error?","<python><python-2.7><character-encoding>","3","2","400"
"50652026","2018-06-01 23:15:00","4","","<p>By default, Python 2 source code should contain only ASCII characters, therefore this is syntax error. You cannot catch it, because it makes the whole file invalid.</p>

<p>There are two things you can do:</p>

<ul>
<li><a href=""https://www.python.org/dev/peps/pep-0263/"" rel=""nofollow noreferrer"">change the code page</a> of the fie from ASCII to something else:</li>
</ul>

<p><code># coding: utf-8</code></p>

<ul>
<li>or encode the non-ascii character</li>
</ul>

<p><code>address = '\xC3\x81R11E'  # this would be utf-8</code></p>

<p><code>address = '\xC3\x81R11E'.decode('utf-8')  # this would be unicode</code></p>

<p>or</p>

<p><code>address = u'\N{LATIN CAPITAL LETTER A WITH ACUTE}R11E'</code></p>

<p>or</p>

<p><code>address = u'\u0381R11E'</code></p>
","389289","389289","2018-07-20 11:55:21","10","769","zvone","2010-07-12 08:25:02","11303","980","601","663","50652004","50652026","2018-06-01 23:10:27","0","150","<p>I am trying to catch this error <code>SyntaxError: Non-ASCII character '\xc3'</code> using this code:</p>

<pre><code>try:
    address = 'ÁR11E'
except:
    print 'hello'
</code></pre>

<p>I can never get ""hello"" printed. The error is treated as unhandled and stops the process. How can I catch and handle this type of error?</p>

<p>I only have to catch the error without solving it for now.</p>
","7332645","355230","2018-06-02 01:19:10","How to catch a non-ASCII character error?","<python><python-2.7><character-encoding>","3","2","400"
"50652031","2018-06-01 23:15:54","2","","<p>I see what happened. On some of the servers, when I tried to update <code>pip</code> the first time, there was an error in the middle of the process. And the folder <code>C:\Python34\Lib\site-packages\pip-10.0.1.dist-info</code> was created, even though the install hadn't finished. If I remove that folder and then run the upgrade, it works fine.</p>
","2213647","","","0","355","twasbrillig","2013-03-26 22:54:25","7848","850","2219","115","50652030","50652031","2018-06-01 23:15:54","2","37","<p>I installed ActiveState Python 3.4 on a lot of servers.</p>

<p>To bring <code>pip</code> up to date, I'm running this command on each server:</p>

<pre><code>""C:\Python34\python.exe"" -m pip install --upgrade pip
</code></pre>

<p>Some of the time, it upgrades to pip version 10.0.1:</p>

<pre><code>Downloading/unpacking pip from https://files.pythonhosted.org/packages/0f/74/ecd13431bcc456ed390b44c8a6e917c1820365cbebcb6a8974d1cd045ab4/pip-10.0.1-py2.py3-none-any.whl#sha256=717cdffb2833be8409433a93746744b59505f42146e8d37de6c62b430e25d6d7
Installing collected packages: pip   
Found existing installation: pip 1.5.6
    Uninstalling pip:
      Successfully uninstalled pip
Successfully installed pip
Cleaning up...
</code></pre>

<p>But, some of the time it doesn't upgrade at all and it keeps pip at version 1.5.6:</p>

<pre><code>Requirement already up-to-date: pip in c:\python34\lib\site-packages
Cleaning up...
</code></pre>

<p>Why is it behaving differently on different servers? Is there a way to force the upgrade to happen?</p>
","2213647","","","Sometimes pip updates but sometimes it doesn't","<python><python-3.x><pip>","1","0","1044"
"50652047","2018-06-01 23:17:57","1","","<p>First create a file.py to write code to and execute.</p>

<p>You can use javascript to send a <code>XMLHttpRequest</code> to a python file</p>

<pre><code>var xhr = new XMLHttpRequest();
xhr.open(""POST"", ""exec?text="" + code, true);
xhr.onload = function(e) {
    var output = JSON.parse(xhr.response);
    // do something with the output
}
xhr.send();
</code></pre>

<p>And using Flask and subprocess in the python file:</p>

<pre><code>import subprocess
from flask import Flask, request
app = Flask(__name__)
@app.route('/exec', methods=['POST']) // route app to /exec
def result():
    with open('file.py', 'w') as code: // 'w' means to override existing code in the file
        code.write(request['code']) // write the code
    return subprocess.check_output([""python"", ""file.py""]) // execute the code using the terminal
</code></pre>

<p>Return the output of the code. Please tell me if this does not work. Thanks!</p>
","1811541","1811541","2018-06-03 15:21:59","0","927","MBJH","2012-11-09 07:50:16","812","254","64","16","50651652","50651804","2018-06-01 22:21:41","0","66","<p>Alright, so basically, I own this website: <a href=""https://quarkedit.ga"" rel=""nofollow noreferrer"">https://quarkedit.ga</a> which right now has a HTML/CSS/JS editor using Ace. Now what I need is I also have a python language one, but I don't know how to make the terminal or whatever show up. I'm going for something like <a href=""https://repl.it"" rel=""nofollow noreferrer"">https://repl.it</a> 's python thing. Just wondering if there is any API which i could use, something like</p>

<pre><code>output = evaluatePythonCode(""print(\""test\"")"");
</code></pre>

<p>I basically want to know and find out these things:</p>

<ul>
<li>What API's can I use to do this?</li>
<li>What would the syntax be?</li>
<li>Can I do this with pure HTML/CSS/JS or would I have to use a JS 
Library?</li>
</ul>

<p>I have the input for the code done, and the syntax highlighting (Ace) But what I need is the:</p>

<ul>
<li>Execution</li>
<li><p>Output</p>

<p>All help will be appreciated, and if this question isn't appropriate or anything just comment and I'll remove it. </p></li>
</ul>

<p>Thanks!</p>
","9414889","9414889","2018-06-01 22:26:16","How would I go about adding a python code runner to my website","<javascript><python><html>","2","7","1089"
"50652066","2018-06-01 23:21:02","0","","<p>Your approach is wrong in quite a number of ways.</p>

<p>First, <code>minimize</code> takes a sequence as constraint, so that your Nx3 array is first flattened before it is passed to constraint functions leaving you with an array of only one dimension. Therefore you can't index with a tuple except you <code>reshape</code> your array inside the constraint functions to the original Nx3; could be pretty expensive for large N:</p>

<pre><code>return x.reshape(-1, 3)[i,1] - xyz0cyl[i,1]
</code></pre>

<p>Secondly, closures in Python are late binding; all of the constraints functions will use the last value of <code>i</code> after the <em>for</em> loop as completed. You'll only finding out later on after fixing the first bug that the optimisation does not go as expected. See <a href=""https://stackoverflow.com/questions/233673/how-do-lexical-closures-work"">How do lexical closures work?</a> to learn more. </p>

<p>A better approach is to actually make the y-axis (i.e. 1st column) stationary in your energy function or simply passing a Nx2 matrix instead to <code>fmin_cobyla</code>.</p>
","3125566","3125566","2018-06-01 23:33:08","3","1098","Moses Koledoye","2013-12-21 15:45:56","65121","6414","1080","1558","50651997","50652066","2018-06-01 23:09:46","1","422","<p>I want to do constrained optimisation using a vector of constraints using the <code>scipy.optimize</code> library. In particular, I am supplying a vector of 3d coordinates <code>r0</code> of <code>N</code> points -- hence a matrix of size <code>N x 3</code> -- as input to the function. The coordinates are Cartesian, and I wish to freeze out all y dependence. So that means that I need the second column of my <code>N x 3</code> matrix to be held to a constant, <code>y0</code> say. How do I go about defining such a list of constraints?</p>

<p>To be concrete, let's the consider the COBYLA algorithm (<a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_cobyla.html#scipy.optimize.fmin_cobyla"" rel=""nofollow noreferrer"">https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_cobyla.html#scipy.optimize.fmin_cobyla</a>). I tried the following construction:</p>

<pre><code>cons = []
for i in range(xyz0.shape[0]):
    def f(x):
        return x[i,1]-xyz0cyl[i,1]
    cons.append(f)
fmin_cobyla(energy, xyz0, cons, rhoend=1e-7)
</code></pre>

<p>and got the error: </p>

<pre><code>41 for i in range(xyz0.shape[0]):
42     def f(x):
---&gt; 43         return x[i,1]-xyz0cyl[i,1]
 44     cons.append(f)
 45 

IndexError: too many indices for array
</code></pre>

<p>What is going on?</p>
","7250309","","","Using a vector of constraints to a scipy.optimize function","<python><optimization><scipy>","1","1","1338"
"50652075","2018-06-01 23:23:19","2","","<p>In short: we can <strong>create a function</strong> to calculate the next day.</p>

<p>We can not add a function with a <code>timedelta(..)</code> object, since nor the function has an <code>__add__</code> method, nor a <code>timedelta</code> object has a <code>__radd__</code> method to add functions and <code>timedelta</code>s together, therefore Python/Django can not construct a new function with this syntax.</p>

<p>We can however solve this problem by creating a function that calculates the default time:</p>

<pre><code><b>def tomorrow():
    return datetime.now() + timedelta(days=1)</b>

class ActivateCode(models.Model):
    """""" """"""
    user = models.ForeignKey(User, on_delete=models.CASCADE)
    code = models.CharField(max_length=100)
    date_expired = models.DateTimeField(default=<b>tomorrow</b>)</code></pre>

<p>We here do <em>not</em> invoke the function we set as <code>default=...</code>, we only pass a reference to our <code>tomorrow</code> function, such that if Django creates a new object, it will call the <code>tomorrow(..)</code> function, and thus calcuate exactly the next day.</p>
","67579","","","0","1119","Willem Van Onsem","2009-02-17 21:39:52","199073","27204","11746","1497","50652057","50652075","2018-06-01 23:19:19","2","44","<p>I'd like to set <code>timedelta</code> for a data model</p>

<pre><code>class ActivateCode(models.Model):
    """""" """"""
    user = models.ForeignKey(User, on_delete=models.CASCADE)
    code = models.CharField(max_length=100)
    date_expired = models.DateTimeField(default=datetime.now + timedelta(days=1))
</code></pre>

<p>It seem not a proper solution because <code>datetime.now</code> is not called but <code>timedetla(days=1)</code> is invoked.</p>

<p>How to deal with such an issue?</p>
","7301792","67579","2018-06-01 23:27:29","Deal with timedelta () and datetime.now","<python><django><django-models><timedelta>","1","0","495"
"50652109","2018-06-01 23:28:28","2","","<p>There was an update to syntax highlighting in BUILD 3127 (Which includes: Significant improvements to Python syntax highlighting).</p>

<p>However, a couple users have stated that in BUILD 3176 syntax highlighting still is not set to correctly highlight Python expressions that are located within f strings. According to @Jollywatt, it is set to source.python <code>f""string.quoted.double.block {constant.other.placeholder}""</code> rather than <code>f""string.quoted.double.block {source.python}""</code></p>

<p>It looks like Sublime uses this tool, <a href=""https://github.com/SublimeText/PackageDev"" rel=""nofollow noreferrer"">PackageDev</a>, ""to ease the creation of snippets, syntax definitions, etc. for Sublime Text.""</p>
","1830793","1830793","2018-08-30 15:08:30","3","729","Zev","2012-11-16 20:51:03","2334","215","463","115","40965686","","2016-12-05 01:07:24","18","1471","<p>I am trying to modify the default <code>Python.sublime_syntax</code> file to handle Python’s f-string literals properly. My goal is to have expressions in interpolated strings recognised as such:</p>

<pre><code>f""hello {person.name if person else 'there'}""
         -----------source.python----------
------string.quoted.double.block.python------
</code></pre>

<p>Within f-strings, ranges of text between a single <code>{</code> and another <code>}</code> (but terminating before format specifiers such as <code>!r}</code>, <code>:&lt;5}</code>, etc—see <a href=""https://www.python.org/dev/peps/pep-0498/#specification"" rel=""noreferrer"">PEP 498</a>) should be recognised as expressions. As far as I know, that might look a little like this:</p>

<pre><code>...
string:
 - match: ""(?&lt;=[^\{]\{)[^\{].*)(?=(!(s|r|a))?(:.*)?\})"" # I'll need a better regex
   push: expressions
</code></pre>

<p>However, upon inspecting the build-in <code>Python.sublime_syntax</code> file, the string contexts especially are to unwieldy to even approach (~480 lines?) and I have no idea how to begin. Thanks heaps for any info.</p>
","1937384","1937384","2016-12-09 06:30:36","Sublime Text syntax: Python 3.6 f-strings","<python><syntax><sublimetext3><syntax-highlighting>","1","2","1120"
"50652119","2018-06-01 23:30:21","0","","<p>You first gess was right, add <code>val =</code> to your function call. But you should also return a value from your function.</p>

<p>Replace: </p>

<pre><code>print(sess.run(accuracy,
               feed_dict={x: mnist.test.images,
                        y_: mnist.test.labels}))
</code></pre>

<p>by:</p>

<pre><code>return sess.run(accuracy,
             feed_dict={x: mnist.test.images,
                        y_: mnist.test.labels})
</code></pre>
","4870915","","","1","458","Gelineau","2015-05-06 13:55:15","1462","118","1019","92","50651447","50652166","2018-06-01 21:59:07","0","152","<p>Brand new to TensorFlow and am trying to modify some examples they give. For instance:</p>

<p><a href=""https://github.com/tensorflow/tensorflow/blob/4806cb0646bd21f713722bd97c0d0262c575f7e0/tensorflow/examples/tutorials/mnist/mnist_softmax_xla.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/4806cb0646bd21f713722bd97c0d0262c575f7e0/tensorflow/examples/tutorials/mnist/mnist_softmax_xla.py</a></p>

<pre><code>""""""Simple MNIST classifier example with JIT XLA and timelines.
""""""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import sys

import tensorflow as tf

from tensorflow.examples.tutorials.mnist import input_data
from tensorflow.python.client import timeline

FLAGS = None


def main(_):
  # Import data
  mnist = input_data.read_data_sets(FLAGS.data_dir)

  # Create the model
  x = tf.placeholder(tf.float32, [None, 784])
  w = tf.Variable(tf.zeros([784, 10]))
  b = tf.Variable(tf.zeros([10]))
  y = tf.matmul(x, w) + b
  ....
  ....
  ....
  ....

  # Test trained model
  correct_prediction = tf.equal(tf.argmax(y, 1), y_)
  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
  print(sess.run(accuracy,
                 feed_dict={x: mnist.test.images,
                            y_: mnist.test.labels}))
  sess.close()


if __name__ == '__main__':
  parser = argparse.ArgumentParser()
  parser.add_argument(
      '--data_dir',
      type=str,
      default='/tmp/tensorflow/mnist/input_data',
      help='Directory for storing input data')
  parser.add_argument(
      '--xla', type=bool, default=True, help='Turn xla via JIT on')
  FLAGS, unparsed = parser.parse_known_args()
  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
</code></pre>

<p>This will print ""0.9202"" on the commandline. How do I return the value so that I can use it in other functions?</p>

<pre><code> val = tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
 print(val)
</code></pre>

<p>I get:</p>

<pre><code>[pylint] E1111:Assigning to function call which doesn't return
</code></pre>

<p>Also, can't do anything after the function executes. If I try to print(""this string"") then the program exits before printing that.</p>

<p>EDIT:
Answers so far give the same error:</p>

<pre><code>[pylint] E1111:Assigning to function call which doesn't return
</code></pre>

<p>I have looked through a lot of the examples for TF but cannot find an example of how to return the value rather than printing it to the console.</p>
","522962","522962","2018-06-02 02:45:40","How to return variables from main()?","<python><tensorflow>","2","0","2541"
"50652120","2018-06-01 23:30:34","-1","","<p>I have to agree. This is a foolproof way fo solving your problem, and it is the simplest way to circumvent dealing with rendering issues. Here is some code of mine with no sprites:</p>

<pre><code> import pygame

pygame.init()
game_display = pygame.display.set_mode((800, 800))


# fixed variables at the start
x_pos = 400
y_pos = 400
current_speed = 15

def jump_coords(y_position, speed):
    if speed &gt;= 0:
        #to move up, reduce the y-coordinate
        y_position -= speed
    return y_position

game_exit = False

# main loop
while not game_exit: 
    for event in pygame.event.get():
        if event.type == pygame.KEYDOWN:
            if event.key == pygame.K_SPACE:
                y_pos = jump_coords(y_pos, current_speed)
                # 1 represents gravity value
                current_speed -= 1

    rect_one = pygame.Rect(x_pos, y_pos, 10, 10)  
    pygame.draw.rect(game_display, (255, 0, 0), rect_one)
    pygame.display.update()`enter code here`
</code></pre>
","9883463","","","0","994","Annabel Beckett","2018-06-01 23:08:35","1","14","0","0","50533105","","2018-05-25 16:05:02","1","115","<p>I have a pygame game, and it works perfectly fine on my laptop, but when I move it over to my raspberry pi, a lot of the sprites don't appear.  They are there because I can still click on them, but they don't seem to actually be being drawn.  I thought it may have been because the pi couldn't handle hundreds of pngs, so I compressed them all, and still no different results.  It's not the same sprites every time that don't appear.  If I relaunch the script, or even just refresh a screen, it's different images that decide to appear.  I then drew it to a capacitive tft screen from Adafruit, and it's still the same results.  The mouse is also inverted which is a bit annoying, but that is a different problem I will figure out.  The sprites are definitely there because I can still click them, they just aren't visible, like it can't load the image, but it doesn't give an error.</p>

<pre><code>import pygame
class base_sprite(pygame.sprite.Sprite):
        def __init__(self, color=(0,0,0), width=0, height=0, image=None,x=0,y=0, scale=None):
            pygame.sprite.Sprite.__init__(self)
            if ""Surface"" in type(image).__name__:
                self.image = image
            else:
                self.image = pygame.image.load(image)
            if scale != None:
                self.image = pygame.transform.scale(self.image, (scale[0], scale[1]))
            pygame.draw.rect(self.image, color, [5000000,5000000,width,height])
            self.rect = self.image.get_rect()
            self.rect.x = x
            self.rect.y = y

pygame.init()
width = 320
height = 240
s = pygame.display.set_mode((width, height))
clock = pygame.time.Clock()

gameGroup = pygame.sprite.Group()
inventoryGroup = pygame.sprite.Group()

back = base_sprite(width=320, height=240, image=""images/back.png"", x=0, y=0)

headBorder = base_sprite(width=64, height=64, image=""images/ItemBorder.png"", x=180, y=10, scale=[50, 50])
handBorder = base_sprite(width=64, height=64, image=""images/ItemBorder.png"", x=250, y=94, scale=[50, 50])
bodyBorder = base_sprite(width=64, height=64, image=""images/ItemBorder.png"", x=180, y=94, scale=[50, 50])
feetBorder = base_sprite(width=64, height=64, image=""images/ItemBorder.png"", x=180, y=178, scale=[50, 50])
spellBorder = base_sprite(width=64, height=64, image=""images/ItemBorder.png"", x=110, y=10, scale=[50, 50])
symbol =  base_sprite(width=100, height=100, image=""images/mageSmall.png"", x=5, y=(height/2)-50)

gameGroup.add(back)

inInventory = False
running = True
while running:
    events = pygame.event.get()
    for event in events:
        if event.type == pygame.KEYDOWN:
            if event.key == pygame.K_i:
                inInventory = not inInventory

    gameGroup.draw(s)
    if inInventory:
        inventoryGroup.add(back)
        inventoryGroup.add(symbol)
        inventoryGroup.add(headBorder)
        inventoryGroup.add(handBorder)
        inventoryGroup.add(bodyBorder)
        inventoryGroup.add(feetBorder)
        inventoryGroup.add(spellBorder)
        inventoryGroup.add(base_sprite(width=64, height=64, image=""images/items/BasicHat.png"", x=180, y=10, scale=[50, 50]))
        inventoryGroup.add(base_sprite(width=64, height=64, image=""images/items/BasicThingToHitPeopleWith.png"", x=250, y=94, scale=[50, 50]))
        inventoryGroup.add(base_sprite(width=64, height=64, image=""images/items/BasicShirt.png"", x=180, y=94, scale=[50, 50]))
        inventoryGroup.add(base_sprite(width=64, height=64, image=""images/items/BasicShoes.png"", x=180, y=178, scale=[50, 50]))
        inventoryGroup.add(base_sprite(width=64, height=64, image=""images/items/BasicBook.png"", x=110, y=10, scale=[50, 50]))
        inventoryGroup.draw(s)
    clock.tick(60)
    pygame.display.flip()
</code></pre>
","8486185","6220679","2018-05-26 07:37:34","Sprites not appearing in pygame on raspberry pi","<python><raspberry-pi><pygame>","1","9","3751"
"50652121","2018-06-01 23:31:07","1","","<p>I used the Transpose function to reshape scaled_train_samples from a 2 by 2100 matrix into a 2100 by 2 matrix. Thanks guys for your contributions.     </p>

<pre><code>#Transpose
scaled_train_samples = scaled_train_samples.transpose()
</code></pre>

<p>However, running the line of code below gives the accuracy of the model. And currently, I am getting an accuracy of 51.52%, is there anything I can do to improve the accuracy of this model?</p>

<pre><code>#Evaluate the model
scores = model.evaluate(scaled_train_samples, train_labels)
print(""\n%s: %.2f%%"" % (model.metrics_names[1], scores[1]*100))
</code></pre>
","9875012","9875012","2018-06-02 01:48:39","0","620","Ifeanyi Ndukwe","2018-05-31 09:08:05","11","1","0","0","50634095","","2018-05-31 23:18:02","0","376","<p>I'm using the Keras API to write a code that can predict using the learned .h5 file.  </p>

<p>The learning model is as follows </p>

<pre><code>#Libraries
import keras
from keras import backend as k
from keras.models import Sequential
from keras.layers import Activation
from keras.layers.core import Dense, Flatten, Reshape
from keras.optimizers import Adam
from keras.metrics import categorical_crossentropy
import numpy as np
from random import randint
from sklearn.preprocessing import MinMaxScaler

#Create 2 numpy lists that will hold both our sample data and raw data
train_labels = []
train_samples = []

#declare array to hold training data as well as label
train_samples_temp_a = []
train_samples_temp_b = []

#Generate data
for i in range(1000):
    #YOUNGER PEOPLE
    random_younger_a = randint(13,64)
    random_younger_b = randint(13,64)
    train_samples_temp_a.append(random_younger_a)
    train_samples_temp_b.append(random_younger_b)
    train_labels.append(0)
    #OLDER PEOPLE
    random_older_a = randint(65,100)
    random_older_b = randint(65,100)
    train_samples_temp_a.append(random_older_a)
    train_samples_temp_b.append(random_older_b)
    train_labels.append(1)

for i in range(50):
    #YOUNGER PEOPLE
    random_younger_a = randint(13,64)
    random_younger_b = randint(13,64)
    train_samples_temp_a.append(random_younger_a)
    train_samples_temp_b.append(random_younger_b)
    train_labels.append(1)
    #OLDER PEOPLE
    random_older_a = randint(65,100)
    random_older_b = randint(65,100)
    train_samples_temp_a.append(random_older_a)
    train_samples_temp_b.append(random_older_b)
    train_labels.append(0)

#Array of Two Arrays 
train_samples.append(train_samples_temp_a)
train_samples.append(train_samples_temp_b)

#Convert both train_label and train_sample list into a numpy array
train_samples = np.array(train_samples)
train_labels = np.array(train_labels)

#Scale down train_samples to numbers between 0 and 1
scaler = MinMaxScaler(feature_range=(0,1))
scaled_train_samples=scaler.fit_transform((train_samples))

#Sequential Model
model = Sequential([
    Dense(16, input_shape=(2,2100), activation='relu'),
    Flatten(),
    Dense(32, activation='relu'),
    Dense(2, activation='softmax')
])

#Compile Model
model.compile(Adam(lr=.0001), loss='sparse_categorical_crossentropy', 
metrics= ['accuracy'])

#Train Model
model.fit(scaled_train_samples, train_labels, validation_split = 0.20, 
batch_size=10, epochs=20, shuffle=True, verbose=2)
</code></pre>

<p><a href=""https://i.stack.imgur.com/9nxvi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9nxvi.png"" alt=""error message""></a></p>
","9875012","2745495","2018-06-01 00:37:41","ValueError: Error when checking input: expected dense_151_input to have 3 dimensions, but got array with shape (2, 2100)","<python><tensorflow><machine-learning><keras><deep-learning>","2","0","2665"
"50652143","2018-06-01 23:34:43","0","","<p>The redirect does not work because the logger that you set up initially has a pointer directly to stdout. See below.</p>

<blockquote>
  <p>logging_test.py</p>
</blockquote>

<pre><code>import logging
import sys

SIMPLE_LOG_FORMAT = '[%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s'
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
formatter = logging.Formatter(SIMPLE_LOG_FORMAT)
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

logger.info(""Message before redirecting stdout and stderr"")
# Checking what logger is writing to
logger.info('Before Redirection. logger writing to {} '.format(logger.handlers[0].stream))

log_file = open('log_file.log', 'w')
sys.stdout = log_file
sys.stderr = log_file
# Checking what logger is writing to
logger.info('After Redirection. logger writing to {} '.format(logger.handlers[0].stream))
logger.info(""Message after redirecting stdout and stderr"")

log_file.close()
</code></pre>

<blockquote>
  <p>Output:</p>
</blockquote>

<pre><code>[2018-06-01 16:27:10,670] {logging_test.py:12} INFO - Message before redirecting stdout and stderr
[2018-06-01 16:27:10,670] {logging_test.py:14} INFO - Before Redirection. logger writing to &lt;open file '&lt;stdout&gt;', mode 'w' at 0x10cd74150&gt; 
[2018-06-01 16:27:10,670] {logging_test.py:20} INFO - After Redirection. logger writing to &lt;open file '&lt;stdout&gt;', mode 'w' at 0x10cd74150&gt; 
[2018-06-01 16:27:10,670] {logging_test.py:21} INFO - Message after redirecting stdout and stderr
</code></pre>

<p>As you can see on the second and third lines of the output that logger still refers to  directly.</p>

<p>One of the ways to fix this is to do something like this </p>

<pre><code>logger.handlers[0].stream = open('log_file.log', 'w')
</code></pre>

<p>Note that if you are using multiple threads a change like this in one will cause output from other threads to also start redirecting output.</p>
","1718324","","","0","2012","kvb","2012-10-03 20:32:29","178","34","19","1","22105465","","2014-02-28 20:46:42","4","2047","<p>There's already a question that answers how to do this regarding <code>sys.stdout</code> and <code>sys.stderr</code> here: <a href=""https://stackoverflow.com/a/14197079/198348"">https://stackoverflow.com/a/14197079/198348</a></p>

<p>But that doesn't work everywhere. The logging module seems to output to <code>sys.stdout</code> and <code>sys.stderr</code>, but I can't capture it with the context manager above.</p>

<p>In the following example code, I'm trying to capture all output inside the context manager, failing to do so for the logger statements:</p>

<pre><code>from __future__ import print_function
import contextlib
import sys
import logging
from StringIO import StringIO

# taken from https://stackoverflow.com/a/14197079/198348
@contextlib.contextmanager
def stdout_redirect(where):
    prev_stdout = sys.stdout
    prev_stderr = sys.stderr
    prev_stdout.flush()
    sys.stdout = where
    sys.stderr = where
    try:
        yield where
    finally:
        where.flush()
        sys.stdout = prev_stdout
        sys.stderr = prev_stderr

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger()

print(""\t\tOUTSIDE: stdout"", file=sys.stdout)
print(""\t\tOUTSIDE: stderr"", file=sys.stderr)
logger.info(""\tOUTSIDE: info"")
logger.debug(""\tOUTSIDE: debug"")
logger.warn(""\tOUTSIDE: warn"")
logger.error(""\tOUTSIDE: error"")
logger.critical(""\tOUTSIDE: critical"")

print(""=============== DIVIDER ================"")

s = """"
with stdout_redirect(StringIO()) as new_stdout:
    print(""\t\tINSIDE: stdout"", file=sys.stdout)
    print(""\t\tINSIDE: stderr"", file=sys.stderr)
    logger.info(""\tINSIDE: info"")
    logger.debug(""\tINSIDE: debug"")
    logger.warn(""\tINSIDE: warn"")
    logger.error(""\tINSIDE: error"")
    logger.critical(""\tINSIDE: critical"")


print(""=============== DIVIDER ==============="")
print(new_stdout.getvalue())

print(""=============== LOGGING ==============="")

print(logger.handlers)
print(logger.root.handlers)
</code></pre>

<p>How can I temporarily redirect the output of the logger(s) that spit out to stdout and capture them? I took a look at <a href=""http://hg.python.org/cpython/file/2.7/Lib/logging/__init__.py#l1492"" rel=""nofollow noreferrer"">logging/<strong>init</strong>.py</a>, but it doesn't immediately tell me what I need to do.</p>

<p>My motivation for doing this is that I want to equip a crufty big codebase with tests, each of which captures the spurious amounts of logging output that each test invokes. I can capture external programs, but I can't seem to capture the tests that I run inside <a href=""http://nose.readthedocs.org/en/latest/"" rel=""nofollow noreferrer"">nose</a>. </p>

<p>Rewriting the verbose parts isn't an option right now, but is definitely a goal for further down the road.</p>

<h2>Edit, regarding ubuntu</h2>

<p>Here's what I've tried running with nosetests:</p>

<pre><code>from __future__ import print_function
import sys

def test_funky_shurane():
    import logging
    logging.basicConfig(level=logging.DEBUG)
    logging.info(""===== shurane info"")
    logging.warn(""===== shurane warn"")
    logging.error(""===== shurane error"")
    logging.critical(""===== shurane critical"")
    print(""===== shurane stdout"", file=sys.stdout)
    print(""===== shurane stderr"", file=sys.stderr)
    assert True
</code></pre>

<p>And then running the above with:</p>

<pre><code>nosetests test_logging.py
nosetests --nocapture test_logging.py
</code></pre>
","198348","-1","2017-05-23 12:23:24","How can I temporarily redirect the output of logging in Python?","<python><python-2.7><logging>","2","4","3435"
"50652157","2018-06-01 23:37:13","1","","<p>According to <a href=""http://appjar.info/pythonDevWidgets/#tree"" rel=""nofollow noreferrer"">appJar documentation</a>, you need to pass an XML string to <code>.addTree()</code>, not an <code>ElementTree</code>. According to <a href=""https://docs.python.org/3.6/library/xml.etree.elementtree.html"" rel=""nofollow noreferrer"">ElementTree documentation</a>, you can use <code>xml.etree.ElementTree.tostring()</code> to build an XML string from your <code>Element</code>:</p>

<pre><code>xml_string = et.tostring(xml)
app.addTree('REGISTER', xml_string)
</code></pre>
","5436698","","","0","564","Phil Krylov","2015-10-12 12:41:06","515","34","65","4","50446859","50652157","2018-05-21 10:27:53","3","79","<p>I am making a GUI using appJar(python Library, uses Tkinter). I have an XML file.
I am parsing the XML file using ElementTree XML parsing library.</p>

<p>I want to see my XML file in a tree view.
So I am parsing the file using Element Tree, Getting the tags in Need to show in the Treeview and Forming a new XML object. and passing the new object in the appJar Function: .addTree().
But I am Getting the error as:</p>

<pre><code>..lib\site-packages\appJar\appjar.py"", line 8764, in addTree
    xmlDoc = parseString(data).

...lib\xml\dom\expatbuilder.py"", line 223, in parseString
    parser.Parse(string, True)
TypeError: a bytes-like object is required, not 'ElementTree'
</code></pre>

<hr>

<pre><code>xml=et.Element(root)
print(xml)
for ele in valList:
    reg=et.SubElement(xml, ele.find('Name').text)
    bitroot= ele.findall('Bit')
    for bit in bitroot:
        et.SubElement(reg, bit.find('Name').text)

xmltree= et.ElementTree(xml)
app.startFrame('bottomleft',1,0,2)
app.setBg('orange')
app.setSticky('news')
app.setStretch('none')
app.addTree('REGISTER', xmltree)
</code></pre>

<blockquote>
  <p>I am getting the error, as far as I can understand is because .addTree() API is unable to read the format of xmltree variable.</p>
</blockquote>
","9822433","6622587","2018-06-01 23:58:38","View an XML file in Tree View using appJar","<python><python-3.x><tkinter>","1","0","1260"
"50652166","2018-06-01 23:38:37","0","","<p>Have a look here <a href=""https://stackoverflow.com/questions/33703624/how-does-tf-app-run-work"">How does tf.app.run() work?</a>.</p>

<p>essentially tf.app.run is a wrapper which calls a main with some arguments. Either you change the print statment to assign the result to a variable and return it or within main you call  your own funtions to write some where</p>

<pre><code>...
result = (sess.run(accuracy,
             feed_dict={x: mnist.test.images,
                        y_: ...
sess.close()
return result
</code></pre>

<p>Maybe the full code of the tutorial makes this clearer:</p>

<p><a href=""https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/examples/tutorials/layers/cnn_mnist.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/examples/tutorials/layers/cnn_mnist.py</a></p>
","8761054","","","1","848","tiennes","2017-10-11 18:45:25","59","10","11","0","50651447","50652166","2018-06-01 21:59:07","0","152","<p>Brand new to TensorFlow and am trying to modify some examples they give. For instance:</p>

<p><a href=""https://github.com/tensorflow/tensorflow/blob/4806cb0646bd21f713722bd97c0d0262c575f7e0/tensorflow/examples/tutorials/mnist/mnist_softmax_xla.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/4806cb0646bd21f713722bd97c0d0262c575f7e0/tensorflow/examples/tutorials/mnist/mnist_softmax_xla.py</a></p>

<pre><code>""""""Simple MNIST classifier example with JIT XLA and timelines.
""""""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import sys

import tensorflow as tf

from tensorflow.examples.tutorials.mnist import input_data
from tensorflow.python.client import timeline

FLAGS = None


def main(_):
  # Import data
  mnist = input_data.read_data_sets(FLAGS.data_dir)

  # Create the model
  x = tf.placeholder(tf.float32, [None, 784])
  w = tf.Variable(tf.zeros([784, 10]))
  b = tf.Variable(tf.zeros([10]))
  y = tf.matmul(x, w) + b
  ....
  ....
  ....
  ....

  # Test trained model
  correct_prediction = tf.equal(tf.argmax(y, 1), y_)
  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
  print(sess.run(accuracy,
                 feed_dict={x: mnist.test.images,
                            y_: mnist.test.labels}))
  sess.close()


if __name__ == '__main__':
  parser = argparse.ArgumentParser()
  parser.add_argument(
      '--data_dir',
      type=str,
      default='/tmp/tensorflow/mnist/input_data',
      help='Directory for storing input data')
  parser.add_argument(
      '--xla', type=bool, default=True, help='Turn xla via JIT on')
  FLAGS, unparsed = parser.parse_known_args()
  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
</code></pre>

<p>This will print ""0.9202"" on the commandline. How do I return the value so that I can use it in other functions?</p>

<pre><code> val = tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
 print(val)
</code></pre>

<p>I get:</p>

<pre><code>[pylint] E1111:Assigning to function call which doesn't return
</code></pre>

<p>Also, can't do anything after the function executes. If I try to print(""this string"") then the program exits before printing that.</p>

<p>EDIT:
Answers so far give the same error:</p>

<pre><code>[pylint] E1111:Assigning to function call which doesn't return
</code></pre>

<p>I have looked through a lot of the examples for TF but cannot find an example of how to return the value rather than printing it to the console.</p>
","522962","522962","2018-06-02 02:45:40","How to return variables from main()?","<python><tensorflow>","2","0","2541"
"50652196","2018-06-01 23:43:14","1","","<p>The culprit is the combination of your command quoting syntax and the Windows terminal interpreter (what you posted would have been fine using Bash for example).</p>

<p>See <a href=""https://stackoverflow.com/questions/14966989/escaping-curl-command-in-windows"">Escaping curl command in Windows</a> for details.</p>

<hr>

<p>The actual error (which you really ought to have posted) looks like this:</p>

<blockquote>
<pre><code>Exception Type: JSONDecodeError at /webhook/webhook_receiver/
Exception Value: Expecting value: line 1 column 1 (char 0)
</code></pre>
</blockquote>

<p>That says the data you are passing into the decoder does not start with a valid character (such as a ""{"" if the JSON is supposed to be a dict, or ""["" for an array). You can probably work out what the problem is really easily by adding a print() for the start of the data, e.g. like:</p>

<pre><code>print('first few characters=&lt;{}&gt;'.format(data[:4]))
</code></pre>
","6332554","1830793","2018-06-02 16:00:48","11","956","Shaheed Haque","2016-05-13 21:53:54","168","24","10","0","50651232","","2018-06-01 21:31:45","3","925","<p>I have an API that is sending me a <strong>POST request (JSON)</strong> for testing. I was doing a lot with the JSON, but all of a sudden it stopped working and giving me a <code>JSONDecodeError</code>. I tried all sorts of things, like using <code>request.POST</code> but nothing worked correctly like I said this was working at one point. Any assistance is appreciated.</p>

<p>Test that gives the error:
In the Windows Command prompt, running:</p>

<pre><code>curl -X POST http://127.0.0.1:8000/webhook/webhook_receiver/ -d '{""foo"": ""bar""}'
</code></pre>

<blockquote>
  <p>Error: <code>json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)</code></p>
</blockquote>

<p>View:</p>

<pre><code>def webhook_receiver(request, *args, **kwargs):
    if request.method == 'POST':
        # Get the incoming JSON Data
        data = request.body.decode('utf-8')
        received_json_data = json.loads(data)
        return HttpResponse(received_json_data)
    else:
        return HttpResponse(""not Post"")
</code></pre>
","5844782","1830793","2018-06-02 09:24:12","JSONDecodeError Django","<python><django>","1","2","1036"
"50652230","2018-06-01 23:48:39","0","","<p>As stated in the edited question, this is a bug that was fixed in a newer release of v2.4 of OpenCV. No further info seems to be currently available about the bug fix.</p>
","1830793","","","0","175","Zev","2012-11-16 20:51:03","2334","215","463","115","15343431","","2013-03-11 16:17:15","10","5159","<p>I'm getting my feet wet with opencv in python, and I figure a good place to start is loading an image. </p>

<p>I've built opencv on my system, and have the python bindings and opencv dlls in the directory tpl/opencv, which is relative to my project. </p>

<p>Here is some code that demonstrates the problem: </p>

<pre><code>from tpl.opencv import cv2
from tpl.opencv.cv2 import cv
from PIL import Image

pil_img =  Image.open('C:/test_file.jpg') #Read a temp file, the input is actually a computed image chip
tmpname =  'C:/tmp.png' 
pil_img.save(tmpname,'PNG') # Write the image chip to disk

im  = cv.LoadImage(tmpname) # This seems to work
im2 = cv2.imread(tmpname)   # This always returns None
</code></pre>

<p>There is no error message, im2 is just always None. Could I have messed something up with the way I'm importing opencv? Is there a simple workaround? </p>

<p>When I create </p>

<pre><code> cvFeatDetector  = cv2.FeatureDetector_create(""MSER"")
</code></pre>

<p>I get , so that part of cv2 seems to work </p>

<hr>

<p>As a workaround I just loaded the image with numpy</p>

<pre><code>        im2 = numpy.imread(inname)
        im  = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY) # This works. I was able to imshow it too
        cvFeatDetector  = cv2.FeatureDetector_create(""MSER"") # Seems to produce a valid object
        cvFeatExtractor = cv2.DescriptorExtractor_create(""SIFT"") # Seems to produce a valid object
        kpts         = cvFeatDetector.detect(im)  # Crashes python
</code></pre>

<p>I find this very odd, because cv2.cvtColor converts the image successfully to grayscale. So some cv2 functions do work, and others fail to varying degrees. </p>

<hr>

<p>Edit. I fixed this by installing the released 2.4 of opencv. </p>
","887074","887074","2013-03-12 15:03:33","Python's OpenCV cv2.imread always returns None and cvFeatDetector crashes python","<python><opencv>","1","2","1752"
"50652260","2018-06-01 23:54:04","0","","<p>It seems you are assembling an url that does not exist based on the error message.</p>

<p>The following line in your code will not yield ""password"" in the initial url unless a properly assembled url would redirect you to it gracefully, which is doubtful.</p>

<pre><code>keep_checking = requests.get('https://{}.com'.format(url))
</code></pre>

<p>Given your parameter <strong>url</strong> you may want to provide the full url and not a part with the predefined <strong>https</strong> and <strong>.com</strong></p>

<p>It is difficult to say if you are trying to run this code as is (as part of a bigger file) or it is modified for pasting here. But in general opening a known url to see if the url changes seems unusual to say and would really only work on pages that gracefully redirect you. In which case you can ""just"" check for the proper 300 block or 404 response code <a href=""https://jigsaw.w3.org/HTTP/300/"" rel=""nofollow noreferrer"">300 Response codes</a></p>

<p>Assuming you are trying to see if you are redirected to another page (that requires that the url you are hitting would properly do that and not throw e.g. a 404) </p>

<p>You should e.g. change it to the following</p>

<pre><code>keep_checking = requests.get(url)
</code></pre>

<p>And provide the full URL, e.g. ""<a href=""https://www.hello-world.com/password"" rel=""nofollow noreferrer"">https://www.hello-world.com/password</a>""
Now if this page does not exist, you may be redirected, then you <strong>may</strong> end up on a page that does not contain password.</p>

<p>There is a wealth of errors that could happen, redirects, timeouts, 404s aso. Given that you may not want to use this for a static webpage you should try to catch a few common issues and not only the error you are seeing right now.</p>
","9802392","9802392","2018-06-05 19:29:40","0","1786","d parolin","2018-05-16 19:48:35","184","23","11","0","50651624","50652559","2018-06-01 22:19:03","1","446","<p>So basically I have a monitor that checks a website changes etc - <a href=""https://helloworld.com/password"" rel=""nofollow noreferrer"">https://helloworld.com/password</a> - That checks every xx minutes to see if the url has changed or not. </p>

<p>Now after a while it came to an end:</p>

<pre><code>HTTPSConnectionPool: Max retries ex
urllib3.connection.VerifiedHTTPSConnection object at 0x00000001A46BC8
60] A connection attempt failed because the connected party did not p
onnection failed because connected host has failed to respond',))
</code></pre>

<p>Basically my code looks pretty simple that does:</p>

<pre><code>def checker(url):
     while True:
            keep_checking = requests.get('https://{}.com'.format(url))
            if ('password' in keep_checking.url):
                randomtime = random.randint(6, 60)
                log(Fore.WHITE + 'Password still up on %s, retrying in %d secs' % (url, randomtime))
                time.sleep(randomtime)

            else:
                log(Fore.GREEN + '%s\'s Password page is gone!!' % url)
                break
</code></pre>

<p>So basically how can I retry the script to continue to check after a failure of Max retries etc? I was thinking maybe a solution is to run the method again?</p>
","9441275","","","Python - Is it possible to automatic retry after ""Max retries exceeded with url requests","<python><python-requests>","2","2","1268"